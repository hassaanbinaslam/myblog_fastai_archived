{
  
    
        "post0": {
            "title": "AWS EFS Sync to S3 Using Lambda",
            "content": "About . This post is to document all the steps required to synchronize AWS EFS with an S3 bucket using a lambda function. The flow of information is from S3 to EFS and not vice versa. . The approach is whenever a new file is uploaded or deleted from the S3 bucket, it will create an event notification. This event will trigger a lambda function. This lambda function will have the efs file system mounted to it. Lambda function synchronizes the files from S3 to EFS. . . Environment Details . Python = 3.9.x | . Steps . Create an S3 bucket . Let&#39;s first create an S3 bucket that will contain our data, and this is the bucket we would like to be in sync with EFS. I am naming the bucket mydata-202203. You may name it as you please. Choose a region of your choice and leave the rest of the settings as defaults. . Create a Lambda function . Now create a lambda function that will receive event notifications from the S3 bucket, and sync files on efs. I am naming it mydata-sync and our runtime will be Python 3.9. Keep the rest of the settings as default, and create the function. . Create S3 event notifications . From the bucket, mydata-sync go to Properties. Scroll down to Event notifications and click create. Give any name to the event. I am calling it object-sync. From the provided event types select . s3:ObjectCreated:Put | s3:ObjectRemoved:Delete | . From the section Destination select Lambda Function, and from the list choose the lambda function name we created in the last section mydata-sync . Click Save Changes . Test S3 notifications . Let&#39;s now test if S3 event notifications are being received by our lambda function. For this update lambda function code and simply print the event received. After updating the lambda function, make sure to deploy it. . import json def lambda_handler(event, context): print(event) return { &#39;statusCode&#39;: 200, &#39;body&#39;: json.dumps(&#39;Hello from Lambda!&#39;) } . Now upload some files in our S3 bucket, and it should trigger our lambda function. For testing, I have uploaded an empty test1.txt file in our bucket. Once successfully uploaded I check the Lambda function logs to see if any event is received. For this go to lambda function mydata-sync &gt; Monitor &gt; Logs &gt; View logs in CloudWatch. For the CloudWatch console view the latest log stream. Below is the event I have received in the logs . {&#39;Records&#39;: [{&#39;eventVersion&#39;: &#39;2.1&#39;, &#39;eventSource&#39;: &#39;aws:s3&#39;, &#39;awsRegion&#39;: &#39;us-east-1&#39;, &#39;eventTime&#39;: &#39;2022-03-28T16:08:00.896Z&#39;, &#39;eventName&#39;: &#39;ObjectCreated:Put&#39;, &#39;userIdentity&#39;: {&#39;principalId&#39;: &#39;AWS:AIDA3VIXXJNKIVU6P5NY3&#39;}, &#39;requestParameters&#39;: {&#39;sourceIPAddress&#39;: &#39;202.163.113.76&#39;}, &#39;responseElements&#39;: {&#39;x-amz-request-id&#39;: &#39;39MD61ZS00SNK2RT&#39;, &#39;x-amz-id-2&#39;: &#39;U+zPUWOrfzTuVi7kbaBONLHoJXKqUICsVqyKBg4yPKYbUV7pQLGc4Z5A2fSIVvDFtSJHC6v29EDJoXhypWsj2wXanUu8YrLocr3z+yK1qoo=&#39;}, &#39;s3&#39;: {&#39;s3SchemaVersion&#39;: &#39;1.0&#39;, &#39;configurationId&#39;: &#39;object-sync&#39;, &#39;bucket&#39;: {&#39;name&#39;: &#39;mydata-202203&#39;, &#39;ownerIdentity&#39;: {&#39;principalId&#39;: &#39;AYAQOSFZ1VPK&#39;}, &#39;arn&#39;: &#39;arn:aws:s3:::mydata-202203&#39;}, &#39;object&#39;: {&#39;key&#39;: &#39;test1.txt&#39;, &#39;size&#39;: 0, &#39;eTag&#39;: &#39;d41d8cd98f00b204e9800998ecf8427e&#39;, &#39;sequencer&#39;: &#39;006241DD60D67A4556&#39;}}}]} . let&#39;s load this event in a dictionary and find some important parameters . event = {&#39;Records&#39;: [{&#39;eventVersion&#39;: &#39;2.1&#39;, &#39;eventSource&#39;: &#39;aws:s3&#39;, &#39;awsRegion&#39;: &#39;us-east-1&#39;, &#39;eventTime&#39;: &#39;2022-03-28T16:08:00.896Z&#39;, &#39;eventName&#39;: &#39;ObjectCreated:Put&#39;, &#39;userIdentity&#39;: {&#39;principalId&#39;: &#39;AWS:AIDA3VIXXJNKIVU6P5NY3&#39;}, &#39;requestParameters&#39;: {&#39;sourceIPAddress&#39;: &#39;202.163.113.76&#39;}, &#39;responseElements&#39;: {&#39;x-amz-request-id&#39;: &#39;39MD61ZS00SNK2RT&#39;, &#39;x-amz-id-2&#39;: &#39;U+zPUWOrfzTuVi7kbaBONLHoJXKqUICsVqyKBg4yPKYbUV7pQLGc4Z5A2fSIVvDFtSJHC6v29EDJoXhypWsj2wXanUu8YrLocr3z+yK1qoo=&#39;}, &#39;s3&#39;: {&#39;s3SchemaVersion&#39;: &#39;1.0&#39;, &#39;configurationId&#39;: &#39;object-sync&#39;, &#39;bucket&#39;: {&#39;name&#39;: &#39;mydata-202203&#39;, &#39;ownerIdentity&#39;: {&#39;principalId&#39;: &#39;AYAQOSFZ1VPK&#39;}, &#39;arn&#39;: &#39;arn:aws:s3:::mydata-202203&#39;}, &#39;object&#39;: {&#39;key&#39;: &#39;test1.txt&#39;, &#39;size&#39;: 0, &#39;eTag&#39;: &#39;d41d8cd98f00b204e9800998ecf8427e&#39;, &#39;sequencer&#39;: &#39;006241DD60D67A4556&#39;}}}]} event . {&#39;Records&#39;: [{&#39;eventVersion&#39;: &#39;2.1&#39;, &#39;eventSource&#39;: &#39;aws:s3&#39;, &#39;awsRegion&#39;: &#39;us-east-1&#39;, &#39;eventTime&#39;: &#39;2022-03-28T16:08:00.896Z&#39;, &#39;eventName&#39;: &#39;ObjectCreated:Put&#39;, &#39;userIdentity&#39;: {&#39;principalId&#39;: &#39;AWS:AIDA3VIXXJNKIVU6P5NY3&#39;}, &#39;requestParameters&#39;: {&#39;sourceIPAddress&#39;: &#39;202.163.113.76&#39;}, &#39;responseElements&#39;: {&#39;x-amz-request-id&#39;: &#39;39MD61ZS00SNK2RT&#39;, &#39;x-amz-id-2&#39;: &#39;U+zPUWOrfzTuVi7kbaBONLHoJXKqUICsVqyKBg4yPKYbUV7pQLGc4Z5A2fSIVvDFtSJHC6v29EDJoXhypWsj2wXanUu8YrLocr3z+yK1qoo=&#39;}, &#39;s3&#39;: {&#39;s3SchemaVersion&#39;: &#39;1.0&#39;, &#39;configurationId&#39;: &#39;object-sync&#39;, &#39;bucket&#39;: {&#39;name&#39;: &#39;mydata-202203&#39;, &#39;ownerIdentity&#39;: {&#39;principalId&#39;: &#39;AYAQOSFZ1VPK&#39;}, &#39;arn&#39;: &#39;arn:aws:s3:::mydata-202203&#39;}, &#39;object&#39;: {&#39;key&#39;: &#39;test1.txt&#39;, &#39;size&#39;: 0, &#39;eTag&#39;: &#39;d41d8cd98f00b204e9800998ecf8427e&#39;, &#39;sequencer&#39;: &#39;006241DD60D67A4556&#39;}}}]} . # event name event[&#39;Records&#39;][0][&#39;eventName&#39;] . &#39;ObjectCreated:Put&#39; . # bucket name event[&#39;Records&#39;][0][&#39;s3&#39;][&#39;bucket&#39;][&#39;name&#39;] . &#39;mydata-202203&#39; . # uploaded object key event[&#39;Records&#39;][0][&#39;s3&#39;][&#39;object&#39;][&#39;key&#39;] . &#39;test1.txt&#39; . Alright, we have seen that we are receiving notifications from S3 bucket so let&#39;s now move on to the next section. . Create an EFS . From EFS console give name as mydata-efs. I am using default VPC for this post. Use Regional availability settings. Click Create. Once file system is created, click on Access points and create an access point for this efs to be mounted in other service. For access point use following settings . name = mydata-ap | root dir path = /efs | POSIX User POSIX UID = 1000 | Group ID = 1000 | . | Root directory creation permissions Owner user id = 1000 | Owner group id = 1000 | POSIX permissions = 777 | . | . Click Create. . Here I have used the root dir path as /efs this means that from this access point my access will be limited to folder /efs. If you want to provide full access to all folders then set to root path to /. . Note on EFS security group settings . In the last section, I have used a default VPC security group (sg) while creating EFS. default sg allows traffic for all protocols and all ports, both for inbound and outbound traffic. But if you are using a custom security group then make sure that you have an inbound rule for . Type = NFS | Protocol = TCP | Port range = 2049 | . Otherwise, you will not be able to access EFS using NFS clients. . Mount EFS to Lambda Function . To mount an EFS to the Lambda function requires some additional steps. . First add permissions to Lambda function. From lambda function &gt; Configurations &gt; Permissions &gt; Execution role. Click on the execution role to open it in IAM concole. For the selected role attach an additional policy AmazonElasticFileSystemFullAccess. . Second, add the lambda to a VPC group in which efs was created. We have created efs in default VPC so let&#39;s add lambda to it. For this from lambda Configurations &gt; VPC click edit. For the next pane select default VPC, all subnets, default VPC security group, and click save. . Now we can add EFS to lambda. Go to lambda Configurations &gt; File Systems &gt; Add file system. Select the file system mydata-efs and associated access point mydata-ap and local mount point as /mnt/efs. The local mount point is the mounted directory from where we can access our EFS from inside the lambda environment. Click Save . Check EFS mount point from Lambda . Let&#39;s verify from lambda that EFS has been mounted and can we access it. So update the lambda code as below and deploy it. . import json import os def lambda_handler(event, context): mount_path = &#39;/mnt/efs&#39; if os.path.exists(mount_path): print(f&quot;{mount_path} exists&quot;) print(os.listdir(&#39;/mnt/efs&#39;)) print(event) return { &#39;statusCode&#39;: 200, &#39;body&#39;: json.dumps(&#39;Hello from Lambda!&#39;) } . Now test this code using a test event S3 Put. For this go to lambda Test &gt; Create new event &gt; Template (s3-put). &#39;S3 Put&#39; test event is similar to the one we saw in the last section. We can use this request template to simulate the event received from S3 bucket. Once the test is successfully executed, check the log output. . START RequestId: 2e307a14-f373-46d5-b763-594d5f406ae6 Version: $LATEST /mnt/efs exists [] {&#39;Records&#39;: [{&#39;eventVersion&#39;: &#39;2.0&#39;, &#39;eventSource&#39;: &#39;aws:s3&#39;, &#39;awsRegion&#39;: &#39;us-east-1&#39;, &#39;eventTime&#39;: &#39;1970-01-01T00:00:00.000Z&#39;, &#39;eventName&#39;: &#39;ObjectCreated:Put&#39;, &#39;userIdentity&#39;: {&#39;principalId&#39;: &#39;EXAMPLE&#39;}, &#39;requestParameters&#39;: {&#39;sourceIPAddress&#39;: &#39;127.0.0.1&#39;}, &#39;responseElements&#39;: {&#39;x-amz-request-id&#39;: &#39;EXAMPLE123456789&#39;, &#39;x-amz-id-2&#39;: &#39;EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH&#39;}, &#39;s3&#39;: {&#39;s3SchemaVersion&#39;: &#39;1.0&#39;, &#39;configurationId&#39;: &#39;testConfigRule&#39;, &#39;bucket&#39;: {&#39;name&#39;: &#39;example-bucket&#39;, &#39;ownerIdentity&#39;: {&#39;principalId&#39;: &#39;EXAMPLE&#39;}, &#39;arn&#39;: &#39;arn:aws:s3:::example-bucket&#39;}, &#39;object&#39;: {&#39;key&#39;: &#39;test%2Fkey&#39;, &#39;size&#39;: 1024, &#39;eTag&#39;: &#39;0123456789abcdef0123456789abcdef&#39;, &#39;sequencer&#39;: &#39;0A1B2C3D4E5F678901&#39;}}}]} END RequestId: 2e307a14-f373-46d5-b763-594d5f406ae6 REPORT RequestId: 2e307a14-f373-46d5-b763-594d5f406ae6 Duration: 7.02 ms Billed Duration: 8 ms Memory Size: 128 MB Max Memory Used: 37 MB Init Duration: 93.81 ms . From the logs we can see that the mounted EFS directory exists /mnt/efs but currently the folder is empty. . Configure VPC endpoint for S3 . Till now we have configured S3 notifications to trigger a lambda function and also mounted EFS to it. Our next step is to process the event received in lambda, and download the file from S3 to EFS. But since our lambda function is configured for a VPC we cannot connect to S3 from it. Even though we can still receive S3 event notification, when we try to connect to S3 to download any file we will get a timeout error. To fix this we will create a VPC endpoint for S3 bucket. . For this go to VPC console &gt; Endpoints &gt; Create endpoint, and set the following . name = mydata-ep | service category = aws services | services = com.amazonaws.us-east-1.s3 (Gateway) | vpc = default | route table = default (main route table) | policy = full access | . Click Create endpoint . Configure S3 permissions for Lambda . For lambda to be able to connect to S3 we also need to give it proper permissions. For this go to Lambda &gt; Configurations &gt; Permissions &gt; Execution Role &gt; click on role name. From the IAM Role console select add permissions, and then select AmazonS3FullAccess . Process S3 event notifications . Our lambda and EFS are ready and we can now process S3 events. Update the lambda code as below to process S3 events. It will download and delete from EFS to keep it in sync with S3 bucket. . import json import boto3 import os s3 = boto3.client(&quot;s3&quot;) def lambda_handler(event, context): event_name = event[&quot;Records&quot;][0][&quot;eventName&quot;] bucket_name = event[&quot;Records&quot;][0][&quot;s3&quot;][&quot;bucket&quot;][&quot;name&quot;] object_key = event[&quot;Records&quot;][0][&quot;s3&quot;][&quot;object&quot;][&quot;key&quot;] efs_file_name = &quot;/mnt/efs/&quot; + object_key # S3 put if event_name == &quot;ObjectCreated:Put&quot;: s3.download_file(bucket_name, object_key, efs_file_name) print(f&quot;file downloaded: {efs_file_name}&quot;) # S3 delete if event_name == &quot;ObjectRemoved:Delete&quot;: # check if file exists on efs if os.path.exists(efs_file_name): os.remove(efs_file_name) print(f&quot;file deleted: {efs_file_name}&quot;) return {&quot;statusCode&quot;: 200, &quot;body&quot;: json.dumps(event)} . We can test this code using the S3-put test event we used last time. Modify the event for bucket name and object key as below. . { &quot;Records&quot;: [ { &quot;eventVersion&quot;: &quot;2.0&quot;, &quot;eventSource&quot;: &quot;aws:s3&quot;, &quot;awsRegion&quot;: &quot;us-east-1&quot;, &quot;eventTime&quot;: &quot;1970-01-01T00:00:00.000Z&quot;, &quot;eventName&quot;: &quot;ObjectCreated:Put&quot;, &quot;userIdentity&quot;: { &quot;principalId&quot;: &quot;EXAMPLE&quot; }, &quot;requestParameters&quot;: { &quot;sourceIPAddress&quot;: &quot;127.0.0.1&quot; }, &quot;responseElements&quot;: { &quot;x-amz-request-id&quot;: &quot;EXAMPLE123456789&quot;, &quot;x-amz-id-2&quot;: &quot;EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH&quot; }, &quot;s3&quot;: { &quot;s3SchemaVersion&quot;: &quot;1.0&quot;, &quot;configurationId&quot;: &quot;testConfigRule&quot;, &quot;bucket&quot;: { &quot;name&quot;: &quot;mydata-202203&quot;, &quot;ownerIdentity&quot;: { &quot;principalId&quot;: &quot;EXAMPLE&quot; }, &quot;arn&quot;: &quot;arn:aws:s3:::example-bucket&quot; }, &quot;object&quot;: { &quot;key&quot;: &quot;test1.txt&quot;, &quot;size&quot;: 1024, &quot;eTag&quot;: &quot;0123456789abcdef0123456789abcdef&quot;, &quot;sequencer&quot;: &quot;0A1B2C3D4E5F678901&quot; } } } ] } . Click test. From the output logs, we can see that our code was able to download the file from S3 bucket and write it on EFS. . START RequestId: 7e9c0dc2-f970-426e-8372-e59b07f5536c Version: $LATEST file downloaded: /mnt/efs/test1.txt END RequestId: 7e9c0dc2-f970-426e-8372-e59b07f5536c REPORT RequestId: 7e9c0dc2-f970-426e-8372-e59b07f5536c Duration: 370.00 ms Billed Duration: 371 ms Memory Size: 128 MB Max Memory Used: 72 MB Init Duration: 367.68 ms . Note that if you get any permission errors then it could be due to the mounting path errors. Please do check the access point path and lambda mount path. . Verify file on EFS . We can verify files on EFS by directly mounting them to an EC2 machine and verifying from there. So let&#39;s do that. . Create an EC2 machine . AMI = Amazon Linux 2 AMI (HVM) - Kernel 5.10, SSD Volume Type | Intance type = t2.micro (free tier) | Instance details Network = default VPC | Auto-assign Public IP = Enable | . | Review and Lanunch &gt; Launch &gt; Proceed without key pair. | . Once the instance is up and running, click on it and connect using EC2 instance connect option. Create a dir &#39;efs&#39; using the command . mkdir efs . In a separate tab open EFS, and click on the file system we have created. Click Attach. From &quot;Mount via DNS&quot; copy command for NFS client. paste that in EC2 bash terminal . sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-0c9526e2f48ece247.efs.us-east-1.amazonaws.com:/ efs . Once successfully mounted verify that the file &#39;test1.txt&#39; exists in EFS. We can also delete the file from S3 and similarly verify from EFS that the file has been removed. . Summary . A summary of all the steps . Create an S3 bucket | Create a Lambda function | Create event notifications on the S3 bucket to trigger the lambda function | Create an EFS file system and its access point. Check the security group setting for inbound rules for NFS traffic | Add EFS and S3 permissions to lambda | Add lambda to VPC | Create VPC endpoint for S3 bucket | Update lambda code to process event notifications | Use EC2 to mount EFS and verify the files | .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/lambda/efs/s3/synchonization/2022/03/28/efs-s3-sync-lambda.html",
            "relUrl": "/aws/lambda/efs/s3/synchonization/2022/03/28/efs-s3-sync-lambda.html",
            "date": " • Mar 28, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "AWS Lambda - Test and Debug Locally in Visual Studio Code",
            "content": "About . This post is about running, and debugging AWS Lambda function locally from Visual Studio Code environment and it extensions AWS Toolkit. . Environment Details . Python = 3.8.x | WSL version = 2 | WSL Ubuntu version = 20.04 | Docker Engine version = 20.10.12 | Docker Desktop version = 4.4.4 | Host OS = Windows 10 | VSCode version = 1.65 | SAM CLI version = 1.40.1 | . Sample Application . For this post we will use a simple hello world application as our focus is on debugging. We will use AWS SAM CLI to create our application. You can follow the steps provided in tutorial AWS SAM Developer Guide&gt;Getting started with AWS SAM to create this application. . From the provided link (SAM Developer Guide): This application implements a basic API backend. It consists of an Amazon API Gateway endpoint and an AWS Lambda function. When you send a GET request to the API Gateway endpoint, the Lambda function is invoked. This function returns a hello world message. . The following diagram shows the components of this application: . . To initialize a serverless app use command . sam init . Complete the SAM initialization setup steps . PS C: MyWorkspace gitrepos 2022-03-16-lambda-debug&gt; sam init You can preselect a particular runtime or package type when using the `sam init` experience. Call `sam init --help` to learn more. Which template source would you like to use? 1 - AWS Quick Start Templates 2 - Custom Template Location Choice: 1 Choose an AWS Quick Start application template 1 - Hello World Example 2 - Multi-step workflow 3 - Serverless API 4 - Scheduled task 5 - Standalone function 6 - Data processing 7 - Infrastructure event management 8 - Machine Learning Template: 1 Use the most popular runtime and package type? (Python and zip) [y/N]: y Project name [sam-app]: Cloning from https://github.com/aws/aws-sam-cli-app-templates (process may take a moment) -- Generating application: -- Name: sam-app Runtime: python3.9 Architectures: x86_64 Dependency Manager: pip Application Template: hello-world Output Directory: . Next steps can be found in the README file at ./sam-app/README.md Commands you can use next ========================= [*] Create pipeline: cd sam-app &amp;&amp; sam pipeline init --bootstrap [*] Test Function in the Cloud: sam sync --stack-name {stack-name} --watch . Once the application is initialized the project structure will look like this . sam-app/ ├── README.md ├── events/ │ └── event.json ├── hello_world/ │ ├── __init__.py │ ├── app.py #Contains your AWS Lambda handler logic. │ └── requirements.txt #Contains any Python dependencies the application requires, used for sam build ├── template.yaml #Contains the AWS SAM template defining your application&#39;s AWS resources. └── tests/ └── unit/ ├── __init__.py └── test_handler.py . There are three especially important files: . template.yaml: Contains the AWS SAM template that defines your application&#39;s AWS resources. | hello_world/app.py: Contains your actual Lambda handler logic. | hello_world/requirements.txt: Contains any Python dependencies that the application requires, and is used for sam build. | . Follow the instructions from the tutorial to build, test, and deploy the application. . All the code used for this post can be obtained from the GitHub repository hassaanbinaslam/2022-03-16-lambda-debug . Project code files | Project zip file | . SAM Project Directory . SAM CLI After project initialization from sam init make sure that you move to project root folder | Project root folder is the one that contain template.yaml defining application AWS resources. In this app case project root folder is sam-app/ | All the subsequest commands including project sam build, sam deploy, invoke and test lambda should be done from project root folder | . | VSCode When you open the project make sure that your project root directory is pointing to sam-app/ folder as shown in image below | . | . . Run Lambda Locally . To invoke lambda function locally use SAM CLI command . sam local invoke . PS C: MyWorkspace gitrepos 2022-03-16-lambda-debug sam-app&gt; sam local invoke Invoking app.lambda_handler (python3.9) Skip pulling image and use local one: public.ecr.aws/sam/emulation-python3.9:rapid-1.40.1-x86_64. Mounting C: MyWorkspace gitrepos 2022-03-16-lambda-debug sam-app hello_world as /var/task:ro,delegated inside runtime container START RequestId: 1d4e45de-38be-4b91-8e9f-4d3c7da8788a Version: $LATEST END RequestId: 1d4e45de-38be-4b91-8e9f-4d3c7da8788a REPORT RequestId: 1d4e45de-38be-4b91-8e9f-4d3c7da8788a Init Duration: 1.40 ms Duration: 990.84 ms Billed Duration: 991 ms Memory Size: 128 MB Max Memory Used: 128 MB {&quot;statusCode&quot;: 200, &quot;body&quot;: &quot;{ &quot;message &quot;: &quot;hello world &quot;}&quot;} . If you have multiple lambda functions in the app, you can invoke a specific lambda function by using it&#39;s name in invoke command as . sam local invoke &quot;HelloWorldFunction&quot; . Run API Gateway Locally . You can run API Gateway locally to test HTTP request response functionality using command . sam local start-api . This command will start a local instance of API Gateway and provide you with a URL that you can use to send a request using CURL commmand . PS C: MyWorkspace gitrepos 2022-03-16-lambda-debug sam-app&gt; sam local start-api Mounting HelloWorldFunction at http://127.0.0.1:3000/hello [GET] You can now browse to the above endpoints to invoke your functions. You do not need to restart/reload SAM CLI while working on your functions, changes will be reflected instantly/automatically. You only need to restart SAM CLI if you update your AWS SAM template 2022-03-17 11:41:55 * Running on http://127.0.0.1:3000/ (Press CTRL+C to quit) . From terminal output you can find tha HelloWorldFunction is mounted at http://127.0.0.1:3000/hello. From another terminal we can call this URL . PS C: MyWorkspace gitrepos 2022-03-16-lambda-debug sam-app&gt; curl http://127.0.0.1:3000/hello StatusCode : 200 StatusDescription : OK Content : {&quot;message&quot;: &quot;hello world&quot;} RawContent : HTTP/1.0 200 OK Content-Length: 26 Content-Type: application/json Date: Thu, 17 Mar 2022 06:43:08 GMT Server: Werkzeug/1.0.1 Python/3.8.8 {&quot;message&quot;: &quot;hello world&quot;} Forms : {} Headers : {[Content-Length, 26], [Content-Type, application/json], [Date, Thu, 17 Mar 2022 06:43:08 GMT], [Server, Werkzeug/1.0.1 Python/3.8.8]} Images : {} InputFields : {} Links : {} ParsedHtml : mshtml.HTMLDocumentClass RawContentLength : 26 . Debug Lambda Application Locally . To debug a lambda function we have following options . Option 1: Debug through SAM template . From VSCode open template.yaml, and go to the resources section of the template that defines serverless resources. Click on the lambda function resource, which in our case is HelloWorldFunction. A tooltip will appear over it saying AWS: Add Debug Configuration. Click it as shown below. . . This will create a new folder in the project with debug launch configuration launch.json. . . Let&#39;s add a breakpoint in our lambda handler code hello_world/app.py, and start debugging by clicking the green &quot;play&quot; button in the RUN view. When the debugging sessions starts, the DEBUG CONSOLE panel shows debugging output and displays any values returned by the Lambda function. . . Option 2: Debug Lambda Directly from Code . From VSCode open lambda handler code sam-app/hello_world/app.py. A tooltip will appear above the lambda_handler function with options . AWS: Add Debug Configuration | AWS: Edit Debug Configuration | . . Click on AWS: Add Debug Configuration and it will show two further options . template.yaml:HelloWorldFunction (to debug only the lambda function) | template.yaml:HelloWorldFunction (API Event: HelloWorld) (to debug lambda function along with API gateway) | . . Let&#39;s select API option this time. It will again create a launch configuration, and now we can debug our code. Click on the green &quot;play&quot; button again to start the debug session with request request coming from API Gateway to Lambda function. . You can also edit the debug config visually by selecting the AWS: Edit Debug Configuration, and a side pane will appear from where we can easily edit and update our debug configuration. . . All the code used for this post can be obtained from the GitHub repository hassaanbinaslam/2022-03-16-lambda-debug . Project code files | Project zip file | .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/lambda/docker/2022/03/16/lambda-debug.html",
            "relUrl": "/aws/lambda/docker/2022/03/16/lambda-debug.html",
            "date": " • Mar 16, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Docker - Debugging Python Application",
            "content": "About . This post is about debugging a Python application running on a Docker container inside WSL2 linux environment. Highlight of this post is Visual Studio Code environment and it extensions Remote Containers. . Environment Details . Python = 3.8.x | WSL version = 2 | WSL Ubuntu version = 20.04 | Docker Engine version = 20.10.12 | Docker Desktop version = 4.4.4 | Host OS = Windows 10 | VSCode version = 1.65 | . Sample Application . For this post I will use a a simple hello world application that will print &quot;hello world&quot; messages to stdout, and also logs them in a logfilelog. After each message the application sleeps for 5 seconds, and keeps on doing this for 5 mins (300 sec). After this the program exists. . This application was created as part of the previous blog post Docker - Accessing Python Application Logs. It is a very simple application, and you can find all the code in GitHub repository snapshots-docker-post-11032022 . Project code files | Project zip file | . Project structure of this application is . app/ ├── src/ │ ├── commons/ │ │ └── logger.py │ └── hello.py └── Dockerfile . Where . app/ is the project root folder | src/ folder contain the python application code | src/commons/logger.py is the logging module | src/hello.py is the main application | app/Dockerfile is the Docker image build file | . When I run the src/hello.py file from my local machine (Windows 10) I get the output on the termial with hello world messages like this. . . A &quot;logfile.log&quot; will also appear in the project directory containing the logged messages. . . ├── app/ │ ├── src/ │ │ ├── commons/ │ │ │ └── logger.py │ │ └── hello.py │ └── Dockerfile └── **logfile.log** . Contents of &quot;logfile.log&quot; file will look like this . INFO 2022-03-11 13:01:56,451 - hello world at 11/03/2022 13:01:56 INFO 2022-03-11 13:02:01,464 - hello world at 11/03/2022 13:02:01 INFO 2022-03-11 13:02:06,466 - hello world at 11/03/2022 13:02:06 INFO 2022-03-11 13:02:11,480 - hello world at 11/03/2022 13:02:11 INFO 2022-03-11 13:02:16,496 - hello world at 11/03/2022 13:02:16 . We can build our docker image and run it using commands . docker build --tag python-docker . docker run --name helloworld python-docker . Output on the terminal will be like this . Notice the difference in the print message when the application was is locally, and from the docker container. . Local (Win10) message = hello world at 14/03/2022 18:04:02 from Windows | Docker container message = hello world at 14/03/2022 13:12:14 from Linux | . Debug Docker Application . To debug the application from inside the docker container we will use VSCode extention Visual Studio Code Remote - Containers. From the extension docs . The Visual Studio Code Remote - Containers extension lets you use a Docker container as a full-featured development environment. It allows you to open any folder inside (or mounted into) a container and take advantage of Visual Studio Code&#39;s full feature set. . Once this extension is installed a new icon ( Remote Window ) will appear at the bottom left corner of the VSCode window. Once clicked on the icon, a dropped down will appear as shown below. From this drop down choose option Reopen in Container . . Now it is important to understand that Visual Studio Code Remote - Containers extension let&#39;s you use a Docker container as a full-featured development environment. It allows you to open any folder or repository inside a container and take advantage of Visual Studio Code&#39;s full feature set including code debugging, linting, formatting, intellisense, and other tooling. VSCode also provides its own prebuild docker images with all the necessary tools installed into them. Or we can we instruct the VSCode to create a new development container using our docker file. You can find a list of prebuild docker images here: microsoft-vscode-devcontainers . VSCode uses a configuration file called &quot;devcontainer.json&quot; to store instructions on how to create and attach to a development container. You can read more about this config file here: devcontainerjson-reference . Now let&#39;s create a new docker development environment using our Dockerfile. . Open VSCode Commands Palette (F1 or CTL+Shift+P on Win10) | Select &quot;Remote-Containers: Add Development Container Configuration Files&quot; option from dropdown | Then select &quot;from Docker&quot; since we want our development container environment same as defined in the Dockerfile | . If this option is not available, and the drop down is showing other options like in the image below, then VSCode is unable to find a Dockerfile associated with the project. . . Notice my project dir in the image below. The root folder of my project snapshots-docker-post-11032022 does not contain any Dockerfile. . . VSCode remote extension assumes that there is a Docker file at the root of the project directory. My project root contain app/ folder and inside this folder Dockerfile is located. When we select &quot;Remote-Containers: Add Development Container Configuration Files&quot; option docker extension checks the project root folder for a Dockerfile. It could not find one in my project and that is why it removed &#39;From Dockerfile&#39; option from the dropdown. Let&#39;s correctly open the project with app/ as the root folder (or place the Dockerfile at the project root folder). After correcting this mistake, my project in VSCode looks like this . . Now use the extension one more time to create a development container. This time you will find the option &quot;From Dockerfile&quot; in the dropdown menu . . Once this option is selected, VSCode will add a folder &quot;.devcontainer&quot; in the project root containing instructions on how to build and launch the development container. Then it will run those instruction to launch a container and connect to it. VSCode terminal will show the logs of all the commands used in launching that container, and at bottom left of VSCode window it will show the name of the container to which it is currently connected. . . Note that at this point we are actually working from inside a container. But to actually develop and debug the code from this container you will be required to install more extensions to it. If we had used a VSCode prebuild image then all the required extensions will be automatically available. To install required extension we can use VSCode extensions tab. . . Python extension will be required to work with Python code. So let&#39;s intall in our working container. You can also copy the names of installed extensions and paste them in the &quot;.devcontainer&quot; config file as shown below . // Add the IDs of extensions you want installed when the container is created. &quot;extensions&quot;: [ &quot;ms-python.python&quot;, &quot;ms-python.vscode-pylance&quot; ] . This way when next time we use this config file to launch a new dev container, all these extensions will be automatically installed for us. To customise the config file you can take help from this template provided by VSCode team python-3/.devcontainer . Installation of the extensions can be verified from the VSCode terminal logs . . We can now run our Python code from inside this container . . We can also easily debug our code directly from inside the container . . To close the remote connection, click on the Remote Window Icon at the bottom left corner. Use &quot;Reopen Folder Locally&quot; option to return back to local environment. Or &quot;Close Remote Connection&quot; to close the remote connection and also close the project. . . All the code used for this post can be obtained from the GitHub repository hassaanbinaslam/snapshots-docker-post-11032022 . Project code files | Project zip file | .",
            "url": "https://hassaanbinaslam.github.io/myblog/docker/python/2022/03/14/docker-app-debug.html",
            "relUrl": "/docker/python/2022/03/14/docker-app-debug.html",
            "date": " • Mar 14, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Docker - Accessing Python Application Logs",
            "content": "About . This post is about the challenges on accessing Python application logs running on a Docker container inside WSL2 linux environment. . Environment Details . Python = 3.8.x | WSL version = 2 | WSL Ubuntu version = 20.04 | Docker Engine version = 20.10.12 | Docker Desktop version = 4.4.4 | Host OS = Windows 10 | . Sample Application . Let us create a simple hello world application that will print &quot;hello world&quot; message to stdout, and also logs them in a logfile. After each message the application sleeps for 5 seconds, and keeps on doing this for 5 mins (300 sec). After this the program exists. . Project structure of this application is . app/ └── src/ ├── commons/ │ └── logger.py └── hello.py . Where . app/ is the project root folder | src/ folder contain the python application code | src/commons/logger.py is the logging module | src/hello.py is the main application | . Code files are provided below . # app/src/commons/logger.py import logging import os logformat = &quot;%(levelname)s %(asctime)s - %(message)s&quot; filename = &quot;logfile.log&quot; # Setting the config of the log object logging.basicConfig( format=logformat, filename=filename, level=logging.INFO, ) . # app/src/hello.py from datetime import datetime import time import commons.logger as logger def main(): # run for about 5 min: 300 sec for i in range(60): now = datetime.now() dt_string = now.strftime(&quot;%d/%m/%Y %H:%M:%S&quot;) # prepare message msg = f&quot;hello world at {dt_string}&quot; # put message to stdout and logs print(msg) logger.logging.info(msg) # sleep for some seconds time.sleep(5) if __name__ == &quot;__main__&quot;: main() . When I run the hello.py file I get the output on the termial with hello world messages like this. . . When we run the application a &quot;logfile.log&quot; will also appear in the project directory containing the logged messages. . . ├── app/ │ └── src/ │ ├── commons/ │ │ └── logger.py │ └── hello.py └── **logfile.log** . Contents of &quot;logfile.log&quot; file will look like this . INFO 2022-03-11 13:01:56,451 - hello world at 11/03/2022 13:01:56 INFO 2022-03-11 13:02:01,464 - hello world at 11/03/2022 13:02:01 INFO 2022-03-11 13:02:06,466 - hello world at 11/03/2022 13:02:06 INFO 2022-03-11 13:02:11,480 - hello world at 11/03/2022 13:02:11 INFO 2022-03-11 13:02:16,496 - hello world at 11/03/2022 13:02:16 . All the code till this point can be found at GitHub repository https://github.com/hassaanbinaslam/snapshots-docker-post-11032022 . Project code files | Project zip file | . Dockerize the application . Our hello-world application is ready now, and we can put it inside a docker container. For this let&#39;s create a Dockerfile and place it in app/ folder. . # app/Dockerfile FROM python:3.8-slim-buster # set the working directory in the container WORKDIR /app # copy the content of the local src directory to the working directory COPY src/ . # command to run on container start CMD [ &quot;python3&quot;, &quot;./hello.py&quot;] . We can build our docker image by running the command from terminal at folder app/ . docker build --tag python-docker . . Output of this command will look like this . We can check the created docker image using command from terminal . docker images . Output of this command will look like this . So our docker image is ready, we can now run it using command . docker run --name helloworld python-docker . After running this command you will observe that there is no output on the terminal. Even though we have run our image in an attached mode but still there is no output. We know that the container is running as control on terminal has not return back to us. We can also verify that the container is running by running command docker ps in a separate terminal. Output from this command will look like this . . We can also verify that the container is running from Docker Desktop container apps menu. Running container instance will appear like this . . The reason for logs not appearing on the terminal is because they are being buffered by docker internally. You will get all of them once docker container has finished execution and stopped. You can read more about docker buffereing the output from these StackOverflow posts . disable-output-buffering | python-app-does-not-print-anything-when-running-detached-in-docker | . To disable the output buffering we need to change the CMD in our docker file as . CMD [ &quot;python3&quot;, &quot;-u&quot;, &quot;./hello.py&quot;] . # app/Dockerfile FROM python:3.8-slim-buster # set the working directory in the container WORKDIR /app # copy the content of the local src directory to the working directory COPY src/ . # command to run on container start CMD [ &quot;python3&quot;, &quot;-u&quot;, &quot;./hello.py&quot;] . We need to rebuild our docker image and then run it. Let&#39;s do that will the following commands . docker build --tag python-docker . docker run --name helloworld python-docker . this time you can see the output directly on the terminal . We don&#39;t have to run the docker container in an attached mode, and can still get the logs from a running container using docker logs command. Let&#39;s do that then . first run the docker image in a detached mode . docker run -d --name helloworld python-docker . this command will give the running container ID which will look something like this 06918cf824210c015b08ef072feed01768be30ee569e5dbc0eef8d3cd855ab47. We can pass this ID to our next command to view the logs stream from a running container. Use this command . docker logs -f 06918cf824210c015b08ef072feed01768be30ee569e5dbc0eef8d3cd855ab47 . Also note that we don&#39;t need to provide the full container id, and can can also provide the first 2 or 3 digits of the ID that can uniquely identify the running container. So following command will also work . docker logs -f 069 . Output on the terminal will look like this . All the project files till this point can be found at . project code files | . Log Files Physical Location . Docker STDOUT / STDERR logs are also stored on host system as JSON files. . On linux You can find them at . /var/lib/docker/containers/&lt;container-id&gt;/&lt;container-id&gt;-json.log . On Windows You can find them at . wsl$ docker-desktop-data version-pack-data community docker containers &lt;container-id&gt;/&lt;container-id&gt;-json.log . In the above paths replace &lt;container-id&gt; with your full cotaniner ID. Also note that on Windows 10 you can open the folder location by directly pasting the path in folder explorer search bar. . You can read more about them in the following StackOverflow post . windows-10-and-docker-container-logs-docker-logging-driver | . Application Log Files . So far we have talked about the docker logs that were generated by application feed to STDOUT or STDERR. But now we are interested in app logs generated by logging module like the one we saw in our first example &quot;logfile.log&quot;. For this let&#39;s first connect to running docker container and see where is this file located inside the docker running instance. . Run a new docker container again using command . docker run -d --name helloworld python-docker . If container already exists you can just start it using command . docker start &lt;container-id&gt; . Or you can also use the docker desktop to start existing container by click the play button over it. . . Once the docker container is running, you can connect to it by either using the CLI from docker desktop . . or from the command below . docker exec -it &lt;cotainer-id&gt; /bin/bash . Remember that container-id of a running container can be obtained by command docker ps. Output from the above command will look like this . . Note that the location of &quot;logfile.log&quot; is under app/ folder . Docker Volume . We have seen the application logs by connecting to the docker container, but we would like to have these logs readily available outside the docker container so they can be consumed in real time for debugging. Fot this docker recommends using volume. Let&#39;s create a volume and then mount our application logs to it. . To create a volume use command . docker volume create applogs . To remove a volume use command . docker rm create applogs . To inspect a created volume use command . docker volume inspect applogs . Output of this command will be like this where Mountpoint is the location of logs on host system. . [ { &quot;CreatedAt&quot;: &quot;2022-03-11T13:12:57Z&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: {}, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/applogs/_data&quot;, &quot;Name&quot;: &quot;applogs&quot;, &quot;Options&quot;: {}, &quot;Scope&quot;: &quot;local&quot; } ] . On Windows 10 you can find the location of these volumes at . wsl$ docker-desktop-data version-pack-data community docker volumes applogs _data . Now let&#39;s run our docker container with this volume mounted to it. Command for this is . docker run -d --name helloworld -v applogs:/app python-docker . Docker container will run as before, but if we check the volume on our host we will find that this time all the files and folders available in app/ folder from inside the docker container are available. And they will persist on the host OS even if the container is stopped. . . Note that the log file &#39;logfile.log&#39; is also available outside the container, and we can poll it for debugging. But having all the contents of app/ folder exposed can be a security issue, as they can contain secrets and passwords. We should only mount the logfile.log file on the volume as a best practice. So let&#39;s do that next. . Application log files from the docker container . To do this we need to slightly update our application, and create the logfile.log file in a designated log/ folder inside the app/. This way we can only mount app/log/ folder on the volume. In our application we will update the logging module as . # app/src/commons/logger.py import logging import os if not os.path.exists(&quot;logs&quot;): os.makedirs(&quot;logs&quot;) logformat = &quot;%(levelname)s %(asctime)s - %(message)s&quot; filename = &quot;logs/logfile.log&quot; # Setting the config of the log object logging.basicConfig( format=logformat, filename=filename, level=logging.INFO, ) . This is all we need to change in our application. To cleanup the volume we can either remove the old one and recreate a new one. Or we can directly delete all the files &amp; folders from the host OS from the directory . wsl$ docker-desktop-data version-pack-data community docker volumes applogs _data . Since we have updated the application code we need to rebuild our docker image. Then create a new docker container, and this time only mount the logs/ folder on the volume. Command for this is . # delete the old volume docker volume rm applogs # create a new volume docker volume create applogs # build the docker image docker build --tag python-docker . # run the docker container and mount a specific folder on volume docker run -d --name helloworld -v applogs:/app/logs python-docker . If we check the mounted volume, this time only logfile.log is exposed. . . All the code for this post can be found at GitHub repository https://github.com/hassaanbinaslam/snapshots-docker-post-11032022 . Project code files | Project zip file | .",
            "url": "https://hassaanbinaslam.github.io/myblog/docker/python/2022/03/11/docker-app-logs.html",
            "relUrl": "/docker/python/2022/03/11/docker-app-logs.html",
            "date": " • Mar 11, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Sklearn Pipeline and Transformers Deep Dive",
            "content": "About . This notebook shows various ways to work with Skearn Pipelines. . We will start with some of the limitations of pipelines and how to overcome them | We will discuss getting a dataframe from a pipeline instead of a NumPy array, and the benefits of this approach | We will learn how to use CustomTransformer and a FunctionTrasformer | We will also build a custom transformer to do some feature engineering | Along the way, we will also see how to avoid common mistakes while creating pipelines | . Setup . Environment Details . from platform import python_version import sklearn, numpy, matplotlib, pandas print(&quot;python==&quot; + python_version()) print(&quot;sklearn==&quot; + sklearn.__version__) print(&quot;numpy==&quot; + numpy.__version__) print(&quot;pandas==&quot; + pandas.__version__) print(&quot;matplotlib==&quot; + matplotlib.__version__) . . python==3.8.8 sklearn==1.0.2 numpy==1.20.1 pandas==1.2.3 matplotlib==3.5.1 . Loading Data . For this example we will use the original Titanic dataset, describing the survival status of individual passengers on the Titanic ship. . Some notes from original source: . The variables on our extracted dataset are &#39;pclass&#39;, &#39;name&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;ticket&#39;, &#39;fare&#39;,&#39;cabin&#39;, &#39;embarked&#39;, &#39;boat&#39;, &#39;body&#39;, and &#39;home.dest&#39;. | pclass refers to passenger class (1st, 2nd, 3rd), and is a proxy for socio-economic class. | Age is in years, and some infants had fractional values. | sibsp = Number of Siblings/Spouses aboard | parch = Number of Parents/Children aboard | The target is either a person survived or not (1 or 0) | . Important note: The purpose of this notebook is not to train a best model on titanic data, but to understand the working of Sklearn pipeline and transformers. So please be mindful of that. . from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split import numpy as np np.random.seed(42) # for consistency # Load data from https://www.openml.org/d/40945 X, y = fetch_openml(&quot;titanic&quot;, version=1, as_frame=True, return_X_y=True) X.head() . pclass name sex age sibsp parch ticket fare cabin embarked boat body home.dest . 0 1.0 | Allen, Miss. Elisabeth Walton | female | 29.0000 | 0.0 | 0.0 | 24160 | 211.3375 | B5 | S | 2 | NaN | St Louis, MO | . 1 1.0 | Allison, Master. Hudson Trevor | male | 0.9167 | 1.0 | 2.0 | 113781 | 151.5500 | C22 C26 | S | 11 | NaN | Montreal, PQ / Chesterville, ON | . 2 1.0 | Allison, Miss. Helen Loraine | female | 2.0000 | 1.0 | 2.0 | 113781 | 151.5500 | C22 C26 | S | None | NaN | Montreal, PQ / Chesterville, ON | . 3 1.0 | Allison, Mr. Hudson Joshua Creighton | male | 30.0000 | 1.0 | 2.0 | 113781 | 151.5500 | C22 C26 | S | None | 135.0 | Montreal, PQ / Chesterville, ON | . 4 1.0 | Allison, Mrs. Hudson J C (Bessie Waldo Daniels) | female | 25.0000 | 1.0 | 2.0 | 113781 | 151.5500 | C22 C26 | S | None | NaN | Montreal, PQ / Chesterville, ON | . # let&#39;s check the frequency of missing values in each feature X.isnull().sum().sort_values(ascending=False) . body 1188 cabin 1014 boat 823 home.dest 564 age 263 embarked 2 fare 1 pclass 0 name 0 sex 0 sibsp 0 parch 0 ticket 0 dtype: int64 . # let&#39;s drop top 4 features with highest percentage of missing data # This step is done to make our working with pipeline simpler and easier to understand X.drop([&#39;body&#39;, &#39;cabin&#39;, &#39;boat&#39;, &#39;home.dest&#39;], axis=1, inplace=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2) X_train.head() . pclass name sex age sibsp parch ticket fare embarked . 999 3.0 | McCarthy, Miss. Catherine &#39;Katie&#39; | female | NaN | 0.0 | 0.0 | 383123 | 7.7500 | Q | . 392 2.0 | del Carlo, Mrs. Sebastiano (Argenia Genovesi) | female | 24.0 | 1.0 | 0.0 | SC/PARIS 2167 | 27.7208 | C | . 628 3.0 | Andersson, Miss. Sigrid Elisabeth | female | 11.0 | 4.0 | 2.0 | 347082 | 31.2750 | S | . 1165 3.0 | Saad, Mr. Khalil | male | 25.0 | 0.0 | 0.0 | 2672 | 7.2250 | C | . 604 3.0 | Abelseth, Miss. Karen Marie | female | 16.0 | 0.0 | 0.0 | 348125 | 7.6500 | S | . Some Terminology First . Datasets . Scikit-learn deals with learning information from one or more datasets that are represented as 2D arrays. They can be understood as a list of multi-dimensional observations. We say that the first axis of these arrays is the samples axis, while the second is the features axis. . (n_samples, n_features) . # for our titanic dataset: # n_samples = 1309 # n_features = 9 X.shape . (1309, 9) . Estimator . An estimator is any object that learns from data; it may be a classification, regression or clustering algorithm or a transformer that extracts/filters useful features from raw data. . All estimator objects expose a fit method that takes a dataset (usually a 2-d array) . estimator.fit(data) . Transformer . An estimator supporting transform and/or fit_transform methods. . A transformer, transforms the input, usually only X, into some transformed space. Output is an array or sparse matrix of length n_samples and with the number of columns fixed after fitting. . Fit . The fit method is provided on every estimator. It usually takes some samples X, targets y if the model is supervised, and potentially other sample properties such as sample_weight. . It should: . clear any prior attributes stored on the estimator, unless warm_start is used | validate and interpret any parameters, ideally raising an error if invalid | validate the input data | estimate and store model attributes from the estimated parameters and provided data; and | return the now fitted estimator to facilitate method chaining | . Note: . Fitting = Calling fit (or fit_transform, fit_predict) method on an estimator. | Fitted = The state of an estimator after fitting. | . Sklearn Pipeline . class sklearn.pipeline.Pipeline(steps, *, memory=None, verbose=False) . It is a pipeline of transformers with a final estimator. . It sequentially applies a list of transforms and a final estimator. Intermediate steps of the pipeline must be transforms, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. . Lets create a simple pipeline to better understand its componets. Steps in our pipeline will be . replace missing values using the mean along each numerical feature column; and | then scale them | . from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler pipe = Pipeline(steps=[ (&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler()) ]) # our first pipeline has been initialized pipe . Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler())]) . We can also visualize the pipeline as a diagram. It has two steps: imputer and scaler in sequence. . from sklearn import set_config set_config(display=&quot;diagram&quot;) pipe . Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputer()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]) . Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputer()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]) . SimpleImputerSimpleImputer() . StandardScalerStandardScaler() . now lets call fit_transform method to run this pipeline, and preprocess our loaded data . pipe.fit_transform(X_train, y_train) . ValueError Traceback (most recent call last) Input In [8], in &lt;module&gt; 1 #collapse-output -&gt; 2 pipe.fit_transform(X_train, y_train) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn pipeline.py:426, in Pipeline.fit_transform(self, X, y, **fit_params) 399 &#34;&#34;&#34;Fit the model and transform with the final estimator. 400 401 Fits all the transformers one after the other and transform the (...) 423 Transformed samples. 424 &#34;&#34;&#34; 425 fit_params_steps = self._check_fit_params(**fit_params) --&gt; 426 Xt = self._fit(X, y, **fit_params_steps) 428 last_step = self._final_estimator 429 with _print_elapsed_time(&#34;Pipeline&#34;, self._log_message(len(self.steps) - 1)): File ~ anaconda3 envs sc_mlflow lib site-packages sklearn pipeline.py:348, in Pipeline._fit(self, X, y, **fit_params_steps) 346 cloned_transformer = clone(transformer) 347 # Fit or load from cache the current transformer --&gt; 348 X, fitted_transformer = fit_transform_one_cached( 349 cloned_transformer, 350 X, 351 y, 352 None, 353 message_clsname=&#34;Pipeline&#34;, 354 message=self._log_message(step_idx), 355 **fit_params_steps[name], 356 ) 357 # Replace the transformer of the step with the fitted 358 # transformer. This is necessary when loading the transformer 359 # from the cache. 360 self.steps[step_idx] = (name, fitted_transformer) File ~ anaconda3 envs sc_mlflow lib site-packages joblib memory.py:352, in NotMemorizedFunc.__call__(self, *args, **kwargs) 351 def __call__(self, *args, **kwargs): --&gt; 352 return self.func(*args, **kwargs) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn pipeline.py:893, in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params) 891 with _print_elapsed_time(message_clsname, message): 892 if hasattr(transformer, &#34;fit_transform&#34;): --&gt; 893 res = transformer.fit_transform(X, y, **fit_params) 894 else: 895 res = transformer.fit(X, y, **fit_params).transform(X) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn base.py:855, in TransformerMixin.fit_transform(self, X, y, **fit_params) 852 return self.fit(X, **fit_params).transform(X) 853 else: 854 # fit method of arity 2 (supervised transformation) --&gt; 855 return self.fit(X, y, **fit_params).transform(X) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn impute _base.py:319, in SimpleImputer.fit(self, X, y) 302 def fit(self, X, y=None): 303 &#34;&#34;&#34;Fit the imputer on `X`. 304 305 Parameters (...) 317 Fitted estimator. 318 &#34;&#34;&#34; --&gt; 319 X = self._validate_input(X, in_fit=True) 321 # default fill_value is 0 for numerical input and &#34;missing_value&#34; 322 # otherwise 323 if self.fill_value is None: File ~ anaconda3 envs sc_mlflow lib site-packages sklearn impute _base.py:285, in SimpleImputer._validate_input(self, X, in_fit) 279 if &#34;could not convert&#34; in str(ve): 280 new_ve = ValueError( 281 &#34;Cannot use {} strategy with non-numeric data: n{}&#34;.format( 282 self.strategy, ve 283 ) 284 ) --&gt; 285 raise new_ve from None 286 else: 287 raise ve ValueError: Cannot use mean strategy with non-numeric data: could not convert string to float: &#34;McCarthy, Miss. Catherine &#39;Katie&#39;&#34; . . Aaargh! this is not what we intended. Let us try to understand why our pipeline did not work and then fix it. The exception message says: . ValueError: Cannot use mean strategy with non-numeric data: could not convert string to float: &quot;McCarthy, Miss. Catherine &#39;Katie&#39; . From the error message we can deduce that Pipeline is trying to apply its transformers on all columns in the dataset. This was not our intention, as we wanted to apply the transformers to numeric data only. Let&#39;s limit our simple pipeline to numerical columns and run again. . num_cols = [&#39;age&#39;, &#39;fare&#39;] pipe.fit_transform(X_train[num_cols], y_train) . array([[ 0. , -0.49963779], [-0.43641134, -0.09097855], [-1.44872891, -0.01824953], ..., [-0.98150542, -0.49349894], [-0.82576425, -0.44336498], [-0.59215251, -0.49349894]]) . Alright, our pipeline has run now and we can also observe a few outcomes. . When we apply a pipeline to a dataset it will run transformers to all features in the dataset. | Output from one transformer will be passed on to the next one until we reach the end of the pipeline | If we want to apply different transformers for numerical and categorical features (heterogeneous data) then the pipeline will not work for us. We would have to create separate pipelines for the different feature sets and then join the output. | . To overcome the limitation of a pipeline for heterogeneous data, Sklearn recommends using ColumnTransformer. With ColumnTransformer we can provide column names against the transformers on which we want to apply them. . ColumnTransformer . Let&#39;s see our first ColumnTransformer in action. . from sklearn.compose import ColumnTransformer from sklearn.preprocessing import OneHotEncoder # Note the sequence when creating a ColumnTransformer # 1. a name for the transformer # 2. the transformer # 3. the column names pipe = ColumnTransformer([ (&#39;standardscaler&#39;, StandardScaler(), [&#39;age&#39;, &#39;fare&#39;] ), (&#39;onehotencoder&#39;, OneHotEncoder(), [&#39;sex&#39;]) ]) pipe . ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])]) . Please rerun this cell to show the HTML repr or trust the notebook.ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])]) . standardscaler[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder() . # Let&#39;s execute our pipeline to see the output. pipe.fit_transform(X_train, y_train) . array([[ nan, -0.49939913, 1. , 0. ], [-0.39043136, -0.09093509, 1. , 0. ], [-1.2960919 , -0.01824081, 1. , 0. ], ..., [-0.87809473, -0.49326321, 0. , 1. ], [-0.73876234, -0.4431532 , 0. , 1. ], [-0.52976375, -0.49326321, 0. , 1. ]]) . get_feature_names_out(input_features=None) . At this point I will also introduce a very useful function get_feature_names_out(input_features=None). Using this method we can get output feature names as well. . pipe.get_feature_names_out() . array([&#39;standardscaler__age&#39;, &#39;standardscaler__fare&#39;, &#39;onehotencoder__sex_female&#39;, &#39;onehotencoder__sex_male&#39;], dtype=object) . Notice the output . Output feature names appear as &lt;transformer_name&gt;__&lt;feature_name&gt; | For OneHotEncoded feature &quot;sex&quot;, output feature names have the label attached to them | . make_column_transformer . Sklean also provides a wrapper function for ColumnTransformer where we don&#39;t have to provide names for the transformers. . from sklearn.compose import make_column_transformer # Note the sequence when using make_column_transformer # 1. the transformer # 2. the column names pipe = make_column_transformer( (StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (OneHotEncoder(), [&#39;sex&#39;] ), verbose_feature_names_out=False # to keep output feature names simple ) pipe . ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False) . Please rerun this cell to show the HTML repr or trust the notebook.ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False) . standardscaler[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder() . # Let&#39;s execute our pipeline. pipe.fit_transform(X_train, y_train) . array([[ nan, -0.49939913, 1. , 0. ], [-0.39043136, -0.09093509, 1. , 0. ], [-1.2960919 , -0.01824081, 1. , 0. ], ..., [-0.87809473, -0.49326321, 0. , 1. ], [-0.73876234, -0.4431532 , 0. , 1. ], [-0.52976375, -0.49326321, 0. , 1. ]]) . # notice the feature names this time. they are shorter. # we have used attribute &quot;verbose_feature_names_out=False&quot; in our pipeline above. pipe.get_feature_names_out() . array([&#39;age&#39;, &#39;fare&#39;, &#39;sex_female&#39;, &#39;sex_male&#39;], dtype=object) . Important difference between Pipeline and ColumnTransformer . Pipeline applies transformer in sequence on all columns | ColumnTranformer applies transformers in parallel to specified columns and then concats the output | . Open questions? . So our ColumnTransformer is working. But we have a few more questions to address. . Why is the output from our pipeline or ColumnTransformer not shown as a dataframe with output features nicely separated in different columns? | Our input dataset had more features besides age, fare, and sex. Why are they not present in the output? | What happens if I change the sequence of transformers, and feature names in my ColumnTransformer? | . In the coming sections, we will try to address these questions. . Why is the output not a dataframe? . The output from a pipeline or a ColumnTransformer is an nd-array where the first index is the number of samples, and second index are the output features (n_samples, n_output_features). Since we are only getting numpy array as an output, we are losing information about the column names. . temp = pipe.fit_transform(X_train, y_train) print(type(temp)) print(temp.shape) . &lt;class &#39;numpy.ndarray&#39;&gt; (1047, 4) . Can we get the feature names back? . We have already seen that we can get the output feature names using method get_feature_names_out. But this time let&#39;s try to analyze our ColumnsTransformer more closely. The transformer attributes discussed here also applies to Pipeline object. . # print the internals of ColumnTransformer set_config(display=&#39;text&#39;) pipe . ColumnTransformer(transformers=[(&#39;standardscaler&#39;, StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (&#39;onehotencoder&#39;, OneHotEncoder(), [&#39;sex&#39;])], verbose_feature_names_out=False) . ColumnTransformer has an attribute &#39;transformers&#39; that is keeping a list of all the provided transformers. Let&#39;s print it. . pipe.transformers . [(&#39;standardscaler&#39;, StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (&#39;onehotencoder&#39;, OneHotEncoder(), [&#39;sex&#39;])] . These are the transformers list at the initialization time. If we want to check the transformers after fit function has been called, then we need to print a different attribute transformers_. . pipe.transformers_ . [(&#39;standardscaler&#39;, StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (&#39;onehotencoder&#39;, OneHotEncoder(), [&#39;sex&#39;]), (&#39;remainder&#39;, &#39;drop&#39;, [0, 1, 4, 5, 6, 8])] . You can see the difference. There is an extra transformer with the name remainder at the end. It was not present at the initialization time. What it does is that it drops all remaining columns from the dataset that have not been explicitly used in the ColumnTransformer. Since, at the initialization time, ColumnTransformer does not know about the other columns that it needs to drop this transformer is missing. During fit it sees the dataset and knows about the other columns, it then keeps a list of them to drop (0, 1, 4, 5, 6, 8). . We can also index through the transformers as well to fetch anyone from the list. . # second transformer from the list pipe.transformers_[1] . (&#39;onehotencoder&#39;, OneHotEncoder(), [&#39;sex&#39;]) . Notice the tuple sequence. . First is the name | Second is the transformer | Third are the column names | . We can also call get_feature_names_out method on a separate transformer from the list. . # output features from second tranformer pipe.transformers_[1][1].get_feature_names_out() . array([&#39;sex_female&#39;, &#39;sex_male&#39;], dtype=object) . # output features from last tranformer pipe.transformers_[-1][1].get_feature_names_out() # No. We cannot do this on last transformer (remainder). . AttributeError Traceback (most recent call last) Input In [22], in &lt;module&gt; 1 #collapse-output 2 # output features from last tranformer -&gt; 3 pipe.transformers_[-1][1].get_feature_names_out() AttributeError: &#39;str&#39; object has no attribute &#39;get_feature_names_out&#39; . We now have output feature names, and the output (nd-array). Can we convert them to a DataFrame? . import pandas as pd temp = pipe.fit_transform(X_train, y_train) col_names = pipe.get_feature_names_out() output = pd.DataFrame(temp.T, col_names).T output.head() . age fare sex_female sex_male . 0 NaN | -0.499399 | 1.0 | 0.0 | . 1 -0.390431 | -0.090935 | 1.0 | 0.0 | . 2 -1.296092 | -0.018241 | 1.0 | 0.0 | . 3 -0.320765 | -0.510137 | 0.0 | 1.0 | . 4 -0.947761 | -0.501444 | 1.0 | 0.0 | . FunctionTransformer . We know how to convert the transformer output to a DataFrame. It would be much simpler if we don&#39;t have to do an extra step, and can directly get a Dataframe from our fitted ColumnTransformer. . For this we can take the help of FunctionTransformer . A FunctionTransformer forwards its X (and optionally y) arguments to a user-defined function or function object and returns the result of this function. . Let&#39;s see a FunctionTransformer in action. . from sklearn.preprocessing import FunctionTransformer preprocessor = make_column_transformer( (StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (OneHotEncoder(), [&#39;sex&#39;] ), verbose_feature_names_out=False ) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;,FunctionTransformer(lambda x: pd.DataFrame(x, columns = preprocessor.get_feature_names_out()))) ]) set_config(display=&quot;diagram&quot;) pipe . Pipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;dataframer&amp;#x27;, FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x0000016196A1B040&gt;))]) . Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;dataframer&amp;#x27;, FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x0000016196A1B040&gt;))]) . preprocess: ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False) . standardscaler[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder() . FunctionTransformerFunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x0000016196A1B040&gt;) . Notice that we have applied FunctionTransformer after ColumnTransformer in a Pipeline. When we fit our pipeline on the dataset, ColumnTransformer will be fitted first and then the FunctionTransformer. Since the ColumnTransformer has been fitted first, we will be able to call get_feature_names_out on it while passing data to FunctionTransformer. . # let&#39;s run our pipeline again temp = pipe.fit_transform(X_train, y_train) temp.head() . age fare sex_female sex_male . 0 NaN | -0.499399 | 1.0 | 0.0 | . 1 -0.390431 | -0.090935 | 1.0 | 0.0 | . 2 -1.296092 | -0.018241 | 1.0 | 0.0 | . 3 -0.320765 | -0.510137 | 0.0 | 1.0 | . 4 -0.947761 | -0.501444 | 1.0 | 0.0 | . This is looking good. We are now getting back a dataframe directly from the pipeline. With a dataframe it is a lot easier to view and verify the output from the preprocessor. . But we have to be very careful with FunctionTransformer. In Sklearn docs, it says . Note: If a lambda is used as the function, then the resulting transformer will not be pickleable. . Huh! that is a very concerning point. We have also used a lambda function, and we will not be able to pickle it. Let&#39;s check it first. . import pickle # save our pipeline s1 = pickle.dumps(pipe) # reload it s2 = pickle.loads(s1) s2 . PicklingError Traceback (most recent call last) Input In [26], in &lt;module&gt; 1 import pickle 3 # save our pipeline -&gt; 4 s1 = pickle.dumps(pipe) 6 # reload it 7 s2 = pickle.loads(s1) PicklingError: Can&#39;t pickle &lt;function &lt;lambda&gt; at 0x0000016196A1B040&gt;: attribute lookup &lt;lambda&gt; on __main__ failed . The documentation was right about it. We have used a Lambda function in our FunctionTranformer and we got a pickle error. Since, the limitation is said for Lambda function, changing it with a normal function should work. Let&#39;s do that. . def get_dataframe(X, transformer): &quot;&quot;&quot; x: an nd-array transformer: fitted transformer &quot;&quot;&quot; col_names = transformer.get_feature_names_out() output = pd.DataFrame(X.T, col_names).T return output preprocessor = make_column_transformer( (StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (OneHotEncoder(), [&#39;sex&#39;] ), verbose_feature_names_out=False ) dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer) ]) set_config(display=&quot;diagram&quot;) pipe . Pipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;dataframer&amp;#x27;, FunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)}))]) . Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;dataframer&amp;#x27;, FunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)}))]) . preprocess: ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False) . standardscaler[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder() . FunctionTransformerFunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)}) . Notice the arguments for FunctionTransformer in the above code. . first argument is the function to be called | second argument are the parameters to be passed to our function | . The sequence of arguments for the callable function will be . first argument will be the output from any previous step in the pipeline (if there is any). In our case, it is nd-array coming from ColumnTransformer. It will be mapped to X. We don&#39;t have to do anything about it. | second argument (if any) we want to pass to function. In our case we need it to be the fitted transformer from the previous step so we have explicitly passed it using kw_args as key-value pair. Where key name is the same as callable method argument name (&#39;transformer&#39; in our case). | . Now let&#39;s do our pickle test one more time. . # save our pipeline s1 = pickle.dumps(pipe) # reload it s2 = pickle.loads(s1) s2 . Pipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;dataframer&amp;#x27;, FunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)}))]) . Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;dataframer&amp;#x27;, FunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)}))]) . preprocess: ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False) . standardscaler[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder() . FunctionTransformerFunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)}) . Alright, no more issues so let&#39;s proceed to our next question. . Where are the rest of the columns? . By default, only the specified columns in transformers are transformed and combined in the output, and the non-specified columns are dropped. (default of remainder=&#39;drop&#39;). By specifying remainder=&#39;passthrough&#39;, all remaining columns that were not specified in transformers will be automatically passed through. This subset of columns is concatenated with the output of the transformers. . Let&#39;s see it in action. . preprocessor = make_column_transformer( (StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (OneHotEncoder(), [&#39;sex&#39;] ), verbose_feature_names_out=False, remainder=&#39;passthrough&#39; ) # get_dataframe is already defined in last section. Intentionally omitted here. dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer) ]) temp = pipe.fit_transform(X_train, y_train) temp.head() . age fare sex_female sex_male pclass name sibsp parch ticket embarked . 0 NaN | -0.499399 | 1.0 | 0.0 | 3.0 | McCarthy, Miss. Catherine &#39;Katie&#39; | 0.0 | 0.0 | 383123 | Q | . 1 -0.390431 | -0.090935 | 1.0 | 0.0 | 2.0 | del Carlo, Mrs. Sebastiano (Argenia Genovesi) | 1.0 | 0.0 | SC/PARIS 2167 | C | . 2 -1.296092 | -0.018241 | 1.0 | 0.0 | 3.0 | Andersson, Miss. Sigrid Elisabeth | 4.0 | 2.0 | 347082 | S | . 3 -0.320765 | -0.510137 | 0.0 | 1.0 | 3.0 | Saad, Mr. Khalil | 0.0 | 0.0 | 2672 | C | . 4 -0.947761 | -0.501444 | 1.0 | 0.0 | 3.0 | Abelseth, Miss. Karen Marie | 0.0 | 0.0 | 348125 | S | . We have our remaining features back now, so let&#39;s proceed to our next question. . What happens if I change the sequence in ColumnTranformer? . It is better to make some changes and then see the results. I am making two changes in ColumnTransformer . Changed the order of transformers (OHE before scaling) | Changed the order of features inside the transformer (&#39;fare&#39; before &#39;age&#39;) | preprocessor = make_column_transformer( (OneHotEncoder(), [&#39;sex&#39;] ), (StandardScaler(), [&#39;fare&#39;, &#39;age&#39;]), verbose_feature_names_out=False, remainder=&#39;passthrough&#39; ) # get_dataframe is already defined in last section. Intentionally omitted here. dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer) ]) temp = pipe.fit_transform(X_train, y_train) temp.head() . sex_female sex_male fare age pclass name sibsp parch ticket embarked . 0 1.0 | 0.0 | -0.499399 | NaN | 3.0 | McCarthy, Miss. Catherine &#39;Katie&#39; | 0.0 | 0.0 | 383123 | Q | . 1 1.0 | 0.0 | -0.090935 | -0.390431 | 2.0 | del Carlo, Mrs. Sebastiano (Argenia Genovesi) | 1.0 | 0.0 | SC/PARIS 2167 | C | . 2 1.0 | 0.0 | -0.018241 | -1.296092 | 3.0 | Andersson, Miss. Sigrid Elisabeth | 4.0 | 2.0 | 347082 | S | . 3 0.0 | 1.0 | -0.510137 | -0.320765 | 3.0 | Saad, Mr. Khalil | 0.0 | 0.0 | 2672 | C | . 4 1.0 | 0.0 | -0.501444 | -0.947761 | 3.0 | Abelseth, Miss. Karen Marie | 0.0 | 0.0 | 348125 | S | . We can see that changing the sequence in ColumnTransformer does change the output. Also note . Specified columns in transformers are transformed and combined in the output | Transformers sequence in ColumnTransformer also represents the columns sequence in the output | When remainder=passthrough is used then remaining columns will be appended at the end. Remainder columns sequence will be same as in the input. | . Pipeline inside ColumnTransformer . Let&#39;s assume we have more requirements this time. I want . for numerical features (age, fare): impute the missing values first, and then scale them | for categorical features (sex): one hot encode them | . Our pipeline will look like this. . numeric_features = [&quot;age&quot;, &quot;fare&quot;] numeric_transformer = Pipeline( steps=[ (&quot;imputer&quot;, SimpleImputer(strategy=&quot;median&quot;)), (&quot;scaler&quot;, StandardScaler()) ]) categorical_features = [&quot;sex&quot;] categorical_transformer = OneHotEncoder(handle_unknown=&quot;ignore&quot;) preprocessor = make_column_transformer( (numeric_transformer, numeric_features), # note &quot;numeric_transformer&quot; is a pipeline this time (categorical_transformer, categorical_features), remainder=&#39;passthrough&#39;, verbose_feature_names_out=False ) dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer) ]) temp = pipe.fit_transform(X_train, y_train) temp.head() . AttributeError Traceback (most recent call last) Input In [31], in &lt;module&gt; 18 dataframer = FunctionTransformer(func=get_dataframe, kw_args={&#34;transformer&#34;: preprocessor}) 19 pipe = Pipeline([ 20 (&#34;preprocess&#34;, preprocessor), 21 (&#34;dataframer&#34;, dataframer) 22 ]) &gt; 24 temp = pipe.fit_transform(X_train, y_train) 25 temp.head() File ~ anaconda3 envs sc_mlflow lib site-packages sklearn pipeline.py:434, in Pipeline.fit_transform(self, X, y, **fit_params) 432 fit_params_last_step = fit_params_steps[self.steps[-1][0]] 433 if hasattr(last_step, &#34;fit_transform&#34;): --&gt; 434 return last_step.fit_transform(Xt, y, **fit_params_last_step) 435 else: 436 return last_step.fit(Xt, y, **fit_params_last_step).transform(Xt) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn base.py:855, in TransformerMixin.fit_transform(self, X, y, **fit_params) 852 return self.fit(X, **fit_params).transform(X) 853 else: 854 # fit method of arity 2 (supervised transformation) --&gt; 855 return self.fit(X, y, **fit_params).transform(X) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn preprocessing _function_transformer.py:182, in FunctionTransformer.transform(self, X) 169 &#34;&#34;&#34;Transform X using the forward function. 170 171 Parameters (...) 179 Transformed input. 180 &#34;&#34;&#34; 181 X = self._check_input(X, reset=False) --&gt; 182 return self._transform(X, func=self.func, kw_args=self.kw_args) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn preprocessing _function_transformer.py:205, in FunctionTransformer._transform(self, X, func, kw_args) 202 if func is None: 203 func = _identity --&gt; 205 return func(X, **(kw_args if kw_args else {})) Input In [27], in get_dataframe(X, transformer) 1 def get_dataframe(X, transformer): 2 &#34;&#34;&#34; 3 x: an nd-array 4 transformer: fitted transformer 5 &#34;&#34;&#34; -&gt; 6 col_names = transformer.get_feature_names_out() 7 output = pd.DataFrame(X.T, col_names).T 8 return output File ~ anaconda3 envs sc_mlflow lib site-packages sklearn compose _column_transformer.py:481, in ColumnTransformer.get_feature_names_out(self, input_features) 479 transformer_with_feature_names_out = [] 480 for name, trans, column, _ in self._iter(fitted=True): --&gt; 481 feature_names_out = self._get_feature_name_out_for_transformer( 482 name, trans, column, input_features 483 ) 484 if feature_names_out is None: 485 continue File ~ anaconda3 envs sc_mlflow lib site-packages sklearn compose _column_transformer.py:454, in ColumnTransformer._get_feature_name_out_for_transformer(self, name, trans, column, feature_names_in) 450 if isinstance(column, Iterable) and not all( 451 isinstance(col, str) for col in column 452 ): 453 column = _safe_indexing(feature_names_in, column) --&gt; 454 return trans.get_feature_names_out(column) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn pipeline.py:751, in Pipeline.get_feature_names_out(self, input_features) 749 for _, name, transform in self._iter(): 750 if not hasattr(transform, &#34;get_feature_names_out&#34;): --&gt; 751 raise AttributeError( 752 &#34;Estimator {} does not provide get_feature_names_out. &#34; 753 &#34;Did you mean to call pipeline[:-1].get_feature_names_out&#34; 754 &#34;()?&#34;.format(name) 755 ) 756 feature_names_out = transform.get_feature_names_out(feature_names_out) 757 return feature_names_out AttributeError: Estimator imputer does not provide get_feature_names_out. Did you mean to call pipeline[:-1].get_feature_names_out()? . . Oh geez! What went wrong this time. The error message says . AttributeError: Estimator imputer does not provide get_feature_names_out. Did you mean to call pipeline[:-1].get_feature_names_out()? . From the error message I am getting that . Estimator imputer does not provide get_feature_names_out . Hmmm, this is strange. Why is this estimator missing a very useful function? Let&#39;s check the docs first on SimpleImputer. For the docs I indeed could not find this method get_feature_names_out() for this transformer. A little googling lead me to this Sklearn Github issue page Implement get_feature_names_out for all estimators. Developers are actively adding get_feature_names_out() to all estimators and transformers, and it looks like this feature has not been implemented for SimpleImputer till Sklearn version==1.0.2. But no worries we can overcome this limitation, and implement this feature ourselves through a custom transformer. . Custom Transformer . We can create a custom transformer or an estimator simply by inheriting a class from BaseEstimator and optionally the mixin classes in sklearn.base. Sklean provides a template that we can use to create our custom transformer. Template link is here: https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/_template.py#L146 . Let us use the same pipeline as in last cell but replace SimpleImputer with a custom one. . from sklearn.base import BaseEstimator, TransformerMixin class SimpleImputerCustom(BaseEstimator, TransformerMixin): def __init__(self, strategy=&#39;mean&#39;): self.strategy = strategy self.imputer = SimpleImputer(strategy=self.strategy) def fit(self, X, y): self.imputer.fit(X, y) return self def transform(self, X): return self.imputer.transform(X) def get_feature_names_out(self, input_features=None): # we have returned the input features name as out features will have the same name return input_features numeric_features = [&quot;age&quot;, &quot;fare&quot;] numeric_transformer = Pipeline( steps=[ (&quot;imputer&quot;, SimpleImputerCustom(strategy=&#39;mean&#39;)), (&quot;scaler&quot;, StandardScaler()) ] ) categorical_features = [&quot;sex&quot;] categorical_transformer = OneHotEncoder(handle_unknown=&quot;ignore&quot;) preprocessor = make_column_transformer( (numeric_transformer, numeric_features), # note &quot;numeric_transformer&quot; is a pipeline (categorical_transformer, categorical_features), remainder=&#39;passthrough&#39;, verbose_feature_names_out=False ) dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer) ]) temp = pipe.fit_transform(X_train, y_train) temp.head() . age fare sex_female sex_male pclass name sibsp parch ticket embarked . 0 0.0 | -0.499638 | 1.0 | 0.0 | 3.0 | McCarthy, Miss. Catherine &#39;Katie&#39; | 0.0 | 0.0 | 383123 | Q | . 1 -0.436411 | -0.090979 | 1.0 | 0.0 | 2.0 | del Carlo, Mrs. Sebastiano (Argenia Genovesi) | 1.0 | 0.0 | SC/PARIS 2167 | C | . 2 -1.448729 | -0.01825 | 1.0 | 0.0 | 3.0 | Andersson, Miss. Sigrid Elisabeth | 4.0 | 2.0 | 347082 | S | . 3 -0.358541 | -0.510381 | 0.0 | 1.0 | 3.0 | Saad, Mr. Khalil | 0.0 | 0.0 | 2672 | C | . 4 -1.059376 | -0.501684 | 1.0 | 0.0 | 3.0 | Abelseth, Miss. Karen Marie | 0.0 | 0.0 | 348125 | S | . Feature Engineering with Custom Transformer . So far, so good! Let&#39;s assume that we have another requirement and it is about feature engineering. We have to combine &#39;sibsp&#39; and &#39;parch&#39; into two new features: family_size and is_alone. . Let&#39;s implement this now. . class FamilyFeatureTransformer(BaseEstimator, TransformerMixin): def __init__(self): pass def fit(self, X, y): return self def transform(self, X): X[&#39;family_size&#39;] = X[&#39;parch&#39;] + X[&#39;sibsp&#39;] X.drop([&#39;parch&#39;, &#39;sibsp&#39;], axis=1, inplace=True) # we can drop this feature now X[&#39;is_alone&#39;] = 1 X.loc[X[&#39;family_size&#39;] &gt; 1, &#39;is_alone&#39;] = 0 return X def get_feature_names_out(self, input_features=None): # this time we have created new features. Their names are different from input features. # so we have explicitly mentioned them here. return [&#39;family_size&#39;, &#39;is_alone&#39;] numeric_features = [&quot;age&quot;, &quot;fare&quot;] numeric_transformer = Pipeline( steps=[ (&quot;imputer&quot;, SimpleImputerCustom(strategy=&#39;mean&#39;)), (&quot;scaler&quot;, StandardScaler()) ] ) categorical_features = [&quot;sex&quot;] categorical_transformer = OneHotEncoder(handle_unknown=&quot;ignore&quot;) family_features = [&quot;parch&quot;, &quot;sibsp&quot;] family_transformer = FamilyFeatureTransformer() preprocessor = make_column_transformer( (numeric_transformer, numeric_features), (categorical_transformer, categorical_features), (family_transformer, family_features), remainder=&#39;drop&#39;, # let&#39;s drop extra features this time verbose_feature_names_out=False ) dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer) ]) temp = pipe.fit_transform(X_train, y_train) temp.head() . age fare sex_female sex_male family_size is_alone . 0 0.000000 | -0.499638 | 1.0 | 0.0 | 0.0 | 1.0 | . 1 -0.436411 | -0.090979 | 1.0 | 0.0 | 1.0 | 1.0 | . 2 -1.448729 | -0.018250 | 1.0 | 0.0 | 6.0 | 0.0 | . 3 -0.358541 | -0.510381 | 0.0 | 1.0 | 0.0 | 1.0 | . 4 -1.059376 | -0.501684 | 1.0 | 0.0 | 0.0 | 1.0 | . Sklean Pipeline with Feature Importance . Alright, we have our required features ready and we can now pass them to a classifier. Let&#39;s use RandomForrest as our classifier and run our pipeline with it. . from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score # &#39;preprocessor&#39; and &#39;dataframer&#39; are already declared in last section, and intentionally omitted here. pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer), (&#39;rf_estimator&#39;, RandomForestClassifier()) ]) temp = pipe.fit_transform(X_train, y_train) temp.head() . AttributeError Traceback (most recent call last) Input In [34], in &lt;module&gt; 4 # &#39;preprocessor&#39; and &#39;dataframer&#39; are already declared in last section, and intentionally omitted here. 5 pipe = Pipeline([ 6 (&#34;preprocess&#34;, preprocessor), 7 (&#34;dataframer&#34;, dataframer), 8 (&#39;rf_estimator&#39;, RandomForestClassifier()) 9 10 ]) &gt; 12 temp = pipe.fit_transform(X_train, y_train) 13 temp.head() File ~ anaconda3 envs sc_mlflow lib site-packages sklearn pipeline.py:436, in Pipeline.fit_transform(self, X, y, **fit_params) 434 return last_step.fit_transform(Xt, y, **fit_params_last_step) 435 else: --&gt; 436 return last_step.fit(Xt, y, **fit_params_last_step).transform(Xt) AttributeError: &#39;RandomForestClassifier&#39; object has no attribute &#39;transform&#39; . Okay, looks like we have made a mistake here. Error message is saying . AttributeError: &#39;RandomForestClassifier&#39; object has no attribute &#39;transform&#39; . I get that. In our pipeline we have an estimator that does not have a transform method defined for it. We should use predict method instead. . Note: . Estimators implement predict method (Template reference Estimator, Template reference Classifier) | Transformers implement transform method (Template reference Transformer) | fit_transform is same calling fit and then transform | . Let us fix the error and run our pipeline again. . # pipeline created in last section and intentionally omitted here. pipe.fit(X_train, y_train) y_pred = pipe.predict(X_test) accuracy_score(y_test, y_pred) . 0.7862595419847328 . Let&#39;s see how our final pipeline looks visually. . # set_config(display=&#39;text&#39;) pipe . Pipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;data... kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)})), (&amp;#x27;rf_estimator&amp;#x27;, RandomForestClassifier())]) . Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;data... kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)})), (&amp;#x27;rf_estimator&amp;#x27;, RandomForestClassifier())]) . preprocess: ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False) . pipeline[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . SimpleImputerCustomSimpleImputerCustom() . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;) . familyfeaturetransformer[&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;] . FamilyFeatureTransformerFamilyFeatureTransformer() . FunctionTransformerFunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)}) . RandomForestClassifierRandomForestClassifier() . We can also get the importance of features in our dataset from RandomForrest classifier. . import matplotlib.pyplot as plt clf = pipe[-1] # last estimator is the RF classifier importances = clf.feature_importances_ features = clf.feature_names_in_ indices = np.argsort(importances) plt.title(&#39;Feature Importances&#39;) plt.barh(range(len(indices)), importances[indices], color=&#39;b&#39;, align=&#39;center&#39;) plt.yticks(range(len(indices)), [features[i] for i in indices]) plt.xlabel(&#39;Relative Importance&#39;) plt.show() . Note that all the feature names were passed to the RF Classifier and that is why we were able to get them back using its attribute feature_names_in_. This can be super useful when you have many model deployed in the environment, and you can just use the model object to get information about the features it was trained on. . For a moment let&#39;s also remove the feature names from our pipeline and see how it will effect our feature importance plot. . pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), # (&quot;dataframer&quot;, dataframer), (&#39;rf_estimator&#39;, RandomForestClassifier()) ]) # fit the pipeline pipe.fit(X_train, y_train) # get the feature importance plot clf = pipe[-1] importances = clf.feature_importances_ features = clf.feature_names_in_ indices = np.argsort(importances) plt.title(&#39;Feature Importances&#39;) plt.barh(range(len(indices)), importances[indices], color=&#39;b&#39;, align=&#39;center&#39;) plt.yticks(range(len(indices)), [features[i] for i in indices]) plt.xlabel(&#39;Relative Importance&#39;) plt.show() . AttributeError Traceback (most recent call last) Input In [38], in &lt;module&gt; 12 clf = pipe[-1] 13 importances = clf.feature_importances_ &gt; 14 features = clf.feature_names_in_ 16 indices = np.argsort(importances) 18 plt.title(&#39;Feature Importances&#39;) AttributeError: &#39;RandomForestClassifier&#39; object has no attribute &#39;feature_names_in_&#39; . No feature names were passed to our classifier this time and it is missing feature_names_in_ attribute. We can circumvent this and still get feature importance plot. . pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), # (&quot;dataframer&quot;, dataframer), (&#39;rf_estimator&#39;, RandomForestClassifier()) ]) # fit the pipeline pipe.fit(X_train, y_train) # get the feature importance plot clf = pipe[-1] importances = clf.feature_importances_ # features = clf.feature_names_in_ indices = np.argsort(importances) plt.title(&#39;Feature Importances&#39;) plt.barh(range(len(indices)), importances[indices], color=&#39;b&#39;, align=&#39;center&#39;) plt.yticks(range(len(indices)), [i for i in indices]) plt.xlabel(&#39;Relative Importance&#39;) plt.show() . This time we get the same plot but not withOUT feature names, and it is not useful anymore. So definitely we need to keep the feature names with the final estimator. Feature names can help us a lot in interpreting the model. . The complete Pipeline . For an easy reference, let&#39;s put the whole pipeline in one place. . Load Data . from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split import pandas as pd import numpy as np np.random.seed(42) # for consistency X, y = fetch_openml(&quot;titanic&quot;, version=1, as_frame=True, return_X_y=True) X.drop([&#39;body&#39;, &#39;cabin&#39;, &#39;boat&#39;, &#39;home.dest&#39;], axis=1, inplace=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2) X_train.head() . pclass name sex age sibsp parch ticket fare embarked . 999 3.0 | McCarthy, Miss. Catherine &#39;Katie&#39; | female | NaN | 0.0 | 0.0 | 383123 | 7.7500 | Q | . 392 2.0 | del Carlo, Mrs. Sebastiano (Argenia Genovesi) | female | 24.0 | 1.0 | 0.0 | SC/PARIS 2167 | 27.7208 | C | . 628 3.0 | Andersson, Miss. Sigrid Elisabeth | female | 11.0 | 4.0 | 2.0 | 347082 | 31.2750 | S | . 1165 3.0 | Saad, Mr. Khalil | male | 25.0 | 0.0 | 0.0 | 2672 | 7.2250 | C | . 604 3.0 | Abelseth, Miss. Karen Marie | female | 16.0 | 0.0 | 0.0 | 348125 | 7.6500 | S | . Train Model . from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler from sklearn.base import BaseEstimator, TransformerMixin from sklearn.compose import make_column_transformer from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score from sklearn.impute import SimpleImputer class FamilyFeatureTransformer(BaseEstimator, TransformerMixin): def __init__(self): pass def fit(self, X, y): return self def transform(self, X): X[&#39;family_size&#39;] = X[&#39;parch&#39;] + X[&#39;sibsp&#39;] X.drop([&#39;parch&#39;, &#39;sibsp&#39;], axis=1, inplace=True) # we can drop this feature now X[&#39;is_alone&#39;] = 1 X.loc[X[&#39;family_size&#39;] &gt; 1, &#39;is_alone&#39;] = 0 return X def get_feature_names_out(self, input_features=None): # this time we have created new features. Their names are different from input features. # so we have explicitly mentioned them here. return [&#39;family_size&#39;, &#39;is_alone&#39;] class SimpleImputerCustom(BaseEstimator, TransformerMixin): def __init__(self, strategy=&#39;mean&#39;): self.strategy = strategy self.imputer = SimpleImputer(strategy=self.strategy) def fit(self, X, y): self.imputer.fit(X, y) return self def transform(self, X): return self.imputer.transform(X) def get_feature_names_out(self, input_features=None): # we have returned the input features name as out features will have the same name return input_features def get_dataframe(X, transformer): &quot;&quot;&quot; x: an nd-array transformer: fitted transformer &quot;&quot;&quot; col_names = transformer.get_feature_names_out() output = pd.DataFrame(X.T, col_names).T return output numeric_features = [&quot;age&quot;, &quot;fare&quot;] numeric_transformer = Pipeline( steps=[ (&quot;imputer&quot;, SimpleImputerCustom(strategy=&#39;mean&#39;)), (&quot;scaler&quot;, StandardScaler()) ] ) categorical_features = [&quot;sex&quot;] categorical_transformer = OneHotEncoder(handle_unknown=&quot;ignore&quot;) family_features = [&quot;parch&quot;, &quot;sibsp&quot;] family_transformer = FamilyFeatureTransformer() preprocessor = make_column_transformer( (numeric_transformer, numeric_features), (categorical_transformer, categorical_features), (family_transformer, family_features), remainder=&#39;drop&#39;, # let&#39;s drop extra features this time verbose_feature_names_out=False ) dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer), (&#39;rf_estimator&#39;, RandomForestClassifier()) ]) pipe.fit(X_train, y_train) y_pred = pipe.predict(X_test) accuracy_score(y_test, y_pred) . 0.8015267175572519 . Plot Feature Importance . import matplotlib.pyplot as plt clf = pipe[-1] # last estimator is the RF classifier importances = clf.feature_importances_ features = clf.feature_names_in_ indices = np.argsort(importances) plt.title(&#39;Feature Importances&#39;) plt.barh(range(len(indices)), importances[indices], color=&#39;b&#39;, align=&#39;center&#39;) plt.yticks(range(len(indices)), [features[i] for i in indices]) plt.xlabel(&#39;Relative Importance&#39;) plt.show() . Pickle Test . from sklearn import set_config set_config(display=&quot;diagram&quot;) import pickle # save our pipeline s1 = pickle.dumps(pipe) # reload it s2 = pickle.loads(s1) s2 . Pipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;data... kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)})), (&amp;#x27;rf_estimator&amp;#x27;, RandomForestClassifier())]) . Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;data... kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)})), (&amp;#x27;rf_estimator&amp;#x27;, RandomForestClassifier())]) . preprocess: ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False) . pipeline[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . SimpleImputerCustomSimpleImputerCustom() . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;) . familyfeaturetransformer[&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;] . FamilyFeatureTransformerFamilyFeatureTransformer() . FunctionTransformerFunctionTransformer(func=&lt;function get_dataframe at 0x00000161997DF3A0&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)}) . RandomForestClassifierRandomForestClassifier() .",
            "url": "https://hassaanbinaslam.github.io/myblog/ml/sklearn/2022/03/04/sklearn-pipeline-deep-dive.html",
            "relUrl": "/ml/sklearn/2022/03/04/sklearn-pipeline-deep-dive.html",
            "date": " • Mar 4, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "AWS CloudFormation Template, Functions, and Commands",
            "content": "About . This post is a collection of useful notes on various sections of AWS CloudFormation template, and intrinsic functions. Knowledge about them is often tested in AWS certifications. For more details on this subject refer to its user guide (https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html) . Template Anatomy . The following is an example of AWS CloudFormation template and its sections in YAML format. There is no sequence to writing these sections besides that if there is a Description section then it must be put after AWSTemplateFormatVersion. . AWSTemplateFormatVersion: &quot;version date&quot; Description: String Metadata: template metadata Parameters: set of parameters Rules: set of rules Mappings: set of mappings Conditions: set of conditions Transform: set of transforms Resources: set of resources Outputs: set of outputs . Template Sections . AWSTemplateFormatVersion (optional) . The AWS CloudFormation template version that the template conforms to. . Syntax . AWSTemplateFormatVersion: &quot;2010-09-09&quot; . Description (optional) . A text string that describes the template. This section must always follow the template format version section. . Syntax . Description: &gt; Here are some details about the template. . Metadata (optional) . Objects that provide additional information about the template. . Difference between Metadata and Description is that some cloudformation features can refer to the objects that are defined in Metadata section. For example, you can use a metadata key AWS::CloudFormation::Interface to define how parameters are grouped and sorted on AWS cloudformation console. By default, cloudformation console alphbetically sorts the parameters by their logical ID. | AWS strongly recommends not to use this section for storing sensitive information such as passwords or secrets. | . Syntax . Metadata: Instances: Description: &quot;Information about the instances&quot; Databases: Description: &quot;Information about the databases&quot; . Parameters (optional) . Parameters enable you to input custom values to your template each time you create or update a stack. You can refer to parameters from the Resources and Outputs sections of the template using Ref intrinsic function. . CloudFormation currently supports the following parameter types . String – A literal string | Number – An integer or float | List&lt;Number&gt; – An array of integers or floats | CommaDelimitedList – An array of literal strings that are separated by commas | AWS::EC2::KeyPair::KeyName – An Amazon EC2 key pair name | AWS::EC2::SecurityGroup::Id – A security group ID | AWS::EC2::Subnet::Id – A subnet ID | AWS::EC2::VPC::Id – A VPC ID | List&lt;AWS::EC2::VPC::Id&gt; – An array of VPC IDs | List&lt;AWS::EC2::SecurityGroup::Id&gt; – An array of security group IDs | List&lt;AWS::EC2::Subnet::Id&gt; – An array of subnet IDs | . Syntax . The following example declares a parameter named InstanceTypeParameter. This parameter lets you specify the Amazon EC2 instance type for the stack to use when you create or update the stack. . Note that InstanceTypeParameter has a default value of t2.micro. This is the value that AWS CloudFormation will use to provision the stack unless another value is provided. . Parameters: InstanceTypeParameter: Type: String Default: t2.micro AllowedValues: - t2.micro - m1.small - m1.large Description: Enter t2.micro, m1.small, or m1.large. Default is t2.micro. . Referencing a parameter in template (Ref function) . In the following example, the InstanceType property of the EC2 instance resource references the InstanceTypeParameter parameter value. . Ec2Instance: Type: AWS::EC2::Instance Properties: InstanceType: Ref: InstanceTypeParameter ImageId: ami-0ff8a91507f77f867 . Rules (optional) . Validates a parameter or a combination of parameters that are passed to a template during a stack creation or stack update. . You can use the following rule-specific intrinsic functions to define rule conditions and assertions: . Fn::And | Fn::Contains | Fn::EachMemberEquals | Fn::EachMemberIn | Fn::Equals | Fn::If | Fn::Not | Fn::Or | Fn::RefAll | Fn::ValueOf | Fn::ValueOfAll | . Syntax . In the following example, the rule checks the value of the InstanceType parameter. The user must specify a1.medium, if the value of the environment parameter is test. . Rules: testInstanceType: RuleCondition: !Equals - !Ref Environment - test Assertions: - Assert: &#39;Fn::Contains&#39;: - - a1.medium - !Ref InstanceType AssertDescription: &#39;For a test environment, the instance type must be a1.medium&#39; . Mappings (optional) . The optional Mappings section matches a key to a corresponding set of named values similar to a lookup table. For example, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region. You use the Fn::FindInMap intrinsic function in the Resources and Outputs to retrieve values in a map. Note that you can&#39;t include parameters, pseudo parameters, or intrinsic functions in the Mappings section. . Fn::FindInMap . The intrinsic function Fn::FindInMap returns the value corresponding to keys in a two-level map that&#39;s declared in the Mappings section. . Syntax for the short form: . !FindInMap [ MapName, TopLevelKey, SecondLevelKey ] . Parameters . MapName The logical name of a mapping declared in the Mappings section that contains the keys and values. | . | TopLevelKey The top-level key name. Its value is a list of key-value pairs. | . | SecondLevelKey The second-level key name, which is set to one of the keys from the list assigned to TopLevelKey. | . | . A more concrete example . Mappings: RegionMap: us-east-1: HVM64: &quot;ami-0ff8a91507f77f867&quot; HVMG2: &quot;ami-0a584ac55a7631c0c&quot; Resources: myEC2Instance: Type: &quot;AWS::EC2::Instance&quot; Properties: ImageId: !FindInMap - RegionMap - !Ref &#39;AWS::Region&#39; # us-east-1 - HVM64 InstanceType: m1.small . Conditions (optional) . Conditions that control whether certain resources are created or whether certain resource properties are assigned a value during stack creation or update. For example, you could conditionally create a resource that depends on whether the stack is for a production or test environment. . Conditions are defined in Conditions section, and are then applied in following sections. . Parameters | Resources | Outputs | . You can use following intrinsic functions to define your conditions . Fn::And | Fn::Equals | Fn::If | Fn::Not | Fn::Or | . Syntax . Conditions: Logical ID: Intrinsic function . A more concrete example . AWSTemplateFormatVersion: 2010-09-09 Parameters: EnvType: Description: Environment type. Default: test Type: String AllowedValues: - prod - test ConstraintDescription: must specify prod or test. Conditions: CreateProdResources: !Equals - !Ref EnvType - prod Resources: EC2Instance: Type: &#39;AWS::EC2::Instance&#39; Properties: ImageId: ami-0ff8a91507f77f867 MountPoint: Type: &#39;AWS::EC2::VolumeAttachment&#39; Condition: CreateProdResources Properties: InstanceId: !Ref EC2Instance VolumeId: !Ref NewVolume Device: /dev/sdh NewVolume: Type: &#39;AWS::EC2::Volume&#39; Condition: CreateProdResources Properties: Size: 100 AvailabilityZone: !GetAtt - EC2Instance - AvailabilityZone . Difference between Rules and Conditions usage? . Rules are used to evaluate the input given by the user in Parameters | Conditions turn come after all rules have been evaluated | Conditions are not limited to Parameters and can also work with Resources and Outputs | . Transform (optional) . For serverless applications (also referred to as Lambda-based applications), specifies the version of the AWS Serverless Application Model (AWS SAM) to use. When you specify a transform, you can use AWS SAM syntax to declare resources in your template. The model defines the syntax that you can use and how it&#39;s processed. . You can also use AWS::Include transforms to work with template snippets that are stored separately from the main AWS CloudFormation template. You can store your snippet files in an Amazon S3 bucket and then reuse the functions across multiple templates. . Syntax . Transform: - MyMacro - &#39;AWS::Serverless&#39; . AWS::Include transform . Use the AWS::Include transform, which is a macro hosted by AWS CloudFormation, to insert boilerplate content into your templates. The AWS::Include transform lets you create a reference to a template snippet in an Amazon S3 bucket. The AWS::Include function behaves similarly to an include, copy, or import directive in programming languages. . Example . Transform: Name: &#39;AWS::Include&#39; Parameters: Location: &#39;s3://MyAmazonS3BucketName/MyFileName.yaml&#39; . Resources (required) . Specifies the stack resources and their properties, such as an Amazon Elastic Compute Cloud instance or an Amazon Simple Storage Service bucket. You can refer to resources in the Resources and Outputs sections of the template. . Syntax . Resources: Logical ID: Type: Resource type Properties: Set of properties . A more concrete example . Resources: MyEC2Instance: Type: &quot;AWS::EC2::Instance&quot; Properties: ImageId: &quot;ami-0ff8a91507f77f867&quot; . Outputs (optional) . The optional Outputs section declares output values that you can import into other stacks (to create cross-stack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name from a stack to make the bucket easier to find. . Notes . You can declare a maximum of 200 outputs in a template. | AWS strongly recommend you don&#39;t use this section to output sensitive information, such as passwords or secrets | Output values are available after the stack operation is complete. Stack output values aren&#39;t available when a stack status is in any of the IN_PROGRESS status. | AWS also does not recommend establishing dependencies between a service runtime and the stack output value because output values might not be available at all times. | . Syntax . Outputs: Logical ID: Description: Information about the value Value: Value to return Export: Name: Name of resource to export . A more concrete example where certain values are shown as output at the end of stack creation. . Outputs: BackupLoadBalancerDNSName: Description: The DNSName of the backup load balancer Value: !GetAtt BackupLoadBalancer.DNSName Condition: CreateProdResources InstanceID: Description: The Instance ID Value: !Ref EC2Instance . For Cross-Stack output use Export tag. Values outputed with &quot;Export&quot; tag can be imported in other stacks &quot;in the same region&quot;. Then, use the Fn::ImportValue intrinsic function to import the value in another stack &quot;in the same region&quot;. . Outputs: StackVPC: Description: The ID of the VPC Value: !Ref MyVPC Export: Name: !Sub &quot;${AWS::StackName}-VPCID&quot; . Some other important Intrinsic Functions . Fn::GetAtt . The Fn::GetAtt intrinsic function returns the value of an attribute from a resource in the template. . Syntax . !GetAtt logicalNameOfResource.attributeName . logicalNameOfResource The logical name (also called logical ID) of the resource that contains the attribute that you want. | . | attributeName The name of the resource-specific attribute whose value you want. See the resource&#39;s reference page for details about the attributes available for that resource type. | . | Return value The attribute value. | . | . A more concrete example . !GetAtt myELB.DNSName . Notes: . For the Fn::GetAtt logical resource name, you can&#39;t use functions. You must specify a string that&#39;s a resource&#39;s logical ID. | For the Fn::GetAtt attribute name, you can use the Ref function. | . Fn::ImportValue . The intrinsic function Fn::ImportValue returns the value of an output exported by another stack. You typically use this function to create cross-stack references. . Notes: . For each AWS account, Export names must be unique within a region. | You can&#39;t create cross-stack references across regions. You can use the intrinsic function Fn::ImportValue to import only values that have been exported within the same region. | You can&#39;t delete a stack if another stack references one of its outputs. | You can&#39;t modify or remove an output value that is referenced by another stack. | . Syntax . !ImportValue sharedValueToImport . A more concrete example. . Fn::ImportValue: !Sub &quot;${NetworkStackName}-SecurityGroupID&quot; . Fn::Sub . The intrinsic function Fn::Sub substitutes variables in an input string with values that you specify. In your templates, you can use this function to construct commands or outputs that include values that aren&#39;t available until you create or update a stack. . Syntax . !Sub - String - VarName: VarValue . Parameters . String A string with variables that AWS CloudFormation substitutes with their associated values at runtime. Write variables as ${MyVarName}. Variables can be template parameter names, resource logical IDs, resource attributes, or a variable in a key-value map. | . | VarName The name of a variable that you included in the String parameter. | . | VarValue The value that CloudFormation substitutes for the associated variable name at runtime. | . | . A more concrete example. The following example uses a mapping to substitute the ${Domain} variable with the resulting value from the Ref function. . Name: !Sub - &#39;www.${Domain}&#39; - Domain: !Ref RootDomainName . Important CloudFormation CLI Commands . Package a template using aws cloudformation package command | Validate a CloudFormation template using aws cloudformation validate-template command | Deploy a template using the aws cloudformation deploy command | .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/2022/02/28/aws-cloudformation-template.html",
            "relUrl": "/aws/2022/02/28/aws-cloudformation-template.html",
            "date": " • Feb 28, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "AWS IAM Policy Types",
            "content": "About . Access is managed in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. . A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. . AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, Organizations SCPs, ACLs, and session policies. This post is summary of AWS IAM policy and permission types. . AWS Policy Types . Identity-based policies . Identity-based policies – Attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity. . Resource-based policies . Resource-based policies – Attach inline policies to resources. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies. Resource-based policies grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts. . Identity vs Resource based policy . . Identity-based policies are applied to IAM identities, and grant them access to AWS resources. | Resource-based policies are applied to AWS resources, and they grant access to Principals (IAM identities, and applications) | . Permissions boundaries . Permissions boundaries – Use a customer or AWS managed policy as the permissions boundary for an IAM entity (user or role). That policy defines the maximum permissions that the identity-based policies can grant to an entity, but does not grant permissions. Permissions boundaries do not define the maximum permissions that a resource-based policy can grant to an entity. . Organizations SCPs . Organizations SCPs – Use an AWS Organizations service control policy (SCP) to define the maximum permissions for account members of an organization or organizational unit (OU). SCPs limit permissions that identity-based policies or resource-based policies grant to entities (users or roles) within the account, but do not grant permissions. . Permission boundries vs Organization SCP . Both permission boundries and SCP only limit permissions. They don&#39;t give any permissions. | Permission boundries limits permissions of identity-based policies only. | SCP limits permissions on both identity and resource based policies. | . Access control lists (ACLs) . Access control lists (ACLs) – Use ACLs to control which principals in other accounts can access the resource to which the ACL is attached. ACLs are similar to resource-based policies, although they are the only policy type that does not use the JSON policy document structure. ACLs are cross-account permissions policies that grant permissions to the specified principal. ACLs cannot grant permissions to entities within the same account. . ACL vs Resource based policy . Resource based policies can grant permission to entities in same or different account | ACL can only grant permissions to entities in different account | Only a few resources support ACL including AWS Amazon S3, AWS WAF, and Amazon VPC. It is a legacy IAM policy type and AWS recommends not to use it. | . Session policies . Session policies – Pass advanced session policies when you use the AWS CLI or AWS API to assume a role or a federated user. Session policies limit the permissions that the role or user&#39;s identity-based policies grant to the session. Session policies limit permissions for a created session, but do not grant permissions. For more information, see Session Policies. .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/2022/02/23/aws-policy-types.html",
            "relUrl": "/aws/2022/02/23/aws-policy-types.html",
            "date": " • Feb 23, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "AWS DNS Records - A, CNAME, ALIAS, & MX",
            "content": "About . This post summarizes the differences between AWS Route53 DNS records namely A record, CNAME, ALIAS, and MX. Knowledge about these differences is commonly checked in AWS certifications. . Credits . This post takes help from a few other really good articles. Please refer to them if you need more details on this subject . “Demystifying DNS Records – A, CNAME, ALIAS, MX &amp; AAAA” from Whizlabs (https://www.whizlabs.com/blog/dns-records/) . | “Why a domain’s root can’t be a CNAME — and other tidbits about the DNS” from freeCodeCamp (https://www.freecodecamp.org/news/why-cant-a-domain-s-root-be-a-cname-8cbab38e5f5c/) . | . First, some definitions . Domain Name . Domain + TLD = Domain Name | When you buy a ‘domain’ from a a registrar or reseller, you buy the rights to a specific domain name (example.com), and any subdomains you want to create (my-site.example.com, mail.example.com, etc). | The domain name (example.com) is also called the apex, root or naked domain name. | Examples of protocol are http, ftp, TCP, UDP, FTP, SMTP etc. | Examples of top level domains are .org, .net, .com, .ai etc. | . A Record . A record (or an address record) always points to an IP address. This IP address should be static like AWS Elastic IP Addresses (EIP) . Example use cases . You can point your root domain name example.com to an Elastic IP Address 192.0.2.23 . | We can also map EC2 instances IPv4 Public IP Address to an A record. But this is not recommended as EC2 instances public IP addresses change when you stop/start your server. We should always use Elastic IP addresses instead. . | . AAAA Record . AAAA record is similar to A record but for IPv6 addresses. . It always points to an IPv6 address . | Note that AWS currently does not support EIP for IPv6 (https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html) . | . MX Record . MX records (Mail Exchange records) are used for setting up email servers. . CNAME Record . CNAME records must always point to another domain name, never directly to an IP address. Since it does not point to an IP address, it is commonly used along with an A record. . One can, for example, point ftp.example.com and/or www.example.com to the DNS entry example.com, which in turn has an A record that points to the IP address. Then, if the IP address ever changes, one only has to record the change in one place within the network: in the DNS A record for example.com. . Example use cases . NAME TYPE VALUE . www.example.com | CNAME | example.com | . example.com | A | 192.0.2.23 | . An A record for example.com (root domain) points to server IP address . | A CNAME record points www.example.com to example.com . | . Now if the IP address of your server has changed you will have to update it only at one place A record. www.example.com and example.com will automatically inherit the changes. . IMPORTANT . CNAME entry for the root domain is not allowed. . | . NAME TYPE VALUE . example.com | CNAME | app.example.com | . app.example.com | A | 192.0.2.23 | . Alias Record . It is AWS Route 53 specific and only works with it. Alias works similar to CNAME but they are created by AWS to solve their specific problems discussed next. . AWS S3 buckets, Elastic Load Balancers, Elastic Beanstalk, and CloudFront offer you DNS names only and no IP addresses. e.g. when you create an S3 bucket you will get its DNS name bucket_name.s3.amazonaws.com. Now if you want to map your root domain example.com to S3 bucket DNS then we don’t have any options left as . A record points to IP addresses only . | CNAME cannot be used for root domain name . | . For this AWS came up with an Alias record in Route 53. With Alias record, you can point your domain root to another DNS name entry. . NAME TYPE VALUE . www.example.com | CNAME | example.com | . example.com | Alias | bucket_name.s3.amazonaws.com | .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/2022/02/22/aws-dns-records.html",
            "relUrl": "/aws/2022/02/22/aws-dns-records.html",
            "date": " • Feb 22, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Python - A collection of output formatting tips",
            "content": "About . This notebook is a collection of useful tips to format Python string literals and output. . Environment . from platform import python_version print(&quot;python==&quot; + python_version()) . . python==3.8.5 . f-string: Expressions inside a string . r = &#39;red&#39; g = &#39;green&#39; b = 1001 # f-string has a simple syntax. Put &#39;f&#39; at the start of string, and put expressions in {} f&quot;Stop = {r}, Go = {g}&quot; . &#39;Stop = red, Go = green&#39; . # &#39;F&#39; can also be used to start an f-string F&quot;binary = {b}. If you need value in brackets {{{b}}}&quot; . &#39;binary = 1001. If you need value in brackets {1001}&#39; . # f-string can also be started with &quot;&quot;&quot; quotes f&quot;&quot;&quot;{r} or {g}&quot;&quot;&quot; . &#39;red or green&#39; . # f-string on multiple lines. # 1. Use &quot;&quot;&quot; with backslash f&quot;&quot;&quot;{r} or {g}&quot;&quot;&quot; . &#39;red or green&#39; . # f-string on multiple lines. # 2. Use only backslash f&quot;{r}&quot; &quot; or &quot; f&quot;{g}&quot; . &#39;red or green&#39; . # f-string on multiple lines. # 3. Use brackets () (f&quot;{r}&quot; &quot; or &quot; f&quot;{g}&quot;) . &#39;red or green&#39; . # you can also compute an expression in an f-string f&quot;{ 40 + 2}&quot; . &#39;42&#39; . # functions can also be called from inside an f-string f&quot;This is in CAPS: { str.upper(r) }&quot; # same as above f&quot;This is in CAPS: { r.upper() }&quot; . &#39;This is in CAPS: RED&#39; . f-string: Padding the output . # Inside f-string, passing an integer after &#39;:&#39; will cause that field to be a minimum number of characters wide. # This is useful for making columns line up. groups = { &#39;small&#39;: 100, &#39;medium&#39;: 100100, &#39;large&#39;: 100100100 } for group, value in groups.items(): print(f&quot;{value:10} ==&gt; {group:20}&quot;) print(f&quot;{&#39;****&#39;*10}&quot;) # another nice trick for group, value in groups.items(): print(f&quot;{group:10} ==&gt; {value:20}&quot;) . 100 ==&gt; small 100100 ==&gt; medium 100100100 ==&gt; large **************************************** small ==&gt; 100 medium ==&gt; 100100 large ==&gt; 100100100 . f-string: Binary and hexadecimal format . # you can convert integers to binary and hexadecimal format print( f&quot;5 in binary {5:b}&quot; ) print( f&quot;5 in hexadecimal {5:#b}&quot; ) . 5 in binary 101 5 in hexadecimal 0b101 . f-string: Controlling the decimal places . import math print(f&#39;The value of pi is approximately (no formatting) {math.pi}&#39;) print(f&#39;The value of pi is approximately {math.pi :.3f}&#39;) . The value of pi is approximately (no formatting) 3.141592653589793 The value of pi is approximately 3.142 . f-string: Putting commas in numerical output . num = 3214298342.234 f&quot;{num:,}&quot; . &#39;3,214,298,342.234&#39; .",
            "url": "https://hassaanbinaslam.github.io/myblog/python/2022/02/18/python-tips-output-formatting.html",
            "relUrl": "/python/2022/02/18/python-tips-output-formatting.html",
            "date": " • Feb 18, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Python - Getting more information from Tracebacks",
            "content": "About . This notebook demonstrates what the Python Traceback object is, and how can we get more information out of it to better diagnose exception messages. . Credit . This blog post is based on an article originally written in Python Cookbook published by O&#39;Reilly Media, Inc. and released July 2002. In book&#39;s chapter 15, there is a section with the title Getting More Information from Tracebacks written by Bryn Keller. An online version of this article is available at https://www.oreilly.com/library/view/python-cookbook/0596001673/ch14s05.html. . The original article uses Python 2.2, but I have adapted it for Python 3.8. Also, I have added some commentary to give more insights on Python Traceback object. . Environment Details . from platform import python_version print(&quot;python==&quot; + python_version()) . . python==3.8.5 . Discussion . Consider the following toy example where we are getting some data from an external source (an API call, a DB call, etc.), and we need to find the length of individual items provided in the list. We know that items in the list will be of type str so we have used a len() function on it. . We got an exception when we ran our function on received data, and now we are trying to investigate what caused the error. . # this is intentionally hidden as we don&#39;t know about the data received from an external source. data = [&quot;1&quot;, &quot;22&quot;, 333, &quot;4444&quot;] . . # our toy example function. import sys, traceback def get_items_len(items: list) -&gt; list: &quot;&quot;&quot; this function returns the length of items received in a list. &quot;&quot;&quot; items_len = [] for i in items: items_len.append(len(i)) return items_len . # let&#39;s run our function on &quot;data&quot; received from an external source try: get_items_len(data) except Exception as e: print(traceback.print_exc()) . None . Traceback (most recent call last): File &#34;&lt;ipython-input-4-42cd486e1858&gt;&#34;, line 4, in &lt;module&gt; get_items_len(data) File &#34;&lt;ipython-input-3-8421f841ba77&gt;&#34;, line 11, in get_items_len items_len.append(len(i)) TypeError: object of type &#39;int&#39; has no len() . We got an exception while data processing and the Traceback message gives us some details. It tells us that we have received some data of type integer instead of string, and we are trying to call len() function on it. But we don&#39;t know the actual data value that caused the exception, and we don&#39;t know the index of the item in the list that caused this error. Depending on the use case, information about the local variables, or input data that caused the error can be crucial in diagnosing the root cause of an error. . Fortunately, all this information is already available to us in the Traceback object, but there are no built-in methods that give this information directly. Let us try some of the built-in methods on the Traceback object to see the kind of information we could get from them. . # calling traceback module built-in methods try: get_items_len(data) except Exception as e: print(&quot;***** Exception *****&quot;) print(e) exc_type, exc_value, exc_traceback = sys.exc_info() print(&quot; n***** print_tb *****&quot;) traceback.print_tb(exc_traceback, limit=1, file=sys.stdout) print(&quot; n***** print_exception *****&quot;) # exc_type below is ignored on 3.5 and later traceback.print_exception(exc_type, exc_value, exc_traceback, limit=2, file=sys.stdout) print(&quot; n***** print_exc *****&quot;) traceback.print_exc(limit=2, file=sys.stdout) print(&quot; n***** format_exc, first and last line *****&quot;) formatted_lines = traceback.format_exc().splitlines() print(formatted_lines[0]) print(formatted_lines[-1]) print(&quot; n***** format_exception *****&quot;) # exc_type below is ignored on 3.5 and later print(repr(traceback.format_exception(exc_type, exc_value, exc_traceback))) print(&quot; n***** extract_tb *****&quot;) print(repr(traceback.extract_tb(exc_traceback))) print(&quot; n***** format_tb *****&quot;) print(repr(traceback.format_tb(exc_traceback))) print(&quot; n***** tb_lineno *****&quot;, exc_traceback.tb_lineno) . ***** Exception ***** object of type &#39;int&#39; has no len() ***** print_tb ***** File &#34;&lt;ipython-input-5-73d5b316a567&gt;&#34;, line 4, in &lt;module&gt; get_items_len(data) ***** print_exception ***** Traceback (most recent call last): File &#34;&lt;ipython-input-5-73d5b316a567&gt;&#34;, line 4, in &lt;module&gt; get_items_len(data) File &#34;&lt;ipython-input-3-8421f841ba77&gt;&#34;, line 11, in get_items_len items_len.append(len(i)) TypeError: object of type &#39;int&#39; has no len() ***** print_exc ***** Traceback (most recent call last): File &#34;&lt;ipython-input-5-73d5b316a567&gt;&#34;, line 4, in &lt;module&gt; get_items_len(data) File &#34;&lt;ipython-input-3-8421f841ba77&gt;&#34;, line 11, in get_items_len items_len.append(len(i)) TypeError: object of type &#39;int&#39; has no len() ***** format_exc, first and last line ***** Traceback (most recent call last): TypeError: object of type &#39;int&#39; has no len() ***** format_exception ***** [&#39;Traceback (most recent call last): n&#39;, &#39; File &#34;&lt;ipython-input-5-73d5b316a567&gt;&#34;, line 4, in &lt;module&gt; n get_items_len(data) n&#39;, &#39; File &#34;&lt;ipython-input-3-8421f841ba77&gt;&#34;, line 11, in get_items_len n items_len.append(len(i)) n&#39;, &#34;TypeError: object of type &#39;int&#39; has no len() n&#34;] ***** extract_tb ***** [&lt;FrameSummary file &lt;ipython-input-5-73d5b316a567&gt;, line 4 in &lt;module&gt;&gt;, &lt;FrameSummary file &lt;ipython-input-3-8421f841ba77&gt;, line 11 in get_items_len&gt;] ***** format_tb ***** [&#39; File &#34;&lt;ipython-input-5-73d5b316a567&gt;&#34;, line 4, in &lt;module&gt; n get_items_len(data) n&#39;, &#39; File &#34;&lt;ipython-input-3-8421f841ba77&gt;&#34;, line 11, in get_items_len n items_len.append(len(i)) n&#39;] ***** tb_lineno ***** 4 . . All these methods are useful but we are still short on information about the state of local variables when the system crashed. Before writing our custom function to get the variables state at the time of exception, let us spend some time to understand the working of Traceback object. . Traceback Module . https://docs.python.org/3/library/traceback.html This module provides an easy-to-use interface to work with traceback objects. It provides multiple functions that we can use to extract the required information from traceback. So far, we have used methods from this module in the above examples. . Traceback Objects . https://docs.python.org/3/reference/datamodel.html &gt; On this page search for term &quot;Traceback objects&quot; . Traceback objects represent a stack trace of an exception. A traceback object is implicitly created when an exception occurs and may also be explicitly created by initializing an instance of class types.TracebackType. traceback object is also an instance of types.TracebackType class. When an exception occurs, a traceback object is initialized for us, and we can obtain it from any of the following two methods. . It is available as a third item of the tuple returned by sys.exc_info() &quot;(type, value, traceback)&quot; | It is available as the __traceback__ object of the caught exception. &quot;Exception.__traceback__&quot; | A traceback object is a linked list of nodes, where each node is a Frame object. Frame objects form their own linked list but in the opposite direction of traceback objects. Together they work like a doubly-linked list, and we can use them to move back and forth in the stack trace history. It is the frame objects that hold all the stack&#39;s important information. traceback object has some special attributes . tb_next point to the next level in the stack trace (towards the frame where the exception occurred), or None if there is no next level | tb_frame points to the execution frame of the current level | tb_lineno gives the line number where the exception occurred | . # method 1: get traceback object using sys.exc_info() try: get_items_len(data) except Exception as e: print(sys.exc_info()[2]) . &lt;traceback object at 0x7f5c6c60e9c0&gt; . # method 2: get traceback object using Exception.__traceback__ try: get_items_len(data) except Exception as e: print(e.__traceback__ ) . &lt;traceback object at 0x7f5c6c5c0180&gt; . If there is no exception in the system, then calling sys.exc_info() will only return None values. . # no exception is generated so sys.exc_info() will return None values. try: get_items_len([&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;]) except Exception as e: print(sys.exc_info()[2]) . Frame Objects . https://docs.python.org/3/reference/datamodel.html &gt; On this page search for term &quot;Frame objects&quot; . Frame objects represent execution frames. It has some special attributes . f_back is a reference to the previous stack frame (towards the caller), or None if this is the bottom stack frame | f_code is the code object being executed in this frame. We will discuss Code Objects in next the section | f_lineno is the current line number of the frame — writing to this from within a trace function jumps to the given line (only for the bottom-most frame). A debugger can implement a Jump command (aka Set Next Statement) by writing to f_lineno. This attribute will give you the line number in the code on which exception occurred | f_locals is a dictionary used to lookup local variables. From this dictionary we can get all the local variables and their state at the time of exception | f_globals is a dictionary for global varaibles | . Code Objects . https://docs.python.org/3/reference/datamodel.html &gt; On this page search for term &quot;Code Objects&quot; . Code objects represent byte-compiled executable Python code or bytecode. Some of its attributes include . co_name gives the function name being executed | co_filename gives the filename from which the code was compiled | . There are many other helpful attributes in this object, and you may read about them from the docs. . Visual representation of Traceback, Frame and Code Objects . figure 1: Visual representation of Traceback, Frame and Code Objects . Custom fuction for additional exception info . Now with this additional information on stack trace objects, let us create a function to get variables state at the time of exception. . def exc_info_plus(): &quot;&quot;&quot; Provides the usual traceback information, followed by a listing of all the local variables in each frame. &quot;&quot;&quot; tb = sys.exc_info()[2] # iterate forward to the last (most recent) traceback object. while 1: if not tb.tb_next: break tb = tb.tb_next stack = [] # get the most recent traceback frame f = tb.tb_frame # iterate backwards from recent to oldest traceback frame while f: stack.append(f) f = f.f_back # stack.reverse() # uncomment to get innermost (most recent) frame at the last # get exception information and stack trace entries from most recent traceback object exc_msg = traceback.format_exc() exc_msg += &quot; n*** Locals by frame, innermost first ***&quot; for frame in stack: exc_msg += f&quot; nFrame {frame.f_code.co_name} in {frame.f_code.co_filename} at line {frame.f_lineno}&quot; for key, value in frame.f_locals.items(): exc_msg += f&quot; n t {key:20} = &quot; try: data = str(value) # limit variable&#39;s output to a certain number. You can adjust it as per your requirement. # But not to remove it as output from large objects (e.g. Pandas DataFrame) can be troublesome. output_limit = 50 exc_msg += (data[:output_limit] + &quot;...&quot;) if len(data) &gt; output_limit else data except: exc_msg += &quot;&lt;ERROR WHILE PRINTING VALUE&gt;&quot; return exc_msg . . #now let us try our custom exception function and see the ouput try: get_items_len(data) except Exception as e: print(exc_info_plus()) . Traceback (most recent call last): File &#34;&lt;ipython-input-10-01264d9e470a&gt;&#34;, line 4, in &lt;module&gt; get_items_len(data) File &#34;&lt;ipython-input-3-8421f841ba77&gt;&#34;, line 11, in get_items_len items_len.append(len(i)) TypeError: object of type &#39;int&#39; has no len() *** Locals by frame, innermost first *** Frame get_items_len in &lt;ipython-input-3-8421f841ba77&gt; at line 11 items = [&#39;1&#39;, &#39;22&#39;, 333, &#39;4444&#39;] items_len = [1, 2] i = 333 Frame &lt;module&gt; in &lt;ipython-input-10-01264d9e470a&gt; at line 6 __name__ = __main__ __doc__ = Automatically created module for IPython interacti... __package__ = None __loader__ = None __spec__ = None __builtin__ = &lt;module &#39;builtins&#39; (built-in)&gt; __builtins__ = &lt;module &#39;builtins&#39; (built-in)&gt; _ih = [&#39;&#39;, &#39;#collapse-hide nfrom platform import python_... _oh = {} _dh = [&#39;/data/_notebooks&#39;] In = [&#39;&#39;, &#39;#collapse-hide nfrom platform import python_... Out = {} get_ipython = &lt;bound method InteractiveShell.get_ipython of &lt;ipy... exit = &lt;IPython.core.autocall.ZMQExitAutocall object at 0... quit = &lt;IPython.core.autocall.ZMQExitAutocall object at 0... _ = __ = ___ = _i = #collapse-show def exc_info_plus(): &#34;&#34;&#34; Pr... _ii = ## # no exception is generated so sys.exc_info() w... _iii = ## # method 2: get traceback object using Exceptio... _i1 = #collapse-hide from platform import python_version... python_version = &lt;function python_version at 0x7f5c72dbc430&gt; _i2 = #collapse-hide # this is intentionally hidden as w... data = [&#39;1&#39;, &#39;22&#39;, 333, &#39;4444&#39;] _i3 = ## # our toy example function. import sys, traceba... sys = &lt;module &#39;sys&#39; (built-in)&gt; traceback = &lt;module &#39;traceback&#39; from &#39;/usr/lib/python3.8/trace... get_items_len = &lt;function get_items_len at 0x7f5c6c62c790&gt; _i4 = ## # let&#39;s run our function on &#34;data&#34; received fro... _i5 = #collapse-output # calling traceback module built-... exc_type = &lt;class &#39;TypeError&#39;&gt; exc_value = object of type &#39;int&#39; has no len() exc_traceback = &lt;traceback object at 0x7f5c6c5cf700&gt; formatted_lines = [&#39;Traceback (most recent call last):&#39;, &#39; File &#34;&lt;i... _i6 = ## # method 1: get traceback object using sys.exc_... _i7 = ## # method 2: get traceback object using Exceptio... _i8 = ## # no exception is generated so sys.exc_info() w... _i9 = #collapse-show def exc_info_plus(): &#34;&#34;&#34; Pr... exc_info_plus = &lt;function exc_info_plus at 0x7f5c6c62cc10&gt; _i10 = #collapse-output #now let us try our custom except... e = object of type &#39;int&#39; has no len() Frame run_code in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 3418 self = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... code_obj = &lt;code object &lt;module&gt; at 0x7f5c6c62eea0, file &#34;&lt;ip... result = &lt;ExecutionResult object at 7f5c6c5c88e0, execution... async_ = False __tracebackhide__ = __ipython_bottom__ old_excepthook = &lt;bound method IPKernelApp.excepthook of &lt;ipykernel... outflag = True Frame run_ast_nodes in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 3338 self = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... nodelist = [&lt;_ast.Try object at 0x7f5c6c5c8850&gt;] cell_name = &lt;ipython-input-10-01264d9e470a&gt; interactivity = none compiler = &lt;IPython.core.compilerop.CachingCompiler object at... result = &lt;ExecutionResult object at 7f5c6c5c88e0, execution... to_run_exec = [&lt;_ast.Try object at 0x7f5c6c5c8850&gt;] to_run_interactive = [] mod = &lt;_ast.Module object at 0x7f5c6c5c8430&gt; compare = &lt;function InteractiveShell.run_ast_nodes.&lt;locals&gt;.... to_run = [(&lt;_ast.Try object at 0x7f5c6c5c8850&gt;, &#39;exec&#39;)] node = &lt;_ast.Try object at 0x7f5c6c5c8850&gt; mode = exec code = &lt;code object &lt;module&gt; at 0x7f5c6c62eea0, file &#34;&lt;ip... asy = False _async = False Frame run_cell_async in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 3146 raw_cell = #collapse-output #now let us try our custom except... silent = False shell_futures = True transformed_cell = #collapse-output #now let us try our custom except... preprocessing_exc_tuple = None info = &lt;ExecutionInfo object at 7f5c6c5c8be0, raw_cell=&#34;#... error_before_exec = &lt;function InteractiveShell.run_cell_async.&lt;locals&gt;... cell = #collapse-output #now let us try our custom except... compiler = &lt;IPython.core.compilerop.CachingCompiler object at... _run_async = False cell_name = &lt;ipython-input-10-01264d9e470a&gt; code_ast = &lt;_ast.Module object at 0x7f5c6c5c85e0&gt; interactivity = last_expr result = &lt;ExecutionResult object at 7f5c6c5c88e0, execution... self = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... store_history = True Frame _pseudo_sync_runner in /usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py at line 68 coro = &lt;coroutine object InteractiveShell.run_cell_async ... Frame _run_cell in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 2923 self = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... raw_cell = #collapse-output #now let us try our custom except... store_history = True silent = False shell_futures = True preprocessing_exc_tuple = None transformed_cell = #collapse-output #now let us try our custom except... coro = &lt;coroutine object InteractiveShell.run_cell_async ... runner = &lt;function _pseudo_sync_runner at 0x7f5c724ba040&gt; Frame run_cell in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 2877 self = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... raw_cell = #collapse-output #now let us try our custom except... store_history = True silent = False shell_futures = True result = None Frame run_cell in /usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py at line 539 self = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... args = (&#39;#collapse-output n#now let us try our custom exc... kwargs = {&#39;store_history&#39;: True, &#39;silent&#39;: False} __class__ = &lt;class &#39;ipykernel.zmqshell.ZMQInteractiveShell&#39;&gt; Frame do_execute in /usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py at line 302 self = &lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5c... code = #collapse-output #now let us try our custom except... silent = False store_history = True user_expressions = {} allow_stdin = True reply_content = {} run_cell = &lt;bound method InteractiveShell.run_cell_async of &lt;... should_run_async = &lt;bound method InteractiveShell.should_run_async of... shell = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... Frame wrapper in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 234 args = (&lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5... kwargs = {} future = &lt;Future pending&gt; ctx_run = &lt;built-in method run of Context object at 0x7f5c6c... result = &lt;generator object IPythonKernel.do_execute at 0x7f... func = &lt;function IPythonKernel.do_execute at 0x7f5c6f6978... Frame execute_request in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py at line 540 self = &lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5c... stream = &lt;zmq.eventloop.zmqstream.ZMQStream object at 0x7f5... ident = [b&#39;e2e3826d25fb4c63876268cdc5a787ad&#39;] parent = {&#39;header&#39;: {&#39;msg_id&#39;: &#39;218114cb9837444cbd29466d87b... content = {&#39;code&#39;: &#39;#collapse-output n#now let us try our cu... code = #collapse-output #now let us try our custom except... silent = False store_history = True user_expressions = {} allow_stdin = True stop_on_error = True metadata = {&#39;started&#39;: datetime.datetime(2022, 2, 14, 9, 30, ... Frame wrapper in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 234 args = (&lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5... kwargs = {} future = &lt;Future pending&gt; ctx_run = &lt;built-in method run of Context object at 0x7f5c6c... result = &lt;generator object Kernel.execute_request at 0x7f5c... func = &lt;function Kernel.execute_request at 0x7f5c6f747f70... Frame dispatch_shell in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py at line 265 self = &lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5c... stream = &lt;zmq.eventloop.zmqstream.ZMQStream object at 0x7f5... msg = {&#39;header&#39;: {&#39;msg_id&#39;: &#39;218114cb9837444cbd29466d87b... idents = [b&#39;e2e3826d25fb4c63876268cdc5a787ad&#39;] msg_type = execute_request handler = &lt;bound method Kernel.execute_request of &lt;ipykernel... Frame wrapper in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 234 args = (&lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5... kwargs = {} future = &lt;Future pending&gt; ctx_run = &lt;built-in method run of Context object at 0x7f5c6f... result = &lt;generator object Kernel.dispatch_shell at 0x7f5c6... func = &lt;function Kernel.dispatch_shell at 0x7f5c6f7473a0&gt; Frame process_one in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py at line 362 self = &lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5c... wait = True priority = 10 t = 13 dispatch = &lt;bound method Kernel.dispatch_shell of &lt;ipykernel.... args = (&lt;zmq.eventloop.zmqstream.ZMQStream object at 0x7f... Frame run in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 775 self = &lt;tornado.gen.Runner object at 0x7f5c6c60f8e0&gt; future = None exc_info = None value = (10, 13, &lt;bound method Kernel.dispatch_shell of &lt;i... Frame inner in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 814 f = None self = &lt;tornado.gen.Runner object at 0x7f5c6c60f8e0&gt; Frame _run_callback in /usr/local/lib/python3.8/dist-packages/tornado/ioloop.py at line 741 self = &lt;tornado.platform.asyncio.AsyncIOMainLoop object a... callback = functools.partial(&lt;function Runner.handle_yield.&lt;l... Frame &lt;lambda&gt; in /usr/local/lib/python3.8/dist-packages/tornado/ioloop.py at line 688 f = &lt;Future finished result=(10, 13, &lt;bound method...7... callback = &lt;function Runner.handle_yield.&lt;locals&gt;.inner at 0x... future = &lt;Future finished result=(10, 13, &lt;bound method...7... self = &lt;tornado.platform.asyncio.AsyncIOMainLoop object a... Frame _run in /usr/lib/python3.8/asyncio/events.py at line 81 self = &lt;Handle IOLoop.add_future.&lt;locals&gt;.&lt;lambda&gt;(&lt;Futur... Frame _run_once in /usr/lib/python3.8/asyncio/base_events.py at line 1859 self = &lt;_UnixSelectorEventLoop running=True closed=False ... sched_count = 0 handle = &lt;Handle IOLoop.add_future.&lt;locals&gt;.&lt;lambda&gt;(&lt;Futur... timeout = 0 event_list = [] end_time = 113697.83311910101 ntodo = 2 i = 0 Frame run_forever in /usr/lib/python3.8/asyncio/base_events.py at line 570 self = &lt;_UnixSelectorEventLoop running=True closed=False ... old_agen_hooks = asyncgen_hooks(firstiter=None, finalizer=None) Frame start in /usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py at line 199 self = &lt;tornado.platform.asyncio.AsyncIOMainLoop object a... old_loop = &lt;_UnixSelectorEventLoop running=True closed=False ... Frame start in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py at line 612 self = &lt;ipykernel.kernelapp.IPKernelApp object at 0x7f5c7... Frame launch_instance in /usr/local/lib/python3.8/dist-packages/traitlets/config/application.py at line 845 cls = &lt;class &#39;ipykernel.kernelapp.IPKernelApp&#39;&gt; argv = None kwargs = {} app = &lt;ipykernel.kernelapp.IPKernelApp object at 0x7f5c7... Frame &lt;module&gt; in /usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py at line 16 __name__ = __main__ __doc__ = Entry point for launching an IPython kernel. This... __package__ = __loader__ = &lt;_frozen_importlib_external.SourceFileLoader objec... __spec__ = ModuleSpec(name=&#39;ipykernel_launcher&#39;, loader=&lt;_fro... __annotations__ = {} __builtins__ = &lt;module &#39;builtins&#39; (built-in)&gt; __file__ = /usr/local/lib/python3.8/dist-packages/ipykernel_l... __cached__ = /usr/local/lib/python3.8/dist-packages/__pycache__... sys = &lt;module &#39;sys&#39; (built-in)&gt; app = &lt;module &#39;ipykernel.kernelapp&#39; from &#39;/usr/local/lib... Frame _run_code in /usr/lib/python3.8/runpy.py at line 87 code = &lt;code object &lt;module&gt; at 0x7f5c7317e030, file &#34;/us... run_globals = {&#39;__name__&#39;: &#39;__main__&#39;, &#39;__doc__&#39;: &#39;Entry point f... init_globals = None mod_name = __main__ mod_spec = ModuleSpec(name=&#39;ipykernel_launcher&#39;, loader=&lt;_fro... pkg_name = script_name = None loader = &lt;_frozen_importlib_external.SourceFileLoader objec... fname = /usr/local/lib/python3.8/dist-packages/ipykernel_l... cached = /usr/local/lib/python3.8/dist-packages/__pycache__... Frame _run_module_as_main in /usr/lib/python3.8/runpy.py at line 194 mod_name = ipykernel_launcher alter_argv = 1 mod_spec = ModuleSpec(name=&#39;ipykernel_launcher&#39;, loader=&lt;_fro... code = &lt;code object &lt;module&gt; at 0x7f5c7317e030, file &#34;/us... main_globals = {&#39;__name__&#39;: &#39;__main__&#39;, &#39;__doc__&#39;: &#39;Entry point f... . . Note the output from the first stack frame in the above stack trace. It is easy now to see (items) that we received in our function. The item at index i is also available (333) on which our function crashed. Using our custom function unexpected errors are logged in a format that makes it a lot easier to find and fix the errors. Let&#39;s fix our function to handle unexpected integer values. . # let&#39;s fix our function to handle unexpected &#39;int&#39; items by converting them to &#39;str&#39; def get_items_len(items: list) -&gt; list: &quot;&quot;&quot; this function returns the length of items received in a list. &quot;&quot;&quot; items_len = [] for i in map(str, items): items_len.append(len(i)) return items_len # test it again get_items_len(data) . [1, 2, 3, 4] .",
            "url": "https://hassaanbinaslam.github.io/myblog/python/2022/02/11/python-stack-traceback-more-info.html",
            "relUrl": "/python/2022/02/11/python-stack-traceback-more-info.html",
            "date": " • Feb 11, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Python Dictionary - Multiple ways to get items",
            "content": "About . This notebook demonstrates multiple ways to get items from a Python dictionary. . Environment Details . from platform import python_version print(&quot;python==&quot; + python_version()) . . python==3.8.5 . Example Dictionaries . # simple dictionary car = { &quot;brand&quot;: &quot;ford&quot;, &quot;model&quot;: &quot;mustang&quot; } car . {&#39;brand&#39;: &#39;ford&#39;, &#39;model&#39;: &#39;mustang&#39;} . # nested dictionary family = { &#39;gfather&#39; : { &#39;father&#39;: { &#39;son&#39;: {&#39;love&#39;:&#39;python&#39;} } } } family . {&#39;gfather&#39;: {&#39;father&#39;: {&#39;son&#39;: {&#39;love&#39;: &#39;python&#39;}}}} . Method 1: Square brackets . A square bracket is the simplest approach to getting any item from a dictionary. You can get a value from a dictionary by providing it a key in [] brackets. For example, to get a value of model from a car . car[&#39;model&#39;] . &#39;mustang&#39; . Problem with this approach is that if the provided key is not available in the dictionary then it will throw a KeyError exception. . car[&#39;year&#39;] . KeyErrorTraceback (most recent call last) &lt;ipython-input-5-ca220af55913&gt; in &lt;module&gt; -&gt; 1 car[&#39;year&#39;] KeyError: &#39;year&#39; . To avoid KeyError, you can first check if the key is available in dictionary. . if &#39;year&#39; in car: # check if given key is available in dictionary year = car[&#39;year&#39;] # now get the value else: year = &#39;1964&#39; # (Optional) otherwise give this car a default value year . &#39;1964&#39; . An alternate approach could be to use a Try-Except block to handle the KeyError exception. . try: year = car[&#39;year&#39;] except KeyError: year = &#39;1964&#39; # give this car a default value year . &#39;1964&#39; . For nested dictionaries, you can use chained [] brackets. But beware that if any of the Keys is missing in the chain, you will get a KeyError exception. . # this will work. All keys are present. family[&#39;gfather&#39;][&#39;father&#39;][&#39;son&#39;] . {&#39;love&#39;: &#39;python&#39;} . # this will not work. &#39;mother&#39; key is not in dictionary family[&#39;gfather&#39;][&#39;mother&#39;][&#39;son&#39;] . KeyErrorTraceback (most recent call last) &lt;ipython-input-9-3d615db6bfdf&gt; in &lt;module&gt; 1 # this will not work. &#39;mother&#39; key is not in dictionary -&gt; 2 family[&#39;gfather&#39;][&#39;mother&#39;][&#39;son&#39;] KeyError: &#39;mother&#39; . Method 2: Get function . https://docs.python.org/3/library/stdtypes.html#dict.get &gt; get(key[, default]) . Get function will return the value for key if key is in the dictionary. Otherwise, it will return a default value which is None. You can provide your default value as well. . year = car.get(&#39;year&#39;, &#39;1964&#39;) year # year key is not present so get function will return a default value &#39;1964&#39; . &#39;1964&#39; . Depending on your use case there can be confusion with this approach when your item can also have None value. In that case, you will not know whether the None value was returned from the dictionary or it was the Get function. . owner = car.get(&#39;owner&#39;) owner # owner has a None value. But is this value coming from dic or from Get function? # This can be confusing for large nested dictionaries. . For nested dictionaries you can use chained Get functions. But beware that missing Key items needs to be properly handled otherwise you will still get an exception. . # this will work. All keys are present. family.get(&#39;gfather&#39;).get(&#39;father&#39;).get(&#39;son&#39;) . {&#39;love&#39;: &#39;python&#39;} . # this will still work. &#39;daughter&#39; key is missing # but since it is at the end of chain it will return a default None value family.get(&#39;gfather&#39;).get(&#39;father&#39;).get(&#39;daughter&#39;) . # this will NOT work. &#39;mother&#39; key is missing and it returned a default None value. # but since it is not at the end, and we called Get function on returned value &#39;None&#39; family.get(&#39;gfather&#39;).get(&#39;mother&#39;).get(&#39;son&#39;) . AttributeErrorTraceback (most recent call last) &lt;ipython-input-14-a35a8f091991&gt; in &lt;module&gt; 1 # this will NOT work. &#39;mother&#39; key is missing and it returned a default None value. 2 # but since it is not at the end, and we called Get function on returned value &#39;None&#39; -&gt; 3 family.get(&#39;gfather&#39;).get(&#39;mother&#39;).get(&#39;son&#39;) AttributeError: &#39;NoneType&#39; object has no attribute &#39;get&#39; . # this will work. &#39;mother&#39; key is missing and it returned a default value. # but we have properly handled all the default values with empty dictionaries. family.get(&#39;gfather&#39;, {}).get(&#39;mother&#39;, {}).get(&#39;son&#39;, {}) . {} .",
            "url": "https://hassaanbinaslam.github.io/myblog/python/2022/02/10/python-dictionary.html",
            "relUrl": "/python/2022/02/10/python-dictionary.html",
            "date": " • Feb 10, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "My First Blog Post from Jupyter Notebook",
            "content": "Well, this is my first post using Jupyter notebook as a publishing medium. Besides this notebook, I am also using &#39;nbdev&#39; library from FastAI as tooling to convert notebooks into static HTML pages. Once pushed to GitHub they will become new posts on my blog. I need to learn more about this setup, but it is looking very interesting. . # I can also include some code directly into the blog post. No need for GitHub snippets. print(&quot;nbdev and fastpages from Fast.AI are so cool! &quot;) . . nbdev and fastpages from Fast.AI are so cool! .",
            "url": "https://hassaanbinaslam.github.io/myblog/jupyter/fastpages/2022/02/10/hello-world.html",
            "relUrl": "/jupyter/fastpages/2022/02/10/hello-world.html",
            "date": " • Feb 10, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://hassaanbinaslam.github.io/myblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://hassaanbinaslam.github.io/myblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://hassaanbinaslam.github.io/myblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hassaanbinaslam.github.io/myblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}