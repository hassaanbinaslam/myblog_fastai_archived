{
  
    
        "post0": {
            "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
            "content": ". Introduction . You may have trained a model with your favorite ML framework, and now you are asked to move your code to Amazon SageMaker. The good news is that SageMaker&#39;s fully managed training works well with many popular ML frameworks, including scikit-learn. In addition, SageMaker provides its prebuilt container for the scikit-learn framework, enabling us to seamlessly port our scripts to SageMaker and benefit from its training and deployment capabilities. SageMaker&#39;s scikit-learn Container is an open source library for making the scikit-learn framework run on the Amazon SageMaker platform. You can read more about sklearn container features from its GitHub page SageMaker Scikit-learn Container. . Amazon SageMaker also provides open source Python SDK to train and deploy models on SageMaker. SageMaker SDK provides several high-level abstractions (classes), including: . Session Provides a collection of methods for working with SageMaker resources | Estimators Encapsulate training on SageMaker | Predictors Provide real-time inference and transformation using Python data types against a SageMaker endpoint | . You can read more on SageMaker Python SDK from its official site Amazon SageMaker Python SDK . This approach of using a custom training script with SageMaker&#39;s prebuilt container is commonly called as Script Mode. To train a scikit-learn model by using the SageMaker Python SDK involves three steps: . Prepare a training script. The training script is similar to any other scikit-learn training script that you might use outside of SageMaker | Create an Estimator object from class sagemaker.sklearn.SKLearn. Scikit-learn estimator class handles end-to-end training and deployment of custom scikit-learn code. We pass our training script to the SKLearn estimator, and it executes the script within a SageMaker Training Job. This training job is an Amazon-built Docker container that runs functions defined in the provided Python script. | Call the Estimator&#39;s fit method on training data. Training is started by calling fit() on this Estimator. After training is complete, calling deploy() creates a hosted SageMaker endpoint and returns a SKLearnPredictor instance that can be used to perform inference against the hosted model. We will discuss the SKLearn Estimator in more detail later in this post. | To read more about using scikit-learn with the SageMaker Python SDK, you may refer to the official documentation using Scikit-learn with the SageMaker Python SDK. The official documentation is valuable, and I would highly recommend checking it and keeping it as a reference. . In this post we will built a scikit-learn RandomForrestClassifier on iris public dataset. There is a similar example in SageMaker documentation. Train a SKLearn Model using Script Mode. But it does not discuss many important aspects of a scikit-learn container and its environment. In this post, we will learn about them and cover all the details of training a scikit-learn model with script mode. I also noted that the example in the documentation uses RandomForrestRegressor on a classification problem which I believe is a mistake. . We have much to cover and learn, so let&#39;s start. . Environment . This notebook is prepared with AWS SageMaker notebook running on ml.t3.medium instance and &quot;conda_python3&quot; kernel. . !aws --version . aws-cli/1.22.97 Python/3.8.12 Linux/5.10.102-99.473.amzn2.x86_64 botocore/1.24.19 . !cat /etc/os-release . NAME=&#34;Amazon Linux&#34; VERSION=&#34;2&#34; ID=&#34;amzn&#34; ID_LIKE=&#34;centos rhel fedora&#34; VERSION_ID=&#34;2&#34; PRETTY_NAME=&#34;Amazon Linux 2&#34; ANSI_COLOR=&#34;0;33&#34; CPE_NAME=&#34;cpe:2.3:o:amazon:amazon_linux:2&#34; HOME_URL=&#34;https://amazonlinux.com/&#34; . !python3 --version . Python 3.8.12 . !conda env list . # conda environments: # base /home/ec2-user/anaconda3 JupyterSystemEnv /home/ec2-user/anaconda3/envs/JupyterSystemEnv R /home/ec2-user/anaconda3/envs/R amazonei_mxnet_p36 /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36 amazonei_pytorch_latest_p37 /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37 amazonei_tensorflow2_p36 /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36 mxnet_p37 /home/ec2-user/anaconda3/envs/mxnet_p37 python3 * /home/ec2-user/anaconda3/envs/python3 pytorch_p38 /home/ec2-user/anaconda3/envs/pytorch_p38 tensorflow2_p38 /home/ec2-user/anaconda3/envs/tensorflow2_p38 . . Prepare training and test data . We will use Iris flower dataset. It includes three iris species (Iris setosa, Iris virginica, and Iris versicolor) with 50 samples each. Four features were measured for each sample: the length and the width of the sepals and petals, in centimeters. We can train a model to distinguish the species from each other based on the combination of these four features. You can read more about this dataset at Iris flower data set. The dataset has five columns representing. . sepal length in cm | sepal width in cm | petal length in cm | petal width in cm | class: Iris Setosa, Iris Versicolour, Iris Virginica | Download and preprocess data . # download dataset import boto3 import pandas as pd import numpy as np s3 = boto3.client(&quot;s3&quot;) s3.download_file( f&quot;sagemaker-sample-files&quot;, &quot;datasets/tabular/iris/iris.data&quot;, &quot;iris.data&quot; ) df = pd.read_csv( &quot;iris.data&quot;, header=None, names=[&quot;sepal_len&quot;, &quot;sepal_wid&quot;, &quot;petal_len&quot;, &quot;petal_wid&quot;, &quot;class&quot;], ) df.head() . sepal_len sepal_wid petal_len petal_wid class . 0 5.1 | 3.5 | 1.4 | 0.2 | Iris-setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | Iris-setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | Iris-setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | Iris-setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | Iris-setosa | . # Convert the three classes from strings to integers in {0,1,2} df[&quot;class_cat&quot;] = df[&quot;class&quot;].astype(&quot;category&quot;).cat.codes categories_map = dict(enumerate(df[&quot;class&quot;].astype(&quot;category&quot;).cat.categories)) print(categories_map) df.head() . {0: &#39;Iris-setosa&#39;, 1: &#39;Iris-versicolor&#39;, 2: &#39;Iris-virginica&#39;} . sepal_len sepal_wid petal_len petal_wid class class_cat . 0 5.1 | 3.5 | 1.4 | 0.2 | Iris-setosa | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | Iris-setosa | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | Iris-setosa | 0 | . 3 4.6 | 3.1 | 1.5 | 0.2 | Iris-setosa | 0 | . 4 5.0 | 3.6 | 1.4 | 0.2 | Iris-setosa | 0 | . Prepare and store train and test sets as CSV files . # split the data into train and test set from sklearn.model_selection import train_test_split train, test = train_test_split(df, test_size=0.2, random_state=42) print(f&quot;train.shape: {train.shape}&quot;) print(f&quot;test.shape: {test.shape}&quot;) . train.shape: (120, 6) test.shape: (30, 6) . We have our dataset ready. Let&#39;s define a local directory local_path to keep all the files and artifacts related to this post. I will refer to this directory as &#39;workspace&#39;. . # `local_path` will be the root directory for this post. local_path = &quot;./datasets/2022-07-07-sagemaker-script-mode&quot; . We have train and test sets ready. Let&#39;s create two more directories in our workspace and store our data in them. . from pathlib import Path # local paths local_train_path = local_path + &quot;/train&quot; local_test_path = local_path + &quot;/test&quot; # create local directories Path(local_train_path).mkdir(parents=True, exist_ok=True) Path(local_test_path).mkdir(parents=True, exist_ok=True) print(&quot;local_train_path: &quot;, local_train_path) print(&quot;local_test_path: &quot;, local_test_path) # local file names local_train_file = local_train_path + &quot;/train.csv&quot; local_test_file = local_test_path + &quot;/test.csv&quot; # write train and test CSV files train.to_csv(local_train_file, index=False) test.to_csv(local_test_file, index=False) print(&quot;local_train_file: &quot;, local_train_file) print(&quot;local_test_file: &quot;, local_test_file) . local_train_path: ./datasets/2022-07-07-sagemaker-script-mode/train local_test_path: ./datasets/2022-07-07-sagemaker-script-mode/test local_train_file: ./datasets/2022-07-07-sagemaker-script-mode/train/train.csv local_test_file: ./datasets/2022-07-07-sagemaker-script-mode/test/test.csv . Create SageMaker session . import sagemaker session = sagemaker.Session() role = sagemaker.get_execution_role() bucket = session.default_bucket() region = session.boto_region_name print(&quot;sagemaker.__version__: &quot;, sagemaker.__version__) print(&quot;Session: &quot;, session) print(&quot;Role: &quot;, role) print(&quot;Bucket: &quot;, bucket) print(&quot;Region: &quot;, region) . sagemaker.__version__: 2.86.2 Session: &lt;sagemaker.session.Session object at 0x7f80ad720460&gt; Role: arn:aws:iam::801598032724:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole Bucket: sagemaker-us-east-1-801598032724 Region: us-east-1 . What we have done here is . imported the SageMaker Python SDK into our runtime | get a session to work with SageMaker API and other AWS services | get the execution role associated with the user profile. It is the same profile that is available to the user to work from console UI and has AmazonSageMakerFullAccess policy attached to it. | create or get a default bucket to use and return its name. Default bucket name has the format sagemaker-{region}-{account_id}. If it doesn&#39;t exist then our session will automatically create it. You may also use any other bucket in its place given that you have enough permission for reading and writing. | get the region name attached to our session | . Next, we will use this session to upload data to our default bucket. . Upload data to Amazon S3 bucket . # You may choose any other prefix for your bucket. # All the data related to this post will be under this prefix. bucket_prefix = &quot;2022-07-07-sagemaker-script-mode&quot; . Now upload the data. In the output, we will get the complete path (S3 URI) for our uploaded data. . s3_train_uri = session.upload_data(local_train_file, key_prefix=bucket_prefix + &quot;/data&quot;) s3_test_uri = session.upload_data(local_test_file, key_prefix=bucket_prefix + &quot;/data&quot;) print(&quot;s3_train_uri: &quot;, s3_train_uri) print(&quot;s3_test_uri: &quot;, s3_test_uri) . s3_train_uri: s3://sagemaker-us-east-1-801598032724/2022-07-07-sagemaker-script-mode/data/train.csv s3_test_uri: s3://sagemaker-us-east-1-801598032724/2022-07-07-sagemaker-script-mode/data/test.csv . At this point, our data preparation step is complete. Train and test CSV files are available on the local system and in our default Amazon S3 bucket. . Prepare SageMaker local environment . The Amazon SageMaker training environment is managed, but SageMaker Python SDK also supports local mode, allowing you to train and deploy models to your local environment. This is a great way to test training scripts before running them in SageMaker&#39;s managed training or hosting environment. . How SageMaker managed environment works? . When you send a request to SageMaker API (fit or deploy call) . it spins up new instances with the provided specification | loads the algorithm container | pulls the data from S3 | runs the training code | store the results and trained model artifacts to S3 | terminates the new instances | . All this happens behind the scenes with a single line of code and is a huge advantage. Spinning up new hardware every time can be good for repeatability and security, but it can add some friction while testing and debugging our code. We can test our code on a small dataset in our local environment with SageMaker local mode and then switch seamlessly to SageMaker managed environment by changing a single line of code. . Steps to prepare Amazon SageMaker local environment . Install the following pre-requisites if you want to set up Amazon SageMaker on your local system. . Install required Python packages: pip install boto3 sagemaker pandas scikit-learn pip install &#39;sagemaker[local]&#39; . | Docker Desktop installed and running on your computer: docker ps . | You should have AWS credentials configured on your local machine to be able to pull the docker image from ECR. | Instructions for SageMaker notebook instances . You can also set up SageMaker&#39;s local environment in SageMaker notebook instances. Required Python packages and Docker service is already there. You only need to upgrade the sagemaker[local] Python package. . # this is required for SageMaker notebook instances !pip install &#39;sagemaker[local]&#39; --upgrade . Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com Requirement already satisfied: sagemaker[local] in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (2.86.2) Collecting sagemaker[local] Downloading sagemaker-2.99.0.tar.gz (542 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.7/542.7 KB 10.6 MB/s eta 0:00:0000:01 Preparing metadata (setup.py) ... done Requirement already satisfied: attrs&lt;22,&gt;=20.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (20.3.0) Requirement already satisfied: boto3&lt;2.0,&gt;=1.20.21 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (1.21.42) Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (0.2.0) Requirement already satisfied: numpy&lt;2.0,&gt;=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (1.20.3) Requirement already satisfied: protobuf&lt;4.0,&gt;=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (3.19.1) Requirement already satisfied: protobuf3-to-dict&lt;1.0,&gt;=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (0.1.5) Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (1.0.1) Requirement already satisfied: importlib-metadata&lt;5.0,&gt;=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (4.8.2) Requirement already satisfied: packaging&gt;=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (21.3) Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (1.3.4) Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (0.2.8) Requirement already satisfied: urllib3==1.26.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (1.26.8) Requirement already satisfied: docker-compose==1.29.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (1.29.2) Requirement already satisfied: docker~=5.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (5.0.3) Requirement already satisfied: PyYAML==5.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (5.4.1) Requirement already satisfied: texttable&lt;2,&gt;=0.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2-&gt;sagemaker[local]) (1.6.4) Requirement already satisfied: websocket-client&lt;1,&gt;=0.32.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2-&gt;sagemaker[local]) (0.59.0) Requirement already satisfied: docopt&lt;1,&gt;=0.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2-&gt;sagemaker[local]) (0.6.2) Requirement already satisfied: jsonschema&lt;4,&gt;=2.5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2-&gt;sagemaker[local]) (3.2.0) Requirement already satisfied: dockerpty&lt;1,&gt;=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2-&gt;sagemaker[local]) (0.4.1) Requirement already satisfied: distro&lt;2,&gt;=1.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2-&gt;sagemaker[local]) (1.7.0) Requirement already satisfied: python-dotenv&lt;1,&gt;=0.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2-&gt;sagemaker[local]) (0.20.0) Requirement already satisfied: requests&lt;3,&gt;=2.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2-&gt;sagemaker[local]) (2.26.0) Collecting botocore&lt;1.25.0,&gt;=1.24.42 Downloading botocore-1.24.46-py3-none-any.whl (8.7 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.7/8.7 MB 34.3 MB/s eta 0:00:00:00:0100:01 Requirement already satisfied: s3transfer&lt;0.6.0,&gt;=0.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from boto3&lt;2.0,&gt;=1.20.21-&gt;sagemaker[local]) (0.5.2) Requirement already satisfied: jmespath&lt;2.0.0,&gt;=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from boto3&lt;2.0,&gt;=1.20.21-&gt;sagemaker[local]) (0.10.0) Requirement already satisfied: zipp&gt;=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from importlib-metadata&lt;5.0,&gt;=1.4.0-&gt;sagemaker[local]) (3.6.0) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from packaging&gt;=20.0-&gt;sagemaker[local]) (3.0.6) Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from protobuf3-to-dict&lt;1.0,&gt;=0.1.5-&gt;sagemaker[local]) (1.16.0) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas-&gt;sagemaker[local]) (2.8.2) Requirement already satisfied: pytz&gt;=2017.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas-&gt;sagemaker[local]) (2021.3) Requirement already satisfied: multiprocess&gt;=0.70.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pathos-&gt;sagemaker[local]) (0.70.12.2) Requirement already satisfied: pox&gt;=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pathos-&gt;sagemaker[local]) (0.3.0) Requirement already satisfied: dill&gt;=0.3.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pathos-&gt;sagemaker[local]) (0.3.4) Requirement already satisfied: ppft&gt;=1.6.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pathos-&gt;sagemaker[local]) (1.6.6.4) Requirement already satisfied: paramiko&gt;=2.4.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker~=5.0.0-&gt;sagemaker[local]) (2.10.3) Requirement already satisfied: pyrsistent&gt;=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from jsonschema&lt;4,&gt;=2.5.1-&gt;docker-compose==1.29.2-&gt;sagemaker[local]) (0.18.0) Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from jsonschema&lt;4,&gt;=2.5.1-&gt;docker-compose==1.29.2-&gt;sagemaker[local]) (59.4.0) Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.20.0-&gt;docker-compose==1.29.2-&gt;sagemaker[local]) (2.0.8) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.20.0-&gt;docker-compose==1.29.2-&gt;sagemaker[local]) (3.1) Requirement already satisfied: certifi&gt;=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.20.0-&gt;docker-compose==1.29.2-&gt;sagemaker[local]) (2021.10.8) Requirement already satisfied: pynacl&gt;=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from paramiko&gt;=2.4.2-&gt;docker~=5.0.0-&gt;sagemaker[local]) (1.5.0) Requirement already satisfied: cryptography&gt;=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from paramiko&gt;=2.4.2-&gt;docker~=5.0.0-&gt;sagemaker[local]) (36.0.0) Requirement already satisfied: bcrypt&gt;=3.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from paramiko&gt;=2.4.2-&gt;docker~=5.0.0-&gt;sagemaker[local]) (3.2.0) Requirement already satisfied: cffi&gt;=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from bcrypt&gt;=3.1.3-&gt;paramiko&gt;=2.4.2-&gt;docker~=5.0.0-&gt;sagemaker[local]) (1.15.0) Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from cffi&gt;=1.1-&gt;bcrypt&gt;=3.1.3-&gt;paramiko&gt;=2.4.2-&gt;docker~=5.0.0-&gt;sagemaker[local]) (2.21) Building wheels for collected packages: sagemaker Building wheel for sagemaker (setup.py) ... done Created wheel for sagemaker: filename=sagemaker-2.99.0-py2.py3-none-any.whl size=756462 sha256=309b5159cfb7f5c739c6159b8bf309bfa7ce28d2ca402296e824f3e84bc837c1 Stored in directory: /home/ec2-user/.cache/pip/wheels/fc/df/14/14b7871f4cf108cfe8891338510d97e28cfe2da00f37114fcf Successfully built sagemaker Installing collected packages: botocore, sagemaker Attempting uninstall: botocore Found existing installation: botocore 1.24.19 Uninstalling botocore-1.24.19: Successfully uninstalled botocore-1.24.19 Attempting uninstall: sagemaker Found existing installation: sagemaker 2.86.2 Uninstalling sagemaker-2.86.2: Successfully uninstalled sagemaker-2.86.2 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. awscli 1.22.97 requires botocore==1.24.42, but you have botocore 1.24.46 which is incompatible. aiobotocore 2.0.1 requires botocore&lt;1.22.9,&gt;=1.22.8, but you have botocore 1.24.46 which is incompatible. Successfully installed botocore-1.24.46 sagemaker-2.99.0 WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available. You should consider upgrading via the &#39;/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip&#39; command. . . Instructions for SageMaker Studio environment . Note that SageMaker local mode will not work in SageMaker Studio environment as it does not have docker service installed on the provided instances. . Create SageMaker local session . SageMaker local session is required for working in a local environment. Let&#39;s create it. . from sagemaker.local import LocalSession session_local = LocalSession() session_local . &lt;sagemaker.local.local_session.LocalSession at 0x7f80ac223910&gt; . # configure local session session_local.config = {&quot;local&quot;: {&quot;local_code&quot;: True}} . Prepare SageMaker training script . We will call our training script train_and_serve.py and place it in our workspace under the /src folder. Then, we will start with a simple Hello World message code. After that, we will update and complete our training script as we learn more about the SageMaker scikit-learn container environment. . script_file_name = &quot;train_and_serve.py&quot; script_path = local_path + &quot;/src&quot; script_file = script_path + &quot;/&quot; + script_file_name print(&quot;script_file_name: &quot;, script_file_name) print(&quot;script_path: &quot;, script_path) print(&quot;script_file: &quot;, script_file) . script_file_name: train_and_serve.py script_path: ./datasets/2022-07-07-sagemaker-script-mode/src script_file: ./datasets/2022-07-07-sagemaker-script-mode/src/train_and_serve.py . # make sure that the directory exists Path(script_path).mkdir(parents=True, exist_ok=True) . Now the training script. . %%writefile $script_file if __name__ == &quot;__main__&quot;: print(&quot;*** Hello from the SageMaker script mode***&quot;) . Overwriting ./datasets/2022-07-07-sagemaker-script-mode/src/train_and_serve.py . Prepare SageMaker SKLearn estimator . To create SKLearn Estimator object we need to pass it following items . entry_point (str) Path (absolute or relative) to the Python source file, which should be executed as the entry point to training | framework_version (str) Scikit-learn version you want to use for executing your model training code | role (str) An AWS IAM role (either name or full ARN) | instance_type (str) Type of instance to use for training. For local mode use string local | instance_count (int) Number of instances to use for training. Since we will train in the local environment and have a single instance, we will use &#39;1&#39; here | . You can read more about the SKLearn Estimator class from the official documentation Scikit Learn Estimator . Let&#39;s find the SKLearn framework version. . import sklearn print(sklearn.__version__) . 1.0.1 . Note that version number 1.0.1 has to be provided to the SKLearn estimator class as 1.0-1. Otherwise, you will get the following error message. . ValueError: Unsupported sklearn version: 1.0.1. You may need to upgrade your SDK version (pip install -U sagemaker) for newer sklearn versions. Supported sklearn version(s): 0.20.0, 0.23-1, 1.0-1. . Now let us create the SageMaker SKLearn estimator object and pass our training script to it. . from sagemaker.sklearn import SKLearn sk_estimator = SKLearn( entry_point=script_file, role=role, instance_count=1, instance_type=&quot;local&quot;, framework_version=&quot;1.0-1&quot; ) sk_estimator.fit() . WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store . Creating fvm7gkf0bq-algo-1-ju7k8 ... Creating fvm7gkf0bq-algo-1-ju7k8 ... done Attaching to fvm7gkf0bq-algo-1-ju7k8 fvm7gkf0bq-algo-1-ju7k8 | 2022-07-17 15:23:43,041 sagemaker-containers INFO Imported framework sagemaker_sklearn_container.training fvm7gkf0bq-algo-1-ju7k8 | 2022-07-17 15:23:43,045 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) fvm7gkf0bq-algo-1-ju7k8 | 2022-07-17 15:23:43,054 sagemaker_sklearn_container.training INFO Invoking user training script. fvm7gkf0bq-algo-1-ju7k8 | 2022-07-17 15:23:43,272 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) fvm7gkf0bq-algo-1-ju7k8 | 2022-07-17 15:23:43,284 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) fvm7gkf0bq-algo-1-ju7k8 | 2022-07-17 15:23:43,297 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) fvm7gkf0bq-algo-1-ju7k8 | 2022-07-17 15:23:43,306 sagemaker-training-toolkit INFO Invoking user script fvm7gkf0bq-algo-1-ju7k8 | fvm7gkf0bq-algo-1-ju7k8 | Training Env: fvm7gkf0bq-algo-1-ju7k8 | fvm7gkf0bq-algo-1-ju7k8 | { fvm7gkf0bq-algo-1-ju7k8 | &#34;additional_framework_parameters&#34;: {}, fvm7gkf0bq-algo-1-ju7k8 | &#34;channel_input_dirs&#34;: {}, fvm7gkf0bq-algo-1-ju7k8 | &#34;current_host&#34;: &#34;algo-1-ju7k8&#34;, fvm7gkf0bq-algo-1-ju7k8 | &#34;framework_module&#34;: &#34;sagemaker_sklearn_container.training:main&#34;, fvm7gkf0bq-algo-1-ju7k8 | &#34;hosts&#34;: [ fvm7gkf0bq-algo-1-ju7k8 | &#34;algo-1-ju7k8&#34; fvm7gkf0bq-algo-1-ju7k8 | ], fvm7gkf0bq-algo-1-ju7k8 | &#34;hyperparameters&#34;: {}, fvm7gkf0bq-algo-1-ju7k8 | &#34;input_config_dir&#34;: &#34;/opt/ml/input/config&#34;, fvm7gkf0bq-algo-1-ju7k8 | &#34;input_data_config&#34;: {}, fvm7gkf0bq-algo-1-ju7k8 | &#34;input_dir&#34;: &#34;/opt/ml/input&#34;, fvm7gkf0bq-algo-1-ju7k8 | &#34;is_master&#34;: true, fvm7gkf0bq-algo-1-ju7k8 | &#34;job_name&#34;: &#34;sagemaker-scikit-learn-2022-07-17-15-22-17-814&#34;, fvm7gkf0bq-algo-1-ju7k8 | &#34;log_level&#34;: 20, fvm7gkf0bq-algo-1-ju7k8 | &#34;master_hostname&#34;: &#34;algo-1-ju7k8&#34;, fvm7gkf0bq-algo-1-ju7k8 | &#34;model_dir&#34;: &#34;/opt/ml/model&#34;, fvm7gkf0bq-algo-1-ju7k8 | &#34;module_dir&#34;: &#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-22-17-814/source/sourcedir.tar.gz&#34;, fvm7gkf0bq-algo-1-ju7k8 | &#34;module_name&#34;: &#34;train_and_serve&#34;, fvm7gkf0bq-algo-1-ju7k8 | &#34;network_interface_name&#34;: &#34;eth0&#34;, fvm7gkf0bq-algo-1-ju7k8 | &#34;num_cpus&#34;: 2, fvm7gkf0bq-algo-1-ju7k8 | &#34;num_gpus&#34;: 0, fvm7gkf0bq-algo-1-ju7k8 | &#34;output_data_dir&#34;: &#34;/opt/ml/output/data&#34;, fvm7gkf0bq-algo-1-ju7k8 | &#34;output_dir&#34;: &#34;/opt/ml/output&#34;, fvm7gkf0bq-algo-1-ju7k8 | &#34;output_intermediate_dir&#34;: &#34;/opt/ml/output/intermediate&#34;, fvm7gkf0bq-algo-1-ju7k8 | &#34;resource_config&#34;: { fvm7gkf0bq-algo-1-ju7k8 | &#34;current_host&#34;: &#34;algo-1-ju7k8&#34;, fvm7gkf0bq-algo-1-ju7k8 | &#34;hosts&#34;: [ fvm7gkf0bq-algo-1-ju7k8 | &#34;algo-1-ju7k8&#34; fvm7gkf0bq-algo-1-ju7k8 | ] fvm7gkf0bq-algo-1-ju7k8 | }, fvm7gkf0bq-algo-1-ju7k8 | &#34;user_entry_point&#34;: &#34;train_and_serve.py&#34; fvm7gkf0bq-algo-1-ju7k8 | } fvm7gkf0bq-algo-1-ju7k8 | fvm7gkf0bq-algo-1-ju7k8 | Environment variables: fvm7gkf0bq-algo-1-ju7k8 | fvm7gkf0bq-algo-1-ju7k8 | SM_HOSTS=[&#34;algo-1-ju7k8&#34;] fvm7gkf0bq-algo-1-ju7k8 | SM_NETWORK_INTERFACE_NAME=eth0 fvm7gkf0bq-algo-1-ju7k8 | SM_HPS={} fvm7gkf0bq-algo-1-ju7k8 | SM_USER_ENTRY_POINT=train_and_serve.py fvm7gkf0bq-algo-1-ju7k8 | SM_FRAMEWORK_PARAMS={} fvm7gkf0bq-algo-1-ju7k8 | SM_RESOURCE_CONFIG={&#34;current_host&#34;:&#34;algo-1-ju7k8&#34;,&#34;hosts&#34;:[&#34;algo-1-ju7k8&#34;]} fvm7gkf0bq-algo-1-ju7k8 | SM_INPUT_DATA_CONFIG={} fvm7gkf0bq-algo-1-ju7k8 | SM_OUTPUT_DATA_DIR=/opt/ml/output/data fvm7gkf0bq-algo-1-ju7k8 | SM_CHANNELS=[] fvm7gkf0bq-algo-1-ju7k8 | SM_CURRENT_HOST=algo-1-ju7k8 fvm7gkf0bq-algo-1-ju7k8 | SM_MODULE_NAME=train_and_serve fvm7gkf0bq-algo-1-ju7k8 | SM_LOG_LEVEL=20 fvm7gkf0bq-algo-1-ju7k8 | SM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main fvm7gkf0bq-algo-1-ju7k8 | SM_INPUT_DIR=/opt/ml/input fvm7gkf0bq-algo-1-ju7k8 | SM_INPUT_CONFIG_DIR=/opt/ml/input/config fvm7gkf0bq-algo-1-ju7k8 | SM_OUTPUT_DIR=/opt/ml/output fvm7gkf0bq-algo-1-ju7k8 | SM_NUM_CPUS=2 fvm7gkf0bq-algo-1-ju7k8 | SM_NUM_GPUS=0 fvm7gkf0bq-algo-1-ju7k8 | SM_MODEL_DIR=/opt/ml/model fvm7gkf0bq-algo-1-ju7k8 | SM_MODULE_DIR=s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-22-17-814/source/sourcedir.tar.gz fvm7gkf0bq-algo-1-ju7k8 | SM_TRAINING_ENV={&#34;additional_framework_parameters&#34;:{},&#34;channel_input_dirs&#34;:{},&#34;current_host&#34;:&#34;algo-1-ju7k8&#34;,&#34;framework_module&#34;:&#34;sagemaker_sklearn_container.training:main&#34;,&#34;hosts&#34;:[&#34;algo-1-ju7k8&#34;],&#34;hyperparameters&#34;:{},&#34;input_config_dir&#34;:&#34;/opt/ml/input/config&#34;,&#34;input_data_config&#34;:{},&#34;input_dir&#34;:&#34;/opt/ml/input&#34;,&#34;is_master&#34;:true,&#34;job_name&#34;:&#34;sagemaker-scikit-learn-2022-07-17-15-22-17-814&#34;,&#34;log_level&#34;:20,&#34;master_hostname&#34;:&#34;algo-1-ju7k8&#34;,&#34;model_dir&#34;:&#34;/opt/ml/model&#34;,&#34;module_dir&#34;:&#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-22-17-814/source/sourcedir.tar.gz&#34;,&#34;module_name&#34;:&#34;train_and_serve&#34;,&#34;network_interface_name&#34;:&#34;eth0&#34;,&#34;num_cpus&#34;:2,&#34;num_gpus&#34;:0,&#34;output_data_dir&#34;:&#34;/opt/ml/output/data&#34;,&#34;output_dir&#34;:&#34;/opt/ml/output&#34;,&#34;output_intermediate_dir&#34;:&#34;/opt/ml/output/intermediate&#34;,&#34;resource_config&#34;:{&#34;current_host&#34;:&#34;algo-1-ju7k8&#34;,&#34;hosts&#34;:[&#34;algo-1-ju7k8&#34;]},&#34;user_entry_point&#34;:&#34;train_and_serve.py&#34;} fvm7gkf0bq-algo-1-ju7k8 | SM_USER_ARGS=[] fvm7gkf0bq-algo-1-ju7k8 | SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate fvm7gkf0bq-algo-1-ju7k8 | PYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages fvm7gkf0bq-algo-1-ju7k8 | fvm7gkf0bq-algo-1-ju7k8 | Invoking script with the following command: fvm7gkf0bq-algo-1-ju7k8 | fvm7gkf0bq-algo-1-ju7k8 | /miniconda3/bin/python train_and_serve.py fvm7gkf0bq-algo-1-ju7k8 | fvm7gkf0bq-algo-1-ju7k8 | fvm7gkf0bq-algo-1-ju7k8 | *** Hello from the SageMaker script mode*** fvm7gkf0bq-algo-1-ju7k8 | 2022-07-17 15:23:43,332 sagemaker-containers INFO Reporting training SUCCESS fvm7gkf0bq-algo-1-ju7k8 exited with code 0 Aborting on container exit... ===== Job Complete ===== . . # The estimator will pick a local session when we use instance_type=&#39;local&#39; sk_estimator.sagemaker_session . &lt;sagemaker.local.local_session.LocalSession at 0x7f80ac53da90&gt; . When you first run the SKLearn estimator, executing it may take some time as it has to download the scikit-learn container to the local docker environment. You will get the container logs in the output when the container completes the execution. The logs show that the container has successfully run the training script, and the hello message is also printed. But there is a lot more information available in the logs. We will discuss it in the coming section. . . Understanding SKLearn container output and environment varaibles . From the SKLearn estimator output, we can see that our train_and_serve.py script is executed by the container with the following command. . /miniconda3/bin/python train_and_serve.py . Inspecting SageMaker SKLearn docker image . Since the container was executed in the local environment, we can also inspect the SageMaker SKLearn local image. . !docker images . REPOSITORY TAG IMAGE ID CREATED SIZE 683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn 1.0-1-cpu-py3 8a6ea8272ad0 10 days ago 3.7GB . Let&#39;s also inspect the docker image. Notice multiple container environment variables and their default values in the output. . !docker inspect 8a6ea8272ad0 . [ { &#34;Id&#34;: &#34;sha256:8a6ea8272ad003ec816569b0f879b16c770116584301161565f065aadb99436c&#34;, &#34;RepoTags&#34;: [ &#34;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.0-1-cpu-py3&#34; ], &#34;RepoDigests&#34;: [ &#34;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn@sha256:fc8c3a617ff0e436c25f3b64d03e1f485f1d159478c26757f3d1d267fc849445&#34; ], &#34;Parent&#34;: &#34;&#34;, &#34;Comment&#34;: &#34;&#34;, &#34;Created&#34;: &#34;2022-07-06T18:55:02.854297671Z&#34;, &#34;Container&#34;: &#34;11b9a5fec2d61294aee63e549100ed18ceb7aa0de6a4ff198da2f556dfe3ec2f&#34;, &#34;ContainerConfig&#34;: { &#34;Hostname&#34;: &#34;11b9a5fec2d6&#34;, &#34;Domainname&#34;: &#34;&#34;, &#34;User&#34;: &#34;&#34;, &#34;AttachStdin&#34;: false, &#34;AttachStdout&#34;: false, &#34;AttachStderr&#34;: false, &#34;ExposedPorts&#34;: { &#34;8080/tcp&#34;: {} }, &#34;Tty&#34;: false, &#34;OpenStdin&#34;: false, &#34;StdinOnce&#34;: false, &#34;Env&#34;: [ &#34;PATH=/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&#34;, &#34;PYTHONDONTWRITEBYTECODE=1&#34;, &#34;PYTHONUNBUFFERED=1&#34;, &#34;PYTHONIOENCODING=UTF-8&#34;, &#34;LANG=C.UTF-8&#34;, &#34;LC_ALL=C.UTF-8&#34;, &#34;SAGEMAKER_SKLEARN_VERSION=1.0-1&#34;, &#34;SAGEMAKER_TRAINING_MODULE=sagemaker_sklearn_container.training:main&#34;, &#34;SAGEMAKER_SERVING_MODULE=sagemaker_sklearn_container.serving:main&#34;, &#34;SKLEARN_MMS_CONFIG=/home/model-server/config.properties&#34;, &#34;SM_INPUT=/opt/ml/input&#34;, &#34;SM_INPUT_TRAINING_CONFIG_FILE=/opt/ml/input/config/hyperparameters.json&#34;, &#34;SM_INPUT_DATA_CONFIG_FILE=/opt/ml/input/config/inputdataconfig.json&#34;, &#34;SM_CHECKPOINT_CONFIG_FILE=/opt/ml/input/config/checkpointconfig.json&#34;, &#34;SM_MODEL_DIR=/opt/ml/model&#34;, &#34;TEMP=/home/model-server/tmp&#34; ], &#34;Cmd&#34;: [ &#34;/bin/sh&#34;, &#34;-c&#34;, &#34;#(nop) &#34;, &#34;LABEL transform_id=9be8b540-703b-4ecd-a127-c37333a0dcec_sagemaker-scikit-learn-1_0&#34; ], &#34;Image&#34;: &#34;sha256:58b15b990d550868caed6f885423deee97a6c7f525c228a043096bf28e775d18&#34;, &#34;Volumes&#34;: null, &#34;WorkingDir&#34;: &#34;&#34;, &#34;Entrypoint&#34;: null, &#34;OnBuild&#34;: null, &#34;Labels&#34;: { &#34;TRANSFORM_TYPE&#34;: &#34;Aggregate-1.0&#34;, &#34;VERSION_SET_NAME&#34;: &#34;SMFrameworksSKLearn/release-cdk&#34;, &#34;VERSION_SET_REVISION&#34;: &#34;6086988568&#34;, &#34;com.amazonaws.sagemaker.capabilities.accept-bind-to-port&#34;: &#34;true&#34;, &#34;com.amazonaws.sagemaker.capabilities.multi-models&#34;: &#34;true&#34;, &#34;transform_id&#34;: &#34;9be8b540-703b-4ecd-a127-c37333a0dcec_sagemaker-scikit-learn-1_0&#34; } }, &#34;DockerVersion&#34;: &#34;20.10.15&#34;, &#34;Author&#34;: &#34;&#34;, &#34;Config&#34;: { &#34;Hostname&#34;: &#34;&#34;, &#34;Domainname&#34;: &#34;&#34;, &#34;User&#34;: &#34;&#34;, &#34;AttachStdin&#34;: false, &#34;AttachStdout&#34;: false, &#34;AttachStderr&#34;: false, &#34;ExposedPorts&#34;: { &#34;8080/tcp&#34;: {} }, &#34;Tty&#34;: false, &#34;OpenStdin&#34;: false, &#34;StdinOnce&#34;: false, &#34;Env&#34;: [ &#34;PATH=/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&#34;, &#34;PYTHONDONTWRITEBYTECODE=1&#34;, &#34;PYTHONUNBUFFERED=1&#34;, &#34;PYTHONIOENCODING=UTF-8&#34;, &#34;LANG=C.UTF-8&#34;, &#34;LC_ALL=C.UTF-8&#34;, &#34;SAGEMAKER_SKLEARN_VERSION=1.0-1&#34;, &#34;SAGEMAKER_TRAINING_MODULE=sagemaker_sklearn_container.training:main&#34;, &#34;SAGEMAKER_SERVING_MODULE=sagemaker_sklearn_container.serving:main&#34;, &#34;SKLEARN_MMS_CONFIG=/home/model-server/config.properties&#34;, &#34;SM_INPUT=/opt/ml/input&#34;, &#34;SM_INPUT_TRAINING_CONFIG_FILE=/opt/ml/input/config/hyperparameters.json&#34;, &#34;SM_INPUT_DATA_CONFIG_FILE=/opt/ml/input/config/inputdataconfig.json&#34;, &#34;SM_CHECKPOINT_CONFIG_FILE=/opt/ml/input/config/checkpointconfig.json&#34;, &#34;SM_MODEL_DIR=/opt/ml/model&#34;, &#34;TEMP=/home/model-server/tmp&#34; ], &#34;Cmd&#34;: [ &#34;bash&#34; ], &#34;Image&#34;: &#34;sha256:58b15b990d550868caed6f885423deee97a6c7f525c228a043096bf28e775d18&#34;, &#34;Volumes&#34;: null, &#34;WorkingDir&#34;: &#34;&#34;, &#34;Entrypoint&#34;: null, &#34;OnBuild&#34;: null, &#34;Labels&#34;: { &#34;TRANSFORM_TYPE&#34;: &#34;Aggregate-1.0&#34;, &#34;VERSION_SET_NAME&#34;: &#34;SMFrameworksSKLearn/release-cdk&#34;, &#34;VERSION_SET_REVISION&#34;: &#34;6086988568&#34;, &#34;com.amazonaws.sagemaker.capabilities.accept-bind-to-port&#34;: &#34;true&#34;, &#34;com.amazonaws.sagemaker.capabilities.multi-models&#34;: &#34;true&#34;, &#34;transform_id&#34;: &#34;9be8b540-703b-4ecd-a127-c37333a0dcec_sagemaker-scikit-learn-1_0&#34; } }, &#34;Architecture&#34;: &#34;amd64&#34;, &#34;Os&#34;: &#34;linux&#34;, &#34;Size&#34;: 3699696670, &#34;VirtualSize&#34;: 3699696670, &#34;GraphDriver&#34;: { &#34;Data&#34;: { &#34;LowerDir&#34;: &#34;/var/lib/docker/overlay2/01a97258168fa360e9f6aa63ac0c6b2417c0ea0ebe888123edad87eb4a646765/diff:/var/lib/docker/overlay2/3b85b71e8fe52c7a27ae71ed492ff72c7e430cccdeea17046e2a361e8d7fd960/diff:/var/lib/docker/overlay2/7de8e16dd696c868ffd028a3ba1f1a80ef04237b9323229e578bc5e3aa6a29d7/diff:/var/lib/docker/overlay2/5eeb27014ab7ac7a894efdbb166d8a87fb9d4b8b739eccd82546ad6a2b53aa70/diff:/var/lib/docker/overlay2/bbd9a81a7aa5bf4c79e81ecf47670a3f8c098eee9c6682f36f88ec52db8e1946/diff:/var/lib/docker/overlay2/eb0e7f3a5bd45c1d611e4c37ba641d1e978043954312da5908fd4003c41c7e7d/diff:/var/lib/docker/overlay2/3daaedc78711e353befc51544a944ad35954327325d056094f445502bf65ce53/diff:/var/lib/docker/overlay2/9dd41e3edfb9d8f852732a968a7b179ca811e0f9d55614a0b193de753fc6aca0/diff:/var/lib/docker/overlay2/ede189a574c79eebc565041a44ebf8b586247a36a99fe3ff9588b8c940783498/diff:/var/lib/docker/overlay2/6b1d78a9c074a42d78650406b90b7b4f51eb31660a7b1e2dcc6d73cc43d29b6b/diff:/var/lib/docker/overlay2/3e0420f6740f876c9355d526cbdedd9ebde5be94ddf0d93d7dadd4f34cae351b/diff:/var/lib/docker/overlay2/de1a2da7ee1b5d9a1b4e5c3dd1adff213185dde7e1212db96c0435e512f50701/diff:/var/lib/docker/overlay2/bebca69aef394f0553634413c7875eb58228c7e6359a305a7501705e75c2b58b/diff:/var/lib/docker/overlay2/8a410db2a038a175ee6ddfb005383f8776c80b1b1901f5d2feedfc8d837ffa40/diff:/var/lib/docker/overlay2/6f6686a8cb3ccf47b214854717cbe33ba777e0985200e3d7b7f761f99231b274/diff:/var/lib/docker/overlay2/ad8b24fa9173d28a83284e4f31d830f1b3d9fe30a3fcc8cbb37895ec2fded7bf/diff:/var/lib/docker/overlay2/e8b0842f0da5b0dbb5076e350bfe1a70ef291546bbbf207fe1f90ae7ccd64517/diff&#34;, &#34;MergedDir&#34;: &#34;/var/lib/docker/overlay2/632d2d4d01646bd8be2ec147edc70eb44f59fb262aa12b217fd560c464edd4cb/merged&#34;, &#34;UpperDir&#34;: &#34;/var/lib/docker/overlay2/632d2d4d01646bd8be2ec147edc70eb44f59fb262aa12b217fd560c464edd4cb/diff&#34;, &#34;WorkDir&#34;: &#34;/var/lib/docker/overlay2/632d2d4d01646bd8be2ec147edc70eb44f59fb262aa12b217fd560c464edd4cb/work&#34; }, &#34;Name&#34;: &#34;overlay2&#34; }, &#34;RootFS&#34;: { &#34;Type&#34;: &#34;layers&#34;, &#34;Layers&#34;: [ &#34;sha256:1dc52a6b4de8561423dd3ec5a1f7f77f5309fd8cb340f80b8bc3d87fa112003e&#34;, &#34;sha256:b13a10ce059365d68a2113e9dbcac05b17b51f181615fca6d717a0dcf9ba8ffb&#34;, &#34;sha256:790d00cf365a312488151b354f0b0ae826be031edffb8a4de6a1fab048774dc7&#34;, &#34;sha256:323e43c53a1cd5abbd55437588f19da04f716452bc6d05486759b35f3e485390&#34;, &#34;sha256:c99c9d462af0bac5511ed046178ab0de79b8cdad33cd85246e9f661e098426cd&#34;, &#34;sha256:4a3a4d9fb4d250b1b64629b23bc0a477a45ee2659a8410d59a31a181dad70002&#34;, &#34;sha256:27b35f432a27e5e275038e559ebbe1aa7e91447bf417f5da01e3326739ba9366&#34;, &#34;sha256:ee12325fe0b7e7930b76d9a3dc81fcc37fa51a3267b311d2ed7c38703f193d75&#34;, &#34;sha256:7ceb40593535cdc07299efa2ce3a2c2267c2fa683161515fd6ab97f733492bf0&#34;, &#34;sha256:f18dbe0eec054f0aedf54a94aa29dab0d2c0f3d920fb482c99819622b0094f47&#34;, &#34;sha256:df2a7845ea611463f9f3282ccb45156ba883f40b15013ee49bd0a569301738d8&#34;, &#34;sha256:bcbd5416b87e3e37e05c22e46cbff2e3503d9caa0ec283a44931dc63e51c8cb7&#34;, &#34;sha256:5bcbb3ccae766c8a72d98ce494500bfd44c32e5780a1cb153139a4c5c143a8d5&#34;, &#34;sha256:4ecc8a8ffa902f3ea9bebb8d610e02a32ce1ca94c1a3160a31da98b73c1f55a0&#34;, &#34;sha256:a7a7b8b26735eb2d137fd0f91b83c73ad48cf2c4b83e9d0cadece410d6e598ba&#34;, &#34;sha256:ae939a0c9d32674ad6674947853ecfda4ff0530a8137960064448ae5e45fa1c5&#34;, &#34;sha256:6948f39c8f3cf6ec104734ccd1112fcb4af85a7c26c9c3d43495494b9b799f25&#34;, &#34;sha256:affd18c8e88f35e75bd02158e0418f3aeb4eec4269a208ede24cc829fa88c850&#34; ] }, &#34;Metadata&#34;: { &#34;LastTagTime&#34;: &#34;0001-01-01T00:00:00Z&#34; } } ] . . Pass hyperparameters to SKLearn estimator . Let&#39;s pass some dummy hyperparameters to the estimator and see how it affects the output. . sk_estimator = SKLearn( entry_point=script_file, role=role, instance_count=1, instance_type=&#39;local&#39;, framework_version=&quot;1.0-1&quot;, hyperparameters={&quot;dummy_param_1&quot;:&quot;val1&quot;,&quot;dummy_param_2&quot;:&quot;val2&quot;}, ) sk_estimator.fit() . Creating kc4ahx6e84-algo-1-8m8ve ... Creating kc4ahx6e84-algo-1-8m8ve ... done Attaching to kc4ahx6e84-algo-1-8m8ve kc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,385 sagemaker-containers INFO Imported framework sagemaker_sklearn_container.training kc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,389 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) kc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,398 sagemaker_sklearn_container.training INFO Invoking user training script. kc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,595 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) kc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,608 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) kc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,621 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) kc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,630 sagemaker-training-toolkit INFO Invoking user script kc4ahx6e84-algo-1-8m8ve | kc4ahx6e84-algo-1-8m8ve | Training Env: kc4ahx6e84-algo-1-8m8ve | kc4ahx6e84-algo-1-8m8ve | { kc4ahx6e84-algo-1-8m8ve | &#34;additional_framework_parameters&#34;: {}, kc4ahx6e84-algo-1-8m8ve | &#34;channel_input_dirs&#34;: {}, kc4ahx6e84-algo-1-8m8ve | &#34;current_host&#34;: &#34;algo-1-8m8ve&#34;, kc4ahx6e84-algo-1-8m8ve | &#34;framework_module&#34;: &#34;sagemaker_sklearn_container.training:main&#34;, kc4ahx6e84-algo-1-8m8ve | &#34;hosts&#34;: [ kc4ahx6e84-algo-1-8m8ve | &#34;algo-1-8m8ve&#34; kc4ahx6e84-algo-1-8m8ve | ], kc4ahx6e84-algo-1-8m8ve | &#34;hyperparameters&#34;: { kc4ahx6e84-algo-1-8m8ve | &#34;dummy_param_1&#34;: &#34;val1&#34;, kc4ahx6e84-algo-1-8m8ve | &#34;dummy_param_2&#34;: &#34;val2&#34; kc4ahx6e84-algo-1-8m8ve | }, kc4ahx6e84-algo-1-8m8ve | &#34;input_config_dir&#34;: &#34;/opt/ml/input/config&#34;, kc4ahx6e84-algo-1-8m8ve | &#34;input_data_config&#34;: {}, kc4ahx6e84-algo-1-8m8ve | &#34;input_dir&#34;: &#34;/opt/ml/input&#34;, kc4ahx6e84-algo-1-8m8ve | &#34;is_master&#34;: true, kc4ahx6e84-algo-1-8m8ve | &#34;job_name&#34;: &#34;sagemaker-scikit-learn-2022-07-17-15-23-44-284&#34;, kc4ahx6e84-algo-1-8m8ve | &#34;log_level&#34;: 20, kc4ahx6e84-algo-1-8m8ve | &#34;master_hostname&#34;: &#34;algo-1-8m8ve&#34;, kc4ahx6e84-algo-1-8m8ve | &#34;model_dir&#34;: &#34;/opt/ml/model&#34;, kc4ahx6e84-algo-1-8m8ve | &#34;module_dir&#34;: &#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-44-284/source/sourcedir.tar.gz&#34;, kc4ahx6e84-algo-1-8m8ve | &#34;module_name&#34;: &#34;train_and_serve&#34;, kc4ahx6e84-algo-1-8m8ve | &#34;network_interface_name&#34;: &#34;eth0&#34;, kc4ahx6e84-algo-1-8m8ve | &#34;num_cpus&#34;: 2, kc4ahx6e84-algo-1-8m8ve | &#34;num_gpus&#34;: 0, kc4ahx6e84-algo-1-8m8ve | &#34;output_data_dir&#34;: &#34;/opt/ml/output/data&#34;, kc4ahx6e84-algo-1-8m8ve | &#34;output_dir&#34;: &#34;/opt/ml/output&#34;, kc4ahx6e84-algo-1-8m8ve | &#34;output_intermediate_dir&#34;: &#34;/opt/ml/output/intermediate&#34;, kc4ahx6e84-algo-1-8m8ve | &#34;resource_config&#34;: { kc4ahx6e84-algo-1-8m8ve | &#34;current_host&#34;: &#34;algo-1-8m8ve&#34;, kc4ahx6e84-algo-1-8m8ve | &#34;hosts&#34;: [ kc4ahx6e84-algo-1-8m8ve | &#34;algo-1-8m8ve&#34; kc4ahx6e84-algo-1-8m8ve | ] kc4ahx6e84-algo-1-8m8ve | }, kc4ahx6e84-algo-1-8m8ve | &#34;user_entry_point&#34;: &#34;train_and_serve.py&#34; kc4ahx6e84-algo-1-8m8ve | } kc4ahx6e84-algo-1-8m8ve | kc4ahx6e84-algo-1-8m8ve | Environment variables: kc4ahx6e84-algo-1-8m8ve | kc4ahx6e84-algo-1-8m8ve | SM_HOSTS=[&#34;algo-1-8m8ve&#34;] kc4ahx6e84-algo-1-8m8ve | SM_NETWORK_INTERFACE_NAME=eth0 kc4ahx6e84-algo-1-8m8ve | SM_HPS={&#34;dummy_param_1&#34;:&#34;val1&#34;,&#34;dummy_param_2&#34;:&#34;val2&#34;} kc4ahx6e84-algo-1-8m8ve | SM_USER_ENTRY_POINT=train_and_serve.py kc4ahx6e84-algo-1-8m8ve | SM_FRAMEWORK_PARAMS={} kc4ahx6e84-algo-1-8m8ve | SM_RESOURCE_CONFIG={&#34;current_host&#34;:&#34;algo-1-8m8ve&#34;,&#34;hosts&#34;:[&#34;algo-1-8m8ve&#34;]} kc4ahx6e84-algo-1-8m8ve | SM_INPUT_DATA_CONFIG={} kc4ahx6e84-algo-1-8m8ve | SM_OUTPUT_DATA_DIR=/opt/ml/output/data kc4ahx6e84-algo-1-8m8ve | SM_CHANNELS=[] kc4ahx6e84-algo-1-8m8ve | SM_CURRENT_HOST=algo-1-8m8ve kc4ahx6e84-algo-1-8m8ve | SM_MODULE_NAME=train_and_serve kc4ahx6e84-algo-1-8m8ve | SM_LOG_LEVEL=20 kc4ahx6e84-algo-1-8m8ve | SM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main kc4ahx6e84-algo-1-8m8ve | SM_INPUT_DIR=/opt/ml/input kc4ahx6e84-algo-1-8m8ve | SM_INPUT_CONFIG_DIR=/opt/ml/input/config kc4ahx6e84-algo-1-8m8ve | SM_OUTPUT_DIR=/opt/ml/output kc4ahx6e84-algo-1-8m8ve | SM_NUM_CPUS=2 kc4ahx6e84-algo-1-8m8ve | SM_NUM_GPUS=0 kc4ahx6e84-algo-1-8m8ve | SM_MODEL_DIR=/opt/ml/model kc4ahx6e84-algo-1-8m8ve | SM_MODULE_DIR=s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-44-284/source/sourcedir.tar.gz kc4ahx6e84-algo-1-8m8ve | SM_TRAINING_ENV={&#34;additional_framework_parameters&#34;:{},&#34;channel_input_dirs&#34;:{},&#34;current_host&#34;:&#34;algo-1-8m8ve&#34;,&#34;framework_module&#34;:&#34;sagemaker_sklearn_container.training:main&#34;,&#34;hosts&#34;:[&#34;algo-1-8m8ve&#34;],&#34;hyperparameters&#34;:{&#34;dummy_param_1&#34;:&#34;val1&#34;,&#34;dummy_param_2&#34;:&#34;val2&#34;},&#34;input_config_dir&#34;:&#34;/opt/ml/input/config&#34;,&#34;input_data_config&#34;:{},&#34;input_dir&#34;:&#34;/opt/ml/input&#34;,&#34;is_master&#34;:true,&#34;job_name&#34;:&#34;sagemaker-scikit-learn-2022-07-17-15-23-44-284&#34;,&#34;log_level&#34;:20,&#34;master_hostname&#34;:&#34;algo-1-8m8ve&#34;,&#34;model_dir&#34;:&#34;/opt/ml/model&#34;,&#34;module_dir&#34;:&#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-44-284/source/sourcedir.tar.gz&#34;,&#34;module_name&#34;:&#34;train_and_serve&#34;,&#34;network_interface_name&#34;:&#34;eth0&#34;,&#34;num_cpus&#34;:2,&#34;num_gpus&#34;:0,&#34;output_data_dir&#34;:&#34;/opt/ml/output/data&#34;,&#34;output_dir&#34;:&#34;/opt/ml/output&#34;,&#34;output_intermediate_dir&#34;:&#34;/opt/ml/output/intermediate&#34;,&#34;resource_config&#34;:{&#34;current_host&#34;:&#34;algo-1-8m8ve&#34;,&#34;hosts&#34;:[&#34;algo-1-8m8ve&#34;]},&#34;user_entry_point&#34;:&#34;train_and_serve.py&#34;} kc4ahx6e84-algo-1-8m8ve | SM_USER_ARGS=[&#34;--dummy_param_1&#34;,&#34;val1&#34;,&#34;--dummy_param_2&#34;,&#34;val2&#34;] kc4ahx6e84-algo-1-8m8ve | SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate kc4ahx6e84-algo-1-8m8ve | SM_HP_DUMMY_PARAM_1=val1 kc4ahx6e84-algo-1-8m8ve | SM_HP_DUMMY_PARAM_2=val2 kc4ahx6e84-algo-1-8m8ve | PYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages kc4ahx6e84-algo-1-8m8ve | kc4ahx6e84-algo-1-8m8ve | Invoking script with the following command: kc4ahx6e84-algo-1-8m8ve | kc4ahx6e84-algo-1-8m8ve | /miniconda3/bin/python train_and_serve.py --dummy_param_1 val1 --dummy_param_2 val2 kc4ahx6e84-algo-1-8m8ve | kc4ahx6e84-algo-1-8m8ve | kc4ahx6e84-algo-1-8m8ve | *** Hello from the SageMaker script mode*** kc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,657 sagemaker-containers INFO Reporting training SUCCESS kc4ahx6e84-algo-1-8m8ve exited with code 0 Aborting on container exit... ===== Job Complete ===== . . . From the output we can see that our hyperparameters are passed to our training script as command line arguments. This is an important point and we will update our script using this information. . SageMaker SKLearn container environment variables . Let&#39;s now discuss some important environment variables we see in the output. . SM_MODULE_DIR . SM_MODULE_DIR=s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-13-13-05-48-675/source/sourcedir.tar.gz . SM_MODULE_DIR points to a location in the S3 bucket where SageMaker will automatically backup our source code for that particular run. SageMaker will create a separate folder in the default bucket for each new run. The default value is s3://sagemaker-{aws-region}-{aws-id}/{training-job-name}/source/sourcedir.tar.gz . Note: We have used local_code for the SKLean estimator, then why is the source code backed up on the S3 bucket. Should it not be backed on the local system and bypass S3 altogether in local mode? Well, this should have been the default behavior, but it looks like SageMaker SDK is doing it otherwise, and even with the local mode it is using the S3 bucket for keeping source code. You can read more about this behavior in this issue ticket Model repack always uploads data to S3 bucket regardless of local mode settings . SM_MODEL_DIR . SM_MODEL_DIR=/opt/ml/model . SM_MODEL_DIR points to a directory located inside the container. When the training job finishes, the container and its file system will be deleted, except for the /opt/ml/model and /opt/ml/output directories. Use /opt/ml/model to save the trained model artifacts. These artifacts are uploaded to S3 for model hosting. . SM_OUTPUT_DATA_DIR . SM_OUTPUT_DIR=/opt/ml/output . SM_OUTPUT_DIR points to a directory in the container to write output artifacts. Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts. These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts. . SM_CHANNELS . SM_CHANNELS=&#39;[&quot;testing&quot;,&quot;training&quot;]&#39; . A channel is a named input source that training algorithms can consume. You can partition your training data into different logical &quot;channels&quot; when you run training. Depending on your problem, some common channel ideas are: &quot;training&quot;, &quot;testing&quot;, &quot;evaluation&quot; or &quot;images&quot; and &quot;labels&quot;. You can read more about the channels from SageMaker API reference Channel . SM CHANNEL {channel_name} . SM_CHANNEL_TRAIN=&#39;/opt/ml/input/data/train&#39; SM_CHANNEL_TEST=&#39;/opt/ml/input/data/test&#39; . Suppose that you have passed two input channels, &#39;train&#39; and &#39;test&#39;, to the Scikit-learn estimator&#39;s fit() method, the following will be set, following the format SM_CHANNEL_[channel_name]: . SM_CHANNEL_TRAIN: it points to the directory in the container that has the train channel data downloaded | SM_CHANNEL_TEST: Same as above, but for the test channel | . Note that the channel names train and test are the conventions. Still, you can use any name here, and the environment variables will be created accordingly. It is important to know that the SageMaker container automatically downloads the data from the provided input channels and makes them available in the respective local directories once it starts executing. The training script can then load the data from the local container directories. . There are more environment variables available, and you can read about them from Environment variables . Pass input channel to SKLearn estimator . Now that we understand the SKLearn container environment more let&#39;s pass the training data channel to the estimator and see if the data becomes available inside the container directory. . Update our script to list all the files in the SM_CHANNEL_TRAIN directory. . %%writefile $script_file import argparse, os, sys if __name__ == &quot;__main__&quot;: print(&quot; *** Hello from SageMaker script container *** &quot;) training_dir = os.environ.get(&quot;SM_CHANNEL_TRAIN&quot;) dir_list = os.listdir(training_dir) print(&quot;training_dir files list: &quot;, dir_list) . Overwriting ./datasets/2022-07-07-sagemaker-script-mode/src/train_and_serve.py . sk_estimator = SKLearn( entry_point=script_file, role=role, instance_count=1, instance_type=&#39;local&#39;, framework_version=&quot;1.0-1&quot;, hyperparameters={&quot;dummy_param_1&quot;:&quot;val1&quot;,&quot;dummy_param_2&quot;:&quot;val2&quot;}, ) sk_estimator.fit({&quot;train&quot;: f&quot;file://{local_train_path}&quot;}) . Creating wp2g5fxyg1-algo-1-o05g1 ... Creating wp2g5fxyg1-algo-1-o05g1 ... done Attaching to wp2g5fxyg1-algo-1-o05g1 wp2g5fxyg1-algo-1-o05g1 | 2022-07-17 15:23:49,444 sagemaker-containers INFO Imported framework sagemaker_sklearn_container.training wp2g5fxyg1-algo-1-o05g1 | 2022-07-17 15:23:49,447 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) wp2g5fxyg1-algo-1-o05g1 | 2022-07-17 15:23:49,456 sagemaker_sklearn_container.training INFO Invoking user training script. wp2g5fxyg1-algo-1-o05g1 | 2022-07-17 15:23:49,638 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) wp2g5fxyg1-algo-1-o05g1 | 2022-07-17 15:23:49,653 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) wp2g5fxyg1-algo-1-o05g1 | 2022-07-17 15:23:49,667 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) wp2g5fxyg1-algo-1-o05g1 | 2022-07-17 15:23:49,676 sagemaker-training-toolkit INFO Invoking user script wp2g5fxyg1-algo-1-o05g1 | wp2g5fxyg1-algo-1-o05g1 | Training Env: wp2g5fxyg1-algo-1-o05g1 | wp2g5fxyg1-algo-1-o05g1 | { wp2g5fxyg1-algo-1-o05g1 | &#34;additional_framework_parameters&#34;: {}, wp2g5fxyg1-algo-1-o05g1 | &#34;channel_input_dirs&#34;: { wp2g5fxyg1-algo-1-o05g1 | &#34;train&#34;: &#34;/opt/ml/input/data/train&#34; wp2g5fxyg1-algo-1-o05g1 | }, wp2g5fxyg1-algo-1-o05g1 | &#34;current_host&#34;: &#34;algo-1-o05g1&#34;, wp2g5fxyg1-algo-1-o05g1 | &#34;framework_module&#34;: &#34;sagemaker_sklearn_container.training:main&#34;, wp2g5fxyg1-algo-1-o05g1 | &#34;hosts&#34;: [ wp2g5fxyg1-algo-1-o05g1 | &#34;algo-1-o05g1&#34; wp2g5fxyg1-algo-1-o05g1 | ], wp2g5fxyg1-algo-1-o05g1 | &#34;hyperparameters&#34;: { wp2g5fxyg1-algo-1-o05g1 | &#34;dummy_param_1&#34;: &#34;val1&#34;, wp2g5fxyg1-algo-1-o05g1 | &#34;dummy_param_2&#34;: &#34;val2&#34; wp2g5fxyg1-algo-1-o05g1 | }, wp2g5fxyg1-algo-1-o05g1 | &#34;input_config_dir&#34;: &#34;/opt/ml/input/config&#34;, wp2g5fxyg1-algo-1-o05g1 | &#34;input_data_config&#34;: { wp2g5fxyg1-algo-1-o05g1 | &#34;train&#34;: { wp2g5fxyg1-algo-1-o05g1 | &#34;TrainingInputMode&#34;: &#34;File&#34; wp2g5fxyg1-algo-1-o05g1 | } wp2g5fxyg1-algo-1-o05g1 | }, wp2g5fxyg1-algo-1-o05g1 | &#34;input_dir&#34;: &#34;/opt/ml/input&#34;, wp2g5fxyg1-algo-1-o05g1 | &#34;is_master&#34;: true, wp2g5fxyg1-algo-1-o05g1 | &#34;job_name&#34;: &#34;sagemaker-scikit-learn-2022-07-17-15-23-47-051&#34;, wp2g5fxyg1-algo-1-o05g1 | &#34;log_level&#34;: 20, wp2g5fxyg1-algo-1-o05g1 | &#34;master_hostname&#34;: &#34;algo-1-o05g1&#34;, wp2g5fxyg1-algo-1-o05g1 | &#34;model_dir&#34;: &#34;/opt/ml/model&#34;, wp2g5fxyg1-algo-1-o05g1 | &#34;module_dir&#34;: &#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-47-051/source/sourcedir.tar.gz&#34;, wp2g5fxyg1-algo-1-o05g1 | &#34;module_name&#34;: &#34;train_and_serve&#34;, wp2g5fxyg1-algo-1-o05g1 | &#34;network_interface_name&#34;: &#34;eth0&#34;, wp2g5fxyg1-algo-1-o05g1 | &#34;num_cpus&#34;: 2, wp2g5fxyg1-algo-1-o05g1 | &#34;num_gpus&#34;: 0, wp2g5fxyg1-algo-1-o05g1 | &#34;output_data_dir&#34;: &#34;/opt/ml/output/data&#34;, wp2g5fxyg1-algo-1-o05g1 | &#34;output_dir&#34;: &#34;/opt/ml/output&#34;, wp2g5fxyg1-algo-1-o05g1 | &#34;output_intermediate_dir&#34;: &#34;/opt/ml/output/intermediate&#34;, wp2g5fxyg1-algo-1-o05g1 | &#34;resource_config&#34;: { wp2g5fxyg1-algo-1-o05g1 | &#34;current_host&#34;: &#34;algo-1-o05g1&#34;, wp2g5fxyg1-algo-1-o05g1 | &#34;hosts&#34;: [ wp2g5fxyg1-algo-1-o05g1 | &#34;algo-1-o05g1&#34; wp2g5fxyg1-algo-1-o05g1 | ] wp2g5fxyg1-algo-1-o05g1 | }, wp2g5fxyg1-algo-1-o05g1 | &#34;user_entry_point&#34;: &#34;train_and_serve.py&#34; wp2g5fxyg1-algo-1-o05g1 | } wp2g5fxyg1-algo-1-o05g1 | wp2g5fxyg1-algo-1-o05g1 | Environment variables: wp2g5fxyg1-algo-1-o05g1 | wp2g5fxyg1-algo-1-o05g1 | SM_HOSTS=[&#34;algo-1-o05g1&#34;] wp2g5fxyg1-algo-1-o05g1 | SM_NETWORK_INTERFACE_NAME=eth0 wp2g5fxyg1-algo-1-o05g1 | SM_HPS={&#34;dummy_param_1&#34;:&#34;val1&#34;,&#34;dummy_param_2&#34;:&#34;val2&#34;} wp2g5fxyg1-algo-1-o05g1 | SM_USER_ENTRY_POINT=train_and_serve.py wp2g5fxyg1-algo-1-o05g1 | SM_FRAMEWORK_PARAMS={} wp2g5fxyg1-algo-1-o05g1 | SM_RESOURCE_CONFIG={&#34;current_host&#34;:&#34;algo-1-o05g1&#34;,&#34;hosts&#34;:[&#34;algo-1-o05g1&#34;]} wp2g5fxyg1-algo-1-o05g1 | SM_INPUT_DATA_CONFIG={&#34;train&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;}} wp2g5fxyg1-algo-1-o05g1 | SM_OUTPUT_DATA_DIR=/opt/ml/output/data wp2g5fxyg1-algo-1-o05g1 | SM_CHANNELS=[&#34;train&#34;] wp2g5fxyg1-algo-1-o05g1 | SM_CURRENT_HOST=algo-1-o05g1 wp2g5fxyg1-algo-1-o05g1 | SM_MODULE_NAME=train_and_serve wp2g5fxyg1-algo-1-o05g1 | SM_LOG_LEVEL=20 wp2g5fxyg1-algo-1-o05g1 | SM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main wp2g5fxyg1-algo-1-o05g1 | SM_INPUT_DIR=/opt/ml/input wp2g5fxyg1-algo-1-o05g1 | SM_INPUT_CONFIG_DIR=/opt/ml/input/config wp2g5fxyg1-algo-1-o05g1 | SM_OUTPUT_DIR=/opt/ml/output wp2g5fxyg1-algo-1-o05g1 | SM_NUM_CPUS=2 wp2g5fxyg1-algo-1-o05g1 | SM_NUM_GPUS=0 wp2g5fxyg1-algo-1-o05g1 | SM_MODEL_DIR=/opt/ml/model wp2g5fxyg1-algo-1-o05g1 | SM_MODULE_DIR=s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-47-051/source/sourcedir.tar.gz wp2g5fxyg1-algo-1-o05g1 | SM_TRAINING_ENV={&#34;additional_framework_parameters&#34;:{},&#34;channel_input_dirs&#34;:{&#34;train&#34;:&#34;/opt/ml/input/data/train&#34;},&#34;current_host&#34;:&#34;algo-1-o05g1&#34;,&#34;framework_module&#34;:&#34;sagemaker_sklearn_container.training:main&#34;,&#34;hosts&#34;:[&#34;algo-1-o05g1&#34;],&#34;hyperparameters&#34;:{&#34;dummy_param_1&#34;:&#34;val1&#34;,&#34;dummy_param_2&#34;:&#34;val2&#34;},&#34;input_config_dir&#34;:&#34;/opt/ml/input/config&#34;,&#34;input_data_config&#34;:{&#34;train&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;}},&#34;input_dir&#34;:&#34;/opt/ml/input&#34;,&#34;is_master&#34;:true,&#34;job_name&#34;:&#34;sagemaker-scikit-learn-2022-07-17-15-23-47-051&#34;,&#34;log_level&#34;:20,&#34;master_hostname&#34;:&#34;algo-1-o05g1&#34;,&#34;model_dir&#34;:&#34;/opt/ml/model&#34;,&#34;module_dir&#34;:&#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-47-051/source/sourcedir.tar.gz&#34;,&#34;module_name&#34;:&#34;train_and_serve&#34;,&#34;network_interface_name&#34;:&#34;eth0&#34;,&#34;num_cpus&#34;:2,&#34;num_gpus&#34;:0,&#34;output_data_dir&#34;:&#34;/opt/ml/output/data&#34;,&#34;output_dir&#34;:&#34;/opt/ml/output&#34;,&#34;output_intermediate_dir&#34;:&#34;/opt/ml/output/intermediate&#34;,&#34;resource_config&#34;:{&#34;current_host&#34;:&#34;algo-1-o05g1&#34;,&#34;hosts&#34;:[&#34;algo-1-o05g1&#34;]},&#34;user_entry_point&#34;:&#34;train_and_serve.py&#34;} wp2g5fxyg1-algo-1-o05g1 | SM_USER_ARGS=[&#34;--dummy_param_1&#34;,&#34;val1&#34;,&#34;--dummy_param_2&#34;,&#34;val2&#34;] wp2g5fxyg1-algo-1-o05g1 | SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate wp2g5fxyg1-algo-1-o05g1 | SM_CHANNEL_TRAIN=/opt/ml/input/data/train wp2g5fxyg1-algo-1-o05g1 | SM_HP_DUMMY_PARAM_1=val1 wp2g5fxyg1-algo-1-o05g1 | SM_HP_DUMMY_PARAM_2=val2 wp2g5fxyg1-algo-1-o05g1 | PYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages wp2g5fxyg1-algo-1-o05g1 | wp2g5fxyg1-algo-1-o05g1 | Invoking script with the following command: wp2g5fxyg1-algo-1-o05g1 | wp2g5fxyg1-algo-1-o05g1 | /miniconda3/bin/python train_and_serve.py --dummy_param_1 val1 --dummy_param_2 val2 wp2g5fxyg1-algo-1-o05g1 | wp2g5fxyg1-algo-1-o05g1 | wp2g5fxyg1-algo-1-o05g1 | *** Hello from SageMaker script container *** wp2g5fxyg1-algo-1-o05g1 | training_dir files list: [&#39;train.csv&#39;] wp2g5fxyg1-algo-1-o05g1 | 2022-07-17 15:23:49,715 sagemaker-containers INFO Reporting training SUCCESS wp2g5fxyg1-algo-1-o05g1 exited with code 0 Aborting on container exit... ===== Job Complete ===== . . . From the output, we can see that train.csv, which was in our local environment, is now available inside the container on path SM_CHANNEL_TRAIN=/opt/ml/input/data/train. . Let&#39;s also test the same with our training data on the S3 bucket. . sk_estimator = SKLearn( entry_point=script_file, role=role, instance_count=1, instance_type=&#39;local&#39;, framework_version=&quot;1.0-1&quot;, hyperparameters={&quot;dummy_param_1&quot;:&quot;val1&quot;,&quot;dummy_param_2&quot;:&quot;val2&quot;}, ) sk_estimator.fit({&quot;train&quot;: s3_train_uri}) . Creating 7ao431iiu5-algo-1-9jid1 ... Creating 7ao431iiu5-algo-1-9jid1 ... done Attaching to 7ao431iiu5-algo-1-9jid1 7ao431iiu5-algo-1-9jid1 | 2022-07-17 15:23:53,073 sagemaker-containers INFO Imported framework sagemaker_sklearn_container.training 7ao431iiu5-algo-1-9jid1 | 2022-07-17 15:23:53,079 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) 7ao431iiu5-algo-1-9jid1 | 2022-07-17 15:23:53,094 sagemaker_sklearn_container.training INFO Invoking user training script. 7ao431iiu5-algo-1-9jid1 | 2022-07-17 15:23:53,335 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) 7ao431iiu5-algo-1-9jid1 | 2022-07-17 15:23:53,348 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) 7ao431iiu5-algo-1-9jid1 | 2022-07-17 15:23:53,360 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) 7ao431iiu5-algo-1-9jid1 | 2022-07-17 15:23:53,369 sagemaker-training-toolkit INFO Invoking user script 7ao431iiu5-algo-1-9jid1 | 7ao431iiu5-algo-1-9jid1 | Training Env: 7ao431iiu5-algo-1-9jid1 | 7ao431iiu5-algo-1-9jid1 | { 7ao431iiu5-algo-1-9jid1 | &#34;additional_framework_parameters&#34;: {}, 7ao431iiu5-algo-1-9jid1 | &#34;channel_input_dirs&#34;: { 7ao431iiu5-algo-1-9jid1 | &#34;train&#34;: &#34;/opt/ml/input/data/train&#34; 7ao431iiu5-algo-1-9jid1 | }, 7ao431iiu5-algo-1-9jid1 | &#34;current_host&#34;: &#34;algo-1-9jid1&#34;, 7ao431iiu5-algo-1-9jid1 | &#34;framework_module&#34;: &#34;sagemaker_sklearn_container.training:main&#34;, 7ao431iiu5-algo-1-9jid1 | &#34;hosts&#34;: [ 7ao431iiu5-algo-1-9jid1 | &#34;algo-1-9jid1&#34; 7ao431iiu5-algo-1-9jid1 | ], 7ao431iiu5-algo-1-9jid1 | &#34;hyperparameters&#34;: { 7ao431iiu5-algo-1-9jid1 | &#34;dummy_param_1&#34;: &#34;val1&#34;, 7ao431iiu5-algo-1-9jid1 | &#34;dummy_param_2&#34;: &#34;val2&#34; 7ao431iiu5-algo-1-9jid1 | }, 7ao431iiu5-algo-1-9jid1 | &#34;input_config_dir&#34;: &#34;/opt/ml/input/config&#34;, 7ao431iiu5-algo-1-9jid1 | &#34;input_data_config&#34;: { 7ao431iiu5-algo-1-9jid1 | &#34;train&#34;: { 7ao431iiu5-algo-1-9jid1 | &#34;TrainingInputMode&#34;: &#34;File&#34; 7ao431iiu5-algo-1-9jid1 | } 7ao431iiu5-algo-1-9jid1 | }, 7ao431iiu5-algo-1-9jid1 | &#34;input_dir&#34;: &#34;/opt/ml/input&#34;, 7ao431iiu5-algo-1-9jid1 | &#34;is_master&#34;: true, 7ao431iiu5-algo-1-9jid1 | &#34;job_name&#34;: &#34;sagemaker-scikit-learn-2022-07-17-15-23-50-077&#34;, 7ao431iiu5-algo-1-9jid1 | &#34;log_level&#34;: 20, 7ao431iiu5-algo-1-9jid1 | &#34;master_hostname&#34;: &#34;algo-1-9jid1&#34;, 7ao431iiu5-algo-1-9jid1 | &#34;model_dir&#34;: &#34;/opt/ml/model&#34;, 7ao431iiu5-algo-1-9jid1 | &#34;module_dir&#34;: &#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-50-077/source/sourcedir.tar.gz&#34;, 7ao431iiu5-algo-1-9jid1 | &#34;module_name&#34;: &#34;train_and_serve&#34;, 7ao431iiu5-algo-1-9jid1 | &#34;network_interface_name&#34;: &#34;eth0&#34;, 7ao431iiu5-algo-1-9jid1 | &#34;num_cpus&#34;: 2, 7ao431iiu5-algo-1-9jid1 | &#34;num_gpus&#34;: 0, 7ao431iiu5-algo-1-9jid1 | &#34;output_data_dir&#34;: &#34;/opt/ml/output/data&#34;, 7ao431iiu5-algo-1-9jid1 | &#34;output_dir&#34;: &#34;/opt/ml/output&#34;, 7ao431iiu5-algo-1-9jid1 | &#34;output_intermediate_dir&#34;: &#34;/opt/ml/output/intermediate&#34;, 7ao431iiu5-algo-1-9jid1 | &#34;resource_config&#34;: { 7ao431iiu5-algo-1-9jid1 | &#34;current_host&#34;: &#34;algo-1-9jid1&#34;, 7ao431iiu5-algo-1-9jid1 | &#34;hosts&#34;: [ 7ao431iiu5-algo-1-9jid1 | &#34;algo-1-9jid1&#34; 7ao431iiu5-algo-1-9jid1 | ] 7ao431iiu5-algo-1-9jid1 | }, 7ao431iiu5-algo-1-9jid1 | &#34;user_entry_point&#34;: &#34;train_and_serve.py&#34; 7ao431iiu5-algo-1-9jid1 | } 7ao431iiu5-algo-1-9jid1 | 7ao431iiu5-algo-1-9jid1 | Environment variables: 7ao431iiu5-algo-1-9jid1 | 7ao431iiu5-algo-1-9jid1 | SM_HOSTS=[&#34;algo-1-9jid1&#34;] 7ao431iiu5-algo-1-9jid1 | SM_NETWORK_INTERFACE_NAME=eth0 7ao431iiu5-algo-1-9jid1 | SM_HPS={&#34;dummy_param_1&#34;:&#34;val1&#34;,&#34;dummy_param_2&#34;:&#34;val2&#34;} 7ao431iiu5-algo-1-9jid1 | SM_USER_ENTRY_POINT=train_and_serve.py 7ao431iiu5-algo-1-9jid1 | SM_FRAMEWORK_PARAMS={} 7ao431iiu5-algo-1-9jid1 | SM_RESOURCE_CONFIG={&#34;current_host&#34;:&#34;algo-1-9jid1&#34;,&#34;hosts&#34;:[&#34;algo-1-9jid1&#34;]} 7ao431iiu5-algo-1-9jid1 | SM_INPUT_DATA_CONFIG={&#34;train&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;}} 7ao431iiu5-algo-1-9jid1 | SM_OUTPUT_DATA_DIR=/opt/ml/output/data 7ao431iiu5-algo-1-9jid1 | SM_CHANNELS=[&#34;train&#34;] 7ao431iiu5-algo-1-9jid1 | SM_CURRENT_HOST=algo-1-9jid1 7ao431iiu5-algo-1-9jid1 | SM_MODULE_NAME=train_and_serve 7ao431iiu5-algo-1-9jid1 | SM_LOG_LEVEL=20 7ao431iiu5-algo-1-9jid1 | SM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main 7ao431iiu5-algo-1-9jid1 | SM_INPUT_DIR=/opt/ml/input 7ao431iiu5-algo-1-9jid1 | SM_INPUT_CONFIG_DIR=/opt/ml/input/config 7ao431iiu5-algo-1-9jid1 | SM_OUTPUT_DIR=/opt/ml/output 7ao431iiu5-algo-1-9jid1 | SM_NUM_CPUS=2 7ao431iiu5-algo-1-9jid1 | SM_NUM_GPUS=0 7ao431iiu5-algo-1-9jid1 | SM_MODEL_DIR=/opt/ml/model 7ao431iiu5-algo-1-9jid1 | SM_MODULE_DIR=s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-50-077/source/sourcedir.tar.gz 7ao431iiu5-algo-1-9jid1 | SM_TRAINING_ENV={&#34;additional_framework_parameters&#34;:{},&#34;channel_input_dirs&#34;:{&#34;train&#34;:&#34;/opt/ml/input/data/train&#34;},&#34;current_host&#34;:&#34;algo-1-9jid1&#34;,&#34;framework_module&#34;:&#34;sagemaker_sklearn_container.training:main&#34;,&#34;hosts&#34;:[&#34;algo-1-9jid1&#34;],&#34;hyperparameters&#34;:{&#34;dummy_param_1&#34;:&#34;val1&#34;,&#34;dummy_param_2&#34;:&#34;val2&#34;},&#34;input_config_dir&#34;:&#34;/opt/ml/input/config&#34;,&#34;input_data_config&#34;:{&#34;train&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;}},&#34;input_dir&#34;:&#34;/opt/ml/input&#34;,&#34;is_master&#34;:true,&#34;job_name&#34;:&#34;sagemaker-scikit-learn-2022-07-17-15-23-50-077&#34;,&#34;log_level&#34;:20,&#34;master_hostname&#34;:&#34;algo-1-9jid1&#34;,&#34;model_dir&#34;:&#34;/opt/ml/model&#34;,&#34;module_dir&#34;:&#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-50-077/source/sourcedir.tar.gz&#34;,&#34;module_name&#34;:&#34;train_and_serve&#34;,&#34;network_interface_name&#34;:&#34;eth0&#34;,&#34;num_cpus&#34;:2,&#34;num_gpus&#34;:0,&#34;output_data_dir&#34;:&#34;/opt/ml/output/data&#34;,&#34;output_dir&#34;:&#34;/opt/ml/output&#34;,&#34;output_intermediate_dir&#34;:&#34;/opt/ml/output/intermediate&#34;,&#34;resource_config&#34;:{&#34;current_host&#34;:&#34;algo-1-9jid1&#34;,&#34;hosts&#34;:[&#34;algo-1-9jid1&#34;]},&#34;user_entry_point&#34;:&#34;train_and_serve.py&#34;} 7ao431iiu5-algo-1-9jid1 | SM_USER_ARGS=[&#34;--dummy_param_1&#34;,&#34;val1&#34;,&#34;--dummy_param_2&#34;,&#34;val2&#34;] 7ao431iiu5-algo-1-9jid1 | SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate 7ao431iiu5-algo-1-9jid1 | SM_CHANNEL_TRAIN=/opt/ml/input/data/train 7ao431iiu5-algo-1-9jid1 | SM_HP_DUMMY_PARAM_1=val1 7ao431iiu5-algo-1-9jid1 | SM_HP_DUMMY_PARAM_2=val2 7ao431iiu5-algo-1-9jid1 | PYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages 7ao431iiu5-algo-1-9jid1 | 7ao431iiu5-algo-1-9jid1 | Invoking script with the following command: 7ao431iiu5-algo-1-9jid1 | 7ao431iiu5-algo-1-9jid1 | /miniconda3/bin/python train_and_serve.py --dummy_param_1 val1 --dummy_param_2 val2 7ao431iiu5-algo-1-9jid1 | 7ao431iiu5-algo-1-9jid1 | 7ao431iiu5-algo-1-9jid1 | *** Hello from SageMaker script container *** 7ao431iiu5-algo-1-9jid1 | training_dir files list: [&#39;train.csv&#39;] 7ao431iiu5-algo-1-9jid1 | 2022-07-17 15:23:53,409 sagemaker-containers INFO Reporting training SUCCESS 7ao431iiu5-algo-1-9jid1 exited with code 0 Aborting on container exit... ===== Job Complete ===== . . Again the results are the same. SageMaker will download the data from the S3 bucket and make it available in the container. In the environment variables section we also learned that two directories are special /opt/ml/model and /opt/ml/output. Container environment variables SM_MODEL_DIR and SM_OUTPUT_DATA_DIR point to them, respectively. Whatever artifacts we put on them will be stored on the S3 bucket when the training job finishes. &quot;SM_MODEL_DIR&quot; is for trained models, and &quot;SM_OUTPUT_DATA_DIR&quot; is for other artifacts like logs, graphs, plots, results, etc. Let&#39;s update our training script and put some dummy data in these directories. Once the job is complete, we will verify the stored artifacts on the S3 bucket. . %%writefile $script_file import argparse, os, sys if __name__ == &quot;__main__&quot;: print(&quot; *** Hello from SageMaker script container *** &quot;) # list files in SM_CHANNEL_TRAIN training_dir = os.environ.get(&quot;SM_CHANNEL_TRAIN&quot;) dir_list = os.listdir(training_dir) print(&quot;training_dir files list: &quot;, dir_list) # write dummy model file to SM_MODEL_DIR sm_model_dir = os.environ.get(&quot;SM_MODEL_DIR&quot;) with open(f&quot;{sm_model_dir}/dummy-model.txt&quot;, &quot;w&quot;) as f: f.write(&quot;this is a dummy model&quot;) # write dummy artifact file to SM_OUTPUT_DATA_DIR sm_output_data_dir = os.environ.get(&quot;SM_OUTPUT_DATA_DIR&quot;) with open(f&quot;{sm_output_data_dir}/dummy-output-data.txt&quot;, &quot;w&quot;) as f: f.write(&quot;this is a dummy output data&quot;) . Overwriting ./datasets/2022-07-07-sagemaker-script-mode/src/train_and_serve.py . sk_estimator = SKLearn( entry_point=script_file, role=role, instance_count=1, instance_type=&#39;local&#39;, framework_version=&quot;1.0-1&quot;, hyperparameters={&quot;dummy_param_1&quot;:&quot;val1&quot;,&quot;dummy_param_2&quot;:&quot;val2&quot;}, ) sk_estimator.fit({&quot;train&quot;: s3_train_uri}) . Creating c30093mavu-algo-1-p87y9 ... Creating c30093mavu-algo-1-p87y9 ... done Attaching to c30093mavu-algo-1-p87y9 c30093mavu-algo-1-p87y9 | 2022-07-17 15:23:56,051 sagemaker-containers INFO Imported framework sagemaker_sklearn_container.training c30093mavu-algo-1-p87y9 | 2022-07-17 15:23:56,055 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) c30093mavu-algo-1-p87y9 | 2022-07-17 15:23:56,065 sagemaker_sklearn_container.training INFO Invoking user training script. c30093mavu-algo-1-p87y9 | 2022-07-17 15:23:56,251 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) c30093mavu-algo-1-p87y9 | 2022-07-17 15:23:56,267 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) c30093mavu-algo-1-p87y9 | 2022-07-17 15:23:56,281 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) c30093mavu-algo-1-p87y9 | 2022-07-17 15:23:56,291 sagemaker-training-toolkit INFO Invoking user script c30093mavu-algo-1-p87y9 | c30093mavu-algo-1-p87y9 | Training Env: c30093mavu-algo-1-p87y9 | c30093mavu-algo-1-p87y9 | { c30093mavu-algo-1-p87y9 | &#34;additional_framework_parameters&#34;: {}, c30093mavu-algo-1-p87y9 | &#34;channel_input_dirs&#34;: { c30093mavu-algo-1-p87y9 | &#34;train&#34;: &#34;/opt/ml/input/data/train&#34; c30093mavu-algo-1-p87y9 | }, c30093mavu-algo-1-p87y9 | &#34;current_host&#34;: &#34;algo-1-p87y9&#34;, c30093mavu-algo-1-p87y9 | &#34;framework_module&#34;: &#34;sagemaker_sklearn_container.training:main&#34;, c30093mavu-algo-1-p87y9 | &#34;hosts&#34;: [ c30093mavu-algo-1-p87y9 | &#34;algo-1-p87y9&#34; c30093mavu-algo-1-p87y9 | ], c30093mavu-algo-1-p87y9 | &#34;hyperparameters&#34;: { c30093mavu-algo-1-p87y9 | &#34;dummy_param_1&#34;: &#34;val1&#34;, c30093mavu-algo-1-p87y9 | &#34;dummy_param_2&#34;: &#34;val2&#34; c30093mavu-algo-1-p87y9 | }, c30093mavu-algo-1-p87y9 | &#34;input_config_dir&#34;: &#34;/opt/ml/input/config&#34;, c30093mavu-algo-1-p87y9 | &#34;input_data_config&#34;: { c30093mavu-algo-1-p87y9 | &#34;train&#34;: { c30093mavu-algo-1-p87y9 | &#34;TrainingInputMode&#34;: &#34;File&#34; c30093mavu-algo-1-p87y9 | } c30093mavu-algo-1-p87y9 | }, c30093mavu-algo-1-p87y9 | &#34;input_dir&#34;: &#34;/opt/ml/input&#34;, c30093mavu-algo-1-p87y9 | &#34;is_master&#34;: true, c30093mavu-algo-1-p87y9 | &#34;job_name&#34;: &#34;sagemaker-scikit-learn-2022-07-17-15-23-53-775&#34;, c30093mavu-algo-1-p87y9 | &#34;log_level&#34;: 20, c30093mavu-algo-1-p87y9 | &#34;master_hostname&#34;: &#34;algo-1-p87y9&#34;, c30093mavu-algo-1-p87y9 | &#34;model_dir&#34;: &#34;/opt/ml/model&#34;, c30093mavu-algo-1-p87y9 | &#34;module_dir&#34;: &#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-53-775/source/sourcedir.tar.gz&#34;, c30093mavu-algo-1-p87y9 | &#34;module_name&#34;: &#34;train_and_serve&#34;, c30093mavu-algo-1-p87y9 | &#34;network_interface_name&#34;: &#34;eth0&#34;, c30093mavu-algo-1-p87y9 | &#34;num_cpus&#34;: 2, c30093mavu-algo-1-p87y9 | &#34;num_gpus&#34;: 0, c30093mavu-algo-1-p87y9 | &#34;output_data_dir&#34;: &#34;/opt/ml/output/data&#34;, c30093mavu-algo-1-p87y9 | &#34;output_dir&#34;: &#34;/opt/ml/output&#34;, c30093mavu-algo-1-p87y9 | &#34;output_intermediate_dir&#34;: &#34;/opt/ml/output/intermediate&#34;, c30093mavu-algo-1-p87y9 | &#34;resource_config&#34;: { c30093mavu-algo-1-p87y9 | &#34;current_host&#34;: &#34;algo-1-p87y9&#34;, c30093mavu-algo-1-p87y9 | &#34;hosts&#34;: [ c30093mavu-algo-1-p87y9 | &#34;algo-1-p87y9&#34; c30093mavu-algo-1-p87y9 | ] c30093mavu-algo-1-p87y9 | }, c30093mavu-algo-1-p87y9 | &#34;user_entry_point&#34;: &#34;train_and_serve.py&#34; c30093mavu-algo-1-p87y9 | } c30093mavu-algo-1-p87y9 | c30093mavu-algo-1-p87y9 | Environment variables: c30093mavu-algo-1-p87y9 | c30093mavu-algo-1-p87y9 | SM_HOSTS=[&#34;algo-1-p87y9&#34;] c30093mavu-algo-1-p87y9 | SM_NETWORK_INTERFACE_NAME=eth0 c30093mavu-algo-1-p87y9 | SM_HPS={&#34;dummy_param_1&#34;:&#34;val1&#34;,&#34;dummy_param_2&#34;:&#34;val2&#34;} c30093mavu-algo-1-p87y9 | SM_USER_ENTRY_POINT=train_and_serve.py c30093mavu-algo-1-p87y9 | SM_FRAMEWORK_PARAMS={} c30093mavu-algo-1-p87y9 | SM_RESOURCE_CONFIG={&#34;current_host&#34;:&#34;algo-1-p87y9&#34;,&#34;hosts&#34;:[&#34;algo-1-p87y9&#34;]} c30093mavu-algo-1-p87y9 | SM_INPUT_DATA_CONFIG={&#34;train&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;}} c30093mavu-algo-1-p87y9 | SM_OUTPUT_DATA_DIR=/opt/ml/output/data c30093mavu-algo-1-p87y9 | SM_CHANNELS=[&#34;train&#34;] c30093mavu-algo-1-p87y9 | SM_CURRENT_HOST=algo-1-p87y9 c30093mavu-algo-1-p87y9 | SM_MODULE_NAME=train_and_serve c30093mavu-algo-1-p87y9 | SM_LOG_LEVEL=20 c30093mavu-algo-1-p87y9 | SM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main c30093mavu-algo-1-p87y9 | SM_INPUT_DIR=/opt/ml/input c30093mavu-algo-1-p87y9 | SM_INPUT_CONFIG_DIR=/opt/ml/input/config c30093mavu-algo-1-p87y9 | SM_OUTPUT_DIR=/opt/ml/output c30093mavu-algo-1-p87y9 | SM_NUM_CPUS=2 c30093mavu-algo-1-p87y9 | SM_NUM_GPUS=0 c30093mavu-algo-1-p87y9 | SM_MODEL_DIR=/opt/ml/model c30093mavu-algo-1-p87y9 | SM_MODULE_DIR=s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-53-775/source/sourcedir.tar.gz c30093mavu-algo-1-p87y9 | SM_TRAINING_ENV={&#34;additional_framework_parameters&#34;:{},&#34;channel_input_dirs&#34;:{&#34;train&#34;:&#34;/opt/ml/input/data/train&#34;},&#34;current_host&#34;:&#34;algo-1-p87y9&#34;,&#34;framework_module&#34;:&#34;sagemaker_sklearn_container.training:main&#34;,&#34;hosts&#34;:[&#34;algo-1-p87y9&#34;],&#34;hyperparameters&#34;:{&#34;dummy_param_1&#34;:&#34;val1&#34;,&#34;dummy_param_2&#34;:&#34;val2&#34;},&#34;input_config_dir&#34;:&#34;/opt/ml/input/config&#34;,&#34;input_data_config&#34;:{&#34;train&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;}},&#34;input_dir&#34;:&#34;/opt/ml/input&#34;,&#34;is_master&#34;:true,&#34;job_name&#34;:&#34;sagemaker-scikit-learn-2022-07-17-15-23-53-775&#34;,&#34;log_level&#34;:20,&#34;master_hostname&#34;:&#34;algo-1-p87y9&#34;,&#34;model_dir&#34;:&#34;/opt/ml/model&#34;,&#34;module_dir&#34;:&#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-53-775/source/sourcedir.tar.gz&#34;,&#34;module_name&#34;:&#34;train_and_serve&#34;,&#34;network_interface_name&#34;:&#34;eth0&#34;,&#34;num_cpus&#34;:2,&#34;num_gpus&#34;:0,&#34;output_data_dir&#34;:&#34;/opt/ml/output/data&#34;,&#34;output_dir&#34;:&#34;/opt/ml/output&#34;,&#34;output_intermediate_dir&#34;:&#34;/opt/ml/output/intermediate&#34;,&#34;resource_config&#34;:{&#34;current_host&#34;:&#34;algo-1-p87y9&#34;,&#34;hosts&#34;:[&#34;algo-1-p87y9&#34;]},&#34;user_entry_point&#34;:&#34;train_and_serve.py&#34;} c30093mavu-algo-1-p87y9 | SM_USER_ARGS=[&#34;--dummy_param_1&#34;,&#34;val1&#34;,&#34;--dummy_param_2&#34;,&#34;val2&#34;] c30093mavu-algo-1-p87y9 | SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate c30093mavu-algo-1-p87y9 | SM_CHANNEL_TRAIN=/opt/ml/input/data/train c30093mavu-algo-1-p87y9 | SM_HP_DUMMY_PARAM_1=val1 c30093mavu-algo-1-p87y9 | SM_HP_DUMMY_PARAM_2=val2 c30093mavu-algo-1-p87y9 | PYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages c30093mavu-algo-1-p87y9 | c30093mavu-algo-1-p87y9 | Invoking script with the following command: c30093mavu-algo-1-p87y9 | c30093mavu-algo-1-p87y9 | /miniconda3/bin/python train_and_serve.py --dummy_param_1 val1 --dummy_param_2 val2 c30093mavu-algo-1-p87y9 | c30093mavu-algo-1-p87y9 | c30093mavu-algo-1-p87y9 | *** Hello from SageMaker script container *** c30093mavu-algo-1-p87y9 | training_dir files list: [&#39;train.csv&#39;] c30093mavu-algo-1-p87y9 | 2022-07-17 15:23:56,328 sagemaker-containers INFO Reporting training SUCCESS c30093mavu-algo-1-p87y9 exited with code 0 Aborting on container exit... . Failed to delete: /tmp/tmpuwvrle8_/algo-1-p87y9 Please remove it manually. . ===== Job Complete ===== . . Our training job is now complete. Let us check the S3 bucket to see if our dummy model and other artifacts are present. . First, we need the S3 URI for these artifacts. For our dummy model (from SM_MODEL_DIR), we can use our estimator object to get the URI. . model_data = sk_estimator.model_data model_data . &#39;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-53-775/model.tar.gz&#39; . Let&#39;s download model_data from S3 to a local directory for verification. For this create a local /tmp to store these downloaded files. . local_tmp_path = local_path + &quot;/tmp&quot; print(local_tmp_path) # create the local &#39;/tmp&#39; directory Path(local_tmp_path).mkdir(parents=True, exist_ok=True) . ./datasets/2022-07-07-sagemaker-script-mode/tmp . We will use SageMaker S3Downloader object to download the model file. . from sagemaker.s3 import S3Downloader S3Downloader.download( s3_uri=model_data, local_path=local_tmp_path, sagemaker_session=session ) . File is downloaded. Let&#39;s uncompress it to verify the model file. . !tar -xzvf $local_tmp_path/model.tar.gz -C $local_tmp_path . dummy-model.txt . Yes, the &quot;dummy-model.txt&quot; file is present. This tells us that SageMaker will automatically upload the files from the model directory (SM_MODEL_DIR) to the S3 bucket. Let&#39;s do the same for the output data directory (SM_OUTPUT_DATA_DIR). There is no direct way to get the S3 URI from the estimator object for the output data directory. But we can prepare it ourselves. So let&#39;s do that next. . print(&quot;estimator.output_path: &quot;, sk_estimator.output_path) print(&quot;estimator.latest_training_job.name: &quot;, sk_estimator.latest_training_job.name) . estimator.output_path: s3://sagemaker-us-east-1-801598032724/ estimator.latest_training_job.name: sagemaker-scikit-learn-2022-07-17-15-23-53-775 . def get_s3_output_uri(estimator): return estimator.output_path + estimator.latest_training_job.name get_s3_output_uri(sk_estimator) . &#39;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-53-775&#39; . # S3 URI for output data artifacts s3_output_uri = get_s3_output_uri(sk_estimator) + &#39;/output.tar.gz&#39; s3_output_uri . &#39;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-53-775/output.tar.gz&#39; . # S3 URI for model artifact. We have already veirifed it. s3_model_uri = get_s3_output_uri(sk_estimator) + &#39;/model.tar.gz&#39; s3_model_uri . &#39;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-53-775/model.tar.gz&#39; . # S3 URI for source code s3_source_uri = get_s3_output_uri(sk_estimator) + &#39;/source/sourcedir.tar.gz&#39; s3_source_uri . &#39;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-53-775/source/sourcedir.tar.gz&#39; . Let&#39;s download these artifacts to our local &#39;/tmp&#39; directory for verification. . !aws s3 cp $s3_output_uri $local_tmp_path !aws s3 cp $s3_source_uri $local_tmp_path . download: s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-53-775/output.tar.gz to datasets/2022-07-07-sagemaker-script-mode/tmp/output.tar.gz download: s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-53-775/source/sourcedir.tar.gz to datasets/2022-07-07-sagemaker-script-mode/tmp/sourcedir.tar.gz . # extract the output data files from &#39;output.tar.gz&#39; !tar -xzvf $local_tmp_path/output.tar.gz -C $local_tmp_path . data/ data/dummy-output-data.txt success . # extract the source code files from &#39;sourcedir.tar.gz&#39; !tar -xzvf $local_tmp_path/sourcedir.tar.gz -C $local_tmp_path . train_and_serve.py . Summary till now . Let&#39;s summarize what we have learned till now. . We can use SageMaker SKLearn local mode to test our code in a local environment | SKLearn container executes our provided script with the command /miniconda3/bin/python train_and_server.py | Hyperparameters passed to the container are passed to our script as command line arguments | Data from input channels will be downloaded by the container and made available for our script to load and process | &#39;/opt/ml/model&#39; and &#39;/opt/ml/output&#39; directories are special. Anything stored on them will be automatically backed up on the S3 bucket when the job finishes. These directories are defined in the container environment variables &#39;SM_MODEL_DIR&#39; and &#39;SM_OUTPUT_DATA_DIR&#39;, respectively. SM_MODEL_DIR should be used to write model artifacts. SM_OUTPUT_DATA_DIR should be used to write any other supporting artifact. | . Let&#39;s use this knowledge to update our script to train a RandomForrestClassifier on the Iris flower dataset. . # cleanup /tmp directory before moving to next section !rm -r $local_tmp_path/* . Prepare training script for RandomForestClassifier . Let&#39;s update our training script to train a scikit-learn random forest classifier model on the iris data set. The script will read training and testing data from input data channel directories and trains a classifier on it. It will then save the model to the model directory and validation results (&#39;y_pred.csv&#39;) to the output data directory. Notice that we have also parsed container environment variables as command line arguments. It makes sense for hyperparameters (&#39;--estimators&#39;) because we know they will be passed to the script as command line parameters. For other environment variables (e.g. &#39;SM_MODEL_DIR&#39;), we have checked first if they are given as command line arguments. If they are, then we parse them to get the values. Otherwise, we read their values from the environment. This is done so we can test our script locally from the command line without setting the environment variables. . %%writefile $script_file import argparse, os import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn import metrics import joblib if __name__ == &quot;__main__&quot;: # Pass in environment variables and hyperparameters parser = argparse.ArgumentParser() # Hyperparameters parser.add_argument(&quot;--estimators&quot;, type=int, default=15) # sm_model_dir: model artifacts stored here after training # sm-channel-train: input training data location # sm-channel-test: input test data location # sm-output-data-dir: output artifacts location parser.add_argument(&quot;--sm-model-dir&quot;, type=str, default=os.environ.get(&quot;SM_MODEL_DIR&quot;)) parser.add_argument(&quot;--sm-channel-train&quot;, type=str, default=os.environ.get(&quot;SM_CHANNEL_TRAIN&quot;)) parser.add_argument(&quot;--sm-channel-test&quot;, type=str, default=os.environ.get(&quot;SM_CHANNEL_TEST&quot;)) parser.add_argument(&quot;--sm-output-data-dir&quot;, type=str, default=os.environ.get(&quot;SM_OUTPUT_DATA_DIR&quot;)) args, _ = parser.parse_known_args() print(&quot;command line arguments: &quot;, args) estimators = args.estimators sm_model_dir = args.sm_model_dir training_dir = args.sm_channel_train testing_dir = args.sm_channel_test output_data_dir = args.sm_output_data_dir print(f&quot;training_dir: {training_dir}&quot;) print(f&quot;training_dir files list: {os.listdir(training_dir)}&quot;) print(f&quot;testing_dir: {testing_dir}&quot;) print(f&quot;testing_dir files list: {os.listdir(testing_dir)}&quot;) print(f&quot;sm_model_dir: {sm_model_dir}&quot;) print(f&quot;output_data_dir: {output_data_dir}&quot;) # Read in data df_train = pd.read_csv(training_dir + &quot;/train.csv&quot;, sep=&quot;,&quot;) df_test = pd.read_csv(testing_dir + &quot;/test.csv&quot;, sep=&quot;,&quot;) # Preprocess data X_train = df_train.drop([&quot;class&quot;, &quot;class_cat&quot;], axis=1) y_train = df_train[&quot;class_cat&quot;] X_test = df_test.drop([&quot;class&quot;, &quot;class_cat&quot;], axis=1) y_test = df_test[&quot;class_cat&quot;] print(f&quot;X_train.shape: {X_train.shape}&quot;) print(f&quot;y_train.shape: {y_train.shape}&quot;) print(f&quot;X_train.shape: {X_test.shape}&quot;) print(f&quot;y_train.shape: {y_test.shape}&quot;) sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) # Build model regressor = RandomForestClassifier(n_estimators=estimators) regressor.fit(X_train, y_train) y_pred = regressor.predict(X_test) # Save the model joblib.dump(regressor, sm_model_dir + &quot;/model.joblib&quot;) # Save the results pd.DataFrame(y_pred).to_csv(output_data_dir + &quot;/y_pred.csv&quot;) . Overwriting ./datasets/2022-07-07-sagemaker-script-mode/src/train_and_serve.py . Now give proper execution rights to the script. . !chmod +x $script_file . Let&#39;s test this script locally before passing it to the SKLearn estimator. We will invoke this script from a command line and pass the required parameters similar to how an estimator container will execute it. For testing this script, we need to pass four directory paths: . sm-model-dir This will point to a directory where our script will store the trained model. We can point it to &#39;/tmp&#39; directory for test purposes | sm-channel-train This will point to a directory containing training data. We already have it as &#39;local_train_path&#39; | sm-channel-test This will point to a directory containing test data. We also have it as &#39;local_test_path&#39; | sm-output-data-dir This will point to a directory where our script will store other artifacts. We can also point it to &#39;/tmp&#39; directory for test purposes | . Once the script is successfully run, we will find the trained model file &#39;model.joblib&#39; and &#39;y_pred.csv&#39; in the &#39;/tmp&#39; directory. . !python3 $script_file --sm-model-dir $local_tmp_path --sm-channel-train $local_train_path --sm-channel-test $local_test_path --sm-output-data-dir $local_tmp_path --estimators 10 . command line arguments: Namespace(estimators=10, sm_channel_test=&#39;./datasets/2022-07-07-sagemaker-script-mode/test&#39;, sm_channel_train=&#39;./datasets/2022-07-07-sagemaker-script-mode/train&#39;, sm_model_dir=&#39;./datasets/2022-07-07-sagemaker-script-mode/tmp&#39;, sm_output_data_dir=&#39;./datasets/2022-07-07-sagemaker-script-mode/tmp&#39;) training_dir: ./datasets/2022-07-07-sagemaker-script-mode/train training_dir files list: [&#39;train.csv&#39;] testing_dir: ./datasets/2022-07-07-sagemaker-script-mode/test testing_dir files list: [&#39;test.csv&#39;] sm_model_dir: ./datasets/2022-07-07-sagemaker-script-mode/tmp output_data_dir: ./datasets/2022-07-07-sagemaker-script-mode/tmp X_train.shape: (120, 4) y_train.shape: (120,) X_train.shape: (30, 4) y_train.shape: (30,) . . Let&#39;s check the local &#39;/tmp&#39; directory for artifacts. . !ls $local_tmp_path . model.joblib y_pred.csv . Now that we have test our script and it is working as expected, let&#39;s pass it to SKLean container. . sk_estimator = SKLearn( entry_point=script_file, role=role, instance_count=1, instance_type=&#39;local&#39;, framework_version=&quot;1.0-1&quot;, hyperparameters={&quot;estimators&quot;:10}, ) sk_estimator.fit({&quot;train&quot;: s3_train_uri, &quot;test&quot;: s3_test_uri}) . Creating aer2alr1w1-algo-1-10beq ... Creating aer2alr1w1-algo-1-10beq ... done Attaching to aer2alr1w1-algo-1-10beq aer2alr1w1-algo-1-10beq | 2022-07-17 15:24:06,011 sagemaker-containers INFO Imported framework sagemaker_sklearn_container.training aer2alr1w1-algo-1-10beq | 2022-07-17 15:24:06,015 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) aer2alr1w1-algo-1-10beq | 2022-07-17 15:24:06,024 sagemaker_sklearn_container.training INFO Invoking user training script. aer2alr1w1-algo-1-10beq | 2022-07-17 15:24:06,226 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) aer2alr1w1-algo-1-10beq | 2022-07-17 15:24:06,239 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) aer2alr1w1-algo-1-10beq | 2022-07-17 15:24:06,251 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) aer2alr1w1-algo-1-10beq | 2022-07-17 15:24:06,260 sagemaker-training-toolkit INFO Invoking user script aer2alr1w1-algo-1-10beq | aer2alr1w1-algo-1-10beq | Training Env: aer2alr1w1-algo-1-10beq | aer2alr1w1-algo-1-10beq | { aer2alr1w1-algo-1-10beq | &#34;additional_framework_parameters&#34;: {}, aer2alr1w1-algo-1-10beq | &#34;channel_input_dirs&#34;: { aer2alr1w1-algo-1-10beq | &#34;train&#34;: &#34;/opt/ml/input/data/train&#34;, aer2alr1w1-algo-1-10beq | &#34;test&#34;: &#34;/opt/ml/input/data/test&#34; aer2alr1w1-algo-1-10beq | }, aer2alr1w1-algo-1-10beq | &#34;current_host&#34;: &#34;algo-1-10beq&#34;, aer2alr1w1-algo-1-10beq | &#34;framework_module&#34;: &#34;sagemaker_sklearn_container.training:main&#34;, aer2alr1w1-algo-1-10beq | &#34;hosts&#34;: [ aer2alr1w1-algo-1-10beq | &#34;algo-1-10beq&#34; aer2alr1w1-algo-1-10beq | ], aer2alr1w1-algo-1-10beq | &#34;hyperparameters&#34;: { aer2alr1w1-algo-1-10beq | &#34;estimators&#34;: 10 aer2alr1w1-algo-1-10beq | }, aer2alr1w1-algo-1-10beq | &#34;input_config_dir&#34;: &#34;/opt/ml/input/config&#34;, aer2alr1w1-algo-1-10beq | &#34;input_data_config&#34;: { aer2alr1w1-algo-1-10beq | &#34;train&#34;: { aer2alr1w1-algo-1-10beq | &#34;TrainingInputMode&#34;: &#34;File&#34; aer2alr1w1-algo-1-10beq | }, aer2alr1w1-algo-1-10beq | &#34;test&#34;: { aer2alr1w1-algo-1-10beq | &#34;TrainingInputMode&#34;: &#34;File&#34; aer2alr1w1-algo-1-10beq | } aer2alr1w1-algo-1-10beq | }, aer2alr1w1-algo-1-10beq | &#34;input_dir&#34;: &#34;/opt/ml/input&#34;, aer2alr1w1-algo-1-10beq | &#34;is_master&#34;: true, aer2alr1w1-algo-1-10beq | &#34;job_name&#34;: &#34;sagemaker-scikit-learn-2022-07-17-15-24-03-447&#34;, aer2alr1w1-algo-1-10beq | &#34;log_level&#34;: 20, aer2alr1w1-algo-1-10beq | &#34;master_hostname&#34;: &#34;algo-1-10beq&#34;, aer2alr1w1-algo-1-10beq | &#34;model_dir&#34;: &#34;/opt/ml/model&#34;, aer2alr1w1-algo-1-10beq | &#34;module_dir&#34;: &#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-24-03-447/source/sourcedir.tar.gz&#34;, aer2alr1w1-algo-1-10beq | &#34;module_name&#34;: &#34;train_and_serve&#34;, aer2alr1w1-algo-1-10beq | &#34;network_interface_name&#34;: &#34;eth0&#34;, aer2alr1w1-algo-1-10beq | &#34;num_cpus&#34;: 2, aer2alr1w1-algo-1-10beq | &#34;num_gpus&#34;: 0, aer2alr1w1-algo-1-10beq | &#34;output_data_dir&#34;: &#34;/opt/ml/output/data&#34;, aer2alr1w1-algo-1-10beq | &#34;output_dir&#34;: &#34;/opt/ml/output&#34;, aer2alr1w1-algo-1-10beq | &#34;output_intermediate_dir&#34;: &#34;/opt/ml/output/intermediate&#34;, aer2alr1w1-algo-1-10beq | &#34;resource_config&#34;: { aer2alr1w1-algo-1-10beq | &#34;current_host&#34;: &#34;algo-1-10beq&#34;, aer2alr1w1-algo-1-10beq | &#34;hosts&#34;: [ aer2alr1w1-algo-1-10beq | &#34;algo-1-10beq&#34; aer2alr1w1-algo-1-10beq | ] aer2alr1w1-algo-1-10beq | }, aer2alr1w1-algo-1-10beq | &#34;user_entry_point&#34;: &#34;train_and_serve.py&#34; aer2alr1w1-algo-1-10beq | } aer2alr1w1-algo-1-10beq | aer2alr1w1-algo-1-10beq | Environment variables: aer2alr1w1-algo-1-10beq | aer2alr1w1-algo-1-10beq | SM_HOSTS=[&#34;algo-1-10beq&#34;] aer2alr1w1-algo-1-10beq | SM_NETWORK_INTERFACE_NAME=eth0 aer2alr1w1-algo-1-10beq | SM_HPS={&#34;estimators&#34;:10} aer2alr1w1-algo-1-10beq | SM_USER_ENTRY_POINT=train_and_serve.py aer2alr1w1-algo-1-10beq | SM_FRAMEWORK_PARAMS={} aer2alr1w1-algo-1-10beq | SM_RESOURCE_CONFIG={&#34;current_host&#34;:&#34;algo-1-10beq&#34;,&#34;hosts&#34;:[&#34;algo-1-10beq&#34;]} aer2alr1w1-algo-1-10beq | SM_INPUT_DATA_CONFIG={&#34;test&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;},&#34;train&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;}} aer2alr1w1-algo-1-10beq | SM_OUTPUT_DATA_DIR=/opt/ml/output/data aer2alr1w1-algo-1-10beq | SM_CHANNELS=[&#34;test&#34;,&#34;train&#34;] aer2alr1w1-algo-1-10beq | SM_CURRENT_HOST=algo-1-10beq aer2alr1w1-algo-1-10beq | SM_MODULE_NAME=train_and_serve aer2alr1w1-algo-1-10beq | SM_LOG_LEVEL=20 aer2alr1w1-algo-1-10beq | SM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main aer2alr1w1-algo-1-10beq | SM_INPUT_DIR=/opt/ml/input aer2alr1w1-algo-1-10beq | SM_INPUT_CONFIG_DIR=/opt/ml/input/config aer2alr1w1-algo-1-10beq | SM_OUTPUT_DIR=/opt/ml/output aer2alr1w1-algo-1-10beq | SM_NUM_CPUS=2 aer2alr1w1-algo-1-10beq | SM_NUM_GPUS=0 aer2alr1w1-algo-1-10beq | SM_MODEL_DIR=/opt/ml/model aer2alr1w1-algo-1-10beq | SM_MODULE_DIR=s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-24-03-447/source/sourcedir.tar.gz aer2alr1w1-algo-1-10beq | SM_TRAINING_ENV={&#34;additional_framework_parameters&#34;:{},&#34;channel_input_dirs&#34;:{&#34;test&#34;:&#34;/opt/ml/input/data/test&#34;,&#34;train&#34;:&#34;/opt/ml/input/data/train&#34;},&#34;current_host&#34;:&#34;algo-1-10beq&#34;,&#34;framework_module&#34;:&#34;sagemaker_sklearn_container.training:main&#34;,&#34;hosts&#34;:[&#34;algo-1-10beq&#34;],&#34;hyperparameters&#34;:{&#34;estimators&#34;:10},&#34;input_config_dir&#34;:&#34;/opt/ml/input/config&#34;,&#34;input_data_config&#34;:{&#34;test&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;},&#34;train&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;}},&#34;input_dir&#34;:&#34;/opt/ml/input&#34;,&#34;is_master&#34;:true,&#34;job_name&#34;:&#34;sagemaker-scikit-learn-2022-07-17-15-24-03-447&#34;,&#34;log_level&#34;:20,&#34;master_hostname&#34;:&#34;algo-1-10beq&#34;,&#34;model_dir&#34;:&#34;/opt/ml/model&#34;,&#34;module_dir&#34;:&#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-24-03-447/source/sourcedir.tar.gz&#34;,&#34;module_name&#34;:&#34;train_and_serve&#34;,&#34;network_interface_name&#34;:&#34;eth0&#34;,&#34;num_cpus&#34;:2,&#34;num_gpus&#34;:0,&#34;output_data_dir&#34;:&#34;/opt/ml/output/data&#34;,&#34;output_dir&#34;:&#34;/opt/ml/output&#34;,&#34;output_intermediate_dir&#34;:&#34;/opt/ml/output/intermediate&#34;,&#34;resource_config&#34;:{&#34;current_host&#34;:&#34;algo-1-10beq&#34;,&#34;hosts&#34;:[&#34;algo-1-10beq&#34;]},&#34;user_entry_point&#34;:&#34;train_and_serve.py&#34;} aer2alr1w1-algo-1-10beq | SM_USER_ARGS=[&#34;--estimators&#34;,&#34;10&#34;] aer2alr1w1-algo-1-10beq | SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate aer2alr1w1-algo-1-10beq | SM_CHANNEL_TRAIN=/opt/ml/input/data/train aer2alr1w1-algo-1-10beq | SM_CHANNEL_TEST=/opt/ml/input/data/test aer2alr1w1-algo-1-10beq | SM_HP_ESTIMATORS=10 aer2alr1w1-algo-1-10beq | PYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages aer2alr1w1-algo-1-10beq | aer2alr1w1-algo-1-10beq | Invoking script with the following command: aer2alr1w1-algo-1-10beq | aer2alr1w1-algo-1-10beq | /miniconda3/bin/python train_and_serve.py --estimators 10 aer2alr1w1-algo-1-10beq | aer2alr1w1-algo-1-10beq | aer2alr1w1-algo-1-10beq | command line arguments: Namespace(estimators=10, sm_channel_test=&#39;/opt/ml/input/data/test&#39;, sm_channel_train=&#39;/opt/ml/input/data/train&#39;, sm_model_dir=&#39;/opt/ml/model&#39;, sm_output_data_dir=&#39;/opt/ml/output/data&#39;) aer2alr1w1-algo-1-10beq | training_dir: /opt/ml/input/data/train aer2alr1w1-algo-1-10beq | training_dir files list: [&#39;train.csv&#39;] aer2alr1w1-algo-1-10beq | testing_dir: /opt/ml/input/data/test aer2alr1w1-algo-1-10beq | testing_dir files list: [&#39;test.csv&#39;] aer2alr1w1-algo-1-10beq | sm_model_dir: /opt/ml/model aer2alr1w1-algo-1-10beq | output_data_dir: /opt/ml/output/data aer2alr1w1-algo-1-10beq | X_train.shape: (120, 4) aer2alr1w1-algo-1-10beq | y_train.shape: (120,) aer2alr1w1-algo-1-10beq | X_train.shape: (30, 4) aer2alr1w1-algo-1-10beq | y_train.shape: (30,) aer2alr1w1-algo-1-10beq | 2022-07-17 15:24:07,286 sagemaker-containers INFO Reporting training SUCCESS aer2alr1w1-algo-1-10beq exited with code 0 Aborting on container exit... . Failed to delete: /tmp/tmp0yb8k7nj/algo-1-10beq Please remove it manually. . ===== Job Complete ===== . . # cleanup /tmp directory before moving to next section !rm -r $local_tmp_path/* . Passing custom libraries and dependencies to SKLean container . We have successfully trained our classifier but assume we have an additional task. One of your colleagues has created a library that takes the confusion matrix array and plots it with seaborn visualization library. You have been told to use this custom library with the training script and save the confusion matrix plot to the output data directory. . Let&#39;s prepare code for this custom library to take an array and return a confusion matrix plot from seaborn. . custom_library_path = local_path + &quot;/my_custom_library&quot; custom_library_file = custom_library_path + &quot;/seaborn_confusion_matrix.py&quot; print(f&quot;custom_library_path: {custom_library_path}&quot;) print(f&quot;custom_library_file: {custom_library_file}&quot;) # make sure the path exists Path(custom_library_path).mkdir(parents=True, exist_ok=True) . custom_library_path: ./datasets/2022-07-07-sagemaker-script-mode/my_custom_library custom_library_file: ./datasets/2022-07-07-sagemaker-script-mode/my_custom_library/seaborn_confusion_matrix.py . Now the code to plot the confusion matrix. . %%writefile $custom_library_file import seaborn as sns import numpy as np import argparse, os def save_confusion_matrix(cf_matrix, path=&quot;./&quot;): sns_plot = sns.heatmap(cf_matrix, annot=True) sns_plot.figure.savefig(path + &quot;/output_cm.png&quot;) if __name__ == &quot;__main__&quot;: parser = argparse.ArgumentParser() parser.add_argument(&quot;--path&quot;, type=str, default=&quot;./&quot;) args, _ = parser.parse_known_args() path = args.path dummy_cm = np.array([[23, 5], [3, 30]]) save_confusion_matrix(dummy_cm, path) . Overwriting ./datasets/2022-07-07-sagemaker-script-mode/my_custom_library/seaborn_confusion_matrix.py . Convert the directory containing seaborn code into a Python package directory. . %%writefile $custom_library_path/__init__.py from .seaborn_confusion_matrix import * . Overwriting ./datasets/2022-07-07-sagemaker-script-mode/my_custom_library/__init__.py . Our custom library has a dependency on the seaborn Python package. So let&#39;s create &#39;requirements.txt&#39; and put all our dependencies in it. Later it will be passed to the SKLean container to install them during initialization. . %%writefile $script_path/requirements.txt seaborn==0.11.2 . Overwriting ./datasets/2022-07-07-sagemaker-script-mode/src/requirements.txt . Let&#39;s first test this library in our local environment. It should plot a dummy confusion matrix in local /tmp directory. . # intall the dependiencies first !pip install -r $script_path/requirements.txt . Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com Requirement already satisfied: seaborn==0.11.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from -r ./datasets/2022-07-07-sagemaker-script-mode/src/requirements.txt (line 2)) (0.11.2) Requirement already satisfied: numpy&gt;=1.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r ./datasets/2022-07-07-sagemaker-script-mode/src/requirements.txt (line 2)) (1.20.3) Requirement already satisfied: matplotlib&gt;=2.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r ./datasets/2022-07-07-sagemaker-script-mode/src/requirements.txt (line 2)) (3.5.0) Requirement already satisfied: scipy&gt;=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r ./datasets/2022-07-07-sagemaker-script-mode/src/requirements.txt (line 2)) (1.5.3) Requirement already satisfied: pandas&gt;=0.23 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r ./datasets/2022-07-07-sagemaker-script-mode/src/requirements.txt (line 2)) (1.3.4) Requirement already satisfied: fonttools&gt;=4.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r ./datasets/2022-07-07-sagemaker-script-mode/src/requirements.txt (line 2)) (4.28.2) Requirement already satisfied: python-dateutil&gt;=2.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r ./datasets/2022-07-07-sagemaker-script-mode/src/requirements.txt (line 2)) (2.8.2) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r ./datasets/2022-07-07-sagemaker-script-mode/src/requirements.txt (line 2)) (1.3.2) Requirement already satisfied: pillow&gt;=6.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r ./datasets/2022-07-07-sagemaker-script-mode/src/requirements.txt (line 2)) (9.0.1) Requirement already satisfied: packaging&gt;=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r ./datasets/2022-07-07-sagemaker-script-mode/src/requirements.txt (line 2)) (21.3) Requirement already satisfied: pyparsing&gt;=2.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r ./datasets/2022-07-07-sagemaker-script-mode/src/requirements.txt (line 2)) (3.0.6) Requirement already satisfied: cycler&gt;=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r ./datasets/2022-07-07-sagemaker-script-mode/src/requirements.txt (line 2)) (0.11.0) Requirement already satisfied: pytz&gt;=2017.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas&gt;=0.23-&gt;seaborn==0.11.2-&gt;-r ./datasets/2022-07-07-sagemaker-script-mode/src/requirements.txt (line 2)) (2021.3) Requirement already satisfied: six&gt;=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r ./datasets/2022-07-07-sagemaker-script-mode/src/requirements.txt (line 2)) (1.16.0) WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available. You should consider upgrading via the &#39;/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip&#39; command. . . # test the custom library !python3 $custom_library_file --path $local_tmp_path . Matplotlib is building the font cache; this may take a moment. . # verify the custom library output from the /tmp directory !ls $local_tmp_path . output_cm.png . So our custom library code works. Let&#39;s update our script to use it. . %%writefile $script_file import argparse, os import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import confusion_matrix import joblib from my_custom_library import save_confusion_matrix if __name__ == &quot;__main__&quot;: # Pass in environment variables and hyperparameters parser = argparse.ArgumentParser() # Hyperparameters parser.add_argument(&quot;--estimators&quot;, type=int, default=15) # sm_model_dir: model artifacts stored here after training # sm-channel-train: input training data location # sm-channel-test: input test data location # sm-output-data-dir: output artifacts location parser.add_argument(&quot;--sm-model-dir&quot;, type=str, default=os.environ.get(&quot;SM_MODEL_DIR&quot;)) parser.add_argument(&quot;--sm-channel-train&quot;, type=str, default=os.environ.get(&quot;SM_CHANNEL_TRAIN&quot;)) parser.add_argument(&quot;--sm-channel-test&quot;, type=str, default=os.environ.get(&quot;SM_CHANNEL_TEST&quot;)) parser.add_argument(&quot;--sm-output-data-dir&quot;, type=str, default=os.environ.get(&quot;SM_OUTPUT_DATA_DIR&quot;)) args, _ = parser.parse_known_args() print(&quot;command line arguments: &quot;, args) estimators = args.estimators sm_model_dir = args.sm_model_dir training_dir = args.sm_channel_train testing_dir = args.sm_channel_test output_data_dir = args.sm_output_data_dir print(f&quot;training_dir: {training_dir}&quot;) print(f&quot;training_dir files list: {os.listdir(training_dir)}&quot;) print(f&quot;testing_dir: {testing_dir}&quot;) print(f&quot;testing_dir files list: {os.listdir(testing_dir)}&quot;) print(f&quot;sm_model_dir: {sm_model_dir}&quot;) print(f&quot;output_data_dir: {output_data_dir}&quot;) # Read in data df_train = pd.read_csv(training_dir + &quot;/train.csv&quot;, sep=&quot;,&quot;) df_test = pd.read_csv(testing_dir + &quot;/test.csv&quot;, sep=&quot;,&quot;) # Preprocess data X_train = df_train.drop([&quot;class&quot;, &quot;class_cat&quot;], axis=1) y_train = df_train[&quot;class_cat&quot;] X_test = df_test.drop([&quot;class&quot;, &quot;class_cat&quot;], axis=1) y_test = df_test[&quot;class_cat&quot;] print(f&quot;X_train.shape: {X_train.shape}&quot;) print(f&quot;y_train.shape: {y_train.shape}&quot;) print(f&quot;X_train.shape: {X_test.shape}&quot;) print(f&quot;y_train.shape: {y_test.shape}&quot;) sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) # Build model regressor = RandomForestClassifier(n_estimators=estimators) regressor.fit(X_train, y_train) y_pred = regressor.predict(X_test) # Save the model joblib.dump(regressor, sm_model_dir + &quot;/model.joblib&quot;) # Save the results pd.DataFrame(y_pred).to_csv(output_data_dir + &quot;/y_pred.csv&quot;) # save the confusion matrix cf_matrix = confusion_matrix(y_test, y_pred) save_confusion_matrix(cf_matrix, output_data_dir) # print sm_model_dir info print(f&quot;sm_model_dir: {sm_model_dir}&quot;) print(f&quot;sm_model_dir files list: {os.listdir(sm_model_dir)}&quot;) # print output_data_dir info print(f&quot;output_data_dir: {output_data_dir}&quot;) print(f&quot;output_data_dir files list: {os.listdir(output_data_dir)}&quot;) . Overwriting ./datasets/2022-07-07-sagemaker-script-mode/src/train_and_serve.py . Finally, all the ingredients are ready. Let&#39;s run our script from the SKLean container. . In the next cell, you can see that we have passed two extra parameters to the estimator. . source_dir this path points to the directory with the entry_point script train_and_serve.py and requirements.txt. If any requirements.txt file is in this directory, the estimator will pick that and install those packages in the container during initialization. | dependencies this points to a list of dependencies (custom libraries) that we want available in the container. | . Our local directory structure is shown below. . local_path/ ├── my_custom_library/ │ ├── seaborn_confusion_matrix.py │ └── __init__.py └── src/ ├── train_and_serve.py └── requirements.txt . sk_estimator = SKLearn( entry_point=script_file_name, source_dir=script_path, dependencies=[custom_library_path], role=role, instance_count=1, instance_type=&#39;local&#39;, framework_version=&quot;1.0-1&quot;, hyperparameters={&quot;estimators&quot;:10}, ) sk_estimator.fit({&quot;train&quot;: s3_train_uri, &quot;test&quot;: s3_test_uri}) . Creating xm0kutxos7-algo-1-8yrs9 ... Creating xm0kutxos7-algo-1-8yrs9 ... done Attaching to xm0kutxos7-algo-1-8yrs9 xm0kutxos7-algo-1-8yrs9 | 2022-07-17 15:24:24,458 sagemaker-containers INFO Imported framework sagemaker_sklearn_container.training xm0kutxos7-algo-1-8yrs9 | 2022-07-17 15:24:24,462 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) xm0kutxos7-algo-1-8yrs9 | 2022-07-17 15:24:24,472 sagemaker_sklearn_container.training INFO Invoking user training script. xm0kutxos7-algo-1-8yrs9 | 2022-07-17 15:24:24,661 sagemaker-training-toolkit INFO Installing dependencies from requirements.txt: xm0kutxos7-algo-1-8yrs9 | /miniconda3/bin/python -m pip install -r requirements.txt xm0kutxos7-algo-1-8yrs9 | Collecting seaborn==0.11.2 xm0kutxos7-algo-1-8yrs9 | Downloading seaborn-0.11.2-py3-none-any.whl (292 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 292.8/292.8 kB 5.1 MB/s eta 0:00:0000:01eta -:--:-- xm0kutxos7-algo-1-8yrs9 | Requirement already satisfied: pandas&gt;=0.23 in /miniconda3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.1.3) xm0kutxos7-algo-1-8yrs9 | Collecting matplotlib&gt;=2.2 xm0kutxos7-algo-1-8yrs9 | Downloading matplotlib-3.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/11.3 MB 31.0 MB/s eta 0:00:0000:0100:01:--:-- xm0kutxos7-algo-1-8yrs9 | Requirement already satisfied: numpy&gt;=1.15 in /miniconda3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.21.0) xm0kutxos7-algo-1-8yrs9 | Requirement already satisfied: scipy&gt;=1.0 in /miniconda3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.5.3) xm0kutxos7-algo-1-8yrs9 | Collecting kiwisolver&gt;=1.0.1 xm0kutxos7-algo-1-8yrs9 | Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 18.6 MB/s eta 0:00:00:00:01ta -:--:-- xm0kutxos7-algo-1-8yrs9 | Collecting cycler&gt;=0.10 xm0kutxos7-algo-1-8yrs9 | Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB) xm0kutxos7-algo-1-8yrs9 | Collecting fonttools&gt;=4.22.0 xm0kutxos7-algo-1-8yrs9 | Downloading fonttools-4.34.4-py3-none-any.whl (944 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 944.1/944.1 kB 102.1 MB/s eta 0:00:0031m? eta -:--:-- xm0kutxos7-algo-1-8yrs9 | Collecting packaging&gt;=20.0 xm0kutxos7-algo-1-8yrs9 | Downloading packaging-21.3-py3-none-any.whl (40 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 11.9 MB/s eta 0:00:001m? eta -:--:-- xm0kutxos7-algo-1-8yrs9 | Requirement already satisfied: pillow&gt;=6.2.0 in /miniconda3/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (9.1.1) xm0kutxos7-algo-1-8yrs9 | Collecting pyparsing&gt;=2.2.1 xm0kutxos7-algo-1-8yrs9 | Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.3/98.3 kB 24.8 MB/s eta 0:00:001m? eta -:--:-- xm0kutxos7-algo-1-8yrs9 | Requirement already satisfied: python-dateutil&gt;=2.7 in /miniconda3/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (2.8.1) xm0kutxos7-algo-1-8yrs9 | Requirement already satisfied: pytz&gt;=2017.2 in /miniconda3/lib/python3.8/site-packages (from pandas&gt;=0.23-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (2022.1) xm0kutxos7-algo-1-8yrs9 | Requirement already satisfied: six&gt;=1.5 in /miniconda3/lib/python3.8/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.15.0) xm0kutxos7-algo-1-8yrs9 | Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, packaging, matplotlib, seaborn xm0kutxos7-algo-1-8yrs9 | Successfully installed cycler-0.11.0 fonttools-4.34.4 kiwisolver-1.4.4 matplotlib-3.5.2 packaging-21.3 pyparsing-3.0.9 seaborn-0.11.2 xm0kutxos7-algo-1-8yrs9 | WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv xm0kutxos7-algo-1-8yrs9 | 2022-07-17 15:24:30,839 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) xm0kutxos7-algo-1-8yrs9 | 2022-07-17 15:24:30,859 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) xm0kutxos7-algo-1-8yrs9 | 2022-07-17 15:24:30,879 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) xm0kutxos7-algo-1-8yrs9 | 2022-07-17 15:24:30,894 sagemaker-training-toolkit INFO Invoking user script xm0kutxos7-algo-1-8yrs9 | xm0kutxos7-algo-1-8yrs9 | Training Env: xm0kutxos7-algo-1-8yrs9 | xm0kutxos7-algo-1-8yrs9 | { xm0kutxos7-algo-1-8yrs9 | &#34;additional_framework_parameters&#34;: {}, xm0kutxos7-algo-1-8yrs9 | &#34;channel_input_dirs&#34;: { xm0kutxos7-algo-1-8yrs9 | &#34;train&#34;: &#34;/opt/ml/input/data/train&#34;, xm0kutxos7-algo-1-8yrs9 | &#34;test&#34;: &#34;/opt/ml/input/data/test&#34; xm0kutxos7-algo-1-8yrs9 | }, xm0kutxos7-algo-1-8yrs9 | &#34;current_host&#34;: &#34;algo-1-8yrs9&#34;, xm0kutxos7-algo-1-8yrs9 | &#34;framework_module&#34;: &#34;sagemaker_sklearn_container.training:main&#34;, xm0kutxos7-algo-1-8yrs9 | &#34;hosts&#34;: [ xm0kutxos7-algo-1-8yrs9 | &#34;algo-1-8yrs9&#34; xm0kutxos7-algo-1-8yrs9 | ], xm0kutxos7-algo-1-8yrs9 | &#34;hyperparameters&#34;: { xm0kutxos7-algo-1-8yrs9 | &#34;estimators&#34;: 10 xm0kutxos7-algo-1-8yrs9 | }, xm0kutxos7-algo-1-8yrs9 | &#34;input_config_dir&#34;: &#34;/opt/ml/input/config&#34;, xm0kutxos7-algo-1-8yrs9 | &#34;input_data_config&#34;: { xm0kutxos7-algo-1-8yrs9 | &#34;train&#34;: { xm0kutxos7-algo-1-8yrs9 | &#34;TrainingInputMode&#34;: &#34;File&#34; xm0kutxos7-algo-1-8yrs9 | }, xm0kutxos7-algo-1-8yrs9 | &#34;test&#34;: { xm0kutxos7-algo-1-8yrs9 | &#34;TrainingInputMode&#34;: &#34;File&#34; xm0kutxos7-algo-1-8yrs9 | } xm0kutxos7-algo-1-8yrs9 | }, xm0kutxos7-algo-1-8yrs9 | &#34;input_dir&#34;: &#34;/opt/ml/input&#34;, xm0kutxos7-algo-1-8yrs9 | &#34;is_master&#34;: true, xm0kutxos7-algo-1-8yrs9 | &#34;job_name&#34;: &#34;sagemaker-scikit-learn-2022-07-17-15-24-22-270&#34;, xm0kutxos7-algo-1-8yrs9 | &#34;log_level&#34;: 20, xm0kutxos7-algo-1-8yrs9 | &#34;master_hostname&#34;: &#34;algo-1-8yrs9&#34;, xm0kutxos7-algo-1-8yrs9 | &#34;model_dir&#34;: &#34;/opt/ml/model&#34;, xm0kutxos7-algo-1-8yrs9 | &#34;module_dir&#34;: &#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-24-22-270/source/sourcedir.tar.gz&#34;, xm0kutxos7-algo-1-8yrs9 | &#34;module_name&#34;: &#34;train_and_serve&#34;, xm0kutxos7-algo-1-8yrs9 | &#34;network_interface_name&#34;: &#34;eth0&#34;, xm0kutxos7-algo-1-8yrs9 | &#34;num_cpus&#34;: 2, xm0kutxos7-algo-1-8yrs9 | &#34;num_gpus&#34;: 0, xm0kutxos7-algo-1-8yrs9 | &#34;output_data_dir&#34;: &#34;/opt/ml/output/data&#34;, xm0kutxos7-algo-1-8yrs9 | &#34;output_dir&#34;: &#34;/opt/ml/output&#34;, xm0kutxos7-algo-1-8yrs9 | &#34;output_intermediate_dir&#34;: &#34;/opt/ml/output/intermediate&#34;, xm0kutxos7-algo-1-8yrs9 | &#34;resource_config&#34;: { xm0kutxos7-algo-1-8yrs9 | &#34;current_host&#34;: &#34;algo-1-8yrs9&#34;, xm0kutxos7-algo-1-8yrs9 | &#34;hosts&#34;: [ xm0kutxos7-algo-1-8yrs9 | &#34;algo-1-8yrs9&#34; xm0kutxos7-algo-1-8yrs9 | ] xm0kutxos7-algo-1-8yrs9 | }, xm0kutxos7-algo-1-8yrs9 | &#34;user_entry_point&#34;: &#34;train_and_serve.py&#34; xm0kutxos7-algo-1-8yrs9 | } xm0kutxos7-algo-1-8yrs9 | xm0kutxos7-algo-1-8yrs9 | Environment variables: xm0kutxos7-algo-1-8yrs9 | xm0kutxos7-algo-1-8yrs9 | SM_HOSTS=[&#34;algo-1-8yrs9&#34;] xm0kutxos7-algo-1-8yrs9 | SM_NETWORK_INTERFACE_NAME=eth0 xm0kutxos7-algo-1-8yrs9 | SM_HPS={&#34;estimators&#34;:10} xm0kutxos7-algo-1-8yrs9 | SM_USER_ENTRY_POINT=train_and_serve.py xm0kutxos7-algo-1-8yrs9 | SM_FRAMEWORK_PARAMS={} xm0kutxos7-algo-1-8yrs9 | SM_RESOURCE_CONFIG={&#34;current_host&#34;:&#34;algo-1-8yrs9&#34;,&#34;hosts&#34;:[&#34;algo-1-8yrs9&#34;]} xm0kutxos7-algo-1-8yrs9 | SM_INPUT_DATA_CONFIG={&#34;test&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;},&#34;train&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;}} xm0kutxos7-algo-1-8yrs9 | SM_OUTPUT_DATA_DIR=/opt/ml/output/data xm0kutxos7-algo-1-8yrs9 | SM_CHANNELS=[&#34;test&#34;,&#34;train&#34;] xm0kutxos7-algo-1-8yrs9 | SM_CURRENT_HOST=algo-1-8yrs9 xm0kutxos7-algo-1-8yrs9 | SM_MODULE_NAME=train_and_serve xm0kutxos7-algo-1-8yrs9 | SM_LOG_LEVEL=20 xm0kutxos7-algo-1-8yrs9 | SM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main xm0kutxos7-algo-1-8yrs9 | SM_INPUT_DIR=/opt/ml/input xm0kutxos7-algo-1-8yrs9 | SM_INPUT_CONFIG_DIR=/opt/ml/input/config xm0kutxos7-algo-1-8yrs9 | SM_OUTPUT_DIR=/opt/ml/output xm0kutxos7-algo-1-8yrs9 | SM_NUM_CPUS=2 xm0kutxos7-algo-1-8yrs9 | SM_NUM_GPUS=0 xm0kutxos7-algo-1-8yrs9 | SM_MODEL_DIR=/opt/ml/model xm0kutxos7-algo-1-8yrs9 | SM_MODULE_DIR=s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-24-22-270/source/sourcedir.tar.gz xm0kutxos7-algo-1-8yrs9 | SM_TRAINING_ENV={&#34;additional_framework_parameters&#34;:{},&#34;channel_input_dirs&#34;:{&#34;test&#34;:&#34;/opt/ml/input/data/test&#34;,&#34;train&#34;:&#34;/opt/ml/input/data/train&#34;},&#34;current_host&#34;:&#34;algo-1-8yrs9&#34;,&#34;framework_module&#34;:&#34;sagemaker_sklearn_container.training:main&#34;,&#34;hosts&#34;:[&#34;algo-1-8yrs9&#34;],&#34;hyperparameters&#34;:{&#34;estimators&#34;:10},&#34;input_config_dir&#34;:&#34;/opt/ml/input/config&#34;,&#34;input_data_config&#34;:{&#34;test&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;},&#34;train&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;}},&#34;input_dir&#34;:&#34;/opt/ml/input&#34;,&#34;is_master&#34;:true,&#34;job_name&#34;:&#34;sagemaker-scikit-learn-2022-07-17-15-24-22-270&#34;,&#34;log_level&#34;:20,&#34;master_hostname&#34;:&#34;algo-1-8yrs9&#34;,&#34;model_dir&#34;:&#34;/opt/ml/model&#34;,&#34;module_dir&#34;:&#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-24-22-270/source/sourcedir.tar.gz&#34;,&#34;module_name&#34;:&#34;train_and_serve&#34;,&#34;network_interface_name&#34;:&#34;eth0&#34;,&#34;num_cpus&#34;:2,&#34;num_gpus&#34;:0,&#34;output_data_dir&#34;:&#34;/opt/ml/output/data&#34;,&#34;output_dir&#34;:&#34;/opt/ml/output&#34;,&#34;output_intermediate_dir&#34;:&#34;/opt/ml/output/intermediate&#34;,&#34;resource_config&#34;:{&#34;current_host&#34;:&#34;algo-1-8yrs9&#34;,&#34;hosts&#34;:[&#34;algo-1-8yrs9&#34;]},&#34;user_entry_point&#34;:&#34;train_and_serve.py&#34;} xm0kutxos7-algo-1-8yrs9 | SM_USER_ARGS=[&#34;--estimators&#34;,&#34;10&#34;] xm0kutxos7-algo-1-8yrs9 | SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate xm0kutxos7-algo-1-8yrs9 | SM_CHANNEL_TRAIN=/opt/ml/input/data/train xm0kutxos7-algo-1-8yrs9 | SM_CHANNEL_TEST=/opt/ml/input/data/test xm0kutxos7-algo-1-8yrs9 | SM_HP_ESTIMATORS=10 xm0kutxos7-algo-1-8yrs9 | PYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages xm0kutxos7-algo-1-8yrs9 | xm0kutxos7-algo-1-8yrs9 | Invoking script with the following command: xm0kutxos7-algo-1-8yrs9 | xm0kutxos7-algo-1-8yrs9 | /miniconda3/bin/python train_and_serve.py --estimators 10 xm0kutxos7-algo-1-8yrs9 | xm0kutxos7-algo-1-8yrs9 | xm0kutxos7-algo-1-8yrs9 | command line arguments: Namespace(estimators=10, sm_channel_test=&#39;/opt/ml/input/data/test&#39;, sm_channel_train=&#39;/opt/ml/input/data/train&#39;, sm_model_dir=&#39;/opt/ml/model&#39;, sm_output_data_dir=&#39;/opt/ml/output/data&#39;) xm0kutxos7-algo-1-8yrs9 | training_dir: /opt/ml/input/data/train xm0kutxos7-algo-1-8yrs9 | training_dir files list: [&#39;train.csv&#39;] xm0kutxos7-algo-1-8yrs9 | testing_dir: /opt/ml/input/data/test xm0kutxos7-algo-1-8yrs9 | testing_dir files list: [&#39;test.csv&#39;] xm0kutxos7-algo-1-8yrs9 | sm_model_dir: /opt/ml/model xm0kutxos7-algo-1-8yrs9 | output_data_dir: /opt/ml/output/data xm0kutxos7-algo-1-8yrs9 | X_train.shape: (120, 4) xm0kutxos7-algo-1-8yrs9 | y_train.shape: (120,) xm0kutxos7-algo-1-8yrs9 | X_train.shape: (30, 4) xm0kutxos7-algo-1-8yrs9 | y_train.shape: (30,) xm0kutxos7-algo-1-8yrs9 | sm_model_dir: /opt/ml/model xm0kutxos7-algo-1-8yrs9 | sm_model_dir files list: [&#39;model.joblib&#39;] xm0kutxos7-algo-1-8yrs9 | output_data_dir: /opt/ml/output/data xm0kutxos7-algo-1-8yrs9 | output_data_dir files list: [&#39;y_pred.csv&#39;, &#39;output_cm.png&#39;] xm0kutxos7-algo-1-8yrs9 | 2022-07-17 15:24:33,003 sagemaker-containers INFO Reporting training SUCCESS . Failed to delete: /tmp/tmpee3z9n_9/algo-1-8yrs9 Please remove it manually. . xm0kutxos7-algo-1-8yrs9 exited with code 0 Aborting on container exit... ===== Job Complete ===== . . . SKLearn container output shows that our classifier is successfully trained, and the model and output artifacts are placed in their respective folders. We know from the first section of this post that these artifacts will automatically be uploaded to the S3 bucket. This concludes the model training part of our implementation. Let&#39;s now proceed to model serving part of our solution. . Serve SKLearn model in local mode . At this point, we have our trained model ready. Can we deploy it already? . The answer is no. If we try to deploy this model using command . sk_predictor = sk_estimator.deploy( initial_instance_count=1, instance_type=&#39;local&#39; ) . It will generate an exception message telling us that the estimator does not know how to load the model. So we need to tell the estimator by implementing model_fn function in our script. . [2022-07-09 06:15:45 +0000] [31] [ERROR] Error handling request /ping Traceback (most recent call last): File &quot;/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_functions.py&quot;, line 93, in wrapper return fn(*args, **kwargs) File &quot;/miniconda3/lib/python3.8/site-packages/sagemaker_sklearn_container/serving.py&quot;, line 43, in default_model_fn return transformer.default_model_fn(model_dir) File &quot;/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_transformer.py&quot;, line 35, in default_model_fn raise NotImplementedError( NotImplementedError: Please provide a model_fn implementation. See documentation for model_fn at https://github.com/aws/sagemaker-python-sdk . The model_fn has the following signature: . def model_fn(model_dir) . Besides loading the model, we also need to tell the model server how to get predictions from the loaded model. For this, we need to implement the second function predict_fn, which has the following signature. . def predict_fn(input_data, model) . After we have called the fit function on our SKLearn estimator, we can deploy it by calling the deploy function to create an inference endpoint. Once you call deploy on the estimator two objects are created in response . SageMaker scikit-learn Endpoint: This Endpoint encapsulates a model server running under it. The model server will load the model saved during training and perform inference on it. It requires two helper functions to load the model and make inferences on it: model_fn and predict_fn. | Predictor object: This object is returned in response to the deploy call. It can be used to do inference on the Endpoint hosting our SKLearn model. | . Let&#39;s update our script and add these two functions. . %%writefile $script_file import argparse, os import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import confusion_matrix import joblib from my_custom_library import save_confusion_matrix if __name__ == &quot;__main__&quot;: # Pass in environment variables and hyperparameters parser = argparse.ArgumentParser() # Hyperparameters parser.add_argument(&quot;--estimators&quot;, type=int, default=15) # sm_model_dir: model artifacts stored here after training # sm-channel-train: input training data location # sm-channel-test: input test data location # sm-output-data-dir: output artifacts location parser.add_argument(&quot;--sm-model-dir&quot;, type=str, default=os.environ.get(&quot;SM_MODEL_DIR&quot;)) parser.add_argument(&quot;--sm-channel-train&quot;, type=str, default=os.environ.get(&quot;SM_CHANNEL_TRAIN&quot;)) parser.add_argument(&quot;--sm-channel-test&quot;, type=str, default=os.environ.get(&quot;SM_CHANNEL_TEST&quot;)) parser.add_argument(&quot;--sm-output-data-dir&quot;, type=str, default=os.environ.get(&quot;SM_OUTPUT_DATA_DIR&quot;)) args, _ = parser.parse_known_args() print(&quot;command line arguments: &quot;, args) estimators = args.estimators sm_model_dir = args.sm_model_dir training_dir = args.sm_channel_train testing_dir = args.sm_channel_test output_data_dir = args.sm_output_data_dir print(f&quot;training_dir: {training_dir}&quot;) print(f&quot;training_dir files list: {os.listdir(training_dir)}&quot;) print(f&quot;testing_dir: {testing_dir}&quot;) print(f&quot;testing_dir files list: {os.listdir(testing_dir)}&quot;) print(f&quot;sm_model_dir: {sm_model_dir}&quot;) print(f&quot;output_data_dir: {output_data_dir}&quot;) # Read in data df_train = pd.read_csv(training_dir + &quot;/train.csv&quot;, sep=&quot;,&quot;) df_test = pd.read_csv(testing_dir + &quot;/test.csv&quot;, sep=&quot;,&quot;) # Preprocess data X_train = df_train.drop([&quot;class&quot;, &quot;class_cat&quot;], axis=1) y_train = df_train[&quot;class_cat&quot;] X_test = df_test.drop([&quot;class&quot;, &quot;class_cat&quot;], axis=1) y_test = df_test[&quot;class_cat&quot;] print(f&quot;X_train.shape: {X_train.shape}&quot;) print(f&quot;y_train.shape: {y_train.shape}&quot;) print(f&quot;X_train.shape: {X_test.shape}&quot;) print(f&quot;y_train.shape: {y_test.shape}&quot;) sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) # Build model regressor = RandomForestClassifier(n_estimators=estimators) regressor.fit(X_train, y_train) y_pred = regressor.predict(X_test) # Save the model joblib.dump(regressor, sm_model_dir + &quot;/model.joblib&quot;) # Save the results pd.DataFrame(y_pred).to_csv(output_data_dir + &quot;/y_pred.csv&quot;) # save the confusion matrix cf_matrix = confusion_matrix(y_test, y_pred) save_confusion_matrix(cf_matrix, output_data_dir) # print sm_model_dir info print(f&quot;sm_model_dir: {sm_model_dir}&quot;) print(f&quot;sm_model_dir files list: {os.listdir(sm_model_dir)}&quot;) # print output_data_dir info print(f&quot;output_data_dir: {output_data_dir}&quot;) print(f&quot;output_data_dir files list: {os.listdir(output_data_dir)}&quot;) # Model serving &quot;&quot;&quot; Deserialize fitted model &quot;&quot;&quot; def model_fn(model_dir): print(f&quot;model_fn model_dir: {model_dir}&quot;) model = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;)) return model &quot;&quot;&quot; predict_fn input_data: returned array from input_fn above model (sklearn model) returned model loaded from model_fn above &quot;&quot;&quot; def predict_fn(input_data, model): return model.predict(input_data) . Overwriting ./datasets/2022-07-07-sagemaker-script-mode/src/train_and_serve.py . sk_estimator = SKLearn( entry_point=script_file_name, source_dir=script_path, dependencies=[custom_library_path], role=role, instance_count=1, instance_type=&#39;local&#39;, framework_version=&quot;1.0-1&quot;, hyperparameters={&quot;estimators&quot;:10}, ) sk_estimator.fit({&quot;train&quot;: s3_train_uri, &quot;test&quot;: s3_test_uri}) . Creating wxtcttdsw0-algo-1-jym48 ... Creating wxtcttdsw0-algo-1-jym48 ... done Attaching to wxtcttdsw0-algo-1-jym48 wxtcttdsw0-algo-1-jym48 | 2022-07-17 15:24:36,721 sagemaker-containers INFO Imported framework sagemaker_sklearn_container.training wxtcttdsw0-algo-1-jym48 | 2022-07-17 15:24:36,726 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) wxtcttdsw0-algo-1-jym48 | 2022-07-17 15:24:36,735 sagemaker_sklearn_container.training INFO Invoking user training script. wxtcttdsw0-algo-1-jym48 | 2022-07-17 15:24:36,923 sagemaker-training-toolkit INFO Installing dependencies from requirements.txt: wxtcttdsw0-algo-1-jym48 | /miniconda3/bin/python -m pip install -r requirements.txt wxtcttdsw0-algo-1-jym48 | Collecting seaborn==0.11.2 wxtcttdsw0-algo-1-jym48 | Downloading seaborn-0.11.2-py3-none-any.whl (292 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 292.8/292.8 kB 3.3 MB/s eta 0:00:0000:01eta -:--:-- wxtcttdsw0-algo-1-jym48 | Requirement already satisfied: pandas&gt;=0.23 in /miniconda3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.1.3) wxtcttdsw0-algo-1-jym48 | Collecting matplotlib&gt;=2.2 wxtcttdsw0-algo-1-jym48 | Downloading matplotlib-3.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/11.3 MB 101.6 MB/s eta 0:00:0000:0100:01:--:-- wxtcttdsw0-algo-1-jym48 | Requirement already satisfied: numpy&gt;=1.15 in /miniconda3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.21.0) wxtcttdsw0-algo-1-jym48 | Requirement already satisfied: scipy&gt;=1.0 in /miniconda3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.5.3) wxtcttdsw0-algo-1-jym48 | Collecting packaging&gt;=20.0 wxtcttdsw0-algo-1-jym48 | Downloading packaging-21.3-py3-none-any.whl (40 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 12.9 MB/s eta 0:00:001m? eta -:--:-- wxtcttdsw0-algo-1-jym48 | Requirement already satisfied: pillow&gt;=6.2.0 in /miniconda3/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (9.1.1) wxtcttdsw0-algo-1-jym48 | Collecting cycler&gt;=0.10 wxtcttdsw0-algo-1-jym48 | Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB) wxtcttdsw0-algo-1-jym48 | Collecting kiwisolver&gt;=1.0.1 wxtcttdsw0-algo-1-jym48 | Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 86.1 MB/s eta 0:00:0031m? eta -:--:-- wxtcttdsw0-algo-1-jym48 | Requirement already satisfied: python-dateutil&gt;=2.7 in /miniconda3/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (2.8.1) wxtcttdsw0-algo-1-jym48 | Collecting fonttools&gt;=4.22.0 wxtcttdsw0-algo-1-jym48 | Downloading fonttools-4.34.4-py3-none-any.whl (944 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 944.1/944.1 kB 8.7 MB/s eta 0:00:0000:01eta -:--:-- wxtcttdsw0-algo-1-jym48 | Collecting pyparsing&gt;=2.2.1 wxtcttdsw0-algo-1-jym48 | Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.3/98.3 kB 27.8 MB/s eta 0:00:001m? eta -:--:-- wxtcttdsw0-algo-1-jym48 | Requirement already satisfied: pytz&gt;=2017.2 in /miniconda3/lib/python3.8/site-packages (from pandas&gt;=0.23-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (2022.1) wxtcttdsw0-algo-1-jym48 | Requirement already satisfied: six&gt;=1.5 in /miniconda3/lib/python3.8/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.15.0) wxtcttdsw0-algo-1-jym48 | Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, packaging, matplotlib, seaborn wxtcttdsw0-algo-1-jym48 | Successfully installed cycler-0.11.0 fonttools-4.34.4 kiwisolver-1.4.4 matplotlib-3.5.2 packaging-21.3 pyparsing-3.0.9 seaborn-0.11.2 wxtcttdsw0-algo-1-jym48 | WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv wxtcttdsw0-algo-1-jym48 | 2022-07-17 15:24:41,696 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) wxtcttdsw0-algo-1-jym48 | 2022-07-17 15:24:41,711 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) wxtcttdsw0-algo-1-jym48 | 2022-07-17 15:24:41,723 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) wxtcttdsw0-algo-1-jym48 | 2022-07-17 15:24:41,732 sagemaker-training-toolkit INFO Invoking user script wxtcttdsw0-algo-1-jym48 | wxtcttdsw0-algo-1-jym48 | Training Env: wxtcttdsw0-algo-1-jym48 | wxtcttdsw0-algo-1-jym48 | { wxtcttdsw0-algo-1-jym48 | &#34;additional_framework_parameters&#34;: {}, wxtcttdsw0-algo-1-jym48 | &#34;channel_input_dirs&#34;: { wxtcttdsw0-algo-1-jym48 | &#34;train&#34;: &#34;/opt/ml/input/data/train&#34;, wxtcttdsw0-algo-1-jym48 | &#34;test&#34;: &#34;/opt/ml/input/data/test&#34; wxtcttdsw0-algo-1-jym48 | }, wxtcttdsw0-algo-1-jym48 | &#34;current_host&#34;: &#34;algo-1-jym48&#34;, wxtcttdsw0-algo-1-jym48 | &#34;framework_module&#34;: &#34;sagemaker_sklearn_container.training:main&#34;, wxtcttdsw0-algo-1-jym48 | &#34;hosts&#34;: [ wxtcttdsw0-algo-1-jym48 | &#34;algo-1-jym48&#34; wxtcttdsw0-algo-1-jym48 | ], wxtcttdsw0-algo-1-jym48 | &#34;hyperparameters&#34;: { wxtcttdsw0-algo-1-jym48 | &#34;estimators&#34;: 10 wxtcttdsw0-algo-1-jym48 | }, wxtcttdsw0-algo-1-jym48 | &#34;input_config_dir&#34;: &#34;/opt/ml/input/config&#34;, wxtcttdsw0-algo-1-jym48 | &#34;input_data_config&#34;: { wxtcttdsw0-algo-1-jym48 | &#34;train&#34;: { wxtcttdsw0-algo-1-jym48 | &#34;TrainingInputMode&#34;: &#34;File&#34; wxtcttdsw0-algo-1-jym48 | }, wxtcttdsw0-algo-1-jym48 | &#34;test&#34;: { wxtcttdsw0-algo-1-jym48 | &#34;TrainingInputMode&#34;: &#34;File&#34; wxtcttdsw0-algo-1-jym48 | } wxtcttdsw0-algo-1-jym48 | }, wxtcttdsw0-algo-1-jym48 | &#34;input_dir&#34;: &#34;/opt/ml/input&#34;, wxtcttdsw0-algo-1-jym48 | &#34;is_master&#34;: true, wxtcttdsw0-algo-1-jym48 | &#34;job_name&#34;: &#34;sagemaker-scikit-learn-2022-07-17-15-24-34-114&#34;, wxtcttdsw0-algo-1-jym48 | &#34;log_level&#34;: 20, wxtcttdsw0-algo-1-jym48 | &#34;master_hostname&#34;: &#34;algo-1-jym48&#34;, wxtcttdsw0-algo-1-jym48 | &#34;model_dir&#34;: &#34;/opt/ml/model&#34;, wxtcttdsw0-algo-1-jym48 | &#34;module_dir&#34;: &#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-24-34-114/source/sourcedir.tar.gz&#34;, wxtcttdsw0-algo-1-jym48 | &#34;module_name&#34;: &#34;train_and_serve&#34;, wxtcttdsw0-algo-1-jym48 | &#34;network_interface_name&#34;: &#34;eth0&#34;, wxtcttdsw0-algo-1-jym48 | &#34;num_cpus&#34;: 2, wxtcttdsw0-algo-1-jym48 | &#34;num_gpus&#34;: 0, wxtcttdsw0-algo-1-jym48 | &#34;output_data_dir&#34;: &#34;/opt/ml/output/data&#34;, wxtcttdsw0-algo-1-jym48 | &#34;output_dir&#34;: &#34;/opt/ml/output&#34;, wxtcttdsw0-algo-1-jym48 | &#34;output_intermediate_dir&#34;: &#34;/opt/ml/output/intermediate&#34;, wxtcttdsw0-algo-1-jym48 | &#34;resource_config&#34;: { wxtcttdsw0-algo-1-jym48 | &#34;current_host&#34;: &#34;algo-1-jym48&#34;, wxtcttdsw0-algo-1-jym48 | &#34;hosts&#34;: [ wxtcttdsw0-algo-1-jym48 | &#34;algo-1-jym48&#34; wxtcttdsw0-algo-1-jym48 | ] wxtcttdsw0-algo-1-jym48 | }, wxtcttdsw0-algo-1-jym48 | &#34;user_entry_point&#34;: &#34;train_and_serve.py&#34; wxtcttdsw0-algo-1-jym48 | } wxtcttdsw0-algo-1-jym48 | wxtcttdsw0-algo-1-jym48 | Environment variables: wxtcttdsw0-algo-1-jym48 | wxtcttdsw0-algo-1-jym48 | SM_HOSTS=[&#34;algo-1-jym48&#34;] wxtcttdsw0-algo-1-jym48 | SM_NETWORK_INTERFACE_NAME=eth0 wxtcttdsw0-algo-1-jym48 | SM_HPS={&#34;estimators&#34;:10} wxtcttdsw0-algo-1-jym48 | SM_USER_ENTRY_POINT=train_and_serve.py wxtcttdsw0-algo-1-jym48 | SM_FRAMEWORK_PARAMS={} wxtcttdsw0-algo-1-jym48 | SM_RESOURCE_CONFIG={&#34;current_host&#34;:&#34;algo-1-jym48&#34;,&#34;hosts&#34;:[&#34;algo-1-jym48&#34;]} wxtcttdsw0-algo-1-jym48 | SM_INPUT_DATA_CONFIG={&#34;test&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;},&#34;train&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;}} wxtcttdsw0-algo-1-jym48 | SM_OUTPUT_DATA_DIR=/opt/ml/output/data wxtcttdsw0-algo-1-jym48 | SM_CHANNELS=[&#34;test&#34;,&#34;train&#34;] wxtcttdsw0-algo-1-jym48 | SM_CURRENT_HOST=algo-1-jym48 wxtcttdsw0-algo-1-jym48 | SM_MODULE_NAME=train_and_serve wxtcttdsw0-algo-1-jym48 | SM_LOG_LEVEL=20 wxtcttdsw0-algo-1-jym48 | SM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main wxtcttdsw0-algo-1-jym48 | SM_INPUT_DIR=/opt/ml/input wxtcttdsw0-algo-1-jym48 | SM_INPUT_CONFIG_DIR=/opt/ml/input/config wxtcttdsw0-algo-1-jym48 | SM_OUTPUT_DIR=/opt/ml/output wxtcttdsw0-algo-1-jym48 | SM_NUM_CPUS=2 wxtcttdsw0-algo-1-jym48 | SM_NUM_GPUS=0 wxtcttdsw0-algo-1-jym48 | SM_MODEL_DIR=/opt/ml/model wxtcttdsw0-algo-1-jym48 | SM_MODULE_DIR=s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-24-34-114/source/sourcedir.tar.gz wxtcttdsw0-algo-1-jym48 | SM_TRAINING_ENV={&#34;additional_framework_parameters&#34;:{},&#34;channel_input_dirs&#34;:{&#34;test&#34;:&#34;/opt/ml/input/data/test&#34;,&#34;train&#34;:&#34;/opt/ml/input/data/train&#34;},&#34;current_host&#34;:&#34;algo-1-jym48&#34;,&#34;framework_module&#34;:&#34;sagemaker_sklearn_container.training:main&#34;,&#34;hosts&#34;:[&#34;algo-1-jym48&#34;],&#34;hyperparameters&#34;:{&#34;estimators&#34;:10},&#34;input_config_dir&#34;:&#34;/opt/ml/input/config&#34;,&#34;input_data_config&#34;:{&#34;test&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;},&#34;train&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;}},&#34;input_dir&#34;:&#34;/opt/ml/input&#34;,&#34;is_master&#34;:true,&#34;job_name&#34;:&#34;sagemaker-scikit-learn-2022-07-17-15-24-34-114&#34;,&#34;log_level&#34;:20,&#34;master_hostname&#34;:&#34;algo-1-jym48&#34;,&#34;model_dir&#34;:&#34;/opt/ml/model&#34;,&#34;module_dir&#34;:&#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-24-34-114/source/sourcedir.tar.gz&#34;,&#34;module_name&#34;:&#34;train_and_serve&#34;,&#34;network_interface_name&#34;:&#34;eth0&#34;,&#34;num_cpus&#34;:2,&#34;num_gpus&#34;:0,&#34;output_data_dir&#34;:&#34;/opt/ml/output/data&#34;,&#34;output_dir&#34;:&#34;/opt/ml/output&#34;,&#34;output_intermediate_dir&#34;:&#34;/opt/ml/output/intermediate&#34;,&#34;resource_config&#34;:{&#34;current_host&#34;:&#34;algo-1-jym48&#34;,&#34;hosts&#34;:[&#34;algo-1-jym48&#34;]},&#34;user_entry_point&#34;:&#34;train_and_serve.py&#34;} wxtcttdsw0-algo-1-jym48 | SM_USER_ARGS=[&#34;--estimators&#34;,&#34;10&#34;] wxtcttdsw0-algo-1-jym48 | SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate wxtcttdsw0-algo-1-jym48 | SM_CHANNEL_TRAIN=/opt/ml/input/data/train wxtcttdsw0-algo-1-jym48 | SM_CHANNEL_TEST=/opt/ml/input/data/test wxtcttdsw0-algo-1-jym48 | SM_HP_ESTIMATORS=10 wxtcttdsw0-algo-1-jym48 | PYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages wxtcttdsw0-algo-1-jym48 | wxtcttdsw0-algo-1-jym48 | Invoking script with the following command: wxtcttdsw0-algo-1-jym48 | wxtcttdsw0-algo-1-jym48 | /miniconda3/bin/python train_and_serve.py --estimators 10 wxtcttdsw0-algo-1-jym48 | wxtcttdsw0-algo-1-jym48 | wxtcttdsw0-algo-1-jym48 | command line arguments: Namespace(estimators=10, sm_channel_test=&#39;/opt/ml/input/data/test&#39;, sm_channel_train=&#39;/opt/ml/input/data/train&#39;, sm_model_dir=&#39;/opt/ml/model&#39;, sm_output_data_dir=&#39;/opt/ml/output/data&#39;) wxtcttdsw0-algo-1-jym48 | training_dir: /opt/ml/input/data/train wxtcttdsw0-algo-1-jym48 | training_dir files list: [&#39;train.csv&#39;] wxtcttdsw0-algo-1-jym48 | testing_dir: /opt/ml/input/data/test wxtcttdsw0-algo-1-jym48 | testing_dir files list: [&#39;test.csv&#39;] wxtcttdsw0-algo-1-jym48 | sm_model_dir: /opt/ml/model wxtcttdsw0-algo-1-jym48 | output_data_dir: /opt/ml/output/data wxtcttdsw0-algo-1-jym48 | X_train.shape: (120, 4) wxtcttdsw0-algo-1-jym48 | y_train.shape: (120,) wxtcttdsw0-algo-1-jym48 | X_train.shape: (30, 4) wxtcttdsw0-algo-1-jym48 | y_train.shape: (30,) wxtcttdsw0-algo-1-jym48 | sm_model_dir: /opt/ml/model wxtcttdsw0-algo-1-jym48 | sm_model_dir files list: [&#39;model.joblib&#39;] wxtcttdsw0-algo-1-jym48 | output_data_dir: /opt/ml/output/data wxtcttdsw0-algo-1-jym48 | output_data_dir files list: [&#39;y_pred.csv&#39;, &#39;output_cm.png&#39;] wxtcttdsw0-algo-1-jym48 | 2022-07-17 15:24:43,775 sagemaker-containers INFO Reporting training SUCCESS wxtcttdsw0-algo-1-jym48 exited with code 0 Aborting on container exit... . Failed to delete: /tmp/tmpa3uld7ha/algo-1-jym48 Please remove it manually. . ===== Job Complete ===== . . Our model is trained. Let&#39;s also deploy it in the local model. For model loading model_fn, SageMaker will download the model artifacts from S3 and mount them on /opt/ml/model. This way, our script can load the model from within the container. . sk_predictor = sk_estimator.deploy( initial_instance_count=1, instance_type=&#39;local&#39; ) . Attaching to 3fvyanwal0-algo-1-tz4ow 3fvyanwal0-algo-1-tz4ow | 2022-07-17 15:24:46,644 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed) 3fvyanwal0-algo-1-tz4ow | 2022-07-17 15:24:46,648 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed) 3fvyanwal0-algo-1-tz4ow | 2022-07-17 15:24:46,649 INFO - sagemaker-containers - nginx config: 3fvyanwal0-algo-1-tz4ow | worker_processes auto; 3fvyanwal0-algo-1-tz4ow | daemon off; 3fvyanwal0-algo-1-tz4ow | pid /tmp/nginx.pid; 3fvyanwal0-algo-1-tz4ow | error_log /dev/stderr; 3fvyanwal0-algo-1-tz4ow | 3fvyanwal0-algo-1-tz4ow | worker_rlimit_nofile 4096; 3fvyanwal0-algo-1-tz4ow | 3fvyanwal0-algo-1-tz4ow | events { 3fvyanwal0-algo-1-tz4ow | worker_connections 2048; 3fvyanwal0-algo-1-tz4ow | } 3fvyanwal0-algo-1-tz4ow | 3fvyanwal0-algo-1-tz4ow | http { 3fvyanwal0-algo-1-tz4ow | include /etc/nginx/mime.types; 3fvyanwal0-algo-1-tz4ow | default_type application/octet-stream; 3fvyanwal0-algo-1-tz4ow | access_log /dev/stdout combined; 3fvyanwal0-algo-1-tz4ow | 3fvyanwal0-algo-1-tz4ow | upstream gunicorn { 3fvyanwal0-algo-1-tz4ow | server unix:/tmp/gunicorn.sock; 3fvyanwal0-algo-1-tz4ow | } 3fvyanwal0-algo-1-tz4ow | 3fvyanwal0-algo-1-tz4ow | server { 3fvyanwal0-algo-1-tz4ow | listen 8080 deferred; 3fvyanwal0-algo-1-tz4ow | client_max_body_size 0; 3fvyanwal0-algo-1-tz4ow | 3fvyanwal0-algo-1-tz4ow | keepalive_timeout 3; 3fvyanwal0-algo-1-tz4ow | 3fvyanwal0-algo-1-tz4ow | location ~ ^/(ping|invocations|execution-parameters) { 3fvyanwal0-algo-1-tz4ow | proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 3fvyanwal0-algo-1-tz4ow | proxy_set_header Host $http_host; 3fvyanwal0-algo-1-tz4ow | proxy_redirect off; 3fvyanwal0-algo-1-tz4ow | proxy_read_timeout 60s; 3fvyanwal0-algo-1-tz4ow | proxy_pass http://gunicorn; 3fvyanwal0-algo-1-tz4ow | } 3fvyanwal0-algo-1-tz4ow | 3fvyanwal0-algo-1-tz4ow | location / { 3fvyanwal0-algo-1-tz4ow | return 404 &#34;{}&#34;; 3fvyanwal0-algo-1-tz4ow | } 3fvyanwal0-algo-1-tz4ow | 3fvyanwal0-algo-1-tz4ow | } 3fvyanwal0-algo-1-tz4ow | } 3fvyanwal0-algo-1-tz4ow | 3fvyanwal0-algo-1-tz4ow | 3fvyanwal0-algo-1-tz4ow | 2022-07-17 15:24:46,866 INFO - sagemaker-containers - Module train_and_serve does not provide a setup.py. 3fvyanwal0-algo-1-tz4ow | Generating setup.py 3fvyanwal0-algo-1-tz4ow | 2022-07-17 15:24:46,866 INFO - sagemaker-containers - Generating setup.cfg 3fvyanwal0-algo-1-tz4ow | 2022-07-17 15:24:46,866 INFO - sagemaker-containers - Generating MANIFEST.in 3fvyanwal0-algo-1-tz4ow | 2022-07-17 15:24:46,867 INFO - sagemaker-containers - Installing module with the following command: 3fvyanwal0-algo-1-tz4ow | /miniconda3/bin/python3 -m pip install . -r requirements.txt 3fvyanwal0-algo-1-tz4ow | Processing /opt/ml/code 3fvyanwal0-algo-1-tz4ow | Preparing metadata (setup.py) ... done 3fvyanwal0-algo-1-tz4ow | Collecting seaborn==0.11.2 3fvyanwal0-algo-1-tz4ow | Downloading seaborn-0.11.2-py3-none-any.whl (292 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 292.8/292.8 kB 4.8 MB/s eta 0:00:0000:01eta -:--:-- 3fvyanwal0-algo-1-tz4ow | Collecting matplotlib&gt;=2.2 3fvyanwal0-algo-1-tz4ow | Downloading matplotlib-3.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/11.3 MB 93.0 MB/s eta 0:00:0000:0100:01:--:-- 3fvyanwal0-algo-1-tz4ow | Requirement already satisfied: scipy&gt;=1.0 in /miniconda3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.5.3) 3fvyanwal0-algo-1-tz4ow | Requirement already satisfied: numpy&gt;=1.15 in /miniconda3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.21.0) 3fvyanwal0-algo-1-tz4ow | Requirement already satisfied: pandas&gt;=0.23 in /miniconda3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.1.3) 3fvyanwal0-algo-1-tz4ow | Requirement already satisfied: pillow&gt;=6.2.0 in /miniconda3/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (9.1.1) 3fvyanwal0-algo-1-tz4ow | Collecting packaging&gt;=20.0 3fvyanwal0-algo-1-tz4ow | Downloading packaging-21.3-py3-none-any.whl (40 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 9.1 MB/s eta 0:00:0031m? eta -:--:-- 3fvyanwal0-algo-1-tz4ow | Collecting fonttools&gt;=4.22.0 3fvyanwal0-algo-1-tz4ow | Downloading fonttools-4.34.4-py3-none-any.whl (944 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 944.1/944.1 kB 8.1 MB/s eta 0:00:0000:01eta -:--:-- 3fvyanwal0-algo-1-tz4ow | Requirement already satisfied: python-dateutil&gt;=2.7 in /miniconda3/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (2.8.1) 3fvyanwal0-algo-1-tz4ow | Collecting cycler&gt;=0.10 3fvyanwal0-algo-1-tz4ow | Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB) 3fvyanwal0-algo-1-tz4ow | Collecting pyparsing&gt;=2.2.1 3fvyanwal0-algo-1-tz4ow | Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.3/98.3 kB 21.4 MB/s eta 0:00:001m? eta -:--:-- 3fvyanwal0-algo-1-tz4ow | Collecting kiwisolver&gt;=1.0.1 3fvyanwal0-algo-1-tz4ow | Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 17.1 MB/s eta 0:00:00:00:01ta -:--:-- 3fvyanwal0-algo-1-tz4ow | Requirement already satisfied: pytz&gt;=2017.2 in /miniconda3/lib/python3.8/site-packages (from pandas&gt;=0.23-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (2022.1) 3fvyanwal0-algo-1-tz4ow | Requirement already satisfied: six&gt;=1.5 in /miniconda3/lib/python3.8/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.15.0) 3fvyanwal0-algo-1-tz4ow | Building wheels for collected packages: train-and-serve 3fvyanwal0-algo-1-tz4ow | Building wheel for train-and-serve (setup.py) ... 2022/07/17 15:24:49 [crit] 15#15: *1 connect() to unix:/tmp/gunicorn.sock failed (2: No such file or directory) while connecting to upstream, client: 172.18.0.1, server: , request: &#34;GET /ping HTTP/1.1&#34;, upstream: &#34;http://unix:/tmp/gunicorn.sock:/ping&#34;, host: &#34;localhost:8080&#34; 3fvyanwal0-algo-1-tz4ow | 172.18.0.1 - - [17/Jul/2022:15:24:49 +0000] &#34;GET /ping HTTP/1.1&#34; 502 182 &#34;-&#34; &#34;python-urllib3/1.26.8&#34; 3fvyanwal0-algo-1-tz4ow | done 3fvyanwal0-algo-1-tz4ow | Created wheel for train-and-serve: filename=train_and_serve-1.0.0-py2.py3-none-any.whl size=6122 sha256=914e6ad8ea2651da0216fefbc30c28bc25124ff514c30452de608e5b9807197c 3fvyanwal0-algo-1-tz4ow | Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-2u_4hcln/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00 3fvyanwal0-algo-1-tz4ow | Successfully built train-and-serve 3fvyanwal0-algo-1-tz4ow | Installing collected packages: train-and-serve, pyparsing, kiwisolver, fonttools, cycler, packaging, matplotlib, seaborn 3fvyanwal0-algo-1-tz4ow | Successfully installed cycler-0.11.0 fonttools-4.34.4 kiwisolver-1.4.4 matplotlib-3.5.2 packaging-21.3 pyparsing-3.0.9 seaborn-0.11.2 train-and-serve-1.0.0 3fvyanwal0-algo-1-tz4ow | WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv 3fvyanwal0-algo-1-tz4ow | 2022/07/17 15:24:54 [crit] 15#15: *3 connect() to unix:/tmp/gunicorn.sock failed (2: No such file or directory) while connecting to upstream, client: 172.18.0.1, server: , request: &#34;GET /ping HTTP/1.1&#34;, upstream: &#34;http://unix:/tmp/gunicorn.sock:/ping&#34;, host: &#34;localhost:8080&#34; 3fvyanwal0-algo-1-tz4ow | 172.18.0.1 - - [17/Jul/2022:15:24:54 +0000] &#34;GET /ping HTTP/1.1&#34; 502 182 &#34;-&#34; &#34;python-urllib3/1.26.8&#34; 3fvyanwal0-algo-1-tz4ow | 2022-07-17 15:24:55,286 INFO - matplotlib.font_manager - generated new fontManager 3fvyanwal0-algo-1-tz4ow | [2022-07-17 15:24:55 +0000] [37] [INFO] Starting gunicorn 20.0.4 3fvyanwal0-algo-1-tz4ow | [2022-07-17 15:24:55 +0000] [37] [INFO] Listening at: unix:/tmp/gunicorn.sock (37) 3fvyanwal0-algo-1-tz4ow | [2022-07-17 15:24:55 +0000] [37] [INFO] Using worker: gevent 3fvyanwal0-algo-1-tz4ow | [2022-07-17 15:24:55 +0000] [39] [INFO] Booting worker with pid: 39 3fvyanwal0-algo-1-tz4ow | [2022-07-17 15:24:56 +0000] [40] [INFO] Booting worker with pid: 40 3fvyanwal0-algo-1-tz4ow | 2022-07-17 15:24:59,750 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed) 3fvyanwal0-algo-1-tz4ow | model_fn model_dir: /opt/ml/model !3fvyanwal0-algo-1-tz4ow | 172.18.0.1 - - [17/Jul/2022:15:25:01 +0000] &#34;GET /ping HTTP/1.1&#34; 200 0 &#34;-&#34; &#34;python-urllib3/1.26.8&#34; . . Let&#39;s create a sample request and get a prediction from our local inference endpoint. . request = [[9.0, 3571, 1976, 0.525]] response = sk_predictor.predict(request) response = int(response[0]) response . 3fvyanwal0-algo-1-tz4ow | 2022-07-17 15:25:01,760 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed) 3fvyanwal0-algo-1-tz4ow | model_fn model_dir: /opt/ml/model . 2 . 3fvyanwal0-algo-1-tz4ow | 172.18.0.1 - - [17/Jul/2022:15:25:03 +0000] &#34;POST /invocations HTTP/1.1&#34; 200 136 &#34;-&#34; &#34;python-urllib3/1.26.8&#34; . # map response to correct category type print(&quot;Predicted class category {} ({})&quot;.format(response, categories_map[response])) . Predicted class category 2 (Iris-virginica) . Since the enpoint in running in the local environment we can observe a webserver running in a docker instance. . !docker ps . CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5a22263f860b 683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.0-1-cpu-py3 &#34;serve&#34; 18 seconds ago Up 17 seconds 0.0.0.0:8080-&gt;8080/tcp, :::8080-&gt;8080/tcp 3fvyanwal0-algo-1-tz4ow . # delete the local endpoint sk_predictor.delete_endpoint() . Gracefully stopping... (press Ctrl+C again to force) . Note that in local mode, we can only serve a single model simultaneously. Therefore, if you are getting an error on the deploy call in the local model, then check that there is no other endpoint running. . SKLean model server input and output processing . SageMaker model server breaks the incoming request into three steps: . input processing | prediction, and | output processing | In the last section, we have seen that the predict_fn function in the source code file defines model prediction. Similarly, SageMaker provides two additional functions to control input and output processing, defined as input_fn and output_fn, respectively. Both these function have their default implementations. But we can override them by providing our own implementation for them in the source script. If no definition is provided in the source script, then the SageMaker scikit-learn model server will use the default implementation. . input_fn: Takes request data and deserializes the data into an object for prediction. | output_fn: Takes the prediction result and serializes this according to the response content type. | . Let&#39;s update our script to preprocess input request and output response as JSON objects. . %%writefile $script_file import argparse, os import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import confusion_matrix import joblib import json from my_custom_library import save_confusion_matrix if __name__ == &quot;__main__&quot;: # Pass in environment variables and hyperparameters parser = argparse.ArgumentParser() # Hyperparameters parser.add_argument(&quot;--estimators&quot;, type=int, default=15) # sm_model_dir: model artifacts stored here after training # sm-channel-train: input training data location # sm-channel-test: input test data location # sm-output-data-dir: output artifacts location parser.add_argument(&quot;--sm-model-dir&quot;, type=str, default=os.environ.get(&quot;SM_MODEL_DIR&quot;)) parser.add_argument(&quot;--sm-channel-train&quot;, type=str, default=os.environ.get(&quot;SM_CHANNEL_TRAIN&quot;)) parser.add_argument(&quot;--sm-channel-test&quot;, type=str, default=os.environ.get(&quot;SM_CHANNEL_TEST&quot;)) parser.add_argument(&quot;--sm-output-data-dir&quot;, type=str, default=os.environ.get(&quot;SM_OUTPUT_DATA_DIR&quot;)) args, _ = parser.parse_known_args() print(&quot;command line arguments: &quot;, args) estimators = args.estimators sm_model_dir = args.sm_model_dir training_dir = args.sm_channel_train testing_dir = args.sm_channel_test output_data_dir = args.sm_output_data_dir print(f&quot;training_dir: {training_dir}&quot;) print(f&quot;training_dir files list: {os.listdir(training_dir)}&quot;) print(f&quot;testing_dir: {testing_dir}&quot;) print(f&quot;testing_dir files list: {os.listdir(testing_dir)}&quot;) print(f&quot;sm_model_dir: {sm_model_dir}&quot;) print(f&quot;output_data_dir: {output_data_dir}&quot;) # Read in data df_train = pd.read_csv(training_dir + &quot;/train.csv&quot;, sep=&quot;,&quot;) df_test = pd.read_csv(testing_dir + &quot;/test.csv&quot;, sep=&quot;,&quot;) # Preprocess data X_train = df_train.drop([&quot;class&quot;, &quot;class_cat&quot;], axis=1) y_train = df_train[&quot;class_cat&quot;] X_test = df_test.drop([&quot;class&quot;, &quot;class_cat&quot;], axis=1) y_test = df_test[&quot;class_cat&quot;] print(f&quot;X_train.shape: {X_train.shape}&quot;) print(f&quot;y_train.shape: {y_train.shape}&quot;) print(f&quot;X_train.shape: {X_test.shape}&quot;) print(f&quot;y_train.shape: {y_test.shape}&quot;) sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) # Build model regressor = RandomForestClassifier(n_estimators=estimators) regressor.fit(X_train, y_train) y_pred = regressor.predict(X_test) # Save the model joblib.dump(regressor, sm_model_dir + &quot;/model.joblib&quot;) # Save the results pd.DataFrame(y_pred).to_csv(output_data_dir + &quot;/y_pred.csv&quot;) # save the confusion matrix cf_matrix = confusion_matrix(y_test, y_pred) save_confusion_matrix(cf_matrix, output_data_dir) # print sm_model_dir info print(f&quot;sm_model_dir: {sm_model_dir}&quot;) print(f&quot;sm_model_dir files list: {os.listdir(sm_model_dir)}&quot;) # print output_data_dir info print(f&quot;output_data_dir: {output_data_dir}&quot;) print(f&quot;output_data_dir files list: {os.listdir(output_data_dir)}&quot;) # Model serving &quot;&quot;&quot; Deserialize fitted model &quot;&quot;&quot; def model_fn(model_dir): print(f&quot;model_fn model_dir: {model_dir}&quot;) model = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;)) return model &quot;&quot;&quot; predict_fn input_data: returned array from input_fn above model (sklearn model) returned model loaded from model_fn above &quot;&quot;&quot; def predict_fn(input_data, model): return model.predict(input_data) &quot;&quot;&quot; input_fn request_body: The body of the request sent to the model. request_content_type: (string) specifies the format/variable type of the request &quot;&quot;&quot; def input_fn(request_body, request_content_type): if request_content_type == &quot;application/json&quot;: request_body = json.loads(request_body) inpVar = request_body[&quot;Input&quot;] return inpVar else: raise ValueError(&quot;This model only supports application/json input&quot;) &quot;&quot;&quot; output_fn prediction: the returned value from predict_fn above content_type: the content type the endpoint expects to be returned. Ex: JSON, string &quot;&quot;&quot; def output_fn(prediction, content_type): res = int(prediction[0]) respJSON = {&quot;Output&quot;: res} return respJSON . Overwriting ./datasets/2022-07-07-sagemaker-script-mode/src/train_and_serve.py . # train and deploy model with input and output as JSON objects sk_estimator = SKLearn( entry_point=script_file_name, source_dir=script_path, dependencies=[custom_library_path], role=role, instance_count=1, instance_type=&#39;local&#39;, framework_version=&quot;1.0-1&quot;, hyperparameters={&quot;estimators&quot;:10}, ) sk_estimator.fit({&quot;train&quot;: s3_train_uri, &quot;test&quot;: s3_test_uri}) sk_predictor = sk_estimator.deploy( initial_instance_count=1, instance_type=&#39;local&#39; ) . Creating ubyi50juw8-algo-1-9w0jk ... Creating ubyi50juw8-algo-1-9w0jk ... done Attaching to ubyi50juw8-algo-1-9w0jk ubyi50juw8-algo-1-9w0jk | 2022-07-17 15:25:07,036 sagemaker-containers INFO Imported framework sagemaker_sklearn_container.training ubyi50juw8-algo-1-9w0jk | 2022-07-17 15:25:07,041 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) ubyi50juw8-algo-1-9w0jk | 2022-07-17 15:25:07,050 sagemaker_sklearn_container.training INFO Invoking user training script. ubyi50juw8-algo-1-9w0jk | 2022-07-17 15:25:07,233 sagemaker-training-toolkit INFO Installing dependencies from requirements.txt: ubyi50juw8-algo-1-9w0jk | /miniconda3/bin/python -m pip install -r requirements.txt ubyi50juw8-algo-1-9w0jk | Collecting seaborn==0.11.2 ubyi50juw8-algo-1-9w0jk | Downloading seaborn-0.11.2-py3-none-any.whl (292 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 292.8/292.8 kB 38.8 MB/s eta 0:00:0031m? eta -:--:-- ubyi50juw8-algo-1-9w0jk | Requirement already satisfied: scipy&gt;=1.0 in /miniconda3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.5.3) ubyi50juw8-algo-1-9w0jk | Collecting matplotlib&gt;=2.2 ubyi50juw8-algo-1-9w0jk | Downloading matplotlib-3.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/11.3 MB 94.9 MB/s eta 0:00:0000:0100:01:--:-- ubyi50juw8-algo-1-9w0jk | Requirement already satisfied: pandas&gt;=0.23 in /miniconda3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.1.3) ubyi50juw8-algo-1-9w0jk | Requirement already satisfied: numpy&gt;=1.15 in /miniconda3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.21.0) ubyi50juw8-algo-1-9w0jk | Collecting pyparsing&gt;=2.2.1 ubyi50juw8-algo-1-9w0jk | Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.3/98.3 kB 27.9 MB/s eta 0:00:001m? eta -:--:-- ubyi50juw8-algo-1-9w0jk | Collecting cycler&gt;=0.10 ubyi50juw8-algo-1-9w0jk | Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB) ubyi50juw8-algo-1-9w0jk | Collecting fonttools&gt;=4.22.0 ubyi50juw8-algo-1-9w0jk | Downloading fonttools-4.34.4-py3-none-any.whl (944 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 944.1/944.1 kB 26.2 MB/s eta 0:00:0000:01eta -:--:-- ubyi50juw8-algo-1-9w0jk | Collecting packaging&gt;=20.0 ubyi50juw8-algo-1-9w0jk | Downloading packaging-21.3-py3-none-any.whl (40 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 3.6 MB/s eta 0:00:0031m? eta -:--:-- ubyi50juw8-algo-1-9w0jk | Requirement already satisfied: python-dateutil&gt;=2.7 in /miniconda3/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (2.8.1) ubyi50juw8-algo-1-9w0jk | Collecting kiwisolver&gt;=1.0.1 ubyi50juw8-algo-1-9w0jk | Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 15.9 MB/s eta 0:00:00:00:01ta -:--:-- ubyi50juw8-algo-1-9w0jk | Requirement already satisfied: pillow&gt;=6.2.0 in /miniconda3/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (9.1.1) ubyi50juw8-algo-1-9w0jk | Requirement already satisfied: pytz&gt;=2017.2 in /miniconda3/lib/python3.8/site-packages (from pandas&gt;=0.23-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (2022.1) ubyi50juw8-algo-1-9w0jk | Requirement already satisfied: six&gt;=1.5 in /miniconda3/lib/python3.8/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.15.0) ubyi50juw8-algo-1-9w0jk | Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, packaging, matplotlib, seaborn ubyi50juw8-algo-1-9w0jk | Successfully installed cycler-0.11.0 fonttools-4.34.4 kiwisolver-1.4.4 matplotlib-3.5.2 packaging-21.3 pyparsing-3.0.9 seaborn-0.11.2 ubyi50juw8-algo-1-9w0jk | WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv ubyi50juw8-algo-1-9w0jk | 2022-07-17 15:25:11,747 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) ubyi50juw8-algo-1-9w0jk | 2022-07-17 15:25:11,760 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) ubyi50juw8-algo-1-9w0jk | 2022-07-17 15:25:11,773 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) ubyi50juw8-algo-1-9w0jk | 2022-07-17 15:25:11,781 sagemaker-training-toolkit INFO Invoking user script ubyi50juw8-algo-1-9w0jk | ubyi50juw8-algo-1-9w0jk | Training Env: ubyi50juw8-algo-1-9w0jk | ubyi50juw8-algo-1-9w0jk | { ubyi50juw8-algo-1-9w0jk | &#34;additional_framework_parameters&#34;: {}, ubyi50juw8-algo-1-9w0jk | &#34;channel_input_dirs&#34;: { ubyi50juw8-algo-1-9w0jk | &#34;train&#34;: &#34;/opt/ml/input/data/train&#34;, ubyi50juw8-algo-1-9w0jk | &#34;test&#34;: &#34;/opt/ml/input/data/test&#34; ubyi50juw8-algo-1-9w0jk | }, ubyi50juw8-algo-1-9w0jk | &#34;current_host&#34;: &#34;algo-1-9w0jk&#34;, ubyi50juw8-algo-1-9w0jk | &#34;framework_module&#34;: &#34;sagemaker_sklearn_container.training:main&#34;, ubyi50juw8-algo-1-9w0jk | &#34;hosts&#34;: [ ubyi50juw8-algo-1-9w0jk | &#34;algo-1-9w0jk&#34; ubyi50juw8-algo-1-9w0jk | ], ubyi50juw8-algo-1-9w0jk | &#34;hyperparameters&#34;: { ubyi50juw8-algo-1-9w0jk | &#34;estimators&#34;: 10 ubyi50juw8-algo-1-9w0jk | }, ubyi50juw8-algo-1-9w0jk | &#34;input_config_dir&#34;: &#34;/opt/ml/input/config&#34;, ubyi50juw8-algo-1-9w0jk | &#34;input_data_config&#34;: { ubyi50juw8-algo-1-9w0jk | &#34;train&#34;: { ubyi50juw8-algo-1-9w0jk | &#34;TrainingInputMode&#34;: &#34;File&#34; ubyi50juw8-algo-1-9w0jk | }, ubyi50juw8-algo-1-9w0jk | &#34;test&#34;: { ubyi50juw8-algo-1-9w0jk | &#34;TrainingInputMode&#34;: &#34;File&#34; ubyi50juw8-algo-1-9w0jk | } ubyi50juw8-algo-1-9w0jk | }, ubyi50juw8-algo-1-9w0jk | &#34;input_dir&#34;: &#34;/opt/ml/input&#34;, ubyi50juw8-algo-1-9w0jk | &#34;is_master&#34;: true, ubyi50juw8-algo-1-9w0jk | &#34;job_name&#34;: &#34;sagemaker-scikit-learn-2022-07-17-15-25-04-516&#34;, ubyi50juw8-algo-1-9w0jk | &#34;log_level&#34;: 20, ubyi50juw8-algo-1-9w0jk | &#34;master_hostname&#34;: &#34;algo-1-9w0jk&#34;, ubyi50juw8-algo-1-9w0jk | &#34;model_dir&#34;: &#34;/opt/ml/model&#34;, ubyi50juw8-algo-1-9w0jk | &#34;module_dir&#34;: &#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-25-04-516/source/sourcedir.tar.gz&#34;, ubyi50juw8-algo-1-9w0jk | &#34;module_name&#34;: &#34;train_and_serve&#34;, ubyi50juw8-algo-1-9w0jk | &#34;network_interface_name&#34;: &#34;eth0&#34;, ubyi50juw8-algo-1-9w0jk | &#34;num_cpus&#34;: 2, ubyi50juw8-algo-1-9w0jk | &#34;num_gpus&#34;: 0, ubyi50juw8-algo-1-9w0jk | &#34;output_data_dir&#34;: &#34;/opt/ml/output/data&#34;, ubyi50juw8-algo-1-9w0jk | &#34;output_dir&#34;: &#34;/opt/ml/output&#34;, ubyi50juw8-algo-1-9w0jk | &#34;output_intermediate_dir&#34;: &#34;/opt/ml/output/intermediate&#34;, ubyi50juw8-algo-1-9w0jk | &#34;resource_config&#34;: { ubyi50juw8-algo-1-9w0jk | &#34;current_host&#34;: &#34;algo-1-9w0jk&#34;, ubyi50juw8-algo-1-9w0jk | &#34;hosts&#34;: [ ubyi50juw8-algo-1-9w0jk | &#34;algo-1-9w0jk&#34; ubyi50juw8-algo-1-9w0jk | ] ubyi50juw8-algo-1-9w0jk | }, ubyi50juw8-algo-1-9w0jk | &#34;user_entry_point&#34;: &#34;train_and_serve.py&#34; ubyi50juw8-algo-1-9w0jk | } ubyi50juw8-algo-1-9w0jk | ubyi50juw8-algo-1-9w0jk | Environment variables: ubyi50juw8-algo-1-9w0jk | ubyi50juw8-algo-1-9w0jk | SM_HOSTS=[&#34;algo-1-9w0jk&#34;] ubyi50juw8-algo-1-9w0jk | SM_NETWORK_INTERFACE_NAME=eth0 ubyi50juw8-algo-1-9w0jk | SM_HPS={&#34;estimators&#34;:10} ubyi50juw8-algo-1-9w0jk | SM_USER_ENTRY_POINT=train_and_serve.py ubyi50juw8-algo-1-9w0jk | SM_FRAMEWORK_PARAMS={} ubyi50juw8-algo-1-9w0jk | SM_RESOURCE_CONFIG={&#34;current_host&#34;:&#34;algo-1-9w0jk&#34;,&#34;hosts&#34;:[&#34;algo-1-9w0jk&#34;]} ubyi50juw8-algo-1-9w0jk | SM_INPUT_DATA_CONFIG={&#34;test&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;},&#34;train&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;}} ubyi50juw8-algo-1-9w0jk | SM_OUTPUT_DATA_DIR=/opt/ml/output/data ubyi50juw8-algo-1-9w0jk | SM_CHANNELS=[&#34;test&#34;,&#34;train&#34;] ubyi50juw8-algo-1-9w0jk | SM_CURRENT_HOST=algo-1-9w0jk ubyi50juw8-algo-1-9w0jk | SM_MODULE_NAME=train_and_serve ubyi50juw8-algo-1-9w0jk | SM_LOG_LEVEL=20 ubyi50juw8-algo-1-9w0jk | SM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main ubyi50juw8-algo-1-9w0jk | SM_INPUT_DIR=/opt/ml/input ubyi50juw8-algo-1-9w0jk | SM_INPUT_CONFIG_DIR=/opt/ml/input/config ubyi50juw8-algo-1-9w0jk | SM_OUTPUT_DIR=/opt/ml/output ubyi50juw8-algo-1-9w0jk | SM_NUM_CPUS=2 ubyi50juw8-algo-1-9w0jk | SM_NUM_GPUS=0 ubyi50juw8-algo-1-9w0jk | SM_MODEL_DIR=/opt/ml/model ubyi50juw8-algo-1-9w0jk | SM_MODULE_DIR=s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-25-04-516/source/sourcedir.tar.gz ubyi50juw8-algo-1-9w0jk | SM_TRAINING_ENV={&#34;additional_framework_parameters&#34;:{},&#34;channel_input_dirs&#34;:{&#34;test&#34;:&#34;/opt/ml/input/data/test&#34;,&#34;train&#34;:&#34;/opt/ml/input/data/train&#34;},&#34;current_host&#34;:&#34;algo-1-9w0jk&#34;,&#34;framework_module&#34;:&#34;sagemaker_sklearn_container.training:main&#34;,&#34;hosts&#34;:[&#34;algo-1-9w0jk&#34;],&#34;hyperparameters&#34;:{&#34;estimators&#34;:10},&#34;input_config_dir&#34;:&#34;/opt/ml/input/config&#34;,&#34;input_data_config&#34;:{&#34;test&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;},&#34;train&#34;:{&#34;TrainingInputMode&#34;:&#34;File&#34;}},&#34;input_dir&#34;:&#34;/opt/ml/input&#34;,&#34;is_master&#34;:true,&#34;job_name&#34;:&#34;sagemaker-scikit-learn-2022-07-17-15-25-04-516&#34;,&#34;log_level&#34;:20,&#34;master_hostname&#34;:&#34;algo-1-9w0jk&#34;,&#34;model_dir&#34;:&#34;/opt/ml/model&#34;,&#34;module_dir&#34;:&#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-25-04-516/source/sourcedir.tar.gz&#34;,&#34;module_name&#34;:&#34;train_and_serve&#34;,&#34;network_interface_name&#34;:&#34;eth0&#34;,&#34;num_cpus&#34;:2,&#34;num_gpus&#34;:0,&#34;output_data_dir&#34;:&#34;/opt/ml/output/data&#34;,&#34;output_dir&#34;:&#34;/opt/ml/output&#34;,&#34;output_intermediate_dir&#34;:&#34;/opt/ml/output/intermediate&#34;,&#34;resource_config&#34;:{&#34;current_host&#34;:&#34;algo-1-9w0jk&#34;,&#34;hosts&#34;:[&#34;algo-1-9w0jk&#34;]},&#34;user_entry_point&#34;:&#34;train_and_serve.py&#34;} ubyi50juw8-algo-1-9w0jk | SM_USER_ARGS=[&#34;--estimators&#34;,&#34;10&#34;] ubyi50juw8-algo-1-9w0jk | SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate ubyi50juw8-algo-1-9w0jk | SM_CHANNEL_TRAIN=/opt/ml/input/data/train ubyi50juw8-algo-1-9w0jk | SM_CHANNEL_TEST=/opt/ml/input/data/test ubyi50juw8-algo-1-9w0jk | SM_HP_ESTIMATORS=10 ubyi50juw8-algo-1-9w0jk | PYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages ubyi50juw8-algo-1-9w0jk | ubyi50juw8-algo-1-9w0jk | Invoking script with the following command: ubyi50juw8-algo-1-9w0jk | ubyi50juw8-algo-1-9w0jk | /miniconda3/bin/python train_and_serve.py --estimators 10 ubyi50juw8-algo-1-9w0jk | ubyi50juw8-algo-1-9w0jk | ubyi50juw8-algo-1-9w0jk | command line arguments: Namespace(estimators=10, sm_channel_test=&#39;/opt/ml/input/data/test&#39;, sm_channel_train=&#39;/opt/ml/input/data/train&#39;, sm_model_dir=&#39;/opt/ml/model&#39;, sm_output_data_dir=&#39;/opt/ml/output/data&#39;) ubyi50juw8-algo-1-9w0jk | training_dir: /opt/ml/input/data/train ubyi50juw8-algo-1-9w0jk | training_dir files list: [&#39;train.csv&#39;] ubyi50juw8-algo-1-9w0jk | testing_dir: /opt/ml/input/data/test ubyi50juw8-algo-1-9w0jk | testing_dir files list: [&#39;test.csv&#39;] ubyi50juw8-algo-1-9w0jk | sm_model_dir: /opt/ml/model ubyi50juw8-algo-1-9w0jk | output_data_dir: /opt/ml/output/data ubyi50juw8-algo-1-9w0jk | X_train.shape: (120, 4) ubyi50juw8-algo-1-9w0jk | y_train.shape: (120,) ubyi50juw8-algo-1-9w0jk | X_train.shape: (30, 4) ubyi50juw8-algo-1-9w0jk | y_train.shape: (30,) ubyi50juw8-algo-1-9w0jk | sm_model_dir: /opt/ml/model ubyi50juw8-algo-1-9w0jk | sm_model_dir files list: [&#39;model.joblib&#39;] ubyi50juw8-algo-1-9w0jk | output_data_dir: /opt/ml/output/data ubyi50juw8-algo-1-9w0jk | output_data_dir files list: [&#39;y_pred.csv&#39;, &#39;output_cm.png&#39;] ubyi50juw8-algo-1-9w0jk | 2022-07-17 15:25:13,824 sagemaker-containers INFO Reporting training SUCCESS ubyi50juw8-algo-1-9w0jk exited with code 0 Aborting on container exit... . Failed to delete: /tmp/tmp9kiuooe_/algo-1-9w0jk Please remove it manually. . ===== Job Complete ===== Attaching to e6h08rxuj4-algo-1-bitrj e6h08rxuj4-algo-1-bitrj | 2022-07-17 15:25:16,610 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed) e6h08rxuj4-algo-1-bitrj | 2022-07-17 15:25:16,614 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed) e6h08rxuj4-algo-1-bitrj | 2022-07-17 15:25:16,615 INFO - sagemaker-containers - nginx config: e6h08rxuj4-algo-1-bitrj | worker_processes auto; e6h08rxuj4-algo-1-bitrj | daemon off; e6h08rxuj4-algo-1-bitrj | pid /tmp/nginx.pid; e6h08rxuj4-algo-1-bitrj | error_log /dev/stderr; e6h08rxuj4-algo-1-bitrj | e6h08rxuj4-algo-1-bitrj | worker_rlimit_nofile 4096; e6h08rxuj4-algo-1-bitrj | e6h08rxuj4-algo-1-bitrj | events { e6h08rxuj4-algo-1-bitrj | worker_connections 2048; e6h08rxuj4-algo-1-bitrj | } e6h08rxuj4-algo-1-bitrj | e6h08rxuj4-algo-1-bitrj | http { e6h08rxuj4-algo-1-bitrj | include /etc/nginx/mime.types; e6h08rxuj4-algo-1-bitrj | default_type application/octet-stream; e6h08rxuj4-algo-1-bitrj | access_log /dev/stdout combined; e6h08rxuj4-algo-1-bitrj | e6h08rxuj4-algo-1-bitrj | upstream gunicorn { e6h08rxuj4-algo-1-bitrj | server unix:/tmp/gunicorn.sock; e6h08rxuj4-algo-1-bitrj | } e6h08rxuj4-algo-1-bitrj | e6h08rxuj4-algo-1-bitrj | server { e6h08rxuj4-algo-1-bitrj | listen 8080 deferred; e6h08rxuj4-algo-1-bitrj | client_max_body_size 0; e6h08rxuj4-algo-1-bitrj | e6h08rxuj4-algo-1-bitrj | keepalive_timeout 3; e6h08rxuj4-algo-1-bitrj | e6h08rxuj4-algo-1-bitrj | location ~ ^/(ping|invocations|execution-parameters) { e6h08rxuj4-algo-1-bitrj | proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; e6h08rxuj4-algo-1-bitrj | proxy_set_header Host $http_host; e6h08rxuj4-algo-1-bitrj | proxy_redirect off; e6h08rxuj4-algo-1-bitrj | proxy_read_timeout 60s; e6h08rxuj4-algo-1-bitrj | proxy_pass http://gunicorn; e6h08rxuj4-algo-1-bitrj | } e6h08rxuj4-algo-1-bitrj | e6h08rxuj4-algo-1-bitrj | location / { e6h08rxuj4-algo-1-bitrj | return 404 &#34;{}&#34;; e6h08rxuj4-algo-1-bitrj | } e6h08rxuj4-algo-1-bitrj | e6h08rxuj4-algo-1-bitrj | } e6h08rxuj4-algo-1-bitrj | } e6h08rxuj4-algo-1-bitrj | e6h08rxuj4-algo-1-bitrj | e6h08rxuj4-algo-1-bitrj | 2022-07-17 15:25:16,826 INFO - sagemaker-containers - Module train_and_serve does not provide a setup.py. e6h08rxuj4-algo-1-bitrj | Generating setup.py e6h08rxuj4-algo-1-bitrj | 2022-07-17 15:25:16,826 INFO - sagemaker-containers - Generating setup.cfg e6h08rxuj4-algo-1-bitrj | 2022-07-17 15:25:16,826 INFO - sagemaker-containers - Generating MANIFEST.in e6h08rxuj4-algo-1-bitrj | 2022-07-17 15:25:16,826 INFO - sagemaker-containers - Installing module with the following command: e6h08rxuj4-algo-1-bitrj | /miniconda3/bin/python3 -m pip install . -r requirements.txt e6h08rxuj4-algo-1-bitrj | Processing /opt/ml/code e6h08rxuj4-algo-1-bitrj | Preparing metadata (setup.py) ... done e6h08rxuj4-algo-1-bitrj | Collecting seaborn==0.11.2 e6h08rxuj4-algo-1-bitrj | Downloading seaborn-0.11.2-py3-none-any.whl (292 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 292.8/292.8 kB 36.6 MB/s eta 0:00:0031m? eta -:--:-- e6h08rxuj4-algo-1-bitrj | Requirement already satisfied: numpy&gt;=1.15 in /miniconda3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.21.0) e6h08rxuj4-algo-1-bitrj | Collecting matplotlib&gt;=2.2 e6h08rxuj4-algo-1-bitrj | Downloading matplotlib-3.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/11.3 MB 101.9 MB/s eta 0:00:0000:0100:01:--:-- e6h08rxuj4-algo-1-bitrj | Requirement already satisfied: scipy&gt;=1.0 in /miniconda3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.5.3) e6h08rxuj4-algo-1-bitrj | Requirement already satisfied: pandas&gt;=0.23 in /miniconda3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.1.3) e6h08rxuj4-algo-1-bitrj | Collecting kiwisolver&gt;=1.0.1 e6h08rxuj4-algo-1-bitrj | Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 94.2 MB/s eta 0:00:0031m? eta -:--:-- e6h08rxuj4-algo-1-bitrj | Collecting cycler&gt;=0.10 e6h08rxuj4-algo-1-bitrj | Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB) e6h08rxuj4-algo-1-bitrj | Collecting packaging&gt;=20.0 e6h08rxuj4-algo-1-bitrj | Downloading packaging-21.3-py3-none-any.whl (40 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 11.9 MB/s eta 0:00:001m? eta -:--:-- e6h08rxuj4-algo-1-bitrj | Collecting fonttools&gt;=4.22.0 e6h08rxuj4-algo-1-bitrj | Downloading fonttools-4.34.4-py3-none-any.whl (944 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 944.1/944.1 kB 12.1 MB/s eta 0:00:0000:01eta -:--:-- e6h08rxuj4-algo-1-bitrj | Requirement already satisfied: pillow&gt;=6.2.0 in /miniconda3/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (9.1.1) e6h08rxuj4-algo-1-bitrj | Collecting pyparsing&gt;=2.2.1 e6h08rxuj4-algo-1-bitrj | Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.3/98.3 kB 27.3 MB/s eta 0:00:001m? eta -:--:-- e6h08rxuj4-algo-1-bitrj | Requirement already satisfied: python-dateutil&gt;=2.7 in /miniconda3/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (2.8.1) e6h08rxuj4-algo-1-bitrj | Requirement already satisfied: pytz&gt;=2017.2 in /miniconda3/lib/python3.8/site-packages (from pandas&gt;=0.23-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (2022.1) e6h08rxuj4-algo-1-bitrj | Requirement already satisfied: six&gt;=1.5 in /miniconda3/lib/python3.8/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.15.0) e6h08rxuj4-algo-1-bitrj | Building wheels for collected packages: train-and-serve e6h08rxuj4-algo-1-bitrj | Building wheel for train-and-serve (setup.py) ... -2022/07/17 15:25:19 [crit] 14#14: *1 connect() to unix:/tmp/gunicorn.sock failed (2: No such file or directory) while connecting to upstream, client: 172.18.0.1, server: , request: &#34;GET /ping HTTP/1.1&#34;, upstream: &#34;http://unix:/tmp/gunicorn.sock:/ping&#34;, host: &#34;localhost:8080&#34; e6h08rxuj4-algo-1-bitrj | 172.18.0.1 - - [17/Jul/2022:15:25:19 +0000] &#34;GET /ping HTTP/1.1&#34; 502 182 &#34;-&#34; &#34;python-urllib3/1.26.8&#34; e6h08rxuj4-algo-1-bitrj |done e6h08rxuj4-algo-1-bitrj | Created wheel for train-and-serve: filename=train_and_serve-1.0.0-py2.py3-none-any.whl size=6682 sha256=f4b6952b904adaa9a17270142b81e0746714104ac88739bf2ba644d15a4fe837 e6h08rxuj4-algo-1-bitrj | Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-6muul1xe/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00 e6h08rxuj4-algo-1-bitrj | Successfully built train-and-serve e6h08rxuj4-algo-1-bitrj | Installing collected packages: train-and-serve, pyparsing, kiwisolver, fonttools, cycler, packaging, matplotlib, seaborn e6h08rxuj4-algo-1-bitrj | Successfully installed cycler-0.11.0 fonttools-4.34.4 kiwisolver-1.4.4 matplotlib-3.5.2 packaging-21.3 pyparsing-3.0.9 seaborn-0.11.2 train-and-serve-1.0.0 e6h08rxuj4-algo-1-bitrj | WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv e6h08rxuj4-algo-1-bitrj | 2022/07/17 15:25:24 [crit] 14#14: *3 connect() to unix:/tmp/gunicorn.sock failed (2: No such file or directory) while connecting to upstream, client: 172.18.0.1, server: , request: &#34;GET /ping HTTP/1.1&#34;, upstream: &#34;http://unix:/tmp/gunicorn.sock:/ping&#34;, host: &#34;localhost:8080&#34; e6h08rxuj4-algo-1-bitrj | 172.18.0.1 - - [17/Jul/2022:15:25:24 +0000] &#34;GET /ping HTTP/1.1&#34; 502 182 &#34;-&#34; &#34;python-urllib3/1.26.8&#34; e6h08rxuj4-algo-1-bitrj | 2022-07-17 15:25:24,850 INFO - matplotlib.font_manager - generated new fontManager e6h08rxuj4-algo-1-bitrj | [2022-07-17 15:25:25 +0000] [35] [INFO] Starting gunicorn 20.0.4 e6h08rxuj4-algo-1-bitrj | [2022-07-17 15:25:25 +0000] [35] [INFO] Listening at: unix:/tmp/gunicorn.sock (35) e6h08rxuj4-algo-1-bitrj | [2022-07-17 15:25:25 +0000] [35] [INFO] Using worker: gevent e6h08rxuj4-algo-1-bitrj | [2022-07-17 15:25:25 +0000] [37] [INFO] Booting worker with pid: 37 e6h08rxuj4-algo-1-bitrj | [2022-07-17 15:25:25 +0000] [38] [INFO] Booting worker with pid: 38 e6h08rxuj4-algo-1-bitrj | 2022-07-17 15:25:29,802 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed) e6h08rxuj4-algo-1-bitrj | model_fn model_dir: /opt/ml/model !e6h08rxuj4-algo-1-bitrj | 172.18.0.1 - - [17/Jul/2022:15:25:31 +0000] &#34;GET /ping HTTP/1.1&#34; 200 0 &#34;-&#34; &#34;python-urllib3/1.26.8&#34; . . sk_endpoint_name = sk_predictor.endpoint_name sk_endpoint_name . &#39;sagemaker-scikit-learn-2022-07-17-15-25-14-401&#39; . # send JSON request to endpoint import json client = session_local.sagemaker_runtime_client request_body = {&quot;Input&quot;: [[9.0, 3571, 1976, 0.525]]} data = json.loads(json.dumps(request_body)) payload = json.dumps(data) response = client.invoke_endpoint( EndpointName=sk_endpoint_name, ContentType=&quot;application/json&quot;, Body=payload ) result = json.loads(response[&quot;Body&quot;].read().decode())[&quot;Output&quot;] result . e6h08rxuj4-algo-1-bitrj | 2022-07-17 15:25:31,439 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed) e6h08rxuj4-algo-1-bitrj | model_fn model_dir: /opt/ml/model e6h08rxuj4-algo-1-bitrj | 172.18.0.1 - - [17/Jul/2022:15:25:32 +0000] &#34;POST /invocations HTTP/1.1&#34; 200 13 &#34;-&#34; &#34;python-urllib3/1.26.8&#34; . 2 . # get JSON response from endpoint print(&quot;Predicted class category {} ({})&quot;.format(result, categories_map[result])) . Predicted class category 2 (Iris-virginica) . Make sure we delete the endpoint once we have tested our deployed model. . sk_predictor.delete_endpoint() . Gracefully stopping... (press Ctrl+C again to force) . SKLearn model training and serving in SageMaker managed environment . Now that our script is complete and we have tested it in a local environment, let&#39;s proceed to train and deploy it in Amazon SageMaker managed environment. Moving from a local environment to managed environment is very simple. We only need to change the instance type. . # train and deploy model with input and output as JSON objects sk_estimator = SKLearn( entry_point=script_file_name, source_dir=script_path, dependencies=[custom_library_path], role=role, instance_count=1, instance_type=&#39;ml.m5.large&#39;, framework_version=&quot;1.0-1&quot;, hyperparameters={&quot;estimators&quot;:10}, ) sk_estimator.fit({&quot;train&quot;: s3_train_uri, &quot;test&quot;: s3_test_uri}) . 2022-07-17 15:25:33 Starting - Starting the training job... 2022-07-17 15:25:57 Starting - Preparing the instances for trainingProfilerReport-1658071533: InProgress ......... 2022-07-17 15:27:17 Downloading - Downloading input data... 2022-07-17 15:27:57 Training - Downloading the training image... 2022-07-17 15:28:35 Training - Training image download completed. Training in progress...2022-07-17 15:28:37,458 sagemaker-containers INFO Imported framework sagemaker_sklearn_container.training 2022-07-17 15:28:37,462 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) 2022-07-17 15:28:37,478 sagemaker_sklearn_container.training INFO Invoking user training script. 2022-07-17 15:28:37,948 sagemaker-training-toolkit INFO Installing dependencies from requirements.txt: /miniconda3/bin/python -m pip install -r requirements.txt Collecting seaborn==0.11.2 Downloading seaborn-0.11.2-py3-none-any.whl (292 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 292.8/292.8 kB 11.5 MB/s eta 0:00:00 Requirement already satisfied: numpy&gt;=1.15 in /miniconda3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.21.0) Collecting matplotlib&gt;=2.2 Downloading matplotlib-3.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/11.3 MB 69.0 MB/s eta 0:00:00 Requirement already satisfied: scipy&gt;=1.0 in /miniconda3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.5.3) Requirement already satisfied: pandas&gt;=0.23 in /miniconda3/lib/python3.8/site-packages (from seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.1.3) Requirement already satisfied: pillow&gt;=6.2.0 in /miniconda3/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (9.1.1) Requirement already satisfied: python-dateutil&gt;=2.7 in /miniconda3/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (2.8.1) Collecting kiwisolver&gt;=1.0.1 Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 37.9 MB/s eta 0:00:00 Collecting packaging&gt;=20.0 Downloading packaging-21.3-py3-none-any.whl (40 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 7.4 MB/s eta 0:00:00 Collecting cycler&gt;=0.10 Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB) Collecting pyparsing&gt;=2.2.1 Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.3/98.3 kB 18.2 MB/s eta 0:00:00 Collecting fonttools&gt;=4.22.0 Downloading fonttools-4.34.4-py3-none-any.whl (944 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 944.1/944.1 kB 52.9 MB/s eta 0:00:00 Requirement already satisfied: pytz&gt;=2017.2 in /miniconda3/lib/python3.8/site-packages (from pandas&gt;=0.23-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (2022.1) Requirement already satisfied: six&gt;=1.5 in /miniconda3/lib/python3.8/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&gt;=2.2-&gt;seaborn==0.11.2-&gt;-r requirements.txt (line 2)) (1.15.0) Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, packaging, matplotlib, seaborn Successfully installed cycler-0.11.0 fonttools-4.34.4 kiwisolver-1.4.4 matplotlib-3.5.2 packaging-21.3 pyparsing-3.0.9 seaborn-0.11.2 WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv 2022-07-17 15:28:44,790 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) 2022-07-17 15:28:44,810 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) 2022-07-17 15:28:44,831 sagemaker-training-toolkit INFO No GPUs detected (normal if no gpus installed) 2022-07-17 15:28:44,847 sagemaker-training-toolkit INFO Invoking user script Training Env: { &#34;additional_framework_parameters&#34;: {}, &#34;channel_input_dirs&#34;: { &#34;test&#34;: &#34;/opt/ml/input/data/test&#34;, &#34;train&#34;: &#34;/opt/ml/input/data/train&#34; }, &#34;current_host&#34;: &#34;algo-1&#34;, &#34;framework_module&#34;: &#34;sagemaker_sklearn_container.training:main&#34;, &#34;hosts&#34;: [ &#34;algo-1&#34; ], &#34;hyperparameters&#34;: { &#34;estimators&#34;: 10 }, &#34;input_config_dir&#34;: &#34;/opt/ml/input/config&#34;, &#34;input_data_config&#34;: { &#34;test&#34;: { &#34;TrainingInputMode&#34;: &#34;File&#34;, &#34;S3DistributionType&#34;: &#34;FullyReplicated&#34;, &#34;RecordWrapperType&#34;: &#34;None&#34; }, &#34;train&#34;: { &#34;TrainingInputMode&#34;: &#34;File&#34;, &#34;S3DistributionType&#34;: &#34;FullyReplicated&#34;, &#34;RecordWrapperType&#34;: &#34;None&#34; } }, &#34;input_dir&#34;: &#34;/opt/ml/input&#34;, &#34;is_master&#34;: true, &#34;job_name&#34;: &#34;sagemaker-scikit-learn-2022-07-17-15-25-33-210&#34;, &#34;log_level&#34;: 20, &#34;master_hostname&#34;: &#34;algo-1&#34;, &#34;model_dir&#34;: &#34;/opt/ml/model&#34;, &#34;module_dir&#34;: &#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-25-33-210/source/sourcedir.tar.gz&#34;, &#34;module_name&#34;: &#34;train_and_serve&#34;, &#34;network_interface_name&#34;: &#34;eth0&#34;, &#34;num_cpus&#34;: 2, &#34;num_gpus&#34;: 0, &#34;output_data_dir&#34;: &#34;/opt/ml/output/data&#34;, &#34;output_dir&#34;: &#34;/opt/ml/output&#34;, &#34;output_intermediate_dir&#34;: &#34;/opt/ml/output/intermediate&#34;, &#34;resource_config&#34;: { &#34;current_host&#34;: &#34;algo-1&#34;, &#34;current_instance_type&#34;: &#34;ml.m5.large&#34;, &#34;current_group_name&#34;: &#34;homogeneousCluster&#34;, &#34;hosts&#34;: [ &#34;algo-1&#34; ], &#34;instance_groups&#34;: [ { &#34;instance_group_name&#34;: &#34;homogeneousCluster&#34;, &#34;instance_type&#34;: &#34;ml.m5.large&#34;, &#34;hosts&#34;: [ &#34;algo-1&#34; ] } ], &#34;network_interface_name&#34;: &#34;eth0&#34; }, &#34;user_entry_point&#34;: &#34;train_and_serve.py&#34; } Environment variables: SM_HOSTS=[&#34;algo-1&#34;] SM_NETWORK_INTERFACE_NAME=eth0 SM_HPS={&#34;estimators&#34;:10} SM_USER_ENTRY_POINT=train_and_serve.py SM_FRAMEWORK_PARAMS={} SM_RESOURCE_CONFIG={&#34;current_group_name&#34;:&#34;homogeneousCluster&#34;,&#34;current_host&#34;:&#34;algo-1&#34;,&#34;current_instance_type&#34;:&#34;ml.m5.large&#34;,&#34;hosts&#34;:[&#34;algo-1&#34;],&#34;instance_groups&#34;:[{&#34;hosts&#34;:[&#34;algo-1&#34;],&#34;instance_group_name&#34;:&#34;homogeneousCluster&#34;,&#34;instance_type&#34;:&#34;ml.m5.large&#34;}],&#34;network_interface_name&#34;:&#34;eth0&#34;} SM_INPUT_DATA_CONFIG={&#34;test&#34;:{&#34;RecordWrapperType&#34;:&#34;None&#34;,&#34;S3DistributionType&#34;:&#34;FullyReplicated&#34;,&#34;TrainingInputMode&#34;:&#34;File&#34;},&#34;train&#34;:{&#34;RecordWrapperType&#34;:&#34;None&#34;,&#34;S3DistributionType&#34;:&#34;FullyReplicated&#34;,&#34;TrainingInputMode&#34;:&#34;File&#34;}} SM_OUTPUT_DATA_DIR=/opt/ml/output/data SM_CHANNELS=[&#34;test&#34;,&#34;train&#34;] SM_CURRENT_HOST=algo-1 SM_MODULE_NAME=train_and_serve SM_LOG_LEVEL=20 SM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main SM_INPUT_DIR=/opt/ml/input SM_INPUT_CONFIG_DIR=/opt/ml/input/config SM_OUTPUT_DIR=/opt/ml/output SM_NUM_CPUS=2 SM_NUM_GPUS=0 SM_MODEL_DIR=/opt/ml/model SM_MODULE_DIR=s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-25-33-210/source/sourcedir.tar.gz SM_TRAINING_ENV={&#34;additional_framework_parameters&#34;:{},&#34;channel_input_dirs&#34;:{&#34;test&#34;:&#34;/opt/ml/input/data/test&#34;,&#34;train&#34;:&#34;/opt/ml/input/data/train&#34;},&#34;current_host&#34;:&#34;algo-1&#34;,&#34;framework_module&#34;:&#34;sagemaker_sklearn_container.training:main&#34;,&#34;hosts&#34;:[&#34;algo-1&#34;],&#34;hyperparameters&#34;:{&#34;estimators&#34;:10},&#34;input_config_dir&#34;:&#34;/opt/ml/input/config&#34;,&#34;input_data_config&#34;:{&#34;test&#34;:{&#34;RecordWrapperType&#34;:&#34;None&#34;,&#34;S3DistributionType&#34;:&#34;FullyReplicated&#34;,&#34;TrainingInputMode&#34;:&#34;File&#34;},&#34;train&#34;:{&#34;RecordWrapperType&#34;:&#34;None&#34;,&#34;S3DistributionType&#34;:&#34;FullyReplicated&#34;,&#34;TrainingInputMode&#34;:&#34;File&#34;}},&#34;input_dir&#34;:&#34;/opt/ml/input&#34;,&#34;is_master&#34;:true,&#34;job_name&#34;:&#34;sagemaker-scikit-learn-2022-07-17-15-25-33-210&#34;,&#34;log_level&#34;:20,&#34;master_hostname&#34;:&#34;algo-1&#34;,&#34;model_dir&#34;:&#34;/opt/ml/model&#34;,&#34;module_dir&#34;:&#34;s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-25-33-210/source/sourcedir.tar.gz&#34;,&#34;module_name&#34;:&#34;train_and_serve&#34;,&#34;network_interface_name&#34;:&#34;eth0&#34;,&#34;num_cpus&#34;:2,&#34;num_gpus&#34;:0,&#34;output_data_dir&#34;:&#34;/opt/ml/output/data&#34;,&#34;output_dir&#34;:&#34;/opt/ml/output&#34;,&#34;output_intermediate_dir&#34;:&#34;/opt/ml/output/intermediate&#34;,&#34;resource_config&#34;:{&#34;current_group_name&#34;:&#34;homogeneousCluster&#34;,&#34;current_host&#34;:&#34;algo-1&#34;,&#34;current_instance_type&#34;:&#34;ml.m5.large&#34;,&#34;hosts&#34;:[&#34;algo-1&#34;],&#34;instance_groups&#34;:[{&#34;hosts&#34;:[&#34;algo-1&#34;],&#34;instance_group_name&#34;:&#34;homogeneousCluster&#34;,&#34;instance_type&#34;:&#34;ml.m5.large&#34;}],&#34;network_interface_name&#34;:&#34;eth0&#34;},&#34;user_entry_point&#34;:&#34;train_and_serve.py&#34;} SM_USER_ARGS=[&#34;--estimators&#34;,&#34;10&#34;] SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate SM_CHANNEL_TEST=/opt/ml/input/data/test SM_CHANNEL_TRAIN=/opt/ml/input/data/train SM_HP_ESTIMATORS=10 PYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages Invoking script with the following command: /miniconda3/bin/python train_and_serve.py --estimators 10 command line arguments: Namespace(estimators=10, sm_channel_test=&#39;/opt/ml/input/data/test&#39;, sm_channel_train=&#39;/opt/ml/input/data/train&#39;, sm_model_dir=&#39;/opt/ml/model&#39;, sm_output_data_dir=&#39;/opt/ml/output/data&#39;) training_dir: /opt/ml/input/data/train training_dir files list: [&#39;train.csv&#39;] testing_dir: /opt/ml/input/data/test testing_dir files list: [&#39;test.csv&#39;] sm_model_dir: /opt/ml/model output_data_dir: /opt/ml/output/data X_train.shape: (120, 4) y_train.shape: (120,) X_train.shape: (30, 4) y_train.shape: (30,) sm_model_dir: /opt/ml/model sm_model_dir files list: [&#39;model.joblib&#39;] output_data_dir: /opt/ml/output/data output_data_dir files list: [&#39;y_pred.csv&#39;, &#39;output_cm.png&#39;] 2022-07-17 15:28:48,476 sagemaker-containers INFO Reporting training SUCCESS 2022-07-17 15:28:58 Uploading - Uploading generated training model 2022-07-17 15:29:18 Completed - Training job completed Training seconds: 116 Billable seconds: 116 . . . We have used AWS managed ml.m5.large instance for training. Once the training job is complete, model artifacts are uploaded to the S3 bucket. In the end, it also shows the billable seconds. . Let&#39;s confirm that the session we are using is not local. . sk_estimator.sagemaker_session . &lt;sagemaker.session.Session at 0x7f80a7d15700&gt; . Now deploy it on SageMaker managed ml.t2.medium instance. . sk_predictor = sk_estimator.deploy( initial_instance_count=1, instance_type=&#39;ml.t2.medium&#39; ) . -! . Test deployed model with a sample request. . # send JSON request to endpoint import json client = session.sagemaker_runtime_client request_body = {&quot;Input&quot;: [[9.0, 3571, 1976, 0.525]]} data = json.loads(json.dumps(request_body)) payload = json.dumps(data) response = client.invoke_endpoint( EndpointName=sk_predictor.endpoint_name, ContentType=&quot;application/json&quot;, Body=payload, ) result = json.loads(response[&quot;Body&quot;].read().decode())[&quot;Output&quot;] result . 2 . # get JSON response from endpoint print(&quot;Predicted class category {} ({})&quot;.format(result, categories_map[result])) . Predicted class category 2 (Iris-virginica) . Again, don&#39;t forget to delete the endpoint once you are done with testing. . sk_predictor.delete_endpoint() .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/07/07/sagemaker-script-mode.html",
            "relUrl": "/aws/ml/sagemaker/2022/07/07/sagemaker-script-mode.html",
            "date": " • Jul 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Loading SageMaker Linear Learner Model with Apache MXNet in Python",
            "content": ". About . You have trained a model with Amazon SageMaker&#39;s built-in algorithm Linear Learner. You can test this model by deploying it on a SageMaker endpoint. But you want to test this model in your local environment. In this post, we will learn to use Apache MXNet and Gluon API to load the model in a local environment, extract its parameters, and perform predictions. . Introduction . Apache MXNet is a fully featured, flexibly programmable, and ultra-scalable deep learning framework supporting state of the art in deep learning models, including convolutional neural networks (CNNs) and long short-term memory networks (LSTMs). Amazon has selected MXNet as their deep learning framework of choice (see Amazon CTO, Werner Vogels blog post on this). When you train a deep learning model using Amazon SageMaker builtin algorithm then there are high chances that the model has been trained and saved using MXNet framework. If a model has been saved with MXNet then we can use the same library to load that model in a local environment. . In my last post Demystifying Amazon SageMaker Training for scikit-learn Lovers, I used SageMaker builtin Linear Learner algorithm to train a model on Boston housing dataset. Once the training was complete the model artifacts were stored on the S3 bucket at the following location . s3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/output/linear-learner-2022-06-16-09-04-57-576/output/model.tar.gz . Note that Amazon Linear Learner is built using a Neural Network and is different from scikit-learn linear regression algorithm. Linear Learner documentation does not provide details on the architecture of this neural network but it does mention that it trains using a distributed implementation of stochastic gradient descent (SGD). We can also specify the hyperparameters such as momentum, learning rate, and the learning rate schedule. Also, note that not all SageMaker built-in models are using deep learning e.g. XGBoost which is based on regression trees. If you have trained xgboost model then to load this model in a local environment you will have to use xgboost library, and the MXNet library will not work for it. . Since Linear Learner is based on deep learning, We can use MXNet Gluon API to load this model in our local environment and make some predictions. . This post assumes that you have already trained a Linear Learner model and its artifacts are available on the S3 bucket. If you have not done so then you may use my another post to train a Linear Learner on the Boston housing dataset. . Environment . This notebook is prepared with Amazon SageMaker Studio using Python 3 (MXNet 1.9 Python 3.8 CPU Optimized) Kernel and ml.t3.medium instance. . . # AWS CLI version !aws --version . aws-cli/1.22.42 Python/3.8.10 Linux/4.14.281-212.502.amzn2.x86_64 botocore/1.23.42 . # OS version !cat /etc/os-release . NAME=&#34;Ubuntu&#34; VERSION=&#34;20.04.3 LTS (Focal Fossa)&#34; ID=ubuntu ID_LIKE=debian PRETTY_NAME=&#34;Ubuntu 20.04.3 LTS&#34; VERSION_ID=&#34;20.04&#34; HOME_URL=&#34;https://www.ubuntu.com/&#34; SUPPORT_URL=&#34;https://help.ubuntu.com/&#34; BUG_REPORT_URL=&#34;https://bugs.launchpad.net/ubuntu/&#34; PRIVACY_POLICY_URL=&#34;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&#34; VERSION_CODENAME=focal UBUNTU_CODENAME=focal . Loading a SageMaker Linear Learner model with Apache MXNet in Python . Let&#39;s initialize SageMaker API session. . import sagemaker session = sagemaker.Session() role = sagemaker.get_execution_role() bucket = session.default_bucket() region = session.boto_region_name print(f&quot;sagemaker.__version__: {sagemaker.__version__}&quot;) print(f&quot;Session: {session}&quot;) print(f&quot;Role: {role}&quot;) print(f&quot;Bucket: {bucket}&quot;) print(f&quot;Region: {region}&quot;) . sagemaker.__version__: 2.73.0 Session: &lt;sagemaker.session.Session object at 0x7f6b0500a760&gt; Role: arn:aws:iam::801598032724:role/service-role/AmazonSageMaker-ExecutionRole-20220516T161743 Bucket: sagemaker-us-east-1-801598032724 Region: us-east-1 . We have our trained model artifacts available on S3 bucket. Let&#39;s define that bucket path. . model_data = &quot;s3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/output/linear-learner-2022-06-16-09-04-57-576/output/model.tar.gz&quot; . We will use SageMaker SDK to download model artifacts from the S3 bucket to a local directory. Let&#39;s define the local path. . local_path = &quot;datasets/2022-07-05-aws-linear-learner-apache-mxnet-python/&quot; . Download the model artifacts. . from sagemaker.s3 import S3Downloader S3Downloader.download( s3_uri=model_data, local_path=local_path, sagemaker_session=session ) . Once downloaded, you will find an archive &quot;model.tar.gz&quot; in the local directory. Let&#39;s extract this file. . !tar -xzvf $local_path/model.tar.gz -C $local_path . model_algo-1 . Extracting once will give us a zip file. Let&#39;s unzip it to get the model contents . !unzip $local_path/model_algo-1 -d $local_path . Archive: datasets/2022-07-05-aws-linear-learner-apache-mxnet-python//model_algo-1 extracting: datasets/2022-07-05-aws-linear-learner-apache-mxnet-python/additional-params.json extracting: datasets/2022-07-05-aws-linear-learner-apache-mxnet-python/mx-mod-symbol.json extracting: datasets/2022-07-05-aws-linear-learner-apache-mxnet-python/manifest.json extracting: datasets/2022-07-05-aws-linear-learner-apache-mxnet-python/mx-mod-0000.params . Extracted model has two important files. . mx-mod-symbol.json is the JSON file that defines the computational graph for the model | mx-mod-0000.params is a binary file that contains the parameters for the trained model | . Serializing models as JSON files has the benefit that these models can be loaded from other language bindings like C++ or Scala for faster inference or inference in different environments. You can read more about it here: Saving and Loading Gluon Models. . import mxnet import pprint from mxnet import gluon from json import load as json_load from json import dumps as json_dumps . Gluon API is a wrapper around low level MXNet API to provide a simple interface for deep learning. You may read more about this API here: mxnet.gluon . Let&#39;s read model computational graph. . sym_json = json_load(open(f&quot;{local_path}mx-mod-symbol.json&quot;)) sym_json_string = json_dumps(sym_json) . from pprint import pprint pprint(sym_json) . {&#39;arg_nodes&#39;: [0, 1, 3, 5], &#39;attrs&#39;: {&#39;mxnet_version&#39;: [&#39;int&#39;, 10301]}, &#39;heads&#39;: [[6, 0, 0]], &#39;node_row_ptr&#39;: [0, 1, 2, 3, 4, 5, 6, 7], &#39;nodes&#39;: [{&#39;inputs&#39;: [], &#39;name&#39;: &#39;data&#39;, &#39;op&#39;: &#39;null&#39;}, {&#39;attrs&#39;: {&#39;__shape__&#39;: &#39;(12, 1)&#39;}, &#39;inputs&#39;: [], &#39;name&#39;: &#39;fc0_weight&#39;, &#39;op&#39;: &#39;null&#39;}, {&#39;inputs&#39;: [[0, 0, 0], [1, 0, 0]], &#39;name&#39;: &#39;dot46&#39;, &#39;op&#39;: &#39;dot&#39;}, {&#39;attrs&#39;: {&#39;__lr_mult__&#39;: &#39;10.0&#39;, &#39;__shape__&#39;: &#39;(1, 1)&#39;}, &#39;inputs&#39;: [], &#39;name&#39;: &#39;fc0_bias&#39;, &#39;op&#39;: &#39;null&#39;}, {&#39;inputs&#39;: [[2, 0, 0], [3, 0, 0]], &#39;name&#39;: &#39;broadcast_plus46&#39;, &#39;op&#39;: &#39;broadcast_add&#39;}, {&#39;inputs&#39;: [], &#39;name&#39;: &#39;out_label&#39;, &#39;op&#39;: &#39;null&#39;}, {&#39;inputs&#39;: [[4, 0, 0], [5, 0, 0]], &#39;name&#39;: &#39;linearregressionoutput46&#39;, &#39;op&#39;: &#39;LinearRegressionOutput&#39;}]} . # initialize the model graph model = gluon.nn.SymbolBlock( outputs=mxnet.sym.load_json(sym_json_string), inputs=mxnet.sym.var(&quot;data&quot;) ) . /usr/local/lib/python3.8/dist-packages/mxnet/gluon/block.py:1849: UserWarning: Cannot decide type for the following arguments. Consider providing them as input: data: None input_sym_arg_type = in_param.infer_type()[0] . # load the model parameters model.load_parameters(f&quot;{local_path}mx-mod-0000.params&quot;, allow_missing=True) . # finally initialize our model model.initialize() . /usr/local/lib/python3.8/dist-packages/mxnet/gluon/parameter.py:896: UserWarning: Parameter &#39;fc0_weight&#39; is already initialized, ignoring. Set force_reinit=True to re-initialize. v.initialize(None, ctx, init, force_reinit=force_reinit) /usr/local/lib/python3.8/dist-packages/mxnet/gluon/parameter.py:896: UserWarning: Parameter &#39;fc0_bias&#39; is already initialized, ignoring. Set force_reinit=True to re-initialize. v.initialize(None, ctx, init, force_reinit=force_reinit) . At this point our model is ready in our local environment, and we can use it to make some predictions. . Let&#39;s prepare an input request. This request is same as used in the model training blog post. . input_request = [ 0.00632, 18.00, 2.310, 0, 0.5380, 6.5750, 65.20, 4.0900, 1, 296.0, 15.30, 4.98, ] . We need to convert our request Python list to MXNet array to be used for inference. . input_request_nd = mxnet.nd.array(input_request) . print(f&quot;type(input_request): {type(input_request)}&quot;) print(f&quot;type(input_request_nd): {type(input_request_nd)}&quot;) . type(input_request): &lt;class &#39;list&#39;&gt; type(input_request_nd): &lt;class &#39;mxnet.ndarray.ndarray.NDArray&#39;&gt; . Let&#39;s pass our converted request to model for inference. . model(input_request_nd)[0].asscalar() . Extension horovod.torch has not been built: /usr/local/lib/python3.8/dist-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-38-x86_64-linux-gnu.so not found If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error. Warning! MPI libs are missing, but python applications are still avaiable. [2022-07-05 10:53:31.777 mxnet-1-9-cpu-py38-ub-ml-t3-medium-3179f602905714e1b45dfa06b970:222 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None [2022-07-05 10:53:31.944 mxnet-1-9-cpu-py38-ub-ml-t3-medium-3179f602905714e1b45dfa06b970:222 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled. . 29.986717 . That&#39;s it. We have loaded SageMaker built-in model in our local envionment and have done prediction from it. But we can go one step further and explore this model&#39;s trained parameters. . params = model.collect_params() params . ( Parameter fc0_weight (shape=(12, 1), dtype=&lt;class &#39;numpy.float32&#39;&gt;) Parameter fc0_bias (shape=(1, 1), dtype=&lt;class &#39;numpy.float32&#39;&gt;) Parameter out_label (shape=(1,), dtype=&lt;class &#39;numpy.float32&#39;&gt;) ) . Let&#39;s define a function to extract model&#39;s weights and biases . def extract_weight_and_bias(model): params = model.collect_params() weight = params[&quot;fc0_weight&quot;].data().asnumpy() bias = params[&quot;fc0_bias&quot;].data()[0].asscalar() return {&quot;weight&quot;: weight, &quot;bias&quot;: bias} weight_and_bias = extract_weight_and_bias(model) weight_and_bias . {&#39;weight&#39;: array([[-1.6160294e-01], [ 5.2438524e-02], [ 1.5013154e-02], [-4.4300285e-01], [-2.0226759e+01], [ 3.2423832e+00], [ 7.3540364e-03], [-1.4330027e+00], [ 2.0710023e-01], [-8.0383439e-03], [-1.0465978e+00], [-5.0012934e-01]], dtype=float32), &#39;bias&#39;: 44.62983} . This shows that model has 12 weights, one for each input parameter, and a bias. For linear learner there is no activation function so we can use summation formula to create a prediction using the provided weights and bais. . . # convert the input request to np.array import numpy as np input_request = np.array(input_request) . # extract weights and biases weight = weight_and_bias[&quot;weight&quot;] bias = weight_and_bias[&quot;bias&quot;] . We have all the ingredients ready. Let&#39;s use them to calcualte the prediction ourselves. . # calculate the final prediction np.sum(input_request.reshape((-1, 1)) * weight) + bias . 29.98671686516441 .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/07/05/aws-linear-learner-apache-mxnet-python.html",
            "relUrl": "/aws/ml/sagemaker/2022/07/05/aws-linear-learner-apache-mxnet-python.html",
            "date": " • Jul 5, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Serverless Inference with SageMaker Serverless Endpoints",
            "content": ". About . You have trained and deployed a model using Amazon SageMaker. You have an endpoint and now you are wondering &quot;After I deploy an endpoint, where do I go from there?&quot; Your concerns are valid because SageMaker endpoints are not public but are scoped to an individual account. In this post, we will discuss how to make them public using AWS serverless technologies: AWS Lambda and Function URL. We will also make our endpoints serverless so our ML inference solution is serverless end-to-end. . Introduction . The following diagram shows how a model is called using AWS serverless architecture. . Starting from the client, an application calls the AWS Lambda Function URL and passes parameter values. The Lambda function parses the request and passes it to SageMaker model endpoint. This endpoint can be hosted on an EC2 instance or you have the option to make it serverless. Serverless endpoints behave similarly to Lambda functions. Once a request is received by the endpoint it will perform the prediction and return the predicted values to Lambda. The Lambda function then parses the returned values and sends the final response back to the client. . To train a model using Amazon SageMaker you can follow my other post Demystifying Amazon SageMaker Training for scikit-learn Lovers. There I have trained SageMaker Linear Learner model on Boston housing dataset. . Note that this post assumes that you have already trained a model and is available in SageMaker model repository. . Deploy SageMaker Serverless Endpoint . Through SageMaker Console UI . Let&#39;s first deploy our serverless endpoint through SageMaker console UI. In the next section, we will do the same through SageMaker Python SDK. . Visit the SageMaker model repository to find the registered Linear Learner model. You can find the repository on the SageMaker Inference &gt; Model page. . . Note the mode name linear-learner-2022-06-16-09-10-17-207 as will need it in later steps. . Click on the model name and then Create endpoint . . This will take you to configure endpoint page. Here do the following configurations. . Set Endpoint name to 2022-06-17-sagemaker-endpoint-serverless. You may use any other unique string here. | From Attach endpoint configuration select create a new endpoint configuration | From New endpoint configuration &gt; Endpoint configuration set Endpoint configuration name to config-2022-06-17-sagemaker-endpoint-serverless. You may use any other name here. | Type of endpoint to Serverless | From Production variants click on Add Model and then select the model name we want to deploy. In our case it is linear-learner-2022-06-16-09-10-17-207. Click Save. | . | . . Then Edit the Max Concurrency and set it to 5. | . . Click Create endpoint configuration | . . Click Create endpoint | . . It will take a minute for the created endpoint to become ready. . While we were configuring the concurrency for our endpoint we have given it a value of 5. This is because at this point there is a limit on concurrency per account across all serverless endpoints. The maximum total concurrency for an account is 20, and if you cross this limit you will get an error as shown below. . . Through SageMaker Python SDK . Let&#39;s create another endpoint but using SageMaker SDK. Deploying a model to a serverless endpoint using SDK involves the following steps: . Get session to SageMaker API | Create a serverless endpoint deployment config | Create a reference to a model container | Deploy the model on a serverless endpoint using serverless configuration | . Let&#39;s do it now. . # get a session to sagemaker api import sagemaker session = sagemaker.Session() role = sagemaker.get_execution_role() print(f&quot;sagemaker.__version__: {sagemaker.__version__}&quot;) print(f&quot;Session: {session}&quot;) print(f&quot;Role: {role}&quot;) . sagemaker.__version__: 2.88.1 Session: &lt;sagemaker.session.Session object at 0x7feb1853fc10&gt; Role: arn:aws:iam::801598032724:role/service-role/AmazonSageMaker-ExecutionRole-20220516T161743 . # define a serverless endpoint configuration from sagemaker.serverless import ServerlessInferenceConfig serverless_config = ServerlessInferenceConfig( memory_size_in_mb=1024, max_concurrency=5 ) . Note that here we are only defining the endpoint configuration. It will be created when we will deploy the model. Also, note that we have not passed any configuration name. It will default to the endpoint name. To read more about the serverless inference configuration read the documentation ServerlessInferenceConfig . I could not find a way to give a name to endpoint configuration from SageMaker SDK. Let me know in the comments if there is a way to do it. . # create a SageMaker model. # In our case model is already registered so it will only create a reference to it from sagemaker.model import Model ll_model = Model( image_uri = &#39;382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner&#39;, # find it from the SageMaker mode repository name = &#39;linear-learner-2022-06-16-09-10-17-207&#39;, role=role ) . While creating a SageMaker model you need to provide its container URI, name, and role. The role gives necessary permissions to SageMaker to pull the image container from the ECR repository. To read more about the Model read the docs sagemaker.model.Model . # define the endpoint name endpoint_name = &#39;2022-06-17-sagemaker-endpoint-serverless-sdk&#39; . # deploy the model to serverless endpoint ll_model.deploy( endpoint_name=endpoint_name, serverless_inference_config=serverless_config, ) . Using already existing model: linear-learner-2022-06-16-09-10-17-207 . --! . It will take a minute or so for the serverless endpoint to get provisioned. Once it is ready (InService) you will find it on the SageMaker Inference &gt; Endpoints page. . . model.deploy() command will also create the endpoint configuration with same name as endpoint, and it can be found on SageMaker Inference &gt; Endpoint configurations page . . Deploy Lambda Function with Function URL . Our model&#39;s serverless endpoint is ready, and in this section we will make it public using AWS Lambda and Function URL. Let&#39;s create our Lambda Function. . From AWS Lambda console, click Create Function and make the following configurations. . Under Basic Information . Function name = &#39;linear-learner-boston-demo&#39; | Runtime = &#39;Python 3.9&#39; | Execution Role = &#39;Create a new role with basic Lambda permissions&#39; | . Under Advanced Settings . Check &#39;Enable function URL&#39; | &#39;Auth type&#39; to None. This is for demo purposes. | . Click Create Function . Once the function is created click on it to open its page. . Under Function Overview on the bottom right there is a Function URL that we can call to access it publically. . . For our function to call SageMaker endpoint we first need to give it some extra permissions. For this click on the Lambda Configurations &gt; Permissions &gt; Role name. This will open the IAM page for the Role, and the Policies under that role. Select the Policy attached to this Role and Click Edit. . . On the next page add the following permissions to your policy. . { &quot;Sid&quot;: &quot;VisualEditor2&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;, &quot;Resource&quot;: &quot;*&quot; } . After adding those line your final policy will look similar to this. . { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: &quot;logs:CreateLogGroup&quot;, &quot;Resource&quot;: &quot;arn:aws:logs:us-east-1:801598032724:*&quot; }, { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;logs:CreateLogStream&quot;, &quot;logs:PutLogEvents&quot; ], &quot;Resource&quot;: [ &quot;arn:aws:logs:us-east-1:801598032724:log-group:/aws/lambda/linear-learner-boston-demo:*&quot; ] }, { &quot;Sid&quot;: &quot;VisualEditor2&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;, &quot;Resource&quot;: &quot;*&quot; } ] } . Review policy and save your changes. . Now go back to your Lambda console and use the following code. . import json import boto3 runtime= boto3.client(&#39;runtime.sagemaker&#39;) endpoint_name = &#39;2022-06-17-sagemaker-endpoint-serverless-sdk&#39; def lambda_handler(event, context): print(&quot;Received event: &quot; + json.dumps(event, indent=2)) data = json.loads(json.dumps(event)) payload = data[&#39;body&#39;] #payload = &#39;0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98&#39; response = runtime.invoke_endpoint(EndpointName=endpoint_name, ContentType=&#39;text/csv&#39;, Body=payload) print(response) result = json.loads(response[&#39;Body&#39;].read().decode()) print(result) return { &#39;statusCode&#39;: 200, &#39;body&#39;: json.dumps(result) } . What we have done here is . use boto3 SDK to create a session with SageMaker API. We did not use SageMaker SDK here because it is not available in the Lambda environment as of now. You may read more about it here sagemaker-python-sdk in AWS Lambda | then we have defined the endpoint name that we want to call from this function | in the lambda handler we have parsed the request to get the payload | next we have invoked the serverless endpoint with the payload | then we parsed the response to get the predictions | finally we have returned the prediction | . Note that in the endpoint we have used 2022-06-17-sagemaker-endpoint-serverless-sdk which we have created through SageMaker SDK. You may also use 2022-06-17-sagemaker-endpoint-serverless which we created from UI as both point to the same model. . Let&#39;s deploy our function code, and create a test event. Give it a name and use the following event body . { &quot;body&quot;: &quot;0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98&quot; } . Now test it. The first time I tested it I got a timeout exception. The reason for this is that the default timeout for the Lambda function is 3 seconds. But when the function called serverless endpoint it could not get any response during that time window. This is because of a cold start for the serverless endpoint. . If your endpoint does not receive traffic for a while and then your endpoint suddenly receives new requests, it can take some time for your endpoint to spin up the compute resources to process the requests. This is called a cold start. Since serverless endpoints provision compute resources on demand, your endpoint may experience cold starts. A cold start can also occur if your concurrent requests exceed the current concurrent request usage. The cold start time depends on your model size, how long it takes to download your model, and the start-up time of your container. . . On the next test event I got a successful response from the Lambda function as shown below. . . You can find the logs for your serverless endpoint on AWS CloudWatch under log group /aws/sagemaker/Endpoints/[endpoint-name]. In our case it will be /aws/sagemaker/Endpoints/2022-06-17-sagemaker-endpoint-serverless. If you look at the logs you will find that serverless endpoint is doing the following steps: . loading request and response encoders | loading the model | starting a gunicorn server | starting a server listener | making prediction | returning results | . . Test Serverless Inference through Postman . At this point our inference endpoint is ready to be consumed from external applications. Let&#39;s use Postman for testing. Copy the lambda function URL and paste it in Postman Request UI. For the body use the following text. . 0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98 . Send a POST request and on SUCCESS you will get the predictions as shown below . .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/06/17/sagemaker-endpoint.html",
            "relUrl": "/aws/ml/sagemaker/2022/06/17/sagemaker-endpoint.html",
            "date": " • Jun 17, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Demystifying Amazon SageMaker Training for scikit-learn Lovers",
            "content": ". Enviornment . This notebook is prepared with Amazon SageMaker Studio using Python 3 (Data Science) Kernel and ml.t3.medium instance. . About . This post is about simplifying Amazon SageMaker end-to-end machine learning workflow for newcomers. We will go slowly and try to understand each step involved in training and deploying a model. As an example, we will take Kaggle Boston Housing dataset and train a SageMaker built-in algorithm Linear Learner on it. At the end of this tutorial, you will know how to use Amazon SageMaker to build, train, and deploy a machine learning model. . Introduction . A typical SageMaker machine learning flow has the following five steps. If you have a good understanding of them then you can use this approach to train any other model with SageMaker. . Put data on S3 bucket In most use cases, you will keep your training data on S3 bucket. You may also require to preprocess your data and for this you can use SageMaker Data Wrangler. In this post, we will consider that data has already been processed and will upload it to S3 for training. | Configure the training job While configuring a training job you need to take care of the following requirements a. select the algorithm you want to use for your training b. set the hyperparameters (if any) c. define the infrastructure requirements like how many CPUs or GPUs you want to throw at your training run | Launch training job Here we will tell the training job where the input data is located, and where should the output artifacts be stored once the training is done. With this setting, we are ready to start the training run. Once the training is started SageMaker will automatically provision the required infrastructure for our run, and when the training is complete it will be terminated, and you will only be billed for what you have used. | Deploy model and make predictions Deploy the model to make real-time predictions. Again, you need to define the infrastructure requirements where you want your model to be deployed. | Clean Up (Optional) If you are experimenting, you may want to terminate the machine on which you have deployed your model to avoid any unnecessary charges. | Put Data on S3 Bucket . Reading and Checking the Data . In this post, we will be using Boston Housing Dataset. This is a small dataset with 506 rows and 14 columns. medv is the target variable which means median value of owner-occupied homes in $1000s. This dataset is also available with this notebook. Let&#39;s read it and see how it looks. . import pandas as pd import numpy as np data_location = &quot;./datasets/2022-06-08-sagemaker-training-overview/&quot; df = pd.read_csv(data_location + &#39;housing.csv&#39;) df.head() . crim zn indus chas nox age rm dis rad tax ptratio lstat medv . 0 0.00632 | 18.0 | 2.31 | 0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1 | 296.0 | 15.3 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2 | 242.0 | 17.8 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2 | 242.0 | 17.8 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3 | 222.0 | 18.7 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3 | 222.0 | 18.7 | 5.33 | 36.2 | . let&#39;s quickly check the dimensions of our loaded dataset. . df.shape . (506, 13) . The good thing about this dataset is that it does not require any preprocessing as all the features are already in numerical format (no categorical features), and also there are no missing values. We can quickly verify these assumptions. . Check the feature data types. . df.dtypes . crim float64 zn float64 indus float64 chas int64 nox float64 age float64 rm float64 dis float64 rad int64 tax float64 ptratio float64 lstat float64 medv float64 dtype: object . Check if there are any missing values in our dataset. . df.isnull().values.any() . False . Preparing the Data . At this point, our data is ready to be used for training but we also need to check the algorithm we want to use for any specific requirements. We have selected Linear Learner so let&#39;s check its documentation: Linear Learner Algorithm . In the documentation it says . For training, the linear learner algorithm supports both recordIO-wrapped protobuf and CSV formats. For the application/x-recordio-protobuf input type, only Float32 tensors are supported. For the text/csv input type, the first column is assumed to be the label, which is the target variable for prediction. You can use either File mode or Pipe mode to train linear learner models on data that is formatted as recordIO-wrapped-protobuf or as CSV. . It means that we can use CSV format for our training data. It also mentions that the first column in the training dataset should be the target label. So let&#39;s move our target label medv to the first column. . df = pd.concat([df[&#39;medv&#39;], df.drop(&#39;medv&#39;, axis=&#39;columns&#39;)], axis=&#39;columns&#39;) df.head() . medv crim zn indus chas nox age rm dis rad tax ptratio lstat . 0 24.0 | 0.00632 | 18.0 | 2.31 | 0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1 | 296.0 | 15.3 | 4.98 | . 1 21.6 | 0.02731 | 0.0 | 7.07 | 0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2 | 242.0 | 17.8 | 9.14 | . 2 34.7 | 0.02729 | 0.0 | 7.07 | 0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2 | 242.0 | 17.8 | 4.03 | . 3 33.4 | 0.03237 | 0.0 | 2.18 | 0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3 | 222.0 | 18.7 | 2.94 | . 4 36.2 | 0.06905 | 0.0 | 2.18 | 0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3 | 222.0 | 18.7 | 5.33 | . We can now proceed with splitting our data into train and validation sets. After splitting we will export it as CSV files so it could be uploaded to S3 in the next section. Before exporting our dataframe into any format we should also check SageMaker general instructions for that format. For this use the link Common Data Formats for Training and check section Using CSV Format. Here it mentions . To use data in CSV format for training, in the input data channel specification, specify text/csv as the ContentType. Amazon SageMaker requires that a CSV file does not have a header record and that the target variable is in the first column. . We have already moved our target label to the first column, and while exporting our dataframe to CSV format we should omit column headers as well. . from sklearn.model_selection import train_test_split # out data size is very small so we will use a small test set training_data, validation_data = train_test_split(df, test_size=0.1, random_state=42) training_data.to_csv(data_location + &quot;training_data.csv&quot;, index=False, header=False) validation_data.to_csv(data_location + &quot;validation_data.csv&quot;, index=False, header=False) . The next step is to upload this data to S3 bucket, and for this we will take the help of SageMaker Python SDK. There are two Python SDKs (Software Development Kit) available for SageMaker. . AWS SDK for Python (Boto3). It provides low-level access to SageMaker APIs. | SageMaker Python SDK. It provides a high-level API interface, and you can do more with fewer lines of code. Internally it is calling Boto3 APIs. | We will be using SageMaker Python SDK for this post, and you will see that it has an interface similar to scikit-learn, and is a more natural choice for Data Scientists. SageMaker Python SDK documentation is super helpful, and it provides many examples to understand the working of its interface. Make sure that you check it out as well sagemaker.readthedocs.io. If you don&#39;t have much time then I would suggest at least read the following sections from the documentation as we will be using them in the coming sections. . Initialize a SageMaker Session | Upload local file or directory to S3 | default_bucket | Create an Amazon SageMaker training job | A generic Estimator to train using any supplied algorithm | . Since we are already running this notebook from SageMaker environment, we don&#39;t need to care about credentials and permissions. We can simply start our new session with SageMaker environment using its SDK. . import sagemaker session = sagemaker.Session() role = sagemaker.get_execution_role() bucket = session.default_bucket() region = session.boto_region_name print(f&quot;sagemaker.__version__: {sagemaker.__version__}&quot;) print(f&quot;Session: {session}&quot;) print(f&quot;Role: {role}&quot;) print(f&quot;Bucket: {bucket}&quot;) print(f&quot;Region: {region}&quot;) . sagemaker.__version__: 2.88.1 Session: &lt;sagemaker.session.Session object at 0x7ff91637cd50&gt; Role: arn:aws:iam::801598032724:role/service-role/AmazonSageMaker-ExecutionRole-20220516T161743 Bucket: sagemaker-us-east-1-801598032724 Region: us-east-1 . What we have done is . imported the SageMaker Python SDK into our runtime | get a session to work with SageMaker API and other AWS services | get the execution role associated with the user profile. It is the same profile that is available to the user to work from console UI and has AmazonSageMakerFullAccess policy attached to it. | create or get a default bucket to use and return its name. Default bucket name has the format sagemaker-{region}-{account_id}. If it doesn&#39;t exist then our session will automatically create it. You may also use any other bucket in its place given that you have enough permission for reading and writing. | get the region name attached to our session | . Next, we will use this session to upload data to our default bucket. . # You may choose any other prefix for your bucket. All the data related to this post will be under this prefix. bucket_prefix = &#39;2022-06-08-sagemaker-training-overview&#39; . Let&#39;s upload our training data first. In the output, we will get the complete path (S3 URI) for our uploaded data. . s3_train_data_path = session.upload_data( path=data_location + &quot;training_data.csv&quot;, bucket=bucket, key_prefix=bucket_prefix + &#39;/input/training&#39; ) print(s3_train_data_path) . s3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/input/training/training_data.csv . Let&#39;s do the same for our validation data. . s3_validation_data_path = session.upload_data( path=data_location + &quot;validation_data.csv&quot;, bucket=bucket, key_prefix=bucket_prefix + &#39;/input/validation_data&#39; ) print(s3_validation_data_path) . s3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/input/validation_data/validation_data.csv . At this point we have our data available on S3 bucket. We can now proceed to the next step and configure our training job. . Configure the Training Job . In this section, we will first retrieve the Docker container that is relevant to our training algorithm. Then we will create &quot;sagemaker.estimator.Estimator&quot; class object. This estimator object provides a high-level API interface to control end-to-end SageMaker training and deployment tasks. From the estimator, we will also define our infrastructure and hyperparameter tuning requirements. So let&#39;s get started. . Finding the Right Docker Container . AWS SageMaker built-in algorithms are fully managed containers that can be accessed with one call. Each algorithm has a separate container and is also dependent on the region in which you want to run it. Getting the container URI is not a problem as long as we know about the region and the algorithm framework name. We already have the region name from our SageMaker session. To get the algorithm framework name (for linear learner) visit the AWS Docker Registry Paths page. From this page select your region. In my case it is us-east-1. On the regional docker registry page find the algorithm you want to use; Linear Learner in our case. This will give you the example code and algorithm framework name as shown below. . . So let&#39;s use the provided sample code to get the container URI for our linear learner algorithm. . from sagemaker import image_uris image_uri = image_uris.retrieve(framework=&#39;linear-learner&#39;,region=region) print(image_uri) . 382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:1 . Configure the Estimator Object . To configure our estimator we need to fulfill the following requirements. . output path Define the output path where we want to store the trained model artifacts | instance type Since we have a small dataset and not so complex model so a small machine should suffice. &#39;ml.m5.large&#39; will do. It is a general purpose instance with 2vCPU and 8GiB RAM. | hyperparameters For the hyperparameters check the Linear Learner model documentaion. From the documentation, we find that the most important parameters for our problem are predictor_type: which should be &#39;regressor&#39; in our case | mini_batch_size: default is 1000 which is too large for our small dataset. Let&#39;s use 30 instead. | . | . It is also important to note that the Estimator class will automatically provision a separate ml.m5.large machine to start the training run. This machine will be different from the one on which we are running this Jupyter notebook. Once training is complete this new machine will be terminated and we will be billed for only the time we have used it. This approach makes SageMaker very cost effective. We can keep using small less powerful machines for running Jupyter notebooks, and for training and other heavy workloads, we can provision separate machines for short durations and avoid any unnecessary bills. . # define the output path to store trained model artifacts s3_output_path = f&quot;s3://{bucket}/{bucket_prefix}/output/&quot; print(s3_output_path) . s3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/output/ . from sagemaker.estimator import Estimator ll_estimator = Estimator( image_uri = image_uri, # algorithm container role = role, # execution role with necessary permissions instance_count = 1, instance_type = &#39;ml.m5.large&#39;, sagemaker_session = session, # SageMaker API session output_path = s3_output_path, # training artifacts output path hyperparameters = { &#39;predictor_type&#39;: &#39;regressor&#39;, &#39;mini_batch_size&#39;: 30 } ) . In the above cell, we have defined the hyperparameters within the Estimator object constructor. There is a second way to pass the hyperparameters to the Estimator object using the &#39;set_hyperparameters&#39; function call. This method can be useful when we have a large number of parameters to set, or when we want to change them in multiple training runs. . ll_estimator.set_hyperparameters( predictor_type=&#39;regressor&#39;, mini_batch_size=30) . You might ask that for our problem even a small ml.t3.medium or ml.c5.large machine would have been sufficient. Why have not we used them? The answer to this is that AWS SageMaker at this time supports a limited number of machine types for training jobs and both of them are not supported. If you configure the Estimator object for these instance types you will get an error shown below . An error occurred (ValidationException) when calling the CreateTrainingJob operation: 1 validation error detected: Value &#39;ml.t3.medium&#39; at &#39;resourceConfig.instanceType&#39; failed to satisfy constraint: Member must satisfy enum value set: [ml.p2.xlarge, ml.m5.4xlarge, ml.m4.16xlarge, ml.p4d.24xlarge, ml.g5.2xlarge, ml.c5n.xlarge, ml.p3.16xlarge, ml.m5.large, ml.p2.16xlarge, ml.g5.4xlarge, ml.c4.2xlarge, ml.c5.2xlarge, ml.c4.4xlarge, ml.g5.8xlarge, ml.c5.4xlarge, ml.c5n.18xlarge, ml.g4dn.xlarge, ml.g4dn.12xlarge, ml.c4.8xlarge, ml.g4dn.2xlarge, ml.c5.9xlarge, ml.g4dn.4xlarge, ml.c5.xlarge, ml.g4dn.16xlarge, ml.c4.xlarge, ml.g4dn.8xlarge, ml.g5.xlarge, ml.c5n.2xlarge, ml.g5.12xlarge, ml.g5.24xlarge, ml.c5n.4xlarge, ml.c5.18xlarge, ml.p3dn.24xlarge, ml.g5.48xlarge, ml.g5.16xlarge, ml.p3.2xlarge, ml.m5.xlarge, ml.m4.10xlarge, ml.c5n.9xlarge, ml.m5.12xlarge, ml.m4.xlarge, ml.m5.24xlarge, ml.m4.2xlarge, ml.p2.8xlarge, ml.m5.2xlarge, ml.p3.8xlarge, ml.m4.4xlarge] . Launch Training Job . To start our training run we need to associate our Estimator object with the training data. Our data is available on S3 bucket but we also need to tell our Estimator object in which format this data is provided. Is it in CSV format? Is it compressed or not? For these data format issues, we can use Input Channels. An input channel is configurations for S3 and file system data sources for AWS SageMaker (check the input docs). All SageMaker built-in algorithms require at least one training input channel, and more can be passed for validation and testing. In our case, we have two channels and both are in CSV format. Let&#39;s configure them. . from sagemaker.session import TrainingInput train_input = TrainingInput(s3_train_data_path, content_type=&quot;text/csv&quot;) validation_input = TrainingInput(s3_validation_data_path, content_type=&quot;text/csv&quot;) ll_data = { &#39;train&#39;: train_input, &#39;validation&#39;: validation_input } . Make sure that you use content_type text/csv. Only providing csv will not work and you will get the exception . Error for Training job linear-learner-2022-06-15-07-58-01-908: Failed. Reason: ClientError: No iterator has been registered for ContentType (&#39;csv&#39;, &#39;1.0&#39;), exit code: 2 . Alright, we are ready. To start the training run call estimator fit function and pass the data input channels. You can read more about fit call from docs sagemaker.estimator.Estimator.fit . ll_estimator.fit(ll_data) . 2022-06-16 09:04:57 Starting - Starting the training job... 2022-06-16 09:05:23 Starting - Preparing the instances for trainingProfilerReport-1655370297: InProgress ......... 2022-06-16 09:06:48 Downloading - Downloading input data...... 2022-06-16 09:07:44 Training - Downloading the training image... 2022-06-16 09:08:24 Training - Training image download completed. Training in progress..Docker entrypoint called with argument(s): train Running default environment configuration script [06/16/2022 09:08:32 INFO 139637139089216] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-input.json: {&#39;mini_batch_size&#39;: &#39;1000&#39;, &#39;epochs&#39;: &#39;15&#39;, &#39;feature_dim&#39;: &#39;auto&#39;, &#39;use_bias&#39;: &#39;true&#39;, &#39;binary_classifier_model_selection_criteria&#39;: &#39;accuracy&#39;, &#39;f_beta&#39;: &#39;1.0&#39;, &#39;target_recall&#39;: &#39;0.8&#39;, &#39;target_precision&#39;: &#39;0.8&#39;, &#39;num_models&#39;: &#39;auto&#39;, &#39;num_calibration_samples&#39;: &#39;10000000&#39;, &#39;init_method&#39;: &#39;uniform&#39;, &#39;init_scale&#39;: &#39;0.07&#39;, &#39;init_sigma&#39;: &#39;0.01&#39;, &#39;init_bias&#39;: &#39;0.0&#39;, &#39;optimizer&#39;: &#39;auto&#39;, &#39;loss&#39;: &#39;auto&#39;, &#39;margin&#39;: &#39;1.0&#39;, &#39;quantile&#39;: &#39;0.5&#39;, &#39;loss_insensitivity&#39;: &#39;0.01&#39;, &#39;huber_delta&#39;: &#39;1.0&#39;, &#39;num_classes&#39;: &#39;1&#39;, &#39;accuracy_top_k&#39;: &#39;3&#39;, &#39;wd&#39;: &#39;auto&#39;, &#39;l1&#39;: &#39;auto&#39;, &#39;momentum&#39;: &#39;auto&#39;, &#39;learning_rate&#39;: &#39;auto&#39;, &#39;beta_1&#39;: &#39;auto&#39;, &#39;beta_2&#39;: &#39;auto&#39;, &#39;bias_lr_mult&#39;: &#39;auto&#39;, &#39;bias_wd_mult&#39;: &#39;auto&#39;, &#39;use_lr_scheduler&#39;: &#39;true&#39;, &#39;lr_scheduler_step&#39;: &#39;auto&#39;, &#39;lr_scheduler_factor&#39;: &#39;auto&#39;, &#39;lr_scheduler_minimum_lr&#39;: &#39;auto&#39;, &#39;positive_example_weight_mult&#39;: &#39;1.0&#39;, &#39;balance_multiclass_weights&#39;: &#39;false&#39;, &#39;normalize_data&#39;: &#39;true&#39;, &#39;normalize_label&#39;: &#39;auto&#39;, &#39;unbias_data&#39;: &#39;auto&#39;, &#39;unbias_label&#39;: &#39;auto&#39;, &#39;num_point_for_scaler&#39;: &#39;10000&#39;, &#39;_kvstore&#39;: &#39;auto&#39;, &#39;_num_gpus&#39;: &#39;auto&#39;, &#39;_num_kv_servers&#39;: &#39;auto&#39;, &#39;_log_level&#39;: &#39;info&#39;, &#39;_tuning_objective_metric&#39;: &#39;&#39;, &#39;early_stopping_patience&#39;: &#39;3&#39;, &#39;early_stopping_tolerance&#39;: &#39;0.001&#39;, &#39;_enable_profiler&#39;: &#39;false&#39;} [06/16/2022 09:08:32 INFO 139637139089216] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {&#39;mini_batch_size&#39;: &#39;30&#39;, &#39;predictor_type&#39;: &#39;regressor&#39;} [06/16/2022 09:08:32 INFO 139637139089216] Final configuration: {&#39;mini_batch_size&#39;: &#39;30&#39;, &#39;epochs&#39;: &#39;15&#39;, &#39;feature_dim&#39;: &#39;auto&#39;, &#39;use_bias&#39;: &#39;true&#39;, &#39;binary_classifier_model_selection_criteria&#39;: &#39;accuracy&#39;, &#39;f_beta&#39;: &#39;1.0&#39;, &#39;target_recall&#39;: &#39;0.8&#39;, &#39;target_precision&#39;: &#39;0.8&#39;, &#39;num_models&#39;: &#39;auto&#39;, &#39;num_calibration_samples&#39;: &#39;10000000&#39;, &#39;init_method&#39;: &#39;uniform&#39;, &#39;init_scale&#39;: &#39;0.07&#39;, &#39;init_sigma&#39;: &#39;0.01&#39;, &#39;init_bias&#39;: &#39;0.0&#39;, &#39;optimizer&#39;: &#39;auto&#39;, &#39;loss&#39;: &#39;auto&#39;, &#39;margin&#39;: &#39;1.0&#39;, &#39;quantile&#39;: &#39;0.5&#39;, &#39;loss_insensitivity&#39;: &#39;0.01&#39;, &#39;huber_delta&#39;: &#39;1.0&#39;, &#39;num_classes&#39;: &#39;1&#39;, &#39;accuracy_top_k&#39;: &#39;3&#39;, &#39;wd&#39;: &#39;auto&#39;, &#39;l1&#39;: &#39;auto&#39;, &#39;momentum&#39;: &#39;auto&#39;, &#39;learning_rate&#39;: &#39;auto&#39;, &#39;beta_1&#39;: &#39;auto&#39;, &#39;beta_2&#39;: &#39;auto&#39;, &#39;bias_lr_mult&#39;: &#39;auto&#39;, &#39;bias_wd_mult&#39;: &#39;auto&#39;, &#39;use_lr_scheduler&#39;: &#39;true&#39;, &#39;lr_scheduler_step&#39;: &#39;auto&#39;, &#39;lr_scheduler_factor&#39;: &#39;auto&#39;, &#39;lr_scheduler_minimum_lr&#39;: &#39;auto&#39;, &#39;positive_example_weight_mult&#39;: &#39;1.0&#39;, &#39;balance_multiclass_weights&#39;: &#39;false&#39;, &#39;normalize_data&#39;: &#39;true&#39;, &#39;normalize_label&#39;: &#39;auto&#39;, &#39;unbias_data&#39;: &#39;auto&#39;, &#39;unbias_label&#39;: &#39;auto&#39;, &#39;num_point_for_scaler&#39;: &#39;10000&#39;, &#39;_kvstore&#39;: &#39;auto&#39;, &#39;_num_gpus&#39;: &#39;auto&#39;, &#39;_num_kv_servers&#39;: &#39;auto&#39;, &#39;_log_level&#39;: &#39;info&#39;, &#39;_tuning_objective_metric&#39;: &#39;&#39;, &#39;early_stopping_patience&#39;: &#39;3&#39;, &#39;early_stopping_tolerance&#39;: &#39;0.001&#39;, &#39;_enable_profiler&#39;: &#39;false&#39;, &#39;predictor_type&#39;: &#39;regressor&#39;} [06/16/2022 09:08:32 WARNING 139637139089216] Loggers have already been setup. Process 1 is a worker. [06/16/2022 09:08:32 INFO 139637139089216] Using default worker. [06/16/2022 09:08:33 INFO 139637139089216] Checkpoint loading and saving are disabled. [06/16/2022 09:08:33 INFO 139637139089216] Create Store: local [06/16/2022 09:08:33 INFO 139637139089216] Scaler algorithm parameters &lt;algorithm.scaler.ScalerAlgorithmStable object at 0x7eff58958050&gt; [06/16/2022 09:08:33 INFO 139637139089216] Scaling model computed with parameters: {&#39;stdev_label&#39;: [9.34456] &lt;NDArray 1 @cpu(0)&gt;, &#39;stdev_weight&#39;: [8.76453018e+00 2.35804825e+01 6.91447353e+00 2.53265083e-01 1.16866864e-01 7.11425126e-01 2.81710987e+01 2.16355181e+00 8.68270397e+00 1.67831055e+02 2.19965935e+00 7.17631102e+00] &lt;NDArray 12 @cpu(0)&gt;, &#39;mean_label&#39;: [22.730888] &lt;NDArray 1 @cpu(0)&gt;, &#39;mean_weight&#39;: [3.6912031e+00 1.1747775e+01 1.1070757e+01 6.8888903e-02 5.5524135e-01 6.2997293e+00 6.8247337e+01 3.8249063e+00 9.5533333e+00 4.0815112e+02 1.8405781e+01 1.2513045e+01] &lt;NDArray 12 @cpu(0)&gt;} [06/16/2022 09:08:33 INFO 139637139089216] nvidia-smi: took 0.034 seconds to run. [06/16/2022 09:08:33 INFO 139637139089216] nvidia-smi identified 0 GPUs. [06/16/2022 09:08:33 INFO 139637139089216] Number of GPUs being used: 0 #metrics {&#34;StartTime&#34;: 1655370513.6247761, &#34;EndTime&#34;: 1655370513.6248353, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;Meta&#34;: &#34;init_train_data_iter&#34;}, &#34;Metrics&#34;: {&#34;Total Records Seen&#34;: {&#34;sum&#34;: 485.0, &#34;count&#34;: 1, &#34;min&#34;: 485, &#34;max&#34;: 485}, &#34;Total Batches Seen&#34;: {&#34;sum&#34;: 17.0, &#34;count&#34;: 1, &#34;min&#34;: 17, &#34;max&#34;: 17}, &#34;Max Records Seen Between Resets&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Max Batches Seen Between Resets&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}, &#34;Reset Count&#34;: {&#34;sum&#34;: 2.0, &#34;count&#34;: 1, &#34;min&#34;: 2, &#34;max&#34;: 2}, &#34;Number of Records Since Last Reset&#34;: {&#34;sum&#34;: 0.0, &#34;count&#34;: 1, &#34;min&#34;: 0, &#34;max&#34;: 0}, &#34;Number of Batches Since Last Reset&#34;: {&#34;sum&#34;: 0.0, &#34;count&#34;: 1, &#34;min&#34;: 0, &#34;max&#34;: 0}}} #metrics {&#34;StartTime&#34;: 1655370514.1997795, &#34;EndTime&#34;: 1655370514.199862, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.7342255814870199, &#34;count&#34;: 1, &#34;min&#34;: 0.7342255814870199, &#34;max&#34;: 0.7342255814870199}}} #metrics {&#34;StartTime&#34;: 1655370514.200417, &#34;EndTime&#34;: 1655370514.200444, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.8231188530392117, &#34;count&#34;: 1, &#34;min&#34;: 0.8231188530392117, &#34;max&#34;: 0.8231188530392117}}} #metrics {&#34;StartTime&#34;: 1655370514.2007773, &#34;EndTime&#34;: 1655370514.2008433, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.8803848605685763, &#34;count&#34;: 1, &#34;min&#34;: 0.8803848605685763, &#34;max&#34;: 0.8803848605685763}}} #metrics {&#34;StartTime&#34;: 1655370514.2011683, &#34;EndTime&#34;: 1655370514.201206, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.8761744774712457, &#34;count&#34;: 1, &#34;min&#34;: 0.8761744774712457, &#34;max&#34;: 0.8761744774712457}}} #metrics {&#34;StartTime&#34;: 1655370514.2014973, &#34;EndTime&#34;: 1655370514.2015386, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.656442772547404, &#34;count&#34;: 1, &#34;min&#34;: 0.656442772547404, &#34;max&#34;: 0.656442772547404}}} #metrics {&#34;StartTime&#34;: 1655370514.2018526, &#34;EndTime&#34;: 1655370514.2018926, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9078461117214627, &#34;count&#34;: 1, &#34;min&#34;: 0.9078461117214627, &#34;max&#34;: 0.9078461117214627}}} #metrics {&#34;StartTime&#34;: 1655370514.2021906, &#34;EndTime&#34;: 1655370514.202229, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.6571193493737115, &#34;count&#34;: 1, &#34;min&#34;: 0.6571193493737115, &#34;max&#34;: 0.6571193493737115}}} #metrics {&#34;StartTime&#34;: 1655370514.2025762, &#34;EndTime&#34;: 1655370514.2026186, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.8111394161648221, &#34;count&#34;: 1, &#34;min&#34;: 0.8111394161648221, &#34;max&#34;: 0.8111394161648221}}} #metrics {&#34;StartTime&#34;: 1655370514.2029133, &#34;EndTime&#34;: 1655370514.2029514, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.7890813573201497, &#34;count&#34;: 1, &#34;min&#34;: 0.7890813573201497, &#34;max&#34;: 0.7890813573201497}}} #metrics {&#34;StartTime&#34;: 1655370514.2032437, &#34;EndTime&#34;: 1655370514.2032828, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.6199219311608208, &#34;count&#34;: 1, &#34;min&#34;: 0.6199219311608208, &#34;max&#34;: 0.6199219311608208}}} #metrics {&#34;StartTime&#34;: 1655370514.2035882, &#34;EndTime&#34;: 1655370514.2036273, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9667545784844293, &#34;count&#34;: 1, &#34;min&#34;: 0.9667545784844293, &#34;max&#34;: 0.9667545784844293}}} #metrics {&#34;StartTime&#34;: 1655370514.203925, &#34;EndTime&#34;: 1655370514.2039638, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.8675954903496637, &#34;count&#34;: 1, &#34;min&#34;: 0.8675954903496637, &#34;max&#34;: 0.8675954903496637}}} #metrics {&#34;StartTime&#34;: 1655370514.2042556, &#34;EndTime&#34;: 1655370514.204296, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.87180573993259, &#34;count&#34;: 1, &#34;min&#34;: 0.87180573993259, &#34;max&#34;: 0.87180573993259}}} #metrics {&#34;StartTime&#34;: 1655370514.204613, &#34;EndTime&#34;: 1655370514.2046492, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.6551772255367703, &#34;count&#34;: 1, &#34;min&#34;: 0.6551772255367703, &#34;max&#34;: 0.6551772255367703}}} #metrics {&#34;StartTime&#34;: 1655370514.204947, &#34;EndTime&#34;: 1655370514.2049868, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.8131753158569336, &#34;count&#34;: 1, &#34;min&#34;: 0.8131753158569336, &#34;max&#34;: 0.8131753158569336}}} #metrics {&#34;StartTime&#34;: 1655370514.205287, &#34;EndTime&#34;: 1655370514.205329, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.8455197090572781, &#34;count&#34;: 1, &#34;min&#34;: 0.8455197090572781, &#34;max&#34;: 0.8455197090572781}}} #metrics {&#34;StartTime&#34;: 1655370514.205626, &#34;EndTime&#34;: 1655370514.2056801, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.7571575673421224, &#34;count&#34;: 1, &#34;min&#34;: 0.7571575673421224, &#34;max&#34;: 0.7571575673421224}}} #metrics {&#34;StartTime&#34;: 1655370514.205997, &#34;EndTime&#34;: 1655370514.2060547, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.8838858689202203, &#34;count&#34;: 1, &#34;min&#34;: 0.8838858689202203, &#34;max&#34;: 0.8838858689202203}}} #metrics {&#34;StartTime&#34;: 1655370514.2063773, &#34;EndTime&#34;: 1655370514.2064557, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.7011345269944933, &#34;count&#34;: 1, &#34;min&#34;: 0.7011345269944933, &#34;max&#34;: 0.7011345269944933}}} #metrics {&#34;StartTime&#34;: 1655370514.2067807, &#34;EndTime&#34;: 1655370514.2068167, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.784123322168986, &#34;count&#34;: 1, &#34;min&#34;: 0.784123322168986, &#34;max&#34;: 0.784123322168986}}} #metrics {&#34;StartTime&#34;: 1655370514.207115, &#34;EndTime&#34;: 1655370514.2071867, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.8399419085184733, &#34;count&#34;: 1, &#34;min&#34;: 0.8399419085184733, &#34;max&#34;: 0.8399419085184733}}} #metrics {&#34;StartTime&#34;: 1655370514.2074873, &#34;EndTime&#34;: 1655370514.2075248, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.676321226755778, &#34;count&#34;: 1, &#34;min&#34;: 0.676321226755778, &#34;max&#34;: 0.676321226755778}}} #metrics {&#34;StartTime&#34;: 1655370514.2078495, &#34;EndTime&#34;: 1655370514.2078874, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.6817984845903184, &#34;count&#34;: 1, &#34;min&#34;: 0.6817984845903184, &#34;max&#34;: 0.6817984845903184}}} #metrics {&#34;StartTime&#34;: 1655370514.208195, &#34;EndTime&#34;: 1655370514.208255, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.7986219829983181, &#34;count&#34;: 1, &#34;min&#34;: 0.7986219829983181, &#34;max&#34;: 0.7986219829983181}}} #metrics {&#34;StartTime&#34;: 1655370514.208583, &#34;EndTime&#34;: 1655370514.2086253, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9477216720581054, &#34;count&#34;: 1, &#34;min&#34;: 0.9477216720581054, &#34;max&#34;: 0.9477216720581054}}} #metrics {&#34;StartTime&#34;: 1655370514.2089238, &#34;EndTime&#34;: 1655370514.2089608, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 1.0295849715338814, &#34;count&#34;: 1, &#34;min&#34;: 1.0295849715338814, &#34;max&#34;: 1.0295849715338814}}} #metrics {&#34;StartTime&#34;: 1655370514.2092505, &#34;EndTime&#34;: 1655370514.2092884, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9801304117838542, &#34;count&#34;: 1, &#34;min&#34;: 0.9801304117838542, &#34;max&#34;: 0.9801304117838542}}} #metrics {&#34;StartTime&#34;: 1655370514.2096825, &#34;EndTime&#34;: 1655370514.2097063, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 1.0793715371025934, &#34;count&#34;: 1, &#34;min&#34;: 1.0793715371025934, &#34;max&#34;: 1.0793715371025934}}} #metrics {&#34;StartTime&#34;: 1655370514.210022, &#34;EndTime&#34;: 1655370514.210062, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 1.3322255198160808, &#34;count&#34;: 1, &#34;min&#34;: 1.3322255198160808, &#34;max&#34;: 1.3322255198160808}}} #metrics {&#34;StartTime&#34;: 1655370514.210389, &#34;EndTime&#34;: 1655370514.2104301, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 1.1104522429572212, &#34;count&#34;: 1, &#34;min&#34;: 1.1104522429572212, &#34;max&#34;: 1.1104522429572212}}} #metrics {&#34;StartTime&#34;: 1655370514.2107415, &#34;EndTime&#34;: 1655370514.2107806, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 1.1085011821322972, &#34;count&#34;: 1, &#34;min&#34;: 1.1085011821322972, &#34;max&#34;: 1.1085011821322972}}} #metrics {&#34;StartTime&#34;: 1655370514.2110868, &#34;EndTime&#34;: 1655370514.2111309, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 1.1517539087931314, &#34;count&#34;: 1, &#34;min&#34;: 1.1517539087931314, &#34;max&#34;: 1.1517539087931314}}} [06/16/2022 09:08:34 INFO 139637139089216] #quality_metric: host=algo-1, epoch=0, train mse_objective &lt;loss&gt;=0.7342255814870199 #metrics {&#34;StartTime&#34;: 1655370514.3148293, &#34;EndTime&#34;: 1655370514.3149238, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 27.402461033241423, &#34;count&#34;: 1, &#34;min&#34;: 27.402461033241423, &#34;max&#34;: 27.402461033241423}}} #metrics {&#34;StartTime&#34;: 1655370514.3155208, &#34;EndTime&#34;: 1655370514.3155527, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 30.773424335554534, &#34;count&#34;: 1, &#34;min&#34;: 30.773424335554534, &#34;max&#34;: 30.773424335554534}}} #metrics {&#34;StartTime&#34;: 1655370514.315889, &#34;EndTime&#34;: 1655370514.3159294, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 38.51581049900429, &#34;count&#34;: 1, &#34;min&#34;: 38.51581049900429, &#34;max&#34;: 38.51581049900429}}} #metrics {&#34;StartTime&#34;: 1655370514.3162339, &#34;EndTime&#34;: 1655370514.3162794, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 33.189655378753066, &#34;count&#34;: 1, &#34;min&#34;: 33.189655378753066, &#34;max&#34;: 33.189655378753066}}} #metrics {&#34;StartTime&#34;: 1655370514.3166046, &#34;EndTime&#34;: 1655370514.3166413, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 35.38766299977022, &#34;count&#34;: 1, &#34;min&#34;: 35.38766299977022, &#34;max&#34;: 35.38766299977022}}} #metrics {&#34;StartTime&#34;: 1655370514.316944, &#34;EndTime&#34;: 1655370514.3170278, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 31.813827215456495, &#34;count&#34;: 1, &#34;min&#34;: 31.813827215456495, &#34;max&#34;: 31.813827215456495}}} #metrics {&#34;StartTime&#34;: 1655370514.3173647, &#34;EndTime&#34;: 1655370514.3174055, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 34.474498075597424, &#34;count&#34;: 1, &#34;min&#34;: 34.474498075597424, &#34;max&#34;: 34.474498075597424}}} #metrics {&#34;StartTime&#34;: 1655370514.3177202, &#34;EndTime&#34;: 1655370514.3177552, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 39.658028396905635, &#34;count&#34;: 1, &#34;min&#34;: 39.658028396905635, &#34;max&#34;: 39.658028396905635}}} #metrics {&#34;StartTime&#34;: 1655370514.3180368, &#34;EndTime&#34;: 1655370514.3180761, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 29.30691588158701, &#34;count&#34;: 1, &#34;min&#34;: 29.30691588158701, &#34;max&#34;: 29.30691588158701}}} #metrics {&#34;StartTime&#34;: 1655370514.3183806, &#34;EndTime&#34;: 1655370514.318424, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 22.185959759880514, &#34;count&#34;: 1, &#34;min&#34;: 22.185959759880514, &#34;max&#34;: 22.185959759880514}}} #metrics {&#34;StartTime&#34;: 1655370514.3187418, &#34;EndTime&#34;: 1655370514.3187768, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 36.23678409352022, &#34;count&#34;: 1, &#34;min&#34;: 36.23678409352022, &#34;max&#34;: 36.23678409352022}}} #metrics {&#34;StartTime&#34;: 1655370514.319071, &#34;EndTime&#34;: 1655370514.31911, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 37.74798105277267, &#34;count&#34;: 1, &#34;min&#34;: 37.74798105277267, &#34;max&#34;: 37.74798105277267}}} #metrics {&#34;StartTime&#34;: 1655370514.3194046, &#34;EndTime&#34;: 1655370514.3194447, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 34.08496692133885, &#34;count&#34;: 1, &#34;min&#34;: 34.08496692133885, &#34;max&#34;: 34.08496692133885}}} #metrics {&#34;StartTime&#34;: 1655370514.3198159, &#34;EndTime&#34;: 1655370514.3198373, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 34.66747927198223, &#34;count&#34;: 1, &#34;min&#34;: 34.66747927198223, &#34;max&#34;: 34.66747927198223}}} #metrics {&#34;StartTime&#34;: 1655370514.320155, &#34;EndTime&#34;: 1655370514.3201954, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 37.22522571040135, &#34;count&#34;: 1, &#34;min&#34;: 37.22522571040135, &#34;max&#34;: 37.22522571040135}}} #metrics {&#34;StartTime&#34;: 1655370514.320503, &#34;EndTime&#34;: 1655370514.3205466, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 35.7627383961397, &#34;count&#34;: 1, &#34;min&#34;: 35.7627383961397, &#34;max&#34;: 35.7627383961397}}} #metrics {&#34;StartTime&#34;: 1655370514.3208497, &#34;EndTime&#34;: 1655370514.3208885, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 31.76248946844363, &#34;count&#34;: 1, &#34;min&#34;: 31.76248946844363, &#34;max&#34;: 31.76248946844363}}} #metrics {&#34;StartTime&#34;: 1655370514.3212028, &#34;EndTime&#34;: 1655370514.3212395, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 33.12425561044731, &#34;count&#34;: 1, &#34;min&#34;: 33.12425561044731, &#34;max&#34;: 33.12425561044731}}} #metrics {&#34;StartTime&#34;: 1655370514.3215368, &#34;EndTime&#34;: 1655370514.3215775, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 28.10427916283701, &#34;count&#34;: 1, &#34;min&#34;: 28.10427916283701, &#34;max&#34;: 28.10427916283701}}} #metrics {&#34;StartTime&#34;: 1655370514.3218973, &#34;EndTime&#34;: 1655370514.3219354, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 32.98116885914522, &#34;count&#34;: 1, &#34;min&#34;: 32.98116885914522, &#34;max&#34;: 32.98116885914522}}} #metrics {&#34;StartTime&#34;: 1655370514.3222327, &#34;EndTime&#34;: 1655370514.3222723, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 26.26614559397978, &#34;count&#34;: 1, &#34;min&#34;: 26.26614559397978, &#34;max&#34;: 26.26614559397978}}} #metrics {&#34;StartTime&#34;: 1655370514.322598, &#34;EndTime&#34;: 1655370514.3226388, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 25.331933115042894, &#34;count&#34;: 1, &#34;min&#34;: 25.331933115042894, &#34;max&#34;: 25.331933115042894}}} #metrics {&#34;StartTime&#34;: 1655370514.3229315, &#34;EndTime&#34;: 1655370514.3229926, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 25.81375062231924, &#34;count&#34;: 1, &#34;min&#34;: 25.81375062231924, &#34;max&#34;: 25.81375062231924}}} #metrics {&#34;StartTime&#34;: 1655370514.3232965, &#34;EndTime&#34;: 1655370514.323337, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 29.230767941942403, &#34;count&#34;: 1, &#34;min&#34;: 29.230767941942403, &#34;max&#34;: 29.230767941942403}}} #metrics {&#34;StartTime&#34;: 1655370514.323641, &#34;EndTime&#34;: 1655370514.3236794, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 60.009633980545345, &#34;count&#34;: 1, &#34;min&#34;: 60.009633980545345, &#34;max&#34;: 60.009633980545345}}} #metrics {&#34;StartTime&#34;: 1655370514.323973, &#34;EndTime&#34;: 1655370514.3240116, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.05478084788603, &#34;count&#34;: 1, &#34;min&#34;: 62.05478084788603, &#34;max&#34;: 62.05478084788603}}} #metrics {&#34;StartTime&#34;: 1655370514.3243523, &#34;EndTime&#34;: 1655370514.324393, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 63.59229712392769, &#34;count&#34;: 1, &#34;min&#34;: 63.59229712392769, &#34;max&#34;: 63.59229712392769}}} #metrics {&#34;StartTime&#34;: 1655370514.3247035, &#34;EndTime&#34;: 1655370514.3247397, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 58.148657705269606, &#34;count&#34;: 1, &#34;min&#34;: 58.148657705269606, &#34;max&#34;: 58.148657705269606}}} #metrics {&#34;StartTime&#34;: 1655370514.325044, &#34;EndTime&#34;: 1655370514.3250837, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 74.61402324601715, &#34;count&#34;: 1, &#34;min&#34;: 74.61402324601715, &#34;max&#34;: 74.61402324601715}}} #metrics {&#34;StartTime&#34;: 1655370514.3253865, &#34;EndTime&#34;: 1655370514.3254254, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 83.38874368106617, &#34;count&#34;: 1, &#34;min&#34;: 83.38874368106617, &#34;max&#34;: 83.38874368106617}}} #metrics {&#34;StartTime&#34;: 1655370514.3257341, &#34;EndTime&#34;: 1655370514.3257933, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 81.56805060891544, &#34;count&#34;: 1, &#34;min&#34;: 81.56805060891544, &#34;max&#34;: 81.56805060891544}}} #metrics {&#34;StartTime&#34;: 1655370514.3260648, &#34;EndTime&#34;: 1655370514.326102, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 98.70984126072304, &#34;count&#34;: 1, &#34;min&#34;: 98.70984126072304, &#34;max&#34;: 98.70984126072304}}} [06/16/2022 09:08:34 INFO 139637139089216] #quality_metric: host=algo-1, epoch=0, validation mse_objective &lt;loss&gt;=27.402461033241423 [06/16/2022 09:08:34 INFO 139637139089216] #early_stopping_criteria_metric: host=algo-1, epoch=0, criteria=mse_objective, value=22.185959759880514 [06/16/2022 09:08:34 INFO 139637139089216] Epoch 0: Loss improved. Updating best model [06/16/2022 09:08:34 INFO 139637139089216] Saving model for epoch: 0 [06/16/2022 09:08:34 INFO 139637139089216] Saved checkpoint to &#34;/tmp/tmp7s2wp7d_/mx-mod-0000.params&#34; [06/16/2022 09:08:34 INFO 139637139089216] #progress_metric: host=algo-1, completed 6.666666666666667 % of epochs #metrics {&#34;StartTime&#34;: 1655370513.6251845, &#34;EndTime&#34;: 1655370514.3389657, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 0, &#34;Meta&#34;: &#34;training_data_iter&#34;}, &#34;Metrics&#34;: {&#34;Total Records Seen&#34;: {&#34;sum&#34;: 940.0, &#34;count&#34;: 1, &#34;min&#34;: 940, &#34;max&#34;: 940}, &#34;Total Batches Seen&#34;: {&#34;sum&#34;: 33.0, &#34;count&#34;: 1, &#34;min&#34;: 33, &#34;max&#34;: 33}, &#34;Max Records Seen Between Resets&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Max Batches Seen Between Resets&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}, &#34;Reset Count&#34;: {&#34;sum&#34;: 3.0, &#34;count&#34;: 1, &#34;min&#34;: 3, &#34;max&#34;: 3}, &#34;Number of Records Since Last Reset&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Number of Batches Since Last Reset&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}}} [06/16/2022 09:08:34 INFO 139637139089216] #throughput_metric: host=algo-1, train throughput=637.351961759049 records/second #metrics {&#34;StartTime&#34;: 1655370514.793661, &#34;EndTime&#34;: 1655370514.7937362, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.5166595474878947, &#34;count&#34;: 1, &#34;min&#34;: 0.5166595474878947, &#34;max&#34;: 0.5166595474878947}}} #metrics {&#34;StartTime&#34;: 1655370514.7939193, &#34;EndTime&#34;: 1655370514.7939372, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.5506460995144314, &#34;count&#34;: 1, &#34;min&#34;: 0.5506460995144314, &#34;max&#34;: 0.5506460995144314}}} #metrics {&#34;StartTime&#34;: 1655370514.7942128, &#34;EndTime&#34;: 1655370514.794322, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.6185988553365072, &#34;count&#34;: 1, &#34;min&#34;: 0.6185988553365072, &#34;max&#34;: 0.6185988553365072}}} #metrics {&#34;StartTime&#34;: 1655370514.7944574, &#34;EndTime&#34;: 1655370514.7945628, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.6008805370330811, &#34;count&#34;: 1, &#34;min&#34;: 0.6008805370330811, &#34;max&#34;: 0.6008805370330811}}} #metrics {&#34;StartTime&#34;: 1655370514.794682, &#34;EndTime&#34;: 1655370514.7947862, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.37833006223042803, &#34;count&#34;: 1, &#34;min&#34;: 0.37833006223042803, &#34;max&#34;: 0.37833006223042803}}} #metrics {&#34;StartTime&#34;: 1655370514.7948918, &#34;EndTime&#34;: 1655370514.7949944, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4203815163506402, &#34;count&#34;: 1, &#34;min&#34;: 0.4203815163506402, &#34;max&#34;: 0.4203815163506402}}} #metrics {&#34;StartTime&#34;: 1655370514.7951074, &#34;EndTime&#34;: 1655370514.7951229, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.38922022501627607, &#34;count&#34;: 1, &#34;min&#34;: 0.38922022501627607, &#34;max&#34;: 0.38922022501627607}}} #metrics {&#34;StartTime&#34;: 1655370514.7952247, &#34;EndTime&#34;: 1655370514.79524, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.44315323193868, &#34;count&#34;: 1, &#34;min&#34;: 0.44315323193868, &#34;max&#34;: 0.44315323193868}}} #metrics {&#34;StartTime&#34;: 1655370514.7955458, &#34;EndTime&#34;: 1655370514.795566, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.5617256683773465, &#34;count&#34;: 1, &#34;min&#34;: 0.5617256683773465, &#34;max&#34;: 0.5617256683773465}}} #metrics {&#34;StartTime&#34;: 1655370514.795666, &#34;EndTime&#34;: 1655370514.7956815, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4656750954522027, &#34;count&#34;: 1, &#34;min&#34;: 0.4656750954522027, &#34;max&#34;: 0.4656750954522027}}} #metrics {&#34;StartTime&#34;: 1655370514.7957923, &#34;EndTime&#34;: 1655370514.7959692, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.5940322356753879, &#34;count&#34;: 1, &#34;min&#34;: 0.5940322356753879, &#34;max&#34;: 0.5940322356753879}}} #metrics {&#34;StartTime&#34;: 1655370514.796095, &#34;EndTime&#34;: 1655370514.7961931, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.6260889795091417, &#34;count&#34;: 1, &#34;min&#34;: 0.6260889795091417, &#34;max&#34;: 0.6260889795091417}}} #metrics {&#34;StartTime&#34;: 1655370514.796305, &#34;EndTime&#34;: 1655370514.7964015, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4245594278971354, &#34;count&#34;: 1, &#34;min&#34;: 0.4245594278971354, &#34;max&#34;: 0.4245594278971354}}} #metrics {&#34;StartTime&#34;: 1655370514.7965064, &#34;EndTime&#34;: 1655370514.7966027, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3822390651702881, &#34;count&#34;: 1, &#34;min&#34;: 0.3822390651702881, &#34;max&#34;: 0.3822390651702881}}} #metrics {&#34;StartTime&#34;: 1655370514.796765, &#34;EndTime&#34;: 1655370514.7968645, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.43491101688808864, &#34;count&#34;: 1, &#34;min&#34;: 0.43491101688808864, &#34;max&#34;: 0.43491101688808864}}} #metrics {&#34;StartTime&#34;: 1655370514.7970545, &#34;EndTime&#34;: 1655370514.7971013, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.43132063547770183, &#34;count&#34;: 1, &#34;min&#34;: 0.43132063547770183, &#34;max&#34;: 0.43132063547770183}}} #metrics {&#34;StartTime&#34;: 1655370514.7972143, &#34;EndTime&#34;: 1655370514.79732, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.5299906879001194, &#34;count&#34;: 1, &#34;min&#34;: 0.5299906879001194, &#34;max&#34;: 0.5299906879001194}}} #metrics {&#34;StartTime&#34;: 1655370514.7975247, &#34;EndTime&#34;: 1655370514.797551, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.5932350529564752, &#34;count&#34;: 1, &#34;min&#34;: 0.5932350529564752, &#34;max&#34;: 0.5932350529564752}}} #metrics {&#34;StartTime&#34;: 1655370514.7977629, &#34;EndTime&#34;: 1655370514.797783, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.5233783536487155, &#34;count&#34;: 1, &#34;min&#34;: 0.5233783536487155, &#34;max&#34;: 0.5233783536487155}}} #metrics {&#34;StartTime&#34;: 1655370514.7979352, &#34;EndTime&#34;: 1655370514.7979546, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.5680733150906033, &#34;count&#34;: 1, &#34;min&#34;: 0.5680733150906033, &#34;max&#34;: 0.5680733150906033}}} #metrics {&#34;StartTime&#34;: 1655370514.7980409, &#34;EndTime&#34;: 1655370514.7980552, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.5522896650102403, &#34;count&#34;: 1, &#34;min&#34;: 0.5522896650102403, &#34;max&#34;: 0.5522896650102403}}} #metrics {&#34;StartTime&#34;: 1655370514.7984867, &#34;EndTime&#34;: 1655370514.7985315, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4333591233359443, &#34;count&#34;: 1, &#34;min&#34;: 0.4333591233359443, &#34;max&#34;: 0.4333591233359443}}} #metrics {&#34;StartTime&#34;: 1655370514.7988224, &#34;EndTime&#34;: 1655370514.798846, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4411356327268812, &#34;count&#34;: 1, &#34;min&#34;: 0.4411356327268812, &#34;max&#34;: 0.4411356327268812}}} #metrics {&#34;StartTime&#34;: 1655370514.7991967, &#34;EndTime&#34;: 1655370514.7992182, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.5438913355933296, &#34;count&#34;: 1, &#34;min&#34;: 0.5438913355933296, &#34;max&#34;: 0.5438913355933296}}} #metrics {&#34;StartTime&#34;: 1655370514.7995625, &#34;EndTime&#34;: 1655370514.7995846, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9584204080369737, &#34;count&#34;: 1, &#34;min&#34;: 0.9584204080369737, &#34;max&#34;: 0.9584204080369737}}} #metrics {&#34;StartTime&#34;: 1655370514.7999318, &#34;EndTime&#34;: 1655370514.7999597, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9392160394456651, &#34;count&#34;: 1, &#34;min&#34;: 0.9392160394456651, &#34;max&#34;: 0.9392160394456651}}} #metrics {&#34;StartTime&#34;: 1655370514.8002694, &#34;EndTime&#34;: 1655370514.8002915, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9485654428270128, &#34;count&#34;: 1, &#34;min&#34;: 0.9485654428270128, &#34;max&#34;: 0.9485654428270128}}} #metrics {&#34;StartTime&#34;: 1655370514.8005514, &#34;EndTime&#34;: 1655370514.8005733, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9190441343519423, &#34;count&#34;: 1, &#34;min&#34;: 0.9190441343519423, &#34;max&#34;: 0.9190441343519423}}} #metrics {&#34;StartTime&#34;: 1655370514.8006892, &#34;EndTime&#34;: 1655370514.8007975, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 1.067823766072591, &#34;count&#34;: 1, &#34;min&#34;: 1.067823766072591, &#34;max&#34;: 1.067823766072591}}} #metrics {&#34;StartTime&#34;: 1655370514.8011131, &#34;EndTime&#34;: 1655370514.8011549, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 1.0263462956746419, &#34;count&#34;: 1, &#34;min&#34;: 1.0263462956746419, &#34;max&#34;: 1.0263462956746419}}} #metrics {&#34;StartTime&#34;: 1655370514.8014686, &#34;EndTime&#34;: 1655370514.8015127, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 1.0256339263916017, &#34;count&#34;: 1, &#34;min&#34;: 1.0256339263916017, &#34;max&#34;: 1.0256339263916017}}} #metrics {&#34;StartTime&#34;: 1655370514.8018544, &#34;EndTime&#34;: 1655370514.8019047, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 1.0389580175611708, &#34;count&#34;: 1, &#34;min&#34;: 1.0389580175611708, &#34;max&#34;: 1.0389580175611708}}} [06/16/2022 09:08:34 INFO 139637139089216] #quality_metric: host=algo-1, epoch=1, train mse_objective &lt;loss&gt;=0.5166595474878947 #metrics {&#34;StartTime&#34;: 1655370514.8894382, &#34;EndTime&#34;: 1655370514.8895233, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 20.534227558210784, &#34;count&#34;: 1, &#34;min&#34;: 20.534227558210784, &#34;max&#34;: 20.534227558210784}}} #metrics {&#34;StartTime&#34;: 1655370514.8900254, &#34;EndTime&#34;: 1655370514.890074, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 21.64132391237745, &#34;count&#34;: 1, &#34;min&#34;: 21.64132391237745, &#34;max&#34;: 21.64132391237745}}} #metrics {&#34;StartTime&#34;: 1655370514.8903944, &#34;EndTime&#34;: 1655370514.8904364, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 27.875167547487745, &#34;count&#34;: 1, &#34;min&#34;: 27.875167547487745, &#34;max&#34;: 27.875167547487745}}} #metrics {&#34;StartTime&#34;: 1655370514.8907256, &#34;EndTime&#34;: 1655370514.8907456, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 23.328314089307597, &#34;count&#34;: 1, &#34;min&#34;: 23.328314089307597, &#34;max&#34;: 23.328314089307597}}} #metrics {&#34;StartTime&#34;: 1655370514.8910687, &#34;EndTime&#34;: 1655370514.89111, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 12.65540687710631, &#34;count&#34;: 1, &#34;min&#34;: 12.65540687710631, &#34;max&#34;: 12.65540687710631}}} #metrics {&#34;StartTime&#34;: 1655370514.891395, &#34;EndTime&#34;: 1655370514.8914142, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 23.28372970281863, &#34;count&#34;: 1, &#34;min&#34;: 23.28372970281863, &#34;max&#34;: 23.28372970281863}}} #metrics {&#34;StartTime&#34;: 1655370514.891733, &#34;EndTime&#34;: 1655370514.8917713, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.133567062078738, &#34;count&#34;: 1, &#34;min&#34;: 13.133567062078738, &#34;max&#34;: 13.133567062078738}}} #metrics {&#34;StartTime&#34;: 1655370514.8920536, &#34;EndTime&#34;: 1655370514.8920732, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 27.100325042126226, &#34;count&#34;: 1, &#34;min&#34;: 27.100325042126226, &#34;max&#34;: 27.100325042126226}}} #metrics {&#34;StartTime&#34;: 1655370514.893827, &#34;EndTime&#34;: 1655370514.8938773, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 21.418879490272673, &#34;count&#34;: 1, &#34;min&#34;: 21.418879490272673, &#34;max&#34;: 21.418879490272673}}} #metrics {&#34;StartTime&#34;: 1655370514.8942196, &#34;EndTime&#34;: 1655370514.8942635, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.733844831878063, &#34;count&#34;: 1, &#34;min&#34;: 17.733844831878063, &#34;max&#34;: 17.733844831878063}}} #metrics {&#34;StartTime&#34;: 1655370514.894603, &#34;EndTime&#34;: 1655370514.8946478, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 23.06170534620098, &#34;count&#34;: 1, &#34;min&#34;: 23.06170534620098, &#34;max&#34;: 23.06170534620098}}} #metrics {&#34;StartTime&#34;: 1655370514.8950067, &#34;EndTime&#34;: 1655370514.8950486, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 28.42096066942402, &#34;count&#34;: 1, &#34;min&#34;: 28.42096066942402, &#34;max&#34;: 28.42096066942402}}} #metrics {&#34;StartTime&#34;: 1655370514.8953843, &#34;EndTime&#34;: 1655370514.8954334, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 25.944401161343443, &#34;count&#34;: 1, &#34;min&#34;: 25.944401161343443, &#34;max&#34;: 25.944401161343443}}} #metrics {&#34;StartTime&#34;: 1655370514.8957503, &#34;EndTime&#34;: 1655370514.8957975, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.409918691597733, &#34;count&#34;: 1, &#34;min&#34;: 13.409918691597733, &#34;max&#34;: 13.409918691597733}}} #metrics {&#34;StartTime&#34;: 1655370514.8961208, &#34;EndTime&#34;: 1655370514.8961651, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 26.13724532781863, &#34;count&#34;: 1, &#34;min&#34;: 26.13724532781863, &#34;max&#34;: 26.13724532781863}}} #metrics {&#34;StartTime&#34;: 1655370514.8964906, &#34;EndTime&#34;: 1655370514.8965366, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 26.59408509497549, &#34;count&#34;: 1, &#34;min&#34;: 26.59408509497549, &#34;max&#34;: 26.59408509497549}}} #metrics {&#34;StartTime&#34;: 1655370514.8968673, &#34;EndTime&#34;: 1655370514.89692, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 24.054742551317403, &#34;count&#34;: 1, &#34;min&#34;: 24.054742551317403, &#34;max&#34;: 24.054742551317403}}} #metrics {&#34;StartTime&#34;: 1655370514.897243, &#34;EndTime&#34;: 1655370514.8972852, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 23.887010761335784, &#34;count&#34;: 1, &#34;min&#34;: 23.887010761335784, &#34;max&#34;: 23.887010761335784}}} #metrics {&#34;StartTime&#34;: 1655370514.897665, &#34;EndTime&#34;: 1655370514.8976884, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 22.27529967064951, &#34;count&#34;: 1, &#34;min&#34;: 22.27529967064951, &#34;max&#34;: 22.27529967064951}}} #metrics {&#34;StartTime&#34;: 1655370514.8980236, &#34;EndTime&#34;: 1655370514.89807, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 24.790613511029413, &#34;count&#34;: 1, &#34;min&#34;: 24.790613511029413, &#34;max&#34;: 24.790613511029413}}} #metrics {&#34;StartTime&#34;: 1655370514.8984141, &#34;EndTime&#34;: 1655370514.8984637, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 22.136188581878063, &#34;count&#34;: 1, &#34;min&#34;: 22.136188581878063, &#34;max&#34;: 22.136188581878063}}} #metrics {&#34;StartTime&#34;: 1655370514.8987782, &#34;EndTime&#34;: 1655370514.8988209, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 21.865616143918505, &#34;count&#34;: 1, &#34;min&#34;: 21.865616143918505, &#34;max&#34;: 21.865616143918505}}} #metrics {&#34;StartTime&#34;: 1655370514.8991504, &#34;EndTime&#34;: 1655370514.8991938, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 21.634711770450366, &#34;count&#34;: 1, &#34;min&#34;: 21.634711770450366, &#34;max&#34;: 21.634711770450366}}} #metrics {&#34;StartTime&#34;: 1655370514.8995178, &#34;EndTime&#34;: 1655370514.8995636, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 22.551690793504903, &#34;count&#34;: 1, &#34;min&#34;: 22.551690793504903, &#34;max&#34;: 22.551690793504903}}} #metrics {&#34;StartTime&#34;: 1655370514.899884, &#34;EndTime&#34;: 1655370514.8999379, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 63.2322567210478, &#34;count&#34;: 1, &#34;min&#34;: 63.2322567210478, &#34;max&#34;: 63.2322567210478}}} #metrics {&#34;StartTime&#34;: 1655370514.9002714, &#34;EndTime&#34;: 1655370514.900316, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 59.91164742264093, &#34;count&#34;: 1, &#34;min&#34;: 59.91164742264093, &#34;max&#34;: 59.91164742264093}}} #metrics {&#34;StartTime&#34;: 1655370514.9006438, &#34;EndTime&#34;: 1655370514.9007075, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.350928213082106, &#34;count&#34;: 1, &#34;min&#34;: 61.350928213082106, &#34;max&#34;: 61.350928213082106}}} #metrics {&#34;StartTime&#34;: 1655370514.901036, &#34;EndTime&#34;: 1655370514.90108, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 60.662939931832106, &#34;count&#34;: 1, &#34;min&#34;: 60.662939931832106, &#34;max&#34;: 60.662939931832106}}} #metrics {&#34;StartTime&#34;: 1655370514.9013925, &#34;EndTime&#34;: 1655370514.901439, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 58.422523648131126, &#34;count&#34;: 1, &#34;min&#34;: 58.422523648131126, &#34;max&#34;: 58.422523648131126}}} #metrics {&#34;StartTime&#34;: 1655370514.9017863, &#34;EndTime&#34;: 1655370514.9018378, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 59.99669213388481, &#34;count&#34;: 1, &#34;min&#34;: 59.99669213388481, &#34;max&#34;: 59.99669213388481}}} #metrics {&#34;StartTime&#34;: 1655370514.9021795, &#34;EndTime&#34;: 1655370514.9022245, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 59.20013786764706, &#34;count&#34;: 1, &#34;min&#34;: 59.20013786764706, &#34;max&#34;: 59.20013786764706}}} #metrics {&#34;StartTime&#34;: 1655370514.9025545, &#34;EndTime&#34;: 1655370514.9026, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 59.66762886795343, &#34;count&#34;: 1, &#34;min&#34;: 59.66762886795343, &#34;max&#34;: 59.66762886795343}}} [06/16/2022 09:08:34 INFO 139637139089216] #quality_metric: host=algo-1, epoch=1, validation mse_objective &lt;loss&gt;=20.534227558210784 [06/16/2022 09:08:34 INFO 139637139089216] #early_stopping_criteria_metric: host=algo-1, epoch=1, criteria=mse_objective, value=12.65540687710631 [06/16/2022 09:08:34 INFO 139637139089216] Epoch 1: Loss improved. Updating best model [06/16/2022 09:08:34 INFO 139637139089216] Saving model for epoch: 1 [06/16/2022 09:08:34 INFO 139637139089216] Saved checkpoint to &#34;/tmp/tmppz33zbgv/mx-mod-0000.params&#34; [06/16/2022 09:08:34 INFO 139637139089216] #progress_metric: host=algo-1, completed 13.333333333333334 % of epochs #metrics {&#34;StartTime&#34;: 1655370514.3393297, &#34;EndTime&#34;: 1655370514.9135427, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 1, &#34;Meta&#34;: &#34;training_data_iter&#34;}, &#34;Metrics&#34;: {&#34;Total Records Seen&#34;: {&#34;sum&#34;: 1395.0, &#34;count&#34;: 1, &#34;min&#34;: 1395, &#34;max&#34;: 1395}, &#34;Total Batches Seen&#34;: {&#34;sum&#34;: 49.0, &#34;count&#34;: 1, &#34;min&#34;: 49, &#34;max&#34;: 49}, &#34;Max Records Seen Between Resets&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Max Batches Seen Between Resets&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}, &#34;Reset Count&#34;: {&#34;sum&#34;: 4.0, &#34;count&#34;: 1, &#34;min&#34;: 4, &#34;max&#34;: 4}, &#34;Number of Records Since Last Reset&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Number of Batches Since Last Reset&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}}} [06/16/2022 09:08:34 INFO 139637139089216] #throughput_metric: host=algo-1, train throughput=791.7530590891478 records/second #metrics {&#34;StartTime&#34;: 1655370515.4228156, &#34;EndTime&#34;: 1655370515.4228654, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4330157709121704, &#34;count&#34;: 1, &#34;min&#34;: 0.4330157709121704, &#34;max&#34;: 0.4330157709121704}}} #metrics {&#34;StartTime&#34;: 1655370515.4229321, &#34;EndTime&#34;: 1655370515.4229438, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4542000749376085, &#34;count&#34;: 1, &#34;min&#34;: 0.4542000749376085, &#34;max&#34;: 0.4542000749376085}}} #metrics {&#34;StartTime&#34;: 1655370515.4229865, &#34;EndTime&#34;: 1655370515.4229977, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.507329733106825, &#34;count&#34;: 1, &#34;min&#34;: 0.507329733106825, &#34;max&#34;: 0.507329733106825}}} #metrics {&#34;StartTime&#34;: 1655370515.4230287, &#34;EndTime&#34;: 1655370515.4230385, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.48992490821414525, &#34;count&#34;: 1, &#34;min&#34;: 0.48992490821414525, &#34;max&#34;: 0.48992490821414525}}} #metrics {&#34;StartTime&#34;: 1655370515.423068, &#34;EndTime&#34;: 1655370515.4230778, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3084509409798516, &#34;count&#34;: 1, &#34;min&#34;: 0.3084509409798516, &#34;max&#34;: 0.3084509409798516}}} #metrics {&#34;StartTime&#34;: 1655370515.4231062, &#34;EndTime&#34;: 1655370515.4231162, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.35135615560743544, &#34;count&#34;: 1, &#34;min&#34;: 0.35135615560743544, &#34;max&#34;: 0.35135615560743544}}} #metrics {&#34;StartTime&#34;: 1655370515.4231443, &#34;EndTime&#34;: 1655370515.4231539, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3071141947640313, &#34;count&#34;: 1, &#34;min&#34;: 0.3071141947640313, &#34;max&#34;: 0.3071141947640313}}} #metrics {&#34;StartTime&#34;: 1655370515.423182, &#34;EndTime&#34;: 1655370515.4231913, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.39416544066535103, &#34;count&#34;: 1, &#34;min&#34;: 0.39416544066535103, &#34;max&#34;: 0.39416544066535103}}} #metrics {&#34;StartTime&#34;: 1655370515.4232192, &#34;EndTime&#34;: 1655370515.4232285, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4570247713724772, &#34;count&#34;: 1, &#34;min&#34;: 0.4570247713724772, &#34;max&#34;: 0.4570247713724772}}} #metrics {&#34;StartTime&#34;: 1655370515.4232562, &#34;EndTime&#34;: 1655370515.4232652, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3919076877170139, &#34;count&#34;: 1, &#34;min&#34;: 0.3919076877170139, &#34;max&#34;: 0.3919076877170139}}} #metrics {&#34;StartTime&#34;: 1655370515.4232929, &#34;EndTime&#34;: 1655370515.4233022, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4911240275700887, &#34;count&#34;: 1, &#34;min&#34;: 0.4911240275700887, &#34;max&#34;: 0.4911240275700887}}} #metrics {&#34;StartTime&#34;: 1655370515.4233298, &#34;EndTime&#34;: 1655370515.4233387, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.5087960380978055, &#34;count&#34;: 1, &#34;min&#34;: 0.5087960380978055, &#34;max&#34;: 0.5087960380978055}}} #metrics {&#34;StartTime&#34;: 1655370515.4233673, &#34;EndTime&#34;: 1655370515.423376, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3690242481231689, &#34;count&#34;: 1, &#34;min&#34;: 0.3690242481231689, &#34;max&#34;: 0.3690242481231689}}} #metrics {&#34;StartTime&#34;: 1655370515.4234037, &#34;EndTime&#34;: 1655370515.4234135, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30694677035013834, &#34;count&#34;: 1, &#34;min&#34;: 0.30694677035013834, &#34;max&#34;: 0.30694677035013834}}} #metrics {&#34;StartTime&#34;: 1655370515.4234416, &#34;EndTime&#34;: 1655370515.4234507, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.38157987912495933, &#34;count&#34;: 1, &#34;min&#34;: 0.38157987912495933, &#34;max&#34;: 0.38157987912495933}}} #metrics {&#34;StartTime&#34;: 1655370515.4234781, &#34;EndTime&#34;: 1655370515.4234872, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3797152307298448, &#34;count&#34;: 1, &#34;min&#34;: 0.3797152307298448, &#34;max&#34;: 0.3797152307298448}}} #metrics {&#34;StartTime&#34;: 1655370515.4235144, &#34;EndTime&#34;: 1655370515.4235237, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4626873074637519, &#34;count&#34;: 1, &#34;min&#34;: 0.4626873074637519, &#34;max&#34;: 0.4626873074637519}}} #metrics {&#34;StartTime&#34;: 1655370515.4235513, &#34;EndTime&#34;: 1655370515.4235604, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4949516042073568, &#34;count&#34;: 1, &#34;min&#34;: 0.4949516042073568, &#34;max&#34;: 0.4949516042073568}}} #metrics {&#34;StartTime&#34;: 1655370515.423588, &#34;EndTime&#34;: 1655370515.42361, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4552154101265801, &#34;count&#34;: 1, &#34;min&#34;: 0.4552154101265801, &#34;max&#34;: 0.4552154101265801}}} #metrics {&#34;StartTime&#34;: 1655370515.4236434, &#34;EndTime&#34;: 1655370515.4236536, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.47885233985053166, &#34;count&#34;: 1, &#34;min&#34;: 0.47885233985053166, &#34;max&#34;: 0.47885233985053166}}} #metrics {&#34;StartTime&#34;: 1655370515.423682, &#34;EndTime&#34;: 1655370515.4236915, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4555201307932536, &#34;count&#34;: 1, &#34;min&#34;: 0.4555201307932536, &#34;max&#34;: 0.4555201307932536}}} #metrics {&#34;StartTime&#34;: 1655370515.4237192, &#34;EndTime&#34;: 1655370515.4237285, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4135795752207438, &#34;count&#34;: 1, &#34;min&#34;: 0.4135795752207438, &#34;max&#34;: 0.4135795752207438}}} #metrics {&#34;StartTime&#34;: 1655370515.423756, &#34;EndTime&#34;: 1655370515.423765, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.40918830977545845, &#34;count&#34;: 1, &#34;min&#34;: 0.40918830977545845, &#34;max&#34;: 0.40918830977545845}}} #metrics {&#34;StartTime&#34;: 1655370515.4237924, &#34;EndTime&#34;: 1655370515.4238017, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4457821973164876, &#34;count&#34;: 1, &#34;min&#34;: 0.4457821973164876, &#34;max&#34;: 0.4457821973164876}}} #metrics {&#34;StartTime&#34;: 1655370515.423829, &#34;EndTime&#34;: 1655370515.4238384, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9659426667955187, &#34;count&#34;: 1, &#34;min&#34;: 0.9659426667955187, &#34;max&#34;: 0.9659426667955187}}} #metrics {&#34;StartTime&#34;: 1655370515.4238653, &#34;EndTime&#34;: 1655370515.4238749, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9553767310248481, &#34;count&#34;: 1, &#34;min&#34;: 0.9553767310248481, &#34;max&#34;: 0.9553767310248481}}} #metrics {&#34;StartTime&#34;: 1655370515.4239028, &#34;EndTime&#34;: 1655370515.423913, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9574976030985515, &#34;count&#34;: 1, &#34;min&#34;: 0.9574976030985515, &#34;max&#34;: 0.9574976030985515}}} #metrics {&#34;StartTime&#34;: 1655370515.4239492, &#34;EndTime&#34;: 1655370515.4239597, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9598651504516602, &#34;count&#34;: 1, &#34;min&#34;: 0.9598651504516602, &#34;max&#34;: 0.9598651504516602}}} #metrics {&#34;StartTime&#34;: 1655370515.4239879, &#34;EndTime&#34;: 1655370515.4239972, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9971387354532878, &#34;count&#34;: 1, &#34;min&#34;: 0.9971387354532878, &#34;max&#34;: 0.9971387354532878}}} #metrics {&#34;StartTime&#34;: 1655370515.4240248, &#34;EndTime&#34;: 1655370515.4240344, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.987955216301812, &#34;count&#34;: 1, &#34;min&#34;: 0.987955216301812, &#34;max&#34;: 0.987955216301812}}} #metrics {&#34;StartTime&#34;: 1655370515.424062, &#34;EndTime&#34;: 1655370515.424071, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9868502362569174, &#34;count&#34;: 1, &#34;min&#34;: 0.9868502362569174, &#34;max&#34;: 0.9868502362569174}}} #metrics {&#34;StartTime&#34;: 1655370515.4240985, &#34;EndTime&#34;: 1655370515.424108, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9949375194973415, &#34;count&#34;: 1, &#34;min&#34;: 0.9949375194973415, &#34;max&#34;: 0.9949375194973415}}} [06/16/2022 09:08:35 INFO 139637139089216] #quality_metric: host=algo-1, epoch=2, train mse_objective &lt;loss&gt;=0.4330157709121704 #metrics {&#34;StartTime&#34;: 1655370515.5262842, &#34;EndTime&#34;: 1655370515.5263817, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 18.56622553806679, &#34;count&#34;: 1, &#34;min&#34;: 18.56622553806679, &#34;max&#34;: 18.56622553806679}}} #metrics {&#34;StartTime&#34;: 1655370515.526723, &#34;EndTime&#34;: 1655370515.5267525, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.290706858915442, &#34;count&#34;: 1, &#34;min&#34;: 19.290706858915442, &#34;max&#34;: 19.290706858915442}}} #metrics {&#34;StartTime&#34;: 1655370515.5271566, &#34;EndTime&#34;: 1655370515.5272026, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 22.47011551202512, &#34;count&#34;: 1, &#34;min&#34;: 22.47011551202512, &#34;max&#34;: 22.47011551202512}}} #metrics {&#34;StartTime&#34;: 1655370515.5275395, &#34;EndTime&#34;: 1655370515.5275857, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.617024141199447, &#34;count&#34;: 1, &#34;min&#34;: 19.617024141199447, &#34;max&#34;: 19.617024141199447}}} #metrics {&#34;StartTime&#34;: 1655370515.5279338, &#34;EndTime&#34;: 1655370515.528078, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 16.28774246515012, &#34;count&#34;: 1, &#34;min&#34;: 16.28774246515012, &#34;max&#34;: 16.28774246515012}}} #metrics {&#34;StartTime&#34;: 1655370515.5284405, &#34;EndTime&#34;: 1655370515.529872, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 16.712075027765014, &#34;count&#34;: 1, &#34;min&#34;: 16.712075027765014, &#34;max&#34;: 16.712075027765014}}} #metrics {&#34;StartTime&#34;: 1655370515.5302424, &#34;EndTime&#34;: 1655370515.5302901, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 16.65838742723652, &#34;count&#34;: 1, &#34;min&#34;: 16.65838742723652, &#34;max&#34;: 16.65838742723652}}} #metrics {&#34;StartTime&#34;: 1655370515.5306304, &#34;EndTime&#34;: 1655370515.5306756, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.228813021790746, &#34;count&#34;: 1, &#34;min&#34;: 17.228813021790746, &#34;max&#34;: 17.228813021790746}}} #metrics {&#34;StartTime&#34;: 1655370515.5310178, &#34;EndTime&#34;: 1655370515.531063, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 18.618967093673408, &#34;count&#34;: 1, &#34;min&#34;: 18.618967093673408, &#34;max&#34;: 18.618967093673408}}} #metrics {&#34;StartTime&#34;: 1655370515.5314093, &#34;EndTime&#34;: 1655370515.5314512, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 16.397164737477024, &#34;count&#34;: 1, &#34;min&#34;: 16.397164737477024, &#34;max&#34;: 16.397164737477024}}} #metrics {&#34;StartTime&#34;: 1655370515.531789, &#34;EndTime&#34;: 1655370515.5318391, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.562892539828432, &#34;count&#34;: 1, &#34;min&#34;: 19.562892539828432, &#34;max&#34;: 19.562892539828432}}} #metrics {&#34;StartTime&#34;: 1655370515.5321634, &#34;EndTime&#34;: 1655370515.5322104, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 23.306177475873163, &#34;count&#34;: 1, &#34;min&#34;: 23.306177475873163, &#34;max&#34;: 23.306177475873163}}} #metrics {&#34;StartTime&#34;: 1655370515.5325294, &#34;EndTime&#34;: 1655370515.5325742, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.138356526692707, &#34;count&#34;: 1, &#34;min&#34;: 17.138356526692707, &#34;max&#34;: 17.138356526692707}}} #metrics {&#34;StartTime&#34;: 1655370515.5329034, &#34;EndTime&#34;: 1655370515.5329773, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 16.546471689261644, &#34;count&#34;: 1, &#34;min&#34;: 16.546471689261644, &#34;max&#34;: 16.546471689261644}}} #metrics {&#34;StartTime&#34;: 1655370515.533344, &#34;EndTime&#34;: 1655370515.5333664, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 15.802816353592219, &#34;count&#34;: 1, &#34;min&#34;: 15.802816353592219, &#34;max&#34;: 15.802816353592219}}} #metrics {&#34;StartTime&#34;: 1655370515.533725, &#34;EndTime&#34;: 1655370515.53377, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 16.945695465686274, &#34;count&#34;: 1, &#34;min&#34;: 16.945695465686274, &#34;max&#34;: 16.945695465686274}}} #metrics {&#34;StartTime&#34;: 1655370515.5340865, &#34;EndTime&#34;: 1655370515.5341291, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 21.12312047621783, &#34;count&#34;: 1, &#34;min&#34;: 21.12312047621783, &#34;max&#34;: 21.12312047621783}}} #metrics {&#34;StartTime&#34;: 1655370515.5344512, &#34;EndTime&#34;: 1655370515.534495, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 22.38473331227022, &#34;count&#34;: 1, &#34;min&#34;: 22.38473331227022, &#34;max&#34;: 22.38473331227022}}} #metrics {&#34;StartTime&#34;: 1655370515.5348501, &#34;EndTime&#34;: 1655370515.534893, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 20.743672090418197, &#34;count&#34;: 1, &#34;min&#34;: 20.743672090418197, &#34;max&#34;: 20.743672090418197}}} #metrics {&#34;StartTime&#34;: 1655370515.535216, &#34;EndTime&#34;: 1655370515.5352612, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 21.93802059397978, &#34;count&#34;: 1, &#34;min&#34;: 21.93802059397978, &#34;max&#34;: 21.93802059397978}}} #metrics {&#34;StartTime&#34;: 1655370515.5355756, &#34;EndTime&#34;: 1655370515.535621, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 18.39089786305147, &#34;count&#34;: 1, &#34;min&#34;: 18.39089786305147, &#34;max&#34;: 18.39089786305147}}} #metrics {&#34;StartTime&#34;: 1655370515.535933, &#34;EndTime&#34;: 1655370515.535975, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.713614071116726, &#34;count&#34;: 1, &#34;min&#34;: 19.713614071116726, &#34;max&#34;: 19.713614071116726}}} #metrics {&#34;StartTime&#34;: 1655370515.5362978, &#34;EndTime&#34;: 1655370515.5363426, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.60989320044424, &#34;count&#34;: 1, &#34;min&#34;: 19.60989320044424, &#34;max&#34;: 19.60989320044424}}} #metrics {&#34;StartTime&#34;: 1655370515.5366876, &#34;EndTime&#34;: 1655370515.5367286, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 21.67853441425398, &#34;count&#34;: 1, &#34;min&#34;: 21.67853441425398, &#34;max&#34;: 21.67853441425398}}} #metrics {&#34;StartTime&#34;: 1655370515.537061, &#34;EndTime&#34;: 1655370515.5371015, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 60.557566923253674, &#34;count&#34;: 1, &#34;min&#34;: 60.557566923253674, &#34;max&#34;: 60.557566923253674}}} #metrics {&#34;StartTime&#34;: 1655370515.537462, &#34;EndTime&#34;: 1655370515.5374856, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.31462067248774, &#34;count&#34;: 1, &#34;min&#34;: 61.31462067248774, &#34;max&#34;: 61.31462067248774}}} #metrics {&#34;StartTime&#34;: 1655370515.5388012, &#34;EndTime&#34;: 1655370515.5388503, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.02584300321691, &#34;count&#34;: 1, &#34;min&#34;: 61.02584300321691, &#34;max&#34;: 61.02584300321691}}} #metrics {&#34;StartTime&#34;: 1655370515.5391831, &#34;EndTime&#34;: 1655370515.5392292, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.95382869944853, &#34;count&#34;: 1, &#34;min&#34;: 61.95382869944853, &#34;max&#34;: 61.95382869944853}}} #metrics {&#34;StartTime&#34;: 1655370515.5395522, &#34;EndTime&#34;: 1655370515.5395973, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 60.731210746017155, &#34;count&#34;: 1, &#34;min&#34;: 60.731210746017155, &#34;max&#34;: 60.731210746017155}}} #metrics {&#34;StartTime&#34;: 1655370515.5399215, &#34;EndTime&#34;: 1655370515.5399704, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 60.673804189644606, &#34;count&#34;: 1, &#34;min&#34;: 60.673804189644606, &#34;max&#34;: 60.673804189644606}}} #metrics {&#34;StartTime&#34;: 1655370515.5403278, &#34;EndTime&#34;: 1655370515.5403867, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 60.38941865808823, &#34;count&#34;: 1, &#34;min&#34;: 60.38941865808823, &#34;max&#34;: 60.38941865808823}}} #metrics {&#34;StartTime&#34;: 1655370515.5407243, &#34;EndTime&#34;: 1655370515.540768, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 58.48939424402573, &#34;count&#34;: 1, &#34;min&#34;: 58.48939424402573, &#34;max&#34;: 58.48939424402573}}} [06/16/2022 09:08:35 INFO 139637139089216] #quality_metric: host=algo-1, epoch=2, validation mse_objective &lt;loss&gt;=18.56622553806679 [06/16/2022 09:08:35 INFO 139637139089216] #early_stopping_criteria_metric: host=algo-1, epoch=2, criteria=mse_objective, value=15.802816353592219 [06/16/2022 09:08:35 INFO 139637139089216] Saving model for epoch: 2 [06/16/2022 09:08:35 INFO 139637139089216] Saved checkpoint to &#34;/tmp/tmp5eh525cs/mx-mod-0000.params&#34; [06/16/2022 09:08:35 INFO 139637139089216] #progress_metric: host=algo-1, completed 20.0 % of epochs #metrics {&#34;StartTime&#34;: 1655370514.91442, &#34;EndTime&#34;: 1655370515.5563972, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 2, &#34;Meta&#34;: &#34;training_data_iter&#34;}, &#34;Metrics&#34;: {&#34;Total Records Seen&#34;: {&#34;sum&#34;: 1850.0, &#34;count&#34;: 1, &#34;min&#34;: 1850, &#34;max&#34;: 1850}, &#34;Total Batches Seen&#34;: {&#34;sum&#34;: 65.0, &#34;count&#34;: 1, &#34;min&#34;: 65, &#34;max&#34;: 65}, &#34;Max Records Seen Between Resets&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Max Batches Seen Between Resets&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}, &#34;Reset Count&#34;: {&#34;sum&#34;: 5.0, &#34;count&#34;: 1, &#34;min&#34;: 5, &#34;max&#34;: 5}, &#34;Number of Records Since Last Reset&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Number of Batches Since Last Reset&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}}} [06/16/2022 09:08:35 INFO 139637139089216] #throughput_metric: host=algo-1, train throughput=708.1011329049273 records/second #metrics {&#34;StartTime&#34;: 1655370516.070785, &#34;EndTime&#34;: 1655370516.0708687, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.37687147670321997, &#34;count&#34;: 1, &#34;min&#34;: 0.37687147670321997, &#34;max&#34;: 0.37687147670321997}}} #metrics {&#34;StartTime&#34;: 1655370516.0709586, &#34;EndTime&#34;: 1655370516.0709722, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3899355888366699, &#34;count&#34;: 1, &#34;min&#34;: 0.3899355888366699, &#34;max&#34;: 0.3899355888366699}}} #metrics {&#34;StartTime&#34;: 1655370516.0710146, &#34;EndTime&#34;: 1655370516.0710251, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.42631299919552273, &#34;count&#34;: 1, &#34;min&#34;: 0.42631299919552273, &#34;max&#34;: 0.42631299919552273}}} #metrics {&#34;StartTime&#34;: 1655370516.0710595, &#34;EndTime&#34;: 1655370516.0710688, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.40993305418226456, &#34;count&#34;: 1, &#34;min&#34;: 0.40993305418226456, &#34;max&#34;: 0.40993305418226456}}} #metrics {&#34;StartTime&#34;: 1655370516.0711024, &#34;EndTime&#34;: 1655370516.071112, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30120240105523005, &#34;count&#34;: 1, &#34;min&#34;: 0.30120240105523005, &#34;max&#34;: 0.30120240105523005}}} #metrics {&#34;StartTime&#34;: 1655370516.0711467, &#34;EndTime&#34;: 1655370516.0711563, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3230732864803738, &#34;count&#34;: 1, &#34;min&#34;: 0.3230732864803738, &#34;max&#34;: 0.3230732864803738}}} #metrics {&#34;StartTime&#34;: 1655370516.07119, &#34;EndTime&#34;: 1655370516.0711992, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3055626720852322, &#34;count&#34;: 1, &#34;min&#34;: 0.3055626720852322, &#34;max&#34;: 0.3055626720852322}}} #metrics {&#34;StartTime&#34;: 1655370516.0712337, &#34;EndTime&#34;: 1655370516.071244, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.32883591228061254, &#34;count&#34;: 1, &#34;min&#34;: 0.32883591228061254, &#34;max&#34;: 0.32883591228061254}}} #metrics {&#34;StartTime&#34;: 1655370516.0712812, &#34;EndTime&#34;: 1655370516.0712903, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3894065120485094, &#34;count&#34;: 1, &#34;min&#34;: 0.3894065120485094, &#34;max&#34;: 0.3894065120485094}}} #metrics {&#34;StartTime&#34;: 1655370516.0713246, &#34;EndTime&#34;: 1655370516.0713346, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.34771293534172903, &#34;count&#34;: 1, &#34;min&#34;: 0.34771293534172903, &#34;max&#34;: 0.34771293534172903}}} #metrics {&#34;StartTime&#34;: 1655370516.071371, &#34;EndTime&#34;: 1655370516.0713813, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.41975070582495794, &#34;count&#34;: 1, &#34;min&#34;: 0.41975070582495794, &#34;max&#34;: 0.41975070582495794}}} #metrics {&#34;StartTime&#34;: 1655370516.0714185, &#34;EndTime&#34;: 1655370516.0714288, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.42690609667036267, &#34;count&#34;: 1, &#34;min&#34;: 0.42690609667036267, &#34;max&#34;: 0.42690609667036267}}} #metrics {&#34;StartTime&#34;: 1655370516.071467, &#34;EndTime&#34;: 1655370516.0714767, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3297531509399414, &#34;count&#34;: 1, &#34;min&#34;: 0.3297531509399414, &#34;max&#34;: 0.3297531509399414}}} #metrics {&#34;StartTime&#34;: 1655370516.0715125, &#34;EndTime&#34;: 1655370516.0715642, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3046278773413764, &#34;count&#34;: 1, &#34;min&#34;: 0.3046278773413764, &#34;max&#34;: 0.3046278773413764}}} #metrics {&#34;StartTime&#34;: 1655370516.071608, &#34;EndTime&#34;: 1655370516.0716183, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3233711523479886, &#34;count&#34;: 1, &#34;min&#34;: 0.3233711523479886, &#34;max&#34;: 0.3233711523479886}}} #metrics {&#34;StartTime&#34;: 1655370516.071654, &#34;EndTime&#34;: 1655370516.0716634, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3283268467585246, &#34;count&#34;: 1, &#34;min&#34;: 0.3283268467585246, &#34;max&#34;: 0.3283268467585246}}} #metrics {&#34;StartTime&#34;: 1655370516.0716987, &#34;EndTime&#34;: 1655370516.071709, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.42747476153903535, &#34;count&#34;: 1, &#34;min&#34;: 0.42747476153903535, &#34;max&#34;: 0.42747476153903535}}} #metrics {&#34;StartTime&#34;: 1655370516.0717459, &#34;EndTime&#34;: 1655370516.0717561, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.44515266471438936, &#34;count&#34;: 1, &#34;min&#34;: 0.44515266471438936, &#34;max&#34;: 0.44515266471438936}}} #metrics {&#34;StartTime&#34;: 1655370516.071793, &#34;EndTime&#34;: 1655370516.0718045, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.41816701571146647, &#34;count&#34;: 1, &#34;min&#34;: 0.41816701571146647, &#34;max&#34;: 0.41816701571146647}}} #metrics {&#34;StartTime&#34;: 1655370516.0718422, &#34;EndTime&#34;: 1655370516.071852, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.43047955830891926, &#34;count&#34;: 1, &#34;min&#34;: 0.43047955830891926, &#34;max&#34;: 0.43047955830891926}}} #metrics {&#34;StartTime&#34;: 1655370516.071889, &#34;EndTime&#34;: 1655370516.0718994, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4163850498199463, &#34;count&#34;: 1, &#34;min&#34;: 0.4163850498199463, &#34;max&#34;: 0.4163850498199463}}} #metrics {&#34;StartTime&#34;: 1655370516.0719368, &#34;EndTime&#34;: 1655370516.0719469, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.40793675952487524, &#34;count&#34;: 1, &#34;min&#34;: 0.40793675952487524, &#34;max&#34;: 0.40793675952487524}}} #metrics {&#34;StartTime&#34;: 1655370516.0719829, &#34;EndTime&#34;: 1655370516.0719924, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.40354964680141875, &#34;count&#34;: 1, &#34;min&#34;: 0.40354964680141875, &#34;max&#34;: 0.40354964680141875}}} #metrics {&#34;StartTime&#34;: 1655370516.0720303, &#34;EndTime&#34;: 1655370516.072041, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.40695947647094727, &#34;count&#34;: 1, &#34;min&#34;: 0.40695947647094727, &#34;max&#34;: 0.40695947647094727}}} #metrics {&#34;StartTime&#34;: 1655370516.0720763, &#34;EndTime&#34;: 1655370516.0720863, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9495429229736329, &#34;count&#34;: 1, &#34;min&#34;: 0.9495429229736329, &#34;max&#34;: 0.9495429229736329}}} #metrics {&#34;StartTime&#34;: 1655370516.0721233, &#34;EndTime&#34;: 1655370516.0721338, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9583488655090332, &#34;count&#34;: 1, &#34;min&#34;: 0.9583488655090332, &#34;max&#34;: 0.9583488655090332}}} #metrics {&#34;StartTime&#34;: 1655370516.07217, &#34;EndTime&#34;: 1655370516.0721803, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9557569122314453, &#34;count&#34;: 1, &#34;min&#34;: 0.9557569122314453, &#34;max&#34;: 0.9557569122314453}}} #metrics {&#34;StartTime&#34;: 1655370516.0722191, &#34;EndTime&#34;: 1655370516.0722296, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9582390340169271, &#34;count&#34;: 1, &#34;min&#34;: 0.9582390340169271, &#34;max&#34;: 0.9582390340169271}}} #metrics {&#34;StartTime&#34;: 1655370516.0722644, &#34;EndTime&#34;: 1655370516.0722744, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9759493743048774, &#34;count&#34;: 1, &#34;min&#34;: 0.9759493743048774, &#34;max&#34;: 0.9759493743048774}}} #metrics {&#34;StartTime&#34;: 1655370516.072311, &#34;EndTime&#34;: 1655370516.0723207, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9639809778001573, &#34;count&#34;: 1, &#34;min&#34;: 0.9639809778001573, &#34;max&#34;: 0.9639809778001573}}} #metrics {&#34;StartTime&#34;: 1655370516.0723572, &#34;EndTime&#34;: 1655370516.0723674, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9631755362616645, &#34;count&#34;: 1, &#34;min&#34;: 0.9631755362616645, &#34;max&#34;: 0.9631755362616645}}} #metrics {&#34;StartTime&#34;: 1655370516.072404, &#34;EndTime&#34;: 1655370516.0724137, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9622686470879449, &#34;count&#34;: 1, &#34;min&#34;: 0.9622686470879449, &#34;max&#34;: 0.9622686470879449}}} [06/16/2022 09:08:36 INFO 139637139089216] #quality_metric: host=algo-1, epoch=3, train mse_objective &lt;loss&gt;=0.37687147670321997 #metrics {&#34;StartTime&#34;: 1655370516.172835, &#34;EndTime&#34;: 1655370516.1729176, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 16.9971098058364, &#34;count&#34;: 1, &#34;min&#34;: 16.9971098058364, &#34;max&#34;: 16.9971098058364}}} #metrics {&#34;StartTime&#34;: 1655370516.1747851, &#34;EndTime&#34;: 1655370516.1748104, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.806202009612438, &#34;count&#34;: 1, &#34;min&#34;: 17.806202009612438, &#34;max&#34;: 17.806202009612438}}} #metrics {&#34;StartTime&#34;: 1655370516.174856, &#34;EndTime&#34;: 1655370516.1748667, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.166063495710784, &#34;count&#34;: 1, &#34;min&#34;: 19.166063495710784, &#34;max&#34;: 19.166063495710784}}} #metrics {&#34;StartTime&#34;: 1655370516.1749024, &#34;EndTime&#34;: 1655370516.1749117, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.397721832873774, &#34;count&#34;: 1, &#34;min&#34;: 17.397721832873774, &#34;max&#34;: 17.397721832873774}}} #metrics {&#34;StartTime&#34;: 1655370516.1749458, &#34;EndTime&#34;: 1655370516.174956, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.51783722522212, &#34;count&#34;: 1, &#34;min&#34;: 14.51783722522212, &#34;max&#34;: 14.51783722522212}}} #metrics {&#34;StartTime&#34;: 1655370516.1749904, &#34;EndTime&#34;: 1655370516.1749997, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 16.57377415077359, &#34;count&#34;: 1, &#34;min&#34;: 16.57377415077359, &#34;max&#34;: 16.57377415077359}}} #metrics {&#34;StartTime&#34;: 1655370516.1753297, &#34;EndTime&#34;: 1655370516.1753535, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.622593898399204, &#34;count&#34;: 1, &#34;min&#34;: 14.622593898399204, &#34;max&#34;: 14.622593898399204}}} #metrics {&#34;StartTime&#34;: 1655370516.1754174, &#34;EndTime&#34;: 1655370516.175431, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 15.245516907935048, &#34;count&#34;: 1, &#34;min&#34;: 15.245516907935048, &#34;max&#34;: 15.245516907935048}}} #metrics {&#34;StartTime&#34;: 1655370516.1754696, &#34;EndTime&#34;: 1655370516.17548, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 16.654658897250307, &#34;count&#34;: 1, &#34;min&#34;: 16.654658897250307, &#34;max&#34;: 16.654658897250307}}} #metrics {&#34;StartTime&#34;: 1655370516.1755176, &#34;EndTime&#34;: 1655370516.1755273, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.970694149241728, &#34;count&#34;: 1, &#34;min&#34;: 14.970694149241728, &#34;max&#34;: 14.970694149241728}}} #metrics {&#34;StartTime&#34;: 1655370516.175564, &#34;EndTime&#34;: 1655370516.1755745, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.889209223728553, &#34;count&#34;: 1, &#34;min&#34;: 17.889209223728553, &#34;max&#34;: 17.889209223728553}}} #metrics {&#34;StartTime&#34;: 1655370516.175613, &#34;EndTime&#34;: 1655370516.175624, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.786487654143688, &#34;count&#34;: 1, &#34;min&#34;: 19.786487654143688, &#34;max&#34;: 19.786487654143688}}} #metrics {&#34;StartTime&#34;: 1655370516.17566, &#34;EndTime&#34;: 1655370516.17567, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 16.704469568589154, &#34;count&#34;: 1, &#34;min&#34;: 16.704469568589154, &#34;max&#34;: 16.704469568589154}}} #metrics {&#34;StartTime&#34;: 1655370516.1757064, &#34;EndTime&#34;: 1655370516.1757169, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.202775543811274, &#34;count&#34;: 1, &#34;min&#34;: 14.202775543811274, &#34;max&#34;: 14.202775543811274}}} #metrics {&#34;StartTime&#34;: 1655370516.1757562, &#34;EndTime&#34;: 1655370516.175766, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 15.529443479051777, &#34;count&#34;: 1, &#34;min&#34;: 15.529443479051777, &#34;max&#34;: 15.529443479051777}}} #metrics {&#34;StartTime&#34;: 1655370516.1758022, &#34;EndTime&#34;: 1655370516.1758118, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 15.707206576478248, &#34;count&#34;: 1, &#34;min&#34;: 15.707206576478248, &#34;max&#34;: 15.707206576478248}}} #metrics {&#34;StartTime&#34;: 1655370516.1758478, &#34;EndTime&#34;: 1655370516.1758585, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.462962431066178, &#34;count&#34;: 1, &#34;min&#34;: 19.462962431066178, &#34;max&#34;: 19.462962431066178}}} #metrics {&#34;StartTime&#34;: 1655370516.1758938, &#34;EndTime&#34;: 1655370516.1759043, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 20.904079661649817, &#34;count&#34;: 1, &#34;min&#34;: 20.904079661649817, &#34;max&#34;: 20.904079661649817}}} #metrics {&#34;StartTime&#34;: 1655370516.1759408, &#34;EndTime&#34;: 1655370516.1759512, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 18.759546616498163, &#34;count&#34;: 1, &#34;min&#34;: 18.759546616498163, &#34;max&#34;: 18.759546616498163}}} #metrics {&#34;StartTime&#34;: 1655370516.1759884, &#34;EndTime&#34;: 1655370516.1759996, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.47744571461397, &#34;count&#34;: 1, &#34;min&#34;: 19.47744571461397, &#34;max&#34;: 19.47744571461397}}} #metrics {&#34;StartTime&#34;: 1655370516.1760361, &#34;EndTime&#34;: 1655370516.1760464, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 20.019901649624693, &#34;count&#34;: 1, &#34;min&#34;: 20.019901649624693, &#34;max&#34;: 20.019901649624693}}} #metrics {&#34;StartTime&#34;: 1655370516.176085, &#34;EndTime&#34;: 1655370516.176095, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.474550134995404, &#34;count&#34;: 1, &#34;min&#34;: 19.474550134995404, &#34;max&#34;: 19.474550134995404}}} #metrics {&#34;StartTime&#34;: 1655370516.1761308, &#34;EndTime&#34;: 1655370516.1761403, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.6024169921875, &#34;count&#34;: 1, &#34;min&#34;: 19.6024169921875, &#34;max&#34;: 19.6024169921875}}} #metrics {&#34;StartTime&#34;: 1655370516.1761775, &#34;EndTime&#34;: 1655370516.1761882, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 20.623710482728246, &#34;count&#34;: 1, &#34;min&#34;: 20.623710482728246, &#34;max&#34;: 20.623710482728246}}} #metrics {&#34;StartTime&#34;: 1655370516.176223, &#34;EndTime&#34;: 1655370516.176233, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.43261479396446, &#34;count&#34;: 1, &#34;min&#34;: 61.43261479396446, &#34;max&#34;: 61.43261479396446}}} #metrics {&#34;StartTime&#34;: 1655370516.1762698, &#34;EndTime&#34;: 1655370516.17628, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.60102874157476, &#34;count&#34;: 1, &#34;min&#34;: 61.60102874157476, &#34;max&#34;: 61.60102874157476}}} #metrics {&#34;StartTime&#34;: 1655370516.176317, &#34;EndTime&#34;: 1655370516.1763268, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.91429227941177, &#34;count&#34;: 1, &#34;min&#34;: 61.91429227941177, &#34;max&#34;: 61.91429227941177}}} #metrics {&#34;StartTime&#34;: 1655370516.1764174, &#34;EndTime&#34;: 1655370516.1764324, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.42448634727329, &#34;count&#34;: 1, &#34;min&#34;: 61.42448634727329, &#34;max&#34;: 61.42448634727329}}} #metrics {&#34;StartTime&#34;: 1655370516.1764727, &#34;EndTime&#34;: 1655370516.1764836, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 60.93298579197304, &#34;count&#34;: 1, &#34;min&#34;: 60.93298579197304, &#34;max&#34;: 60.93298579197304}}} #metrics {&#34;StartTime&#34;: 1655370516.1765196, &#34;EndTime&#34;: 1655370516.1765292, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.669333065257355, &#34;count&#34;: 1, &#34;min&#34;: 62.669333065257355, &#34;max&#34;: 62.669333065257355}}} #metrics {&#34;StartTime&#34;: 1655370516.1765668, &#34;EndTime&#34;: 1655370516.176577, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.98301786534927, &#34;count&#34;: 1, &#34;min&#34;: 62.98301786534927, &#34;max&#34;: 62.98301786534927}}} #metrics {&#34;StartTime&#34;: 1655370516.176643, &#34;EndTime&#34;: 1655370516.176657, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 63.35635196461397, &#34;count&#34;: 1, &#34;min&#34;: 63.35635196461397, &#34;max&#34;: 63.35635196461397}}} [06/16/2022 09:08:36 INFO 139637139089216] #quality_metric: host=algo-1, epoch=3, validation mse_objective &lt;loss&gt;=16.9971098058364 [06/16/2022 09:08:36 INFO 139637139089216] #early_stopping_criteria_metric: host=algo-1, epoch=3, criteria=mse_objective, value=14.202775543811274 [06/16/2022 09:08:36 INFO 139637139089216] Saving model for epoch: 3 [06/16/2022 09:08:36 INFO 139637139089216] Saved checkpoint to &#34;/tmp/tmpt4vx3hms/mx-mod-0000.params&#34; [06/16/2022 09:08:36 INFO 139637139089216] #progress_metric: host=algo-1, completed 26.666666666666668 % of epochs #metrics {&#34;StartTime&#34;: 1655370515.5573475, &#34;EndTime&#34;: 1655370516.1865118, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 3, &#34;Meta&#34;: &#34;training_data_iter&#34;}, &#34;Metrics&#34;: {&#34;Total Records Seen&#34;: {&#34;sum&#34;: 2305.0, &#34;count&#34;: 1, &#34;min&#34;: 2305, &#34;max&#34;: 2305}, &#34;Total Batches Seen&#34;: {&#34;sum&#34;: 81.0, &#34;count&#34;: 1, &#34;min&#34;: 81, &#34;max&#34;: 81}, &#34;Max Records Seen Between Resets&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Max Batches Seen Between Resets&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}, &#34;Reset Count&#34;: {&#34;sum&#34;: 6.0, &#34;count&#34;: 1, &#34;min&#34;: 6, &#34;max&#34;: 6}, &#34;Number of Records Since Last Reset&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Number of Batches Since Last Reset&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}}} [06/16/2022 09:08:36 INFO 139637139089216] #throughput_metric: host=algo-1, train throughput=723.0333929796658 records/second #metrics {&#34;StartTime&#34;: 1655370516.870697, &#34;EndTime&#34;: 1655370516.870818, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3436708143022325, &#34;count&#34;: 1, &#34;min&#34;: 0.3436708143022325, &#34;max&#34;: 0.3436708143022325}}} #metrics {&#34;StartTime&#34;: 1655370516.8709064, &#34;EndTime&#34;: 1655370516.8709197, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.35319281154208715, &#34;count&#34;: 1, &#34;min&#34;: 0.35319281154208715, &#34;max&#34;: 0.35319281154208715}}} #metrics {&#34;StartTime&#34;: 1655370516.8709624, &#34;EndTime&#34;: 1655370516.8709724, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3756487560272217, &#34;count&#34;: 1, &#34;min&#34;: 0.3756487560272217, &#34;max&#34;: 0.3756487560272217}}} #metrics {&#34;StartTime&#34;: 1655370516.871007, &#34;EndTime&#34;: 1655370516.8710158, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3615579546822442, &#34;count&#34;: 1, &#34;min&#34;: 0.3615579546822442, &#34;max&#34;: 0.3615579546822442}}} #metrics {&#34;StartTime&#34;: 1655370516.8710494, &#34;EndTime&#34;: 1655370516.8710582, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.29766649776034887, &#34;count&#34;: 1, &#34;min&#34;: 0.29766649776034887, &#34;max&#34;: 0.29766649776034887}}} #metrics {&#34;StartTime&#34;: 1655370516.871093, &#34;EndTime&#34;: 1655370516.8711028, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.31679604689280194, &#34;count&#34;: 1, &#34;min&#34;: 0.31679604689280194, &#34;max&#34;: 0.31679604689280194}}} #metrics {&#34;StartTime&#34;: 1655370516.8711355, &#34;EndTime&#34;: 1655370516.871145, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2943812190161811, &#34;count&#34;: 1, &#34;min&#34;: 0.2943812190161811, &#34;max&#34;: 0.2943812190161811}}} #metrics {&#34;StartTime&#34;: 1655370516.8711789, &#34;EndTime&#34;: 1655370516.8711882, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3115263631608751, &#34;count&#34;: 1, &#34;min&#34;: 0.3115263631608751, &#34;max&#34;: 0.3115263631608751}}} #metrics {&#34;StartTime&#34;: 1655370516.8712254, &#34;EndTime&#34;: 1655370516.8712356, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3494891103108724, &#34;count&#34;: 1, &#34;min&#34;: 0.3494891103108724, &#34;max&#34;: 0.3494891103108724}}} #metrics {&#34;StartTime&#34;: 1655370516.87127, &#34;EndTime&#34;: 1655370516.87128, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3216774410671658, &#34;count&#34;: 1, &#34;min&#34;: 0.3216774410671658, &#34;max&#34;: 0.3216774410671658}}} #metrics {&#34;StartTime&#34;: 1655370516.8713155, &#34;EndTime&#34;: 1655370516.8713257, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.37353869279225665, &#34;count&#34;: 1, &#34;min&#34;: 0.37353869279225665, &#34;max&#34;: 0.37353869279225665}}} #metrics {&#34;StartTime&#34;: 1655370516.8713634, &#34;EndTime&#34;: 1655370516.8713737, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.374813134405348, &#34;count&#34;: 1, &#34;min&#34;: 0.374813134405348, &#34;max&#34;: 0.374813134405348}}} #metrics {&#34;StartTime&#34;: 1655370516.87141, &#34;EndTime&#34;: 1655370516.8714206, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3207968584696452, &#34;count&#34;: 1, &#34;min&#34;: 0.3207968584696452, &#34;max&#34;: 0.3207968584696452}}} #metrics {&#34;StartTime&#34;: 1655370516.871459, &#34;EndTime&#34;: 1655370516.8714685, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2979209719763862, &#34;count&#34;: 1, &#34;min&#34;: 0.2979209719763862, &#34;max&#34;: 0.2979209719763862}}} #metrics {&#34;StartTime&#34;: 1655370516.8715043, &#34;EndTime&#34;: 1655370516.8715136, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30769889089796276, &#34;count&#34;: 1, &#34;min&#34;: 0.30769889089796276, &#34;max&#34;: 0.30769889089796276}}} #metrics {&#34;StartTime&#34;: 1655370516.8715494, &#34;EndTime&#34;: 1655370516.8715594, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3148876725302802, &#34;count&#34;: 1, &#34;min&#34;: 0.3148876725302802, &#34;max&#34;: 0.3148876725302802}}} #metrics {&#34;StartTime&#34;: 1655370516.8715959, &#34;EndTime&#34;: 1655370516.8716052, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.40367749320136176, &#34;count&#34;: 1, &#34;min&#34;: 0.40367749320136176, &#34;max&#34;: 0.40367749320136176}}} #metrics {&#34;StartTime&#34;: 1655370516.8716412, &#34;EndTime&#34;: 1655370516.871651, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.41464082347022163, &#34;count&#34;: 1, &#34;min&#34;: 0.41464082347022163, &#34;max&#34;: 0.41464082347022163}}} #metrics {&#34;StartTime&#34;: 1655370516.871686, &#34;EndTime&#34;: 1655370516.8716955, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.39581135167015924, &#34;count&#34;: 1, &#34;min&#34;: 0.39581135167015924, &#34;max&#34;: 0.39581135167015924}}} #metrics {&#34;StartTime&#34;: 1655370516.871733, &#34;EndTime&#34;: 1655370516.8717432, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4019908942116631, &#34;count&#34;: 1, &#34;min&#34;: 0.4019908942116631, &#34;max&#34;: 0.4019908942116631}}} #metrics {&#34;StartTime&#34;: 1655370516.8717804, &#34;EndTime&#34;: 1655370516.871791, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4012196848127577, &#34;count&#34;: 1, &#34;min&#34;: 0.4012196848127577, &#34;max&#34;: 0.4012196848127577}}} #metrics {&#34;StartTime&#34;: 1655370516.8718286, &#34;EndTime&#34;: 1655370516.871838, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.40464004516601565, &#34;count&#34;: 1, &#34;min&#34;: 0.40464004516601565, &#34;max&#34;: 0.40464004516601565}}} #metrics {&#34;StartTime&#34;: 1655370516.8718755, &#34;EndTime&#34;: 1655370516.8718853, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4007937579684787, &#34;count&#34;: 1, &#34;min&#34;: 0.4007937579684787, &#34;max&#34;: 0.4007937579684787}}} #metrics {&#34;StartTime&#34;: 1655370516.8719227, &#34;EndTime&#34;: 1655370516.8719325, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.40211011038886174, &#34;count&#34;: 1, &#34;min&#34;: 0.40211011038886174, &#34;max&#34;: 0.40211011038886174}}} #metrics {&#34;StartTime&#34;: 1655370516.8719683, &#34;EndTime&#34;: 1655370516.8719783, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9575183508131239, &#34;count&#34;: 1, &#34;min&#34;: 0.9575183508131239, &#34;max&#34;: 0.9575183508131239}}} #metrics {&#34;StartTime&#34;: 1655370516.872014, &#34;EndTime&#34;: 1655370516.8720233, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9550820011562772, &#34;count&#34;: 1, &#34;min&#34;: 0.9550820011562772, &#34;max&#34;: 0.9550820011562772}}} #metrics {&#34;StartTime&#34;: 1655370516.8720608, &#34;EndTime&#34;: 1655370516.872071, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9555004607306586, &#34;count&#34;: 1, &#34;min&#34;: 0.9555004607306586, &#34;max&#34;: 0.9555004607306586}}} #metrics {&#34;StartTime&#34;: 1655370516.8721063, &#34;EndTime&#34;: 1655370516.8721159, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9545310147603353, &#34;count&#34;: 1, &#34;min&#34;: 0.9545310147603353, &#34;max&#34;: 0.9545310147603353}}} #metrics {&#34;StartTime&#34;: 1655370516.872153, &#34;EndTime&#34;: 1655370516.872163, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9699274910820855, &#34;count&#34;: 1, &#34;min&#34;: 0.9699274910820855, &#34;max&#34;: 0.9699274910820855}}} #metrics {&#34;StartTime&#34;: 1655370516.8722003, &#34;EndTime&#34;: 1655370516.8722107, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.974634263780382, &#34;count&#34;: 1, &#34;min&#34;: 0.974634263780382, &#34;max&#34;: 0.974634263780382}}} #metrics {&#34;StartTime&#34;: 1655370516.872249, &#34;EndTime&#34;: 1655370516.8722591, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9733005502488878, &#34;count&#34;: 1, &#34;min&#34;: 0.9733005502488878, &#34;max&#34;: 0.9733005502488878}}} #metrics {&#34;StartTime&#34;: 1655370516.872295, &#34;EndTime&#34;: 1655370516.8723044, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9736014281378852, &#34;count&#34;: 1, &#34;min&#34;: 0.9736014281378852, &#34;max&#34;: 0.9736014281378852}}} [06/16/2022 09:08:36 INFO 139637139089216] #quality_metric: host=algo-1, epoch=4, train mse_objective &lt;loss&gt;=0.3436708143022325 #metrics {&#34;StartTime&#34;: 1655370516.9881036, &#34;EndTime&#34;: 1655370516.9881916, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 15.574556238511029, &#34;count&#34;: 1, &#34;min&#34;: 15.574556238511029, &#34;max&#34;: 15.574556238511029}}} #metrics {&#34;StartTime&#34;: 1655370516.9883027, &#34;EndTime&#34;: 1655370516.988318, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 16.30394011852788, &#34;count&#34;: 1, &#34;min&#34;: 16.30394011852788, &#34;max&#34;: 16.30394011852788}}} #metrics {&#34;StartTime&#34;: 1655370516.9883606, &#34;EndTime&#34;: 1655370516.988371, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 16.47208958046109, &#34;count&#34;: 1, &#34;min&#34;: 16.47208958046109, &#34;max&#34;: 16.47208958046109}}} #metrics {&#34;StartTime&#34;: 1655370516.988405, &#34;EndTime&#34;: 1655370516.9884143, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 15.364203957950368, &#34;count&#34;: 1, &#34;min&#34;: 15.364203957950368, &#34;max&#34;: 15.364203957950368}}} #metrics {&#34;StartTime&#34;: 1655370516.9884467, &#34;EndTime&#34;: 1655370516.9884555, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.79813699161305, &#34;count&#34;: 1, &#34;min&#34;: 14.79813699161305, &#34;max&#34;: 14.79813699161305}}} #metrics {&#34;StartTime&#34;: 1655370516.988487, &#34;EndTime&#34;: 1655370516.9884968, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.44422763001685, &#34;count&#34;: 1, &#34;min&#34;: 14.44422763001685, &#34;max&#34;: 14.44422763001685}}} #metrics {&#34;StartTime&#34;: 1655370516.9885292, &#34;EndTime&#34;: 1655370516.9885387, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.71527219286152, &#34;count&#34;: 1, &#34;min&#34;: 14.71527219286152, &#34;max&#34;: 14.71527219286152}}} #metrics {&#34;StartTime&#34;: 1655370516.9885724, &#34;EndTime&#34;: 1655370516.9885807, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.899299172794118, &#34;count&#34;: 1, &#34;min&#34;: 14.899299172794118, &#34;max&#34;: 14.899299172794118}}} #metrics {&#34;StartTime&#34;: 1655370516.9886107, &#34;EndTime&#34;: 1655370516.9886198, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 15.006472718481923, &#34;count&#34;: 1, &#34;min&#34;: 15.006472718481923, &#34;max&#34;: 15.006472718481923}}} #metrics {&#34;StartTime&#34;: 1655370516.9886525, &#34;EndTime&#34;: 1655370516.9886618, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.139413871017156, &#34;count&#34;: 1, &#34;min&#34;: 14.139413871017156, &#34;max&#34;: 14.139413871017156}}} #metrics {&#34;StartTime&#34;: 1655370516.988696, &#34;EndTime&#34;: 1655370516.9887064, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 16.40328919653799, &#34;count&#34;: 1, &#34;min&#34;: 16.40328919653799, &#34;max&#34;: 16.40328919653799}}} #metrics {&#34;StartTime&#34;: 1655370516.9887393, &#34;EndTime&#34;: 1655370516.9887486, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 16.932415532130822, &#34;count&#34;: 1, &#34;min&#34;: 16.932415532130822, &#34;max&#34;: 16.932415532130822}}} #metrics {&#34;StartTime&#34;: 1655370516.9887826, &#34;EndTime&#34;: 1655370516.9887927, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.679675532322303, &#34;count&#34;: 1, &#34;min&#34;: 14.679675532322303, &#34;max&#34;: 14.679675532322303}}} #metrics {&#34;StartTime&#34;: 1655370516.9888282, &#34;EndTime&#34;: 1655370516.9888382, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 15.153298172296262, &#34;count&#34;: 1, &#34;min&#34;: 15.153298172296262, &#34;max&#34;: 15.153298172296262}}} #metrics {&#34;StartTime&#34;: 1655370516.988875, &#34;EndTime&#34;: 1655370516.988885, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.397280225566789, &#34;count&#34;: 1, &#34;min&#34;: 14.397280225566789, &#34;max&#34;: 14.397280225566789}}} #metrics {&#34;StartTime&#34;: 1655370516.9901557, &#34;EndTime&#34;: 1655370516.990182, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.570991665709252, &#34;count&#34;: 1, &#34;min&#34;: 14.570991665709252, &#34;max&#34;: 14.570991665709252}}} #metrics {&#34;StartTime&#34;: 1655370516.9902332, &#34;EndTime&#34;: 1655370516.9902456, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 18.080331241383274, &#34;count&#34;: 1, &#34;min&#34;: 18.080331241383274, &#34;max&#34;: 18.080331241383274}}} #metrics {&#34;StartTime&#34;: 1655370516.990286, &#34;EndTime&#34;: 1655370516.9902983, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.017707375919116, &#34;count&#34;: 1, &#34;min&#34;: 19.017707375919116, &#34;max&#34;: 19.017707375919116}}} #metrics {&#34;StartTime&#34;: 1655370516.990343, &#34;EndTime&#34;: 1655370516.9903536, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.86836990655637, &#34;count&#34;: 1, &#34;min&#34;: 17.86836990655637, &#34;max&#34;: 17.86836990655637}}} #metrics {&#34;StartTime&#34;: 1655370516.9903905, &#34;EndTime&#34;: 1655370516.9904017, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 18.174327177159928, &#34;count&#34;: 1, &#34;min&#34;: 18.174327177159928, &#34;max&#34;: 18.174327177159928}}} #metrics {&#34;StartTime&#34;: 1655370516.9904926, &#34;EndTime&#34;: 1655370516.9905066, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.918998568665746, &#34;count&#34;: 1, &#34;min&#34;: 19.918998568665746, &#34;max&#34;: 19.918998568665746}}} #metrics {&#34;StartTime&#34;: 1655370516.990547, &#34;EndTime&#34;: 1655370516.9905572, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.432778152765014, &#34;count&#34;: 1, &#34;min&#34;: 19.432778152765014, &#34;max&#34;: 19.432778152765014}}} #metrics {&#34;StartTime&#34;: 1655370516.9905963, &#34;EndTime&#34;: 1655370516.990606, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.30176618987439, &#34;count&#34;: 1, &#34;min&#34;: 19.30176618987439, &#34;max&#34;: 19.30176618987439}}} #metrics {&#34;StartTime&#34;: 1655370516.990642, &#34;EndTime&#34;: 1655370516.9906526, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.07663681927849, &#34;count&#34;: 1, &#34;min&#34;: 19.07663681927849, &#34;max&#34;: 19.07663681927849}}} #metrics {&#34;StartTime&#34;: 1655370516.9906898, &#34;EndTime&#34;: 1655370516.9907, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.44997750076593, &#34;count&#34;: 1, &#34;min&#34;: 61.44997750076593, &#34;max&#34;: 61.44997750076593}}} #metrics {&#34;StartTime&#34;: 1655370516.990737, &#34;EndTime&#34;: 1655370516.9907477, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.01623295802696, &#34;count&#34;: 1, &#34;min&#34;: 61.01623295802696, &#34;max&#34;: 61.01623295802696}}} #metrics {&#34;StartTime&#34;: 1655370516.9907827, &#34;EndTime&#34;: 1655370516.990793, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.438835592830884, &#34;count&#34;: 1, &#34;min&#34;: 61.438835592830884, &#34;max&#34;: 61.438835592830884}}} #metrics {&#34;StartTime&#34;: 1655370516.9908295, &#34;EndTime&#34;: 1655370516.9908397, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.31155934053309, &#34;count&#34;: 1, &#34;min&#34;: 61.31155934053309, &#34;max&#34;: 61.31155934053309}}} #metrics {&#34;StartTime&#34;: 1655370516.9908764, &#34;EndTime&#34;: 1655370516.9908867, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 60.904258578431374, &#34;count&#34;: 1, &#34;min&#34;: 60.904258578431374, &#34;max&#34;: 60.904258578431374}}} #metrics {&#34;StartTime&#34;: 1655370516.9909248, &#34;EndTime&#34;: 1655370516.9909344, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.389016544117645, &#34;count&#34;: 1, &#34;min&#34;: 62.389016544117645, &#34;max&#34;: 62.389016544117645}}} #metrics {&#34;StartTime&#34;: 1655370516.9909713, &#34;EndTime&#34;: 1655370516.9909818, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.984700520833336, &#34;count&#34;: 1, &#34;min&#34;: 61.984700520833336, &#34;max&#34;: 61.984700520833336}}} #metrics {&#34;StartTime&#34;: 1655370516.991019, &#34;EndTime&#34;: 1655370516.9910293, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.03021599264706, &#34;count&#34;: 1, &#34;min&#34;: 62.03021599264706, &#34;max&#34;: 62.03021599264706}}} [06/16/2022 09:08:36 INFO 139637139089216] #quality_metric: host=algo-1, epoch=4, validation mse_objective &lt;loss&gt;=15.574556238511029 [06/16/2022 09:08:36 INFO 139637139089216] #early_stopping_criteria_metric: host=algo-1, epoch=4, criteria=mse_objective, value=14.139413871017156 [06/16/2022 09:08:37 INFO 139637139089216] Saving model for epoch: 4 [06/16/2022 09:08:37 INFO 139637139089216] Saved checkpoint to &#34;/tmp/tmphcewkhmu/mx-mod-0000.params&#34; [06/16/2022 09:08:37 INFO 139637139089216] #progress_metric: host=algo-1, completed 33.333333333333336 % of epochs #metrics {&#34;StartTime&#34;: 1655370516.186767, &#34;EndTime&#34;: 1655370517.0047808, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 4, &#34;Meta&#34;: &#34;training_data_iter&#34;}, &#34;Metrics&#34;: {&#34;Total Records Seen&#34;: {&#34;sum&#34;: 2760.0, &#34;count&#34;: 1, &#34;min&#34;: 2760, &#34;max&#34;: 2760}, &#34;Total Batches Seen&#34;: {&#34;sum&#34;: 97.0, &#34;count&#34;: 1, &#34;min&#34;: 97, &#34;max&#34;: 97}, &#34;Max Records Seen Between Resets&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Max Batches Seen Between Resets&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}, &#34;Reset Count&#34;: {&#34;sum&#34;: 7.0, &#34;count&#34;: 1, &#34;min&#34;: 7, &#34;max&#34;: 7}, &#34;Number of Records Since Last Reset&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Number of Batches Since Last Reset&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}}} [06/16/2022 09:08:37 INFO 139637139089216] #throughput_metric: host=algo-1, train throughput=556.1361122921456 records/second #metrics {&#34;StartTime&#34;: 1655370517.7257173, &#34;EndTime&#34;: 1655370517.7257955, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.32255253050062394, &#34;count&#34;: 1, &#34;min&#34;: 0.32255253050062394, &#34;max&#34;: 0.32255253050062394}}} #metrics {&#34;StartTime&#34;: 1655370517.7258809, &#34;EndTime&#34;: 1655370517.7258937, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3298908233642578, &#34;count&#34;: 1, &#34;min&#34;: 0.3298908233642578, &#34;max&#34;: 0.3298908233642578}}} #metrics {&#34;StartTime&#34;: 1655370517.725934, &#34;EndTime&#34;: 1655370517.7259436, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3429497644636366, &#34;count&#34;: 1, &#34;min&#34;: 0.3429497644636366, &#34;max&#34;: 0.3429497644636366}}} #metrics {&#34;StartTime&#34;: 1655370517.7259765, &#34;EndTime&#34;: 1655370517.7259853, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3313139481014676, &#34;count&#34;: 1, &#34;min&#34;: 0.3313139481014676, &#34;max&#34;: 0.3313139481014676}}} #metrics {&#34;StartTime&#34;: 1655370517.7260177, &#34;EndTime&#34;: 1655370517.7260263, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.29397834565904407, &#34;count&#34;: 1, &#34;min&#34;: 0.29397834565904407, &#34;max&#34;: 0.29397834565904407}}} #metrics {&#34;StartTime&#34;: 1655370517.7260585, &#34;EndTime&#34;: 1655370517.7260673, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30436518563164605, &#34;count&#34;: 1, &#34;min&#34;: 0.30436518563164605, &#34;max&#34;: 0.30436518563164605}}} #metrics {&#34;StartTime&#34;: 1655370517.726102, &#34;EndTime&#34;: 1655370517.7261112, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.293362209531996, &#34;count&#34;: 1, &#34;min&#34;: 0.293362209531996, &#34;max&#34;: 0.293362209531996}}} #metrics {&#34;StartTime&#34;: 1655370517.7261436, &#34;EndTime&#34;: 1655370517.726152, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30295398394266765, &#34;count&#34;: 1, &#34;min&#34;: 0.30295398394266765, &#34;max&#34;: 0.30295398394266765}}} #metrics {&#34;StartTime&#34;: 1655370517.7261856, &#34;EndTime&#34;: 1655370517.7261937, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.32513953526814776, &#34;count&#34;: 1, &#34;min&#34;: 0.32513953526814776, &#34;max&#34;: 0.32513953526814776}}} #metrics {&#34;StartTime&#34;: 1655370517.7262278, &#34;EndTime&#34;: 1655370517.726238, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30606098228030737, &#34;count&#34;: 1, &#34;min&#34;: 0.30606098228030737, &#34;max&#34;: 0.30606098228030737}}} #metrics {&#34;StartTime&#34;: 1655370517.7262726, &#34;EndTime&#34;: 1655370517.7262812, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3439723650614421, &#34;count&#34;: 1, &#34;min&#34;: 0.3439723650614421, &#34;max&#34;: 0.3439723650614421}}} #metrics {&#34;StartTime&#34;: 1655370517.726329, &#34;EndTime&#34;: 1655370517.72634, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3409457641177707, &#34;count&#34;: 1, &#34;min&#34;: 0.3409457641177707, &#34;max&#34;: 0.3409457641177707}}} #metrics {&#34;StartTime&#34;: 1655370517.726377, &#34;EndTime&#34;: 1655370517.7263873, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30998548136817083, &#34;count&#34;: 1, &#34;min&#34;: 0.30998548136817083, &#34;max&#34;: 0.30998548136817083}}} #metrics {&#34;StartTime&#34;: 1655370517.7264242, &#34;EndTime&#34;: 1655370517.7264345, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.29684680302937827, &#34;count&#34;: 1, &#34;min&#34;: 0.29684680302937827, &#34;max&#34;: 0.29684680302937827}}} #metrics {&#34;StartTime&#34;: 1655370517.7264724, &#34;EndTime&#34;: 1655370517.726484, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30128766112857397, &#34;count&#34;: 1, &#34;min&#34;: 0.30128766112857397, &#34;max&#34;: 0.30128766112857397}}} #metrics {&#34;StartTime&#34;: 1655370517.7265213, &#34;EndTime&#34;: 1655370517.726531, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3036911408106486, &#34;count&#34;: 1, &#34;min&#34;: 0.3036911408106486, &#34;max&#34;: 0.3036911408106486}}} #metrics {&#34;StartTime&#34;: 1655370517.7265656, &#34;EndTime&#34;: 1655370517.726575, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.39042553689744736, &#34;count&#34;: 1, &#34;min&#34;: 0.39042553689744736, &#34;max&#34;: 0.39042553689744736}}} #metrics {&#34;StartTime&#34;: 1655370517.726609, &#34;EndTime&#34;: 1655370517.7266188, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3969521771536933, &#34;count&#34;: 1, &#34;min&#34;: 0.3969521771536933, &#34;max&#34;: 0.3969521771536933}}} #metrics {&#34;StartTime&#34;: 1655370517.726655, &#34;EndTime&#34;: 1655370517.7266638, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.386775123808119, &#34;count&#34;: 1, &#34;min&#34;: 0.386775123808119, &#34;max&#34;: 0.386775123808119}}} #metrics {&#34;StartTime&#34;: 1655370517.7267008, &#34;EndTime&#34;: 1655370517.7267108, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.38949665440453424, &#34;count&#34;: 1, &#34;min&#34;: 0.38949665440453424, &#34;max&#34;: 0.38949665440453424}}} #metrics {&#34;StartTime&#34;: 1655370517.7267456, &#34;EndTime&#34;: 1655370517.7267559, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.401328739590115, &#34;count&#34;: 1, &#34;min&#34;: 0.401328739590115, &#34;max&#34;: 0.401328739590115}}} #metrics {&#34;StartTime&#34;: 1655370517.7267923, &#34;EndTime&#34;: 1655370517.7268028, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4058595053354899, &#34;count&#34;: 1, &#34;min&#34;: 0.4058595053354899, &#34;max&#34;: 0.4058595053354899}}} #metrics {&#34;StartTime&#34;: 1655370517.7268393, &#34;EndTime&#34;: 1655370517.7268496, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4034149922264947, &#34;count&#34;: 1, &#34;min&#34;: 0.4034149922264947, &#34;max&#34;: 0.4034149922264947}}} #metrics {&#34;StartTime&#34;: 1655370517.7268867, &#34;EndTime&#34;: 1655370517.7268965, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4018051126268175, &#34;count&#34;: 1, &#34;min&#34;: 0.4018051126268175, &#34;max&#34;: 0.4018051126268175}}} #metrics {&#34;StartTime&#34;: 1655370517.7269359, &#34;EndTime&#34;: 1655370517.7269452, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9557787068684895, &#34;count&#34;: 1, &#34;min&#34;: 0.9557787068684895, &#34;max&#34;: 0.9557787068684895}}} #metrics {&#34;StartTime&#34;: 1655370517.7269807, &#34;EndTime&#34;: 1655370517.7269895, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9561210653516982, &#34;count&#34;: 1, &#34;min&#34;: 0.9561210653516982, &#34;max&#34;: 0.9561210653516982}}} #metrics {&#34;StartTime&#34;: 1655370517.727027, &#34;EndTime&#34;: 1655370517.7270365, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9552011320326064, &#34;count&#34;: 1, &#34;min&#34;: 0.9552011320326064, &#34;max&#34;: 0.9552011320326064}}} #metrics {&#34;StartTime&#34;: 1655370517.727072, &#34;EndTime&#34;: 1655370517.7270818, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.956407413482666, &#34;count&#34;: 1, &#34;min&#34;: 0.956407413482666, &#34;max&#34;: 0.956407413482666}}} #metrics {&#34;StartTime&#34;: 1655370517.727117, &#34;EndTime&#34;: 1655370517.7271266, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9677726003858779, &#34;count&#34;: 1, &#34;min&#34;: 0.9677726003858779, &#34;max&#34;: 0.9677726003858779}}} #metrics {&#34;StartTime&#34;: 1655370517.7271628, &#34;EndTime&#34;: 1655370517.7271729, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9717923842536078, &#34;count&#34;: 1, &#34;min&#34;: 0.9717923842536078, &#34;max&#34;: 0.9717923842536078}}} #metrics {&#34;StartTime&#34;: 1655370517.7272089, &#34;EndTime&#34;: 1655370517.7272189, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.970803885989719, &#34;count&#34;: 1, &#34;min&#34;: 0.970803885989719, &#34;max&#34;: 0.970803885989719}}} #metrics {&#34;StartTime&#34;: 1655370517.7272556, &#34;EndTime&#34;: 1655370517.7272658, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9719388135274252, &#34;count&#34;: 1, &#34;min&#34;: 0.9719388135274252, &#34;max&#34;: 0.9719388135274252}}} [06/16/2022 09:08:37 INFO 139637139089216] #quality_metric: host=algo-1, epoch=5, train mse_objective &lt;loss&gt;=0.32255253050062394 #metrics {&#34;StartTime&#34;: 1655370517.8094492, &#34;EndTime&#34;: 1655370517.809523, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.737356148514094, &#34;count&#34;: 1, &#34;min&#34;: 14.737356148514094, &#34;max&#34;: 14.737356148514094}}} #metrics {&#34;StartTime&#34;: 1655370517.809606, &#34;EndTime&#34;: 1655370517.809619, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 15.317010019339767, &#34;count&#34;: 1, &#34;min&#34;: 15.317010019339767, &#34;max&#34;: 15.317010019339767}}} #metrics {&#34;StartTime&#34;: 1655370517.8096802, &#34;EndTime&#34;: 1655370517.8096912, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.96641690123315, &#34;count&#34;: 1, &#34;min&#34;: 14.96641690123315, &#34;max&#34;: 14.96641690123315}}} #metrics {&#34;StartTime&#34;: 1655370517.8097253, &#34;EndTime&#34;: 1655370517.809734, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.3065933526731, &#34;count&#34;: 1, &#34;min&#34;: 14.3065933526731, &#34;max&#34;: 14.3065933526731}}} #metrics {&#34;StartTime&#34;: 1655370517.8097677, &#34;EndTime&#34;: 1655370517.8097758, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.402994791666666, &#34;count&#34;: 1, &#34;min&#34;: 14.402994791666666, &#34;max&#34;: 14.402994791666666}}} #metrics {&#34;StartTime&#34;: 1655370517.8098073, &#34;EndTime&#34;: 1655370517.8098173, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.821093390969668, &#34;count&#34;: 1, &#34;min&#34;: 14.821093390969668, &#34;max&#34;: 14.821093390969668}}} #metrics {&#34;StartTime&#34;: 1655370517.809851, &#34;EndTime&#34;: 1655370517.809861, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.509930778952207, &#34;count&#34;: 1, &#34;min&#34;: 14.509930778952207, &#34;max&#34;: 14.509930778952207}}} #metrics {&#34;StartTime&#34;: 1655370517.8098931, &#34;EndTime&#34;: 1655370517.8099024, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.395231359145221, &#34;count&#34;: 1, &#34;min&#34;: 14.395231359145221, &#34;max&#34;: 14.395231359145221}}} #metrics {&#34;StartTime&#34;: 1655370517.8099346, &#34;EndTime&#34;: 1655370517.8099427, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.16729856004902, &#34;count&#34;: 1, &#34;min&#34;: 14.16729856004902, &#34;max&#34;: 14.16729856004902}}} #metrics {&#34;StartTime&#34;: 1655370517.8099773, &#34;EndTime&#34;: 1655370517.8099875, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.732995725145527, &#34;count&#34;: 1, &#34;min&#34;: 13.732995725145527, &#34;max&#34;: 13.732995725145527}}} #metrics {&#34;StartTime&#34;: 1655370517.8100216, &#34;EndTime&#34;: 1655370517.8100312, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 15.281980028339461, &#34;count&#34;: 1, &#34;min&#34;: 15.281980028339461, &#34;max&#34;: 15.281980028339461}}} #metrics {&#34;StartTime&#34;: 1655370517.8100672, &#34;EndTime&#34;: 1655370517.8100772, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 15.284403483072916, &#34;count&#34;: 1, &#34;min&#34;: 15.284403483072916, &#34;max&#34;: 15.284403483072916}}} #metrics {&#34;StartTime&#34;: 1655370517.810113, &#34;EndTime&#34;: 1655370517.8101227, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.81549072265625, &#34;count&#34;: 1, &#34;min&#34;: 14.81549072265625, &#34;max&#34;: 14.81549072265625}}} #metrics {&#34;StartTime&#34;: 1655370517.81016, &#34;EndTime&#34;: 1655370517.8101711, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.255533854166666, &#34;count&#34;: 1, &#34;min&#34;: 14.255533854166666, &#34;max&#34;: 14.255533854166666}}} #metrics {&#34;StartTime&#34;: 1655370517.810209, &#34;EndTime&#34;: 1655370517.8102183, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.631469128178615, &#34;count&#34;: 1, &#34;min&#34;: 14.631469128178615, &#34;max&#34;: 14.631469128178615}}} #metrics {&#34;StartTime&#34;: 1655370517.810253, &#34;EndTime&#34;: 1655370517.8102622, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.486032523360906, &#34;count&#34;: 1, &#34;min&#34;: 14.486032523360906, &#34;max&#34;: 14.486032523360906}}} #metrics {&#34;StartTime&#34;: 1655370517.8102968, &#34;EndTime&#34;: 1655370517.8103056, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.717605890012255, &#34;count&#34;: 1, &#34;min&#34;: 17.717605890012255, &#34;max&#34;: 17.717605890012255}}} #metrics {&#34;StartTime&#34;: 1655370517.8103495, &#34;EndTime&#34;: 1655370517.8103595, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 18.296094707414216, &#34;count&#34;: 1, &#34;min&#34;: 18.296094707414216, &#34;max&#34;: 18.296094707414216}}} #metrics {&#34;StartTime&#34;: 1655370517.8103933, &#34;EndTime&#34;: 1655370517.8104043, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.732822193818933, &#34;count&#34;: 1, &#34;min&#34;: 17.732822193818933, &#34;max&#34;: 17.732822193818933}}} #metrics {&#34;StartTime&#34;: 1655370517.8104389, &#34;EndTime&#34;: 1655370517.810449, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.891299378638173, &#34;count&#34;: 1, &#34;min&#34;: 17.891299378638173, &#34;max&#34;: 17.891299378638173}}} #metrics {&#34;StartTime&#34;: 1655370517.8104844, &#34;EndTime&#34;: 1655370517.8104947, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.400882496553308, &#34;count&#34;: 1, &#34;min&#34;: 19.400882496553308, &#34;max&#34;: 19.400882496553308}}} #metrics {&#34;StartTime&#34;: 1655370517.8105319, &#34;EndTime&#34;: 1655370517.810542, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.12230607575061, &#34;count&#34;: 1, &#34;min&#34;: 19.12230607575061, &#34;max&#34;: 19.12230607575061}}} #metrics {&#34;StartTime&#34;: 1655370517.8105783, &#34;EndTime&#34;: 1655370517.8105884, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.357296214384192, &#34;count&#34;: 1, &#34;min&#34;: 19.357296214384192, &#34;max&#34;: 19.357296214384192}}} #metrics {&#34;StartTime&#34;: 1655370517.8106284, &#34;EndTime&#34;: 1655370517.8106377, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.472297818053004, &#34;count&#34;: 1, &#34;min&#34;: 19.472297818053004, &#34;max&#34;: 19.472297818053004}}} #metrics {&#34;StartTime&#34;: 1655370517.8106725, &#34;EndTime&#34;: 1655370517.8106823, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.13913143382353, &#34;count&#34;: 1, &#34;min&#34;: 61.13913143382353, &#34;max&#34;: 61.13913143382353}}} #metrics {&#34;StartTime&#34;: 1655370517.8107197, &#34;EndTime&#34;: 1655370517.8107295, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.21798167509191, &#34;count&#34;: 1, &#34;min&#34;: 61.21798167509191, &#34;max&#34;: 61.21798167509191}}} #metrics {&#34;StartTime&#34;: 1655370517.8107648, &#34;EndTime&#34;: 1655370517.810774, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.512032303155635, &#34;count&#34;: 1, &#34;min&#34;: 61.512032303155635, &#34;max&#34;: 61.512032303155635}}} #metrics {&#34;StartTime&#34;: 1655370517.8108106, &#34;EndTime&#34;: 1655370517.8108199, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.2959534888174, &#34;count&#34;: 1, &#34;min&#34;: 61.2959534888174, &#34;max&#34;: 61.2959534888174}}} #metrics {&#34;StartTime&#34;: 1655370517.8108575, &#34;EndTime&#34;: 1655370517.8108675, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.87495212928921, &#34;count&#34;: 1, &#34;min&#34;: 61.87495212928921, &#34;max&#34;: 61.87495212928921}}} #metrics {&#34;StartTime&#34;: 1655370517.8109033, &#34;EndTime&#34;: 1655370517.8109133, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.793772977941174, &#34;count&#34;: 1, &#34;min&#34;: 62.793772977941174, &#34;max&#34;: 62.793772977941174}}} #metrics {&#34;StartTime&#34;: 1655370517.81095, &#34;EndTime&#34;: 1655370517.8109605, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.707282571231616, &#34;count&#34;: 1, &#34;min&#34;: 62.707282571231616, &#34;max&#34;: 62.707282571231616}}} #metrics {&#34;StartTime&#34;: 1655370517.8109972, &#34;EndTime&#34;: 1655370517.8110077, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.59827138863358, &#34;count&#34;: 1, &#34;min&#34;: 62.59827138863358, &#34;max&#34;: 62.59827138863358}}} [06/16/2022 09:08:37 INFO 139637139089216] #quality_metric: host=algo-1, epoch=5, validation mse_objective &lt;loss&gt;=14.737356148514094 [06/16/2022 09:08:37 INFO 139637139089216] #early_stopping_criteria_metric: host=algo-1, epoch=5, criteria=mse_objective, value=13.732995725145527 [06/16/2022 09:08:37 INFO 139637139089216] Epoch 5: Loss has not improved for 0 epochs. [06/16/2022 09:08:37 INFO 139637139089216] Saving model for epoch: 5 [06/16/2022 09:08:37 INFO 139637139089216] Saved checkpoint to &#34;/tmp/tmpw6ev9kkq/mx-mod-0000.params&#34; [06/16/2022 09:08:37 INFO 139637139089216] #progress_metric: host=algo-1, completed 40.0 % of epochs #metrics {&#34;StartTime&#34;: 1655370517.0050483, &#34;EndTime&#34;: 1655370517.819513, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 5, &#34;Meta&#34;: &#34;training_data_iter&#34;}, &#34;Metrics&#34;: {&#34;Total Records Seen&#34;: {&#34;sum&#34;: 3215.0, &#34;count&#34;: 1, &#34;min&#34;: 3215, &#34;max&#34;: 3215}, &#34;Total Batches Seen&#34;: {&#34;sum&#34;: 113.0, &#34;count&#34;: 1, &#34;min&#34;: 113, &#34;max&#34;: 113}, &#34;Max Records Seen Between Resets&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Max Batches Seen Between Resets&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}, &#34;Reset Count&#34;: {&#34;sum&#34;: 8.0, &#34;count&#34;: 1, &#34;min&#34;: 8, &#34;max&#34;: 8}, &#34;Number of Records Since Last Reset&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Number of Batches Since Last Reset&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}}} [06/16/2022 09:08:37 INFO 139637139089216] #throughput_metric: host=algo-1, train throughput=558.5615843364608 records/second #metrics {&#34;StartTime&#34;: 1655370518.4998982, &#34;EndTime&#34;: 1655370518.4999816, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30897735542721216, &#34;count&#34;: 1, &#34;min&#34;: 0.30897735542721216, &#34;max&#34;: 0.30897735542721216}}} #metrics {&#34;StartTime&#34;: 1655370518.5000718, &#34;EndTime&#34;: 1655370518.5000858, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3147106679280599, &#34;count&#34;: 1, &#34;min&#34;: 0.3147106679280599, &#34;max&#34;: 0.3147106679280599}}} #metrics {&#34;StartTime&#34;: 1655370518.5018978, &#34;EndTime&#34;: 1655370518.5019257, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.32191754553053115, &#34;count&#34;: 1, &#34;min&#34;: 0.32191754553053115, &#34;max&#34;: 0.32191754553053115}}} #metrics {&#34;StartTime&#34;: 1655370518.5021906, &#34;EndTime&#34;: 1655370518.5022106, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3126039653354221, &#34;count&#34;: 1, &#34;min&#34;: 0.3126039653354221, &#34;max&#34;: 0.3126039653354221}}} #metrics {&#34;StartTime&#34;: 1655370518.502449, &#34;EndTime&#34;: 1655370518.5024726, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.298829345703125, &#34;count&#34;: 1, &#34;min&#34;: 0.298829345703125, &#34;max&#34;: 0.298829345703125}}} #metrics {&#34;StartTime&#34;: 1655370518.502766, &#34;EndTime&#34;: 1655370518.502787, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2972593699561225, &#34;count&#34;: 1, &#34;min&#34;: 0.2972593699561225, &#34;max&#34;: 0.2972593699561225}}} #metrics {&#34;StartTime&#34;: 1655370518.5042772, &#34;EndTime&#34;: 1655370518.5043008, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.29779706319173177, &#34;count&#34;: 1, &#34;min&#34;: 0.29779706319173177, &#34;max&#34;: 0.29779706319173177}}} #metrics {&#34;StartTime&#34;: 1655370518.5045617, &#34;EndTime&#34;: 1655370518.5045803, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2949141089121501, &#34;count&#34;: 1, &#34;min&#34;: 0.2949141089121501, &#34;max&#34;: 0.2949141089121501}}} #metrics {&#34;StartTime&#34;: 1655370518.504806, &#34;EndTime&#34;: 1655370518.5048285, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3098793829811944, &#34;count&#34;: 1, &#34;min&#34;: 0.3098793829811944, &#34;max&#34;: 0.3098793829811944}}} #metrics {&#34;StartTime&#34;: 1655370518.5051146, &#34;EndTime&#34;: 1655370518.505134, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2965576050016615, &#34;count&#34;: 1, &#34;min&#34;: 0.2965576050016615, &#34;max&#34;: 0.2965576050016615}}} #metrics {&#34;StartTime&#34;: 1655370518.5058932, &#34;EndTime&#34;: 1655370518.5059142, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3242960876888699, &#34;count&#34;: 1, &#34;min&#34;: 0.3242960876888699, &#34;max&#34;: 0.3242960876888699}}} #metrics {&#34;StartTime&#34;: 1655370518.5061612, &#34;EndTime&#34;: 1655370518.5061798, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.31914955139160156, &#34;count&#34;: 1, &#34;min&#34;: 0.31914955139160156, &#34;max&#34;: 0.31914955139160156}}} #metrics {&#34;StartTime&#34;: 1655370518.5064259, &#34;EndTime&#34;: 1655370518.506444, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3023216772079468, &#34;count&#34;: 1, &#34;min&#34;: 0.3023216772079468, &#34;max&#34;: 0.3023216772079468}}} #metrics {&#34;StartTime&#34;: 1655370518.5066624, &#34;EndTime&#34;: 1655370518.5066795, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30320469750298396, &#34;count&#34;: 1, &#34;min&#34;: 0.30320469750298396, &#34;max&#34;: 0.30320469750298396}}} #metrics {&#34;StartTime&#34;: 1655370518.507678, &#34;EndTime&#34;: 1655370518.5077007, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2953969711727566, &#34;count&#34;: 1, &#34;min&#34;: 0.2953969711727566, &#34;max&#34;: 0.2953969711727566}}} #metrics {&#34;StartTime&#34;: 1655370518.5077674, &#34;EndTime&#34;: 1655370518.507781, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2959550454881456, &#34;count&#34;: 1, &#34;min&#34;: 0.2959550454881456, &#34;max&#34;: 0.2959550454881456}}} #metrics {&#34;StartTime&#34;: 1655370518.507822, &#34;EndTime&#34;: 1655370518.5078332, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3850594668918186, &#34;count&#34;: 1, &#34;min&#34;: 0.3850594668918186, &#34;max&#34;: 0.3850594668918186}}} #metrics {&#34;StartTime&#34;: 1655370518.5079374, &#34;EndTime&#34;: 1655370518.5079515, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3889405075709025, &#34;count&#34;: 1, &#34;min&#34;: 0.3889405075709025, &#34;max&#34;: 0.3889405075709025}}} #metrics {&#34;StartTime&#34;: 1655370518.5079908, &#34;EndTime&#34;: 1655370518.508001, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3830383904774984, &#34;count&#34;: 1, &#34;min&#34;: 0.3830383904774984, &#34;max&#34;: 0.3830383904774984}}} #metrics {&#34;StartTime&#34;: 1655370518.5082567, &#34;EndTime&#34;: 1655370518.508276, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.38429128328959145, &#34;count&#34;: 1, &#34;min&#34;: 0.38429128328959145, &#34;max&#34;: 0.38429128328959145}}} #metrics {&#34;StartTime&#34;: 1655370518.508372, &#34;EndTime&#34;: 1655370518.5083902, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4012879572974311, &#34;count&#34;: 1, &#34;min&#34;: 0.4012879572974311, &#34;max&#34;: 0.4012879572974311}}} #metrics {&#34;StartTime&#34;: 1655370518.5084398, &#34;EndTime&#34;: 1655370518.5084553, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.40971195538838706, &#34;count&#34;: 1, &#34;min&#34;: 0.40971195538838706, &#34;max&#34;: 0.40971195538838706}}} #metrics {&#34;StartTime&#34;: 1655370518.5085006, &#34;EndTime&#34;: 1655370518.508511, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4050375186072456, &#34;count&#34;: 1, &#34;min&#34;: 0.4050375186072456, &#34;max&#34;: 0.4050375186072456}}} #metrics {&#34;StartTime&#34;: 1655370518.5085492, &#34;EndTime&#34;: 1655370518.5085595, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4043354680803087, &#34;count&#34;: 1, &#34;min&#34;: 0.4043354680803087, &#34;max&#34;: 0.4043354680803087}}} #metrics {&#34;StartTime&#34;: 1655370518.508597, &#34;EndTime&#34;: 1655370518.5086062, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9556216049194336, &#34;count&#34;: 1, &#34;min&#34;: 0.9556216049194336, &#34;max&#34;: 0.9556216049194336}}} #metrics {&#34;StartTime&#34;: 1655370518.508642, &#34;EndTime&#34;: 1655370518.508652, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9563503032260471, &#34;count&#34;: 1, &#34;min&#34;: 0.9563503032260471, &#34;max&#34;: 0.9563503032260471}}} #metrics {&#34;StartTime&#34;: 1655370518.508688, &#34;EndTime&#34;: 1655370518.5086977, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9559640820821126, &#34;count&#34;: 1, &#34;min&#34;: 0.9559640820821126, &#34;max&#34;: 0.9559640820821126}}} #metrics {&#34;StartTime&#34;: 1655370518.5087314, &#34;EndTime&#34;: 1655370518.508741, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9559990077548557, &#34;count&#34;: 1, &#34;min&#34;: 0.9559990077548557, &#34;max&#34;: 0.9559990077548557}}} #metrics {&#34;StartTime&#34;: 1655370518.5091834, &#34;EndTime&#34;: 1655370518.509203, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9691147549947103, &#34;count&#34;: 1, &#34;min&#34;: 0.9691147549947103, &#34;max&#34;: 0.9691147549947103}}} #metrics {&#34;StartTime&#34;: 1655370518.5092757, &#34;EndTime&#34;: 1655370518.509291, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9743991894192165, &#34;count&#34;: 1, &#34;min&#34;: 0.9743991894192165, &#34;max&#34;: 0.9743991894192165}}} #metrics {&#34;StartTime&#34;: 1655370518.5093346, &#34;EndTime&#34;: 1655370518.5093465, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9726091236538357, &#34;count&#34;: 1, &#34;min&#34;: 0.9726091236538357, &#34;max&#34;: 0.9726091236538357}}} #metrics {&#34;StartTime&#34;: 1655370518.509553, &#34;EndTime&#34;: 1655370518.5095723, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9732530445522732, &#34;count&#34;: 1, &#34;min&#34;: 0.9732530445522732, &#34;max&#34;: 0.9732530445522732}}} [06/16/2022 09:08:38 INFO 139637139089216] #quality_metric: host=algo-1, epoch=6, train mse_objective &lt;loss&gt;=0.30897735542721216 #metrics {&#34;StartTime&#34;: 1655370518.6367745, &#34;EndTime&#34;: 1655370518.6368623, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.250956217447916, &#34;count&#34;: 1, &#34;min&#34;: 14.250956217447916, &#34;max&#34;: 14.250956217447916}}} #metrics {&#34;StartTime&#34;: 1655370518.6369643, &#34;EndTime&#34;: 1655370518.6369798, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.689021690219056, &#34;count&#34;: 1, &#34;min&#34;: 14.689021690219056, &#34;max&#34;: 14.689021690219056}}} #metrics {&#34;StartTime&#34;: 1655370518.6370223, &#34;EndTime&#34;: 1655370518.6370323, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.083184335746017, &#34;count&#34;: 1, &#34;min&#34;: 14.083184335746017, &#34;max&#34;: 14.083184335746017}}} #metrics {&#34;StartTime&#34;: 1655370518.6370666, &#34;EndTime&#34;: 1655370518.6370754, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.72221284754136, &#34;count&#34;: 1, &#34;min&#34;: 13.72221284754136, &#34;max&#34;: 13.72221284754136}}} #metrics {&#34;StartTime&#34;: 1655370518.6371083, &#34;EndTime&#34;: 1655370518.6371167, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.689798392501531, &#34;count&#34;: 1, &#34;min&#34;: 14.689798392501531, &#34;max&#34;: 14.689798392501531}}} #metrics {&#34;StartTime&#34;: 1655370518.6386778, &#34;EndTime&#34;: 1655370518.6387045, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.515152276731005, &#34;count&#34;: 1, &#34;min&#34;: 14.515152276731005, &#34;max&#34;: 14.515152276731005}}} #metrics {&#34;StartTime&#34;: 1655370518.6387596, &#34;EndTime&#34;: 1655370518.638771, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.671085731655944, &#34;count&#34;: 1, &#34;min&#34;: 14.671085731655944, &#34;max&#34;: 14.671085731655944}}} #metrics {&#34;StartTime&#34;: 1655370518.6389523, &#34;EndTime&#34;: 1655370518.6389682, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.720938888250613, &#34;count&#34;: 1, &#34;min&#34;: 14.720938888250613, &#34;max&#34;: 14.720938888250613}}} #metrics {&#34;StartTime&#34;: 1655370518.639088, &#34;EndTime&#34;: 1655370518.6391027, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.692630543428308, &#34;count&#34;: 1, &#34;min&#34;: 13.692630543428308, &#34;max&#34;: 13.692630543428308}}} #metrics {&#34;StartTime&#34;: 1655370518.6392162, &#34;EndTime&#34;: 1655370518.639231, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.588758281632966, &#34;count&#34;: 1, &#34;min&#34;: 13.588758281632966, &#34;max&#34;: 13.588758281632966}}} #metrics {&#34;StartTime&#34;: 1655370518.639336, &#34;EndTime&#34;: 1655370518.6393514, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.579924340341606, &#34;count&#34;: 1, &#34;min&#34;: 14.579924340341606, &#34;max&#34;: 14.579924340341606}}} #metrics {&#34;StartTime&#34;: 1655370518.6393907, &#34;EndTime&#34;: 1655370518.6394925, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.275383444393382, &#34;count&#34;: 1, &#34;min&#34;: 14.275383444393382, &#34;max&#34;: 14.275383444393382}}} #metrics {&#34;StartTime&#34;: 1655370518.63954, &#34;EndTime&#34;: 1655370518.6395514, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.390708773743873, &#34;count&#34;: 1, &#34;min&#34;: 14.390708773743873, &#34;max&#34;: 14.390708773743873}}} #metrics {&#34;StartTime&#34;: 1655370518.6396685, &#34;EndTime&#34;: 1655370518.6396823, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.988318349800858, &#34;count&#34;: 1, &#34;min&#34;: 14.988318349800858, &#34;max&#34;: 14.988318349800858}}} #metrics {&#34;StartTime&#34;: 1655370518.6397872, &#34;EndTime&#34;: 1655370518.6398005, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.409825343711704, &#34;count&#34;: 1, &#34;min&#34;: 14.409825343711704, &#34;max&#34;: 14.409825343711704}}} #metrics {&#34;StartTime&#34;: 1655370518.6399052, &#34;EndTime&#34;: 1655370518.6399183, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.56963991651348, &#34;count&#34;: 1, &#34;min&#34;: 14.56963991651348, &#34;max&#34;: 14.56963991651348}}} #metrics {&#34;StartTime&#34;: 1655370518.640026, &#34;EndTime&#34;: 1655370518.6400397, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.6280044854856, &#34;count&#34;: 1, &#34;min&#34;: 17.6280044854856, &#34;max&#34;: 17.6280044854856}}} #metrics {&#34;StartTime&#34;: 1655370518.6401417, &#34;EndTime&#34;: 1655370518.6401556, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.974991143918505, &#34;count&#34;: 1, &#34;min&#34;: 17.974991143918505, &#34;max&#34;: 17.974991143918505}}} #metrics {&#34;StartTime&#34;: 1655370518.6402678, &#34;EndTime&#34;: 1655370518.6403284, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.605397542317707, &#34;count&#34;: 1, &#34;min&#34;: 17.605397542317707, &#34;max&#34;: 17.605397542317707}}} #metrics {&#34;StartTime&#34;: 1655370518.6403785, &#34;EndTime&#34;: 1655370518.6403913, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.694723091873467, &#34;count&#34;: 1, &#34;min&#34;: 17.694723091873467, &#34;max&#34;: 17.694723091873467}}} #metrics {&#34;StartTime&#34;: 1655370518.6404295, &#34;EndTime&#34;: 1655370518.6404405, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.302534514782476, &#34;count&#34;: 1, &#34;min&#34;: 19.302534514782476, &#34;max&#34;: 19.302534514782476}}} #metrics {&#34;StartTime&#34;: 1655370518.6405933, &#34;EndTime&#34;: 1655370518.640609, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.0399564855239, &#34;count&#34;: 1, &#34;min&#34;: 19.0399564855239, &#34;max&#34;: 19.0399564855239}}} #metrics {&#34;StartTime&#34;: 1655370518.6407275, &#34;EndTime&#34;: 1655370518.6407945, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.32546398686428, &#34;count&#34;: 1, &#34;min&#34;: 19.32546398686428, &#34;max&#34;: 19.32546398686428}}} #metrics {&#34;StartTime&#34;: 1655370518.6408415, &#34;EndTime&#34;: 1655370518.6409123, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.384449079924938, &#34;count&#34;: 1, &#34;min&#34;: 19.384449079924938, &#34;max&#34;: 19.384449079924938}}} #metrics {&#34;StartTime&#34;: 1655370518.6409583, &#34;EndTime&#34;: 1655370518.64097, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.26442105162378, &#34;count&#34;: 1, &#34;min&#34;: 61.26442105162378, &#34;max&#34;: 61.26442105162378}}} #metrics {&#34;StartTime&#34;: 1655370518.641084, &#34;EndTime&#34;: 1655370518.6411507, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.05339020373774, &#34;count&#34;: 1, &#34;min&#34;: 61.05339020373774, &#34;max&#34;: 61.05339020373774}}} #metrics {&#34;StartTime&#34;: 1655370518.6411977, &#34;EndTime&#34;: 1655370518.64121, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.49389169730392, &#34;count&#34;: 1, &#34;min&#34;: 61.49389169730392, &#34;max&#34;: 61.49389169730392}}} #metrics {&#34;StartTime&#34;: 1655370518.6413193, &#34;EndTime&#34;: 1655370518.641358, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.271278530943626, &#34;count&#34;: 1, &#34;min&#34;: 61.271278530943626, &#34;max&#34;: 61.271278530943626}}} #metrics {&#34;StartTime&#34;: 1655370518.6414037, &#34;EndTime&#34;: 1655370518.6414168, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.872628006280635, &#34;count&#34;: 1, &#34;min&#34;: 61.872628006280635, &#34;max&#34;: 61.872628006280635}}} #metrics {&#34;StartTime&#34;: 1655370518.6414537, &#34;EndTime&#34;: 1655370518.6414974, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.867237764246326, &#34;count&#34;: 1, &#34;min&#34;: 61.867237764246326, &#34;max&#34;: 61.867237764246326}}} #metrics {&#34;StartTime&#34;: 1655370518.6415405, &#34;EndTime&#34;: 1655370518.6415534, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.593525007659316, &#34;count&#34;: 1, &#34;min&#34;: 62.593525007659316, &#34;max&#34;: 62.593525007659316}}} #metrics {&#34;StartTime&#34;: 1655370518.641591, &#34;EndTime&#34;: 1655370518.641602, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.01347321155024, &#34;count&#34;: 1, &#34;min&#34;: 62.01347321155024, &#34;max&#34;: 62.01347321155024}}} [06/16/2022 09:08:38 INFO 139637139089216] #quality_metric: host=algo-1, epoch=6, validation mse_objective &lt;loss&gt;=14.250956217447916 [06/16/2022 09:08:38 INFO 139637139089216] #early_stopping_criteria_metric: host=algo-1, epoch=6, criteria=mse_objective, value=13.588758281632966 [06/16/2022 09:08:38 INFO 139637139089216] Epoch 6: Loss has not improved for 0 epochs. [06/16/2022 09:08:38 INFO 139637139089216] Saving model for epoch: 6 [06/16/2022 09:08:38 INFO 139637139089216] Saved checkpoint to &#34;/tmp/tmp8mtyghhx/mx-mod-0000.params&#34; [06/16/2022 09:08:38 INFO 139637139089216] #progress_metric: host=algo-1, completed 46.666666666666664 % of epochs #metrics {&#34;StartTime&#34;: 1655370517.8198147, &#34;EndTime&#34;: 1655370518.654596, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 6, &#34;Meta&#34;: &#34;training_data_iter&#34;}, &#34;Metrics&#34;: {&#34;Total Records Seen&#34;: {&#34;sum&#34;: 3670.0, &#34;count&#34;: 1, &#34;min&#34;: 3670, &#34;max&#34;: 3670}, &#34;Total Batches Seen&#34;: {&#34;sum&#34;: 129.0, &#34;count&#34;: 1, &#34;min&#34;: 129, &#34;max&#34;: 129}, &#34;Max Records Seen Between Resets&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Max Batches Seen Between Resets&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}, &#34;Reset Count&#34;: {&#34;sum&#34;: 9.0, &#34;count&#34;: 1, &#34;min&#34;: 9, &#34;max&#34;: 9}, &#34;Number of Records Since Last Reset&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Number of Batches Since Last Reset&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}}} [06/16/2022 09:08:38 INFO 139637139089216] #throughput_metric: host=algo-1, train throughput=544.9471874120147 records/second #metrics {&#34;StartTime&#34;: 1655370519.4299972, &#34;EndTime&#34;: 1655370519.430078, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30013485908508303, &#34;count&#34;: 1, &#34;min&#34;: 0.30013485908508303, &#34;max&#34;: 0.30013485908508303}}} #metrics {&#34;StartTime&#34;: 1655370519.4301653, &#34;EndTime&#34;: 1655370519.4301777, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30465739727020263, &#34;count&#34;: 1, &#34;min&#34;: 0.30465739727020263, &#34;max&#34;: 0.30465739727020263}}} #metrics {&#34;StartTime&#34;: 1655370519.4302166, &#34;EndTime&#34;: 1655370519.4302263, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3082283205456204, &#34;count&#34;: 1, &#34;min&#34;: 0.3082283205456204, &#34;max&#34;: 0.3082283205456204}}} #metrics {&#34;StartTime&#34;: 1655370519.4302602, &#34;EndTime&#34;: 1655370519.4302688, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3010712273915609, &#34;count&#34;: 1, &#34;min&#34;: 0.3010712273915609, &#34;max&#34;: 0.3010712273915609}}} #metrics {&#34;StartTime&#34;: 1655370519.4303105, &#34;EndTime&#34;: 1655370519.4303195, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2989350361294217, &#34;count&#34;: 1, &#34;min&#34;: 0.2989350361294217, &#34;max&#34;: 0.2989350361294217}}} #metrics {&#34;StartTime&#34;: 1655370519.4303935, &#34;EndTime&#34;: 1655370519.4304044, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.29621562216016983, &#34;count&#34;: 1, &#34;min&#34;: 0.29621562216016983, &#34;max&#34;: 0.29621562216016983}}} #metrics {&#34;StartTime&#34;: 1655370519.4304388, &#34;EndTime&#34;: 1655370519.430448, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2971478091345893, &#34;count&#34;: 1, &#34;min&#34;: 0.2971478091345893, &#34;max&#34;: 0.2971478091345893}}} #metrics {&#34;StartTime&#34;: 1655370519.4304814, &#34;EndTime&#34;: 1655370519.4304914, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2961660967932807, &#34;count&#34;: 1, &#34;min&#34;: 0.2961660967932807, &#34;max&#34;: 0.2961660967932807}}} #metrics {&#34;StartTime&#34;: 1655370519.4305267, &#34;EndTime&#34;: 1655370519.4305365, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30016189840104845, &#34;count&#34;: 1, &#34;min&#34;: 0.30016189840104845, &#34;max&#34;: 0.30016189840104845}}} #metrics {&#34;StartTime&#34;: 1655370519.430663, &#34;EndTime&#34;: 1655370519.430692, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2906233353084988, &#34;count&#34;: 1, &#34;min&#34;: 0.2906233353084988, &#34;max&#34;: 0.2906233353084988}}} #metrics {&#34;StartTime&#34;: 1655370519.4307392, &#34;EndTime&#34;: 1655370519.4307497, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.31111229684617786, &#34;count&#34;: 1, &#34;min&#34;: 0.31111229684617786, &#34;max&#34;: 0.31111229684617786}}} #metrics {&#34;StartTime&#34;: 1655370519.4307883, &#34;EndTime&#34;: 1655370519.4307983, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3052790975570679, &#34;count&#34;: 1, &#34;min&#34;: 0.3052790975570679, &#34;max&#34;: 0.3052790975570679}}} #metrics {&#34;StartTime&#34;: 1655370519.4308348, &#34;EndTime&#34;: 1655370519.4308448, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.29876067373487686, &#34;count&#34;: 1, &#34;min&#34;: 0.29876067373487686, &#34;max&#34;: 0.29876067373487686}}} #metrics {&#34;StartTime&#34;: 1655370519.4308815, &#34;EndTime&#34;: 1655370519.4308918, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3051757060156928, &#34;count&#34;: 1, &#34;min&#34;: 0.3051757060156928, &#34;max&#34;: 0.3051757060156928}}} #metrics {&#34;StartTime&#34;: 1655370519.4309273, &#34;EndTime&#34;: 1655370519.430937, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.29522416432698567, &#34;count&#34;: 1, &#34;min&#34;: 0.29522416432698567, &#34;max&#34;: 0.29522416432698567}}} #metrics {&#34;StartTime&#34;: 1655370519.4310412, &#34;EndTime&#34;: 1655370519.431064, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2963858434889052, &#34;count&#34;: 1, &#34;min&#34;: 0.2963858434889052, &#34;max&#34;: 0.2963858434889052}}} #metrics {&#34;StartTime&#34;: 1655370519.431106, &#34;EndTime&#34;: 1655370519.431116, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3823889128367106, &#34;count&#34;: 1, &#34;min&#34;: 0.3823889128367106, &#34;max&#34;: 0.3823889128367106}}} #metrics {&#34;StartTime&#34;: 1655370519.4311528, &#34;EndTime&#34;: 1655370519.4311635, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.38452766100565594, &#34;count&#34;: 1, &#34;min&#34;: 0.38452766100565594, &#34;max&#34;: 0.38452766100565594}}} #metrics {&#34;StartTime&#34;: 1655370519.4312005, &#34;EndTime&#34;: 1655370519.4312105, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3809778838687473, &#34;count&#34;: 1, &#34;min&#34;: 0.3809778838687473, &#34;max&#34;: 0.3809778838687473}}} #metrics {&#34;StartTime&#34;: 1655370519.431247, &#34;EndTime&#34;: 1655370519.4312565, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3815756458706326, &#34;count&#34;: 1, &#34;min&#34;: 0.3815756458706326, &#34;max&#34;: 0.3815756458706326}}} #metrics {&#34;StartTime&#34;: 1655370519.4312935, &#34;EndTime&#34;: 1655370519.4313033, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.40013744566175674, &#34;count&#34;: 1, &#34;min&#34;: 0.40013744566175674, &#34;max&#34;: 0.40013744566175674}}} #metrics {&#34;StartTime&#34;: 1655370519.431381, &#34;EndTime&#34;: 1655370519.4314406, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.41109680705600316, &#34;count&#34;: 1, &#34;min&#34;: 0.41109680705600316, &#34;max&#34;: 0.41109680705600316}}} #metrics {&#34;StartTime&#34;: 1655370519.4314873, &#34;EndTime&#34;: 1655370519.4314997, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4048978000217014, &#34;count&#34;: 1, &#34;min&#34;: 0.4048978000217014, &#34;max&#34;: 0.4048978000217014}}} #metrics {&#34;StartTime&#34;: 1655370519.4315376, &#34;EndTime&#34;: 1655370519.4315474, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4058122613694933, &#34;count&#34;: 1, &#34;min&#34;: 0.4058122613694933, &#34;max&#34;: 0.4058122613694933}}} #metrics {&#34;StartTime&#34;: 1655370519.4315846, &#34;EndTime&#34;: 1655370519.4315948, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9561879285176595, &#34;count&#34;: 1, &#34;min&#34;: 0.9561879285176595, &#34;max&#34;: 0.9561879285176595}}} #metrics {&#34;StartTime&#34;: 1655370519.4316313, &#34;EndTime&#34;: 1655370519.431641, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9562840292188857, &#34;count&#34;: 1, &#34;min&#34;: 0.9562840292188857, &#34;max&#34;: 0.9562840292188857}}} #metrics {&#34;StartTime&#34;: 1655370519.4317672, &#34;EndTime&#34;: 1655370519.4317803, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9554361216227214, &#34;count&#34;: 1, &#34;min&#34;: 0.9554361216227214, &#34;max&#34;: 0.9554361216227214}}} #metrics {&#34;StartTime&#34;: 1655370519.431821, &#34;EndTime&#34;: 1655370519.431832, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9560291735331218, &#34;count&#34;: 1, &#34;min&#34;: 0.9560291735331218, &#34;max&#34;: 0.9560291735331218}}} #metrics {&#34;StartTime&#34;: 1655370519.431869, &#34;EndTime&#34;: 1655370519.4318786, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9693400192260743, &#34;count&#34;: 1, &#34;min&#34;: 0.9693400192260743, &#34;max&#34;: 0.9693400192260743}}} #metrics {&#34;StartTime&#34;: 1655370519.431916, &#34;EndTime&#34;: 1655370519.4319263, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.973510250515408, &#34;count&#34;: 1, &#34;min&#34;: 0.973510250515408, &#34;max&#34;: 0.973510250515408}}} #metrics {&#34;StartTime&#34;: 1655370519.4320345, &#34;EndTime&#34;: 1655370519.4320471, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9729773924085829, &#34;count&#34;: 1, &#34;min&#34;: 0.9729773924085829, &#34;max&#34;: 0.9729773924085829}}} #metrics {&#34;StartTime&#34;: 1655370519.4320884, &#34;EndTime&#34;: 1655370519.432099, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9738874668545193, &#34;count&#34;: 1, &#34;min&#34;: 0.9738874668545193, &#34;max&#34;: 0.9738874668545193}}} [06/16/2022 09:08:39 INFO 139637139089216] #quality_metric: host=algo-1, epoch=7, train mse_objective &lt;loss&gt;=0.30013485908508303 #metrics {&#34;StartTime&#34;: 1655370519.5213199, &#34;EndTime&#34;: 1655370519.5214026, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.000498453776041, &#34;count&#34;: 1, &#34;min&#34;: 14.000498453776041, &#34;max&#34;: 14.000498453776041}}} #metrics {&#34;StartTime&#34;: 1655370519.521495, &#34;EndTime&#34;: 1655370519.5215108, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.325359269684435, &#34;count&#34;: 1, &#34;min&#34;: 14.325359269684435, &#34;max&#34;: 14.325359269684435}}} #metrics {&#34;StartTime&#34;: 1655370519.521554, &#34;EndTime&#34;: 1655370519.521565, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.608120188993567, &#34;count&#34;: 1, &#34;min&#34;: 13.608120188993567, &#34;max&#34;: 13.608120188993567}}} #metrics {&#34;StartTime&#34;: 1655370519.5215993, &#34;EndTime&#34;: 1655370519.5216086, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.467373717064952, &#34;count&#34;: 1, &#34;min&#34;: 13.467373717064952, &#34;max&#34;: 13.467373717064952}}} #metrics {&#34;StartTime&#34;: 1655370519.523497, &#34;EndTime&#34;: 1655370519.5235257, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.512777889476103, &#34;count&#34;: 1, &#34;min&#34;: 14.512777889476103, &#34;max&#34;: 14.512777889476103}}} #metrics {&#34;StartTime&#34;: 1655370519.523622, &#34;EndTime&#34;: 1655370519.5236387, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.480385574640012, &#34;count&#34;: 1, &#34;min&#34;: 14.480385574640012, &#34;max&#34;: 14.480385574640012}}} #metrics {&#34;StartTime&#34;: 1655370519.5236857, &#34;EndTime&#34;: 1655370519.5236971, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.48170441272212, &#34;count&#34;: 1, &#34;min&#34;: 14.48170441272212, &#34;max&#34;: 14.48170441272212}}} #metrics {&#34;StartTime&#34;: 1655370519.5237367, &#34;EndTime&#34;: 1655370519.5237474, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.422599044500613, &#34;count&#34;: 1, &#34;min&#34;: 14.422599044500613, &#34;max&#34;: 14.422599044500613}}} #metrics {&#34;StartTime&#34;: 1655370519.5237856, &#34;EndTime&#34;: 1655370519.5237954, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.470084994446998, &#34;count&#34;: 1, &#34;min&#34;: 13.470084994446998, &#34;max&#34;: 13.470084994446998}}} #metrics {&#34;StartTime&#34;: 1655370519.5238717, &#34;EndTime&#34;: 1655370519.523903, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.578092687270221, &#34;count&#34;: 1, &#34;min&#34;: 13.578092687270221, &#34;max&#34;: 13.578092687270221}}} #metrics {&#34;StartTime&#34;: 1655370519.5239477, &#34;EndTime&#34;: 1655370519.5239587, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.167259066712623, &#34;count&#34;: 1, &#34;min&#34;: 14.167259066712623, &#34;max&#34;: 14.167259066712623}}} #metrics {&#34;StartTime&#34;: 1655370519.5239959, &#34;EndTime&#34;: 1655370519.5240064, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.755745083678002, &#34;count&#34;: 1, &#34;min&#34;: 13.755745083678002, &#34;max&#34;: 13.755745083678002}}} #metrics {&#34;StartTime&#34;: 1655370519.5240428, &#34;EndTime&#34;: 1655370519.524087, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.471944173177084, &#34;count&#34;: 1, &#34;min&#34;: 14.471944173177084, &#34;max&#34;: 14.471944173177084}}} #metrics {&#34;StartTime&#34;: 1655370519.5241642, &#34;EndTime&#34;: 1655370519.5241761, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.646896063112745, &#34;count&#34;: 1, &#34;min&#34;: 14.646896063112745, &#34;max&#34;: 14.646896063112745}}} #metrics {&#34;StartTime&#34;: 1655370519.5242143, &#34;EndTime&#34;: 1655370519.5242238, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.481415991689644, &#34;count&#34;: 1, &#34;min&#34;: 14.481415991689644, &#34;max&#34;: 14.481415991689644}}} #metrics {&#34;StartTime&#34;: 1655370519.5242627, &#34;EndTime&#34;: 1655370519.5242732, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.378122965494791, &#34;count&#34;: 1, &#34;min&#34;: 14.378122965494791, &#34;max&#34;: 14.378122965494791}}} #metrics {&#34;StartTime&#34;: 1655370519.5243645, &#34;EndTime&#34;: 1655370519.5243778, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.55825985179228, &#34;count&#34;: 1, &#34;min&#34;: 17.55825985179228, &#34;max&#34;: 17.55825985179228}}} #metrics {&#34;StartTime&#34;: 1655370519.5244193, &#34;EndTime&#34;: 1655370519.5244296, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.773896460439644, &#34;count&#34;: 1, &#34;min&#34;: 17.773896460439644, &#34;max&#34;: 17.773896460439644}}} #metrics {&#34;StartTime&#34;: 1655370519.5244668, &#34;EndTime&#34;: 1655370519.524477, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.5570894129136, &#34;count&#34;: 1, &#34;min&#34;: 17.5570894129136, &#34;max&#34;: 17.5570894129136}}} #metrics {&#34;StartTime&#34;: 1655370519.5245142, &#34;EndTime&#34;: 1655370519.524524, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.60993089862898, &#34;count&#34;: 1, &#34;min&#34;: 17.60993089862898, &#34;max&#34;: 17.60993089862898}}} #metrics {&#34;StartTime&#34;: 1655370519.5246346, &#34;EndTime&#34;: 1655370519.524648, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.35096471449908, &#34;count&#34;: 1, &#34;min&#34;: 19.35096471449908, &#34;max&#34;: 19.35096471449908}}} #metrics {&#34;StartTime&#34;: 1655370519.5246878, &#34;EndTime&#34;: 1655370519.5246992, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.111102534275428, &#34;count&#34;: 1, &#34;min&#34;: 19.111102534275428, &#34;max&#34;: 19.111102534275428}}} #metrics {&#34;StartTime&#34;: 1655370519.524736, &#34;EndTime&#34;: 1655370519.5247462, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.373234767539827, &#34;count&#34;: 1, &#34;min&#34;: 19.373234767539827, &#34;max&#34;: 19.373234767539827}}} #metrics {&#34;StartTime&#34;: 1655370519.5247831, &#34;EndTime&#34;: 1655370519.5247931, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.383114683861827, &#34;count&#34;: 1, &#34;min&#34;: 19.383114683861827, &#34;max&#34;: 19.383114683861827}}} #metrics {&#34;StartTime&#34;: 1655370519.5249014, &#34;EndTime&#34;: 1655370519.5249147, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.15340887331495, &#34;count&#34;: 1, &#34;min&#34;: 61.15340887331495, &#34;max&#34;: 61.15340887331495}}} #metrics {&#34;StartTime&#34;: 1655370519.5249536, &#34;EndTime&#34;: 1655370519.5249636, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.030311734068626, &#34;count&#34;: 1, &#34;min&#34;: 61.030311734068626, &#34;max&#34;: 61.030311734068626}}} #metrics {&#34;StartTime&#34;: 1655370519.5250013, &#34;EndTime&#34;: 1655370519.5250118, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.46714872472427, &#34;count&#34;: 1, &#34;min&#34;: 61.46714872472427, &#34;max&#34;: 61.46714872472427}}} #metrics {&#34;StartTime&#34;: 1655370519.5250814, &#34;EndTime&#34;: 1655370519.525126, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.247446097579655, &#34;count&#34;: 1, &#34;min&#34;: 61.247446097579655, &#34;max&#34;: 61.247446097579655}}} #metrics {&#34;StartTime&#34;: 1655370519.5251713, &#34;EndTime&#34;: 1655370519.525183, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.59733790977329, &#34;count&#34;: 1, &#34;min&#34;: 61.59733790977329, &#34;max&#34;: 61.59733790977329}}} #metrics {&#34;StartTime&#34;: 1655370519.5252213, &#34;EndTime&#34;: 1655370519.5252318, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.56002508425245, &#34;count&#34;: 1, &#34;min&#34;: 61.56002508425245, &#34;max&#34;: 61.56002508425245}}} #metrics {&#34;StartTime&#34;: 1655370519.5252697, &#34;EndTime&#34;: 1655370519.5252807, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.363518210018384, &#34;count&#34;: 1, &#34;min&#34;: 62.363518210018384, &#34;max&#34;: 62.363518210018384}}} #metrics {&#34;StartTime&#34;: 1655370519.5253673, &#34;EndTime&#34;: 1655370519.5253975, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.14849973192402, &#34;count&#34;: 1, &#34;min&#34;: 62.14849973192402, &#34;max&#34;: 62.14849973192402}}} [06/16/2022 09:08:39 INFO 139637139089216] #quality_metric: host=algo-1, epoch=7, validation mse_objective &lt;loss&gt;=14.000498453776041 [06/16/2022 09:08:39 INFO 139637139089216] #early_stopping_criteria_metric: host=algo-1, epoch=7, criteria=mse_objective, value=13.467373717064952 [06/16/2022 09:08:39 INFO 139637139089216] Epoch 7: Loss has not improved for 0 epochs. [06/16/2022 09:08:39 INFO 139637139089216] Saving model for epoch: 7 [06/16/2022 09:08:39 INFO 139637139089216] Saved checkpoint to &#34;/tmp/tmpn_4ygrzt/mx-mod-0000.params&#34; [06/16/2022 09:08:39 INFO 139637139089216] #progress_metric: host=algo-1, completed 53.333333333333336 % of epochs #metrics {&#34;StartTime&#34;: 1655370518.6549575, &#34;EndTime&#34;: 1655370519.5368323, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 7, &#34;Meta&#34;: &#34;training_data_iter&#34;}, &#34;Metrics&#34;: {&#34;Total Records Seen&#34;: {&#34;sum&#34;: 4125.0, &#34;count&#34;: 1, &#34;min&#34;: 4125, &#34;max&#34;: 4125}, &#34;Total Batches Seen&#34;: {&#34;sum&#34;: 145.0, &#34;count&#34;: 1, &#34;min&#34;: 145, &#34;max&#34;: 145}, &#34;Max Records Seen Between Resets&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Max Batches Seen Between Resets&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}, &#34;Reset Count&#34;: {&#34;sum&#34;: 10.0, &#34;count&#34;: 1, &#34;min&#34;: 10, &#34;max&#34;: 10}, &#34;Number of Records Since Last Reset&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Number of Batches Since Last Reset&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}}} [06/16/2022 09:08:39 INFO 139637139089216] #throughput_metric: host=algo-1, train throughput=514.8640084000922 records/second #metrics {&#34;StartTime&#34;: 1655370520.0116363, &#34;EndTime&#34;: 1655370520.0116863, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2941920460595025, &#34;count&#34;: 1, &#34;min&#34;: 0.2941920460595025, &#34;max&#34;: 0.2941920460595025}}} #metrics {&#34;StartTime&#34;: 1655370520.011752, &#34;EndTime&#34;: 1655370520.0117636, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2977325047387017, &#34;count&#34;: 1, &#34;min&#34;: 0.2977325047387017, &#34;max&#34;: 0.2977325047387017}}} #metrics {&#34;StartTime&#34;: 1655370520.0118022, &#34;EndTime&#34;: 1655370520.0118113, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.299101005660163, &#34;count&#34;: 1, &#34;min&#34;: 0.299101005660163, &#34;max&#34;: 0.299101005660163}}} #metrics {&#34;StartTime&#34;: 1655370520.0118444, &#34;EndTime&#34;: 1655370520.0118527, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.29377426359388564, &#34;count&#34;: 1, &#34;min&#34;: 0.29377426359388564, &#34;max&#34;: 0.29377426359388564}}} #metrics {&#34;StartTime&#34;: 1655370520.0118868, &#34;EndTime&#34;: 1655370520.0118957, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2996904797024197, &#34;count&#34;: 1, &#34;min&#34;: 0.2996904797024197, &#34;max&#34;: 0.2996904797024197}}} #metrics {&#34;StartTime&#34;: 1655370520.0119596, &#34;EndTime&#34;: 1655370520.011973, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.29708516014946834, &#34;count&#34;: 1, &#34;min&#34;: 0.29708516014946834, &#34;max&#34;: 0.29708516014946834}}} #metrics {&#34;StartTime&#34;: 1655370520.0120091, &#34;EndTime&#34;: 1655370520.0120187, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2980598428514269, &#34;count&#34;: 1, &#34;min&#34;: 0.2980598428514269, &#34;max&#34;: 0.2980598428514269}}} #metrics {&#34;StartTime&#34;: 1655370520.0120518, &#34;EndTime&#34;: 1655370520.0120609, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.29740745650397404, &#34;count&#34;: 1, &#34;min&#34;: 0.29740745650397404, &#34;max&#34;: 0.29740745650397404}}} #metrics {&#34;StartTime&#34;: 1655370520.0120983, &#34;EndTime&#34;: 1655370520.0121088, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2937090386284722, &#34;count&#34;: 1, &#34;min&#34;: 0.2937090386284722, &#34;max&#34;: 0.2937090386284722}}} #metrics {&#34;StartTime&#34;: 1655370520.0121424, &#34;EndTime&#34;: 1655370520.012152, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2868423456615872, &#34;count&#34;: 1, &#34;min&#34;: 0.2868423456615872, &#34;max&#34;: 0.2868423456615872}}} #metrics {&#34;StartTime&#34;: 1655370520.0122473, &#34;EndTime&#34;: 1655370520.0122707, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3020733462439643, &#34;count&#34;: 1, &#34;min&#34;: 0.3020733462439643, &#34;max&#34;: 0.3020733462439643}}} #metrics {&#34;StartTime&#34;: 1655370520.012336, &#34;EndTime&#34;: 1655370520.0123484, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.29631993611653645, &#34;count&#34;: 1, &#34;min&#34;: 0.29631993611653645, &#34;max&#34;: 0.29631993611653645}}} #metrics {&#34;StartTime&#34;: 1655370520.0123844, &#34;EndTime&#34;: 1655370520.0123942, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2968277194764879, &#34;count&#34;: 1, &#34;min&#34;: 0.2968277194764879, &#34;max&#34;: 0.2968277194764879}}} #metrics {&#34;StartTime&#34;: 1655370520.012429, &#34;EndTime&#34;: 1655370520.0124393, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3076706706153022, &#34;count&#34;: 1, &#34;min&#34;: 0.3076706706153022, &#34;max&#34;: 0.3076706706153022}}} #metrics {&#34;StartTime&#34;: 1655370520.0124753, &#34;EndTime&#34;: 1655370520.0124848, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2947852929433187, &#34;count&#34;: 1, &#34;min&#34;: 0.2947852929433187, &#34;max&#34;: 0.2947852929433187}}} #metrics {&#34;StartTime&#34;: 1655370520.01252, &#34;EndTime&#34;: 1655370520.0125296, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.29740756352742515, &#34;count&#34;: 1, &#34;min&#34;: 0.29740756352742515, &#34;max&#34;: 0.29740756352742515}}} #metrics {&#34;StartTime&#34;: 1655370520.0126102, &#34;EndTime&#34;: 1655370520.0126433, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.38085482279459637, &#34;count&#34;: 1, &#34;min&#34;: 0.38085482279459637, &#34;max&#34;: 0.38085482279459637}}} #metrics {&#34;StartTime&#34;: 1655370520.0126982, &#34;EndTime&#34;: 1655370520.0127106, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3819334877861871, &#34;count&#34;: 1, &#34;min&#34;: 0.3819334877861871, &#34;max&#34;: 0.3819334877861871}}} #metrics {&#34;StartTime&#34;: 1655370520.01275, &#34;EndTime&#34;: 1655370520.0127604, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.38018073505825467, &#34;count&#34;: 1, &#34;min&#34;: 0.38018073505825467, &#34;max&#34;: 0.38018073505825467}}} #metrics {&#34;StartTime&#34;: 1655370520.0127964, &#34;EndTime&#34;: 1655370520.0128067, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3805082925160726, &#34;count&#34;: 1, &#34;min&#34;: 0.3805082925160726, &#34;max&#34;: 0.3805082925160726}}} #metrics {&#34;StartTime&#34;: 1655370520.0128434, &#34;EndTime&#34;: 1655370520.0128539, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4009894773695204, &#34;count&#34;: 1, &#34;min&#34;: 0.4009894773695204, &#34;max&#34;: 0.4009894773695204}}} #metrics {&#34;StartTime&#34;: 1655370520.0128891, &#34;EndTime&#34;: 1655370520.0129316, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4127677832709418, &#34;count&#34;: 1, &#34;min&#34;: 0.4127677832709418, &#34;max&#34;: 0.4127677832709418}}} #metrics {&#34;StartTime&#34;: 1655370520.0130405, &#34;EndTime&#34;: 1655370520.0130532, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4057879130045573, &#34;count&#34;: 1, &#34;min&#34;: 0.4057879130045573, &#34;max&#34;: 0.4057879130045573}}} #metrics {&#34;StartTime&#34;: 1655370520.0130932, &#34;EndTime&#34;: 1655370520.013104, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.40640818066067164, &#34;count&#34;: 1, &#34;min&#34;: 0.40640818066067164, &#34;max&#34;: 0.40640818066067164}}} #metrics {&#34;StartTime&#34;: 1655370520.0131407, &#34;EndTime&#34;: 1655370520.01315, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9558209673563639, &#34;count&#34;: 1, &#34;min&#34;: 0.9558209673563639, &#34;max&#34;: 0.9558209673563639}}} #metrics {&#34;StartTime&#34;: 1655370520.013188, &#34;EndTime&#34;: 1655370520.0131981, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9563378630744086, &#34;count&#34;: 1, &#34;min&#34;: 0.9563378630744086, &#34;max&#34;: 0.9563378630744086}}} #metrics {&#34;StartTime&#34;: 1655370520.0133023, &#34;EndTime&#34;: 1655370520.013317, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.955727530585395, &#34;count&#34;: 1, &#34;min&#34;: 0.955727530585395, &#34;max&#34;: 0.955727530585395}}} #metrics {&#34;StartTime&#34;: 1655370520.0133593, &#34;EndTime&#34;: 1655370520.01337, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9561675283643934, &#34;count&#34;: 1, &#34;min&#34;: 0.9561675283643934, &#34;max&#34;: 0.9561675283643934}}} #metrics {&#34;StartTime&#34;: 1655370520.0134063, &#34;EndTime&#34;: 1655370520.0134165, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9690417692396376, &#34;count&#34;: 1, &#34;min&#34;: 0.9690417692396376, &#34;max&#34;: 0.9690417692396376}}} #metrics {&#34;StartTime&#34;: 1655370520.0134535, &#34;EndTime&#34;: 1655370520.0134635, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9747900433010526, &#34;count&#34;: 1, &#34;min&#34;: 0.9747900433010526, &#34;max&#34;: 0.9747900433010526}}} #metrics {&#34;StartTime&#34;: 1655370520.0135748, &#34;EndTime&#34;: 1655370520.0135884, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9729332139756944, &#34;count&#34;: 1, &#34;min&#34;: 0.9729332139756944, &#34;max&#34;: 0.9729332139756944}}} #metrics {&#34;StartTime&#34;: 1655370520.0136409, &#34;EndTime&#34;: 1655370520.013653, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9750834634568957, &#34;count&#34;: 1, &#34;min&#34;: 0.9750834634568957, &#34;max&#34;: 0.9750834634568957}}} [06/16/2022 09:08:40 INFO 139637139089216] #quality_metric: host=algo-1, epoch=8, train mse_objective &lt;loss&gt;=0.2941920460595025 #metrics {&#34;StartTime&#34;: 1655370520.0893464, &#34;EndTime&#34;: 1655370520.0893962, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.891092337814031, &#34;count&#34;: 1, &#34;min&#34;: 13.891092337814031, &#34;max&#34;: 13.891092337814031}}} #metrics {&#34;StartTime&#34;: 1655370520.0894663, &#34;EndTime&#34;: 1655370520.0894792, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.127114688648897, &#34;count&#34;: 1, &#34;min&#34;: 14.127114688648897, &#34;max&#34;: 14.127114688648897}}} #metrics {&#34;StartTime&#34;: 1655370520.0895193, &#34;EndTime&#34;: 1655370520.0895288, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.375494863472733, &#34;count&#34;: 1, &#34;min&#34;: 13.375494863472733, &#34;max&#34;: 13.375494863472733}}} #metrics {&#34;StartTime&#34;: 1655370520.0895627, &#34;EndTime&#34;: 1655370520.0895712, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.390127742991728, &#34;count&#34;: 1, &#34;min&#34;: 13.390127742991728, &#34;max&#34;: 13.390127742991728}}} #metrics {&#34;StartTime&#34;: 1655370520.0896049, &#34;EndTime&#34;: 1655370520.089613, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.589366239659926, &#34;count&#34;: 1, &#34;min&#34;: 14.589366239659926, &#34;max&#34;: 14.589366239659926}}} #metrics {&#34;StartTime&#34;: 1655370520.0911295, &#34;EndTime&#34;: 1655370520.0911534, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.518709668926164, &#34;count&#34;: 1, &#34;min&#34;: 14.518709668926164, &#34;max&#34;: 14.518709668926164}}} #metrics {&#34;StartTime&#34;: 1655370520.09121, &#34;EndTime&#34;: 1655370520.0912225, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.604721966911764, &#34;count&#34;: 1, &#34;min&#34;: 14.604721966911764, &#34;max&#34;: 14.604721966911764}}} #metrics {&#34;StartTime&#34;: 1655370520.0913134, &#34;EndTime&#34;: 1655370520.0913277, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.566698261335784, &#34;count&#34;: 1, &#34;min&#34;: 14.566698261335784, &#34;max&#34;: 14.566698261335784}}} #metrics {&#34;StartTime&#34;: 1655370520.0914166, &#34;EndTime&#34;: 1655370520.0914292, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.38850492589614, &#34;count&#34;: 1, &#34;min&#34;: 13.38850492589614, &#34;max&#34;: 13.38850492589614}}} #metrics {&#34;StartTime&#34;: 1655370520.0914688, &#34;EndTime&#34;: 1655370520.091479, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.63548667758119, &#34;count&#34;: 1, &#34;min&#34;: 13.63548667758119, &#34;max&#34;: 13.63548667758119}}} #metrics {&#34;StartTime&#34;: 1655370520.091515, &#34;EndTime&#34;: 1655370520.0915265, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.932566923253676, &#34;count&#34;: 1, &#34;min&#34;: 13.932566923253676, &#34;max&#34;: 13.932566923253676}}} #metrics {&#34;StartTime&#34;: 1655370520.0915627, &#34;EndTime&#34;: 1655370520.0915728, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.517542820350796, &#34;count&#34;: 1, &#34;min&#34;: 13.517542820350796, &#34;max&#34;: 13.517542820350796}}} #metrics {&#34;StartTime&#34;: 1655370520.0916889, &#34;EndTime&#34;: 1655370520.0917022, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.374922210094976, &#34;count&#34;: 1, &#34;min&#34;: 14.374922210094976, &#34;max&#34;: 14.374922210094976}}} #metrics {&#34;StartTime&#34;: 1655370520.0917413, &#34;EndTime&#34;: 1655370520.0917516, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.99994016161152, &#34;count&#34;: 1, &#34;min&#34;: 14.99994016161152, &#34;max&#34;: 14.99994016161152}}} #metrics {&#34;StartTime&#34;: 1655370520.091789, &#34;EndTime&#34;: 1655370520.0917997, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.399116067325368, &#34;count&#34;: 1, &#34;min&#34;: 14.399116067325368, &#34;max&#34;: 14.399116067325368}}} #metrics {&#34;StartTime&#34;: 1655370520.0918357, &#34;EndTime&#34;: 1655370520.091881, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.498655431410846, &#34;count&#34;: 1, &#34;min&#34;: 14.498655431410846, &#34;max&#34;: 14.498655431410846}}} #metrics {&#34;StartTime&#34;: 1655370520.0919576, &#34;EndTime&#34;: 1655370520.0919702, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.53235102634804, &#34;count&#34;: 1, &#34;min&#34;: 17.53235102634804, &#34;max&#34;: 17.53235102634804}}} #metrics {&#34;StartTime&#34;: 1655370520.0920084, &#34;EndTime&#34;: 1655370520.0920186, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.663778267654717, &#34;count&#34;: 1, &#34;min&#34;: 17.663778267654717, &#34;max&#34;: 17.663778267654717}}} #metrics {&#34;StartTime&#34;: 1655370520.0920556, &#34;EndTime&#34;: 1655370520.092066, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.548595353668812, &#34;count&#34;: 1, &#34;min&#34;: 17.548595353668812, &#34;max&#34;: 17.548595353668812}}} #metrics {&#34;StartTime&#34;: 1655370520.0921018, &#34;EndTime&#34;: 1655370520.0921464, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.57577335133272, &#34;count&#34;: 1, &#34;min&#34;: 17.57577335133272, &#34;max&#34;: 17.57577335133272}}} #metrics {&#34;StartTime&#34;: 1655370520.0922227, &#34;EndTime&#34;: 1655370520.0922353, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.417114856196385, &#34;count&#34;: 1, &#34;min&#34;: 19.417114856196385, &#34;max&#34;: 19.417114856196385}}} #metrics {&#34;StartTime&#34;: 1655370520.0922737, &#34;EndTime&#34;: 1655370520.0922842, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.064445944393384, &#34;count&#34;: 1, &#34;min&#34;: 19.064445944393384, &#34;max&#34;: 19.064445944393384}}} #metrics {&#34;StartTime&#34;: 1655370520.0923226, &#34;EndTime&#34;: 1655370520.0923333, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.305278104894303, &#34;count&#34;: 1, &#34;min&#34;: 19.305278104894303, &#34;max&#34;: 19.305278104894303}}} #metrics {&#34;StartTime&#34;: 1655370520.0924037, &#34;EndTime&#34;: 1655370520.092449, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.309446447035846, &#34;count&#34;: 1, &#34;min&#34;: 19.309446447035846, &#34;max&#34;: 19.309446447035846}}} #metrics {&#34;StartTime&#34;: 1655370520.092494, &#34;EndTime&#34;: 1655370520.0925057, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.16492895986519, &#34;count&#34;: 1, &#34;min&#34;: 61.16492895986519, &#34;max&#34;: 61.16492895986519}}} #metrics {&#34;StartTime&#34;: 1655370520.0925431, &#34;EndTime&#34;: 1655370520.0925536, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.01647470511642, &#34;count&#34;: 1, &#34;min&#34;: 61.01647470511642, &#34;max&#34;: 61.01647470511642}}} #metrics {&#34;StartTime&#34;: 1655370520.0925922, &#34;EndTime&#34;: 1655370520.092603, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.45632515701593, &#34;count&#34;: 1, &#34;min&#34;: 61.45632515701593, &#34;max&#34;: 61.45632515701593}}} #metrics {&#34;StartTime&#34;: 1655370520.0926373, &#34;EndTime&#34;: 1655370520.0927045, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.2218807444853, &#34;count&#34;: 1, &#34;min&#34;: 61.2218807444853, &#34;max&#34;: 61.2218807444853}}} #metrics {&#34;StartTime&#34;: 1655370520.092766, &#34;EndTime&#34;: 1655370520.0927784, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.41029268152573, &#34;count&#34;: 1, &#34;min&#34;: 61.41029268152573, &#34;max&#34;: 61.41029268152573}}} #metrics {&#34;StartTime&#34;: 1655370520.092816, &#34;EndTime&#34;: 1655370520.0928268, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.66987400428921, &#34;count&#34;: 1, &#34;min&#34;: 61.66987400428921, &#34;max&#34;: 61.66987400428921}}} #metrics {&#34;StartTime&#34;: 1655370520.0928633, &#34;EndTime&#34;: 1655370520.092874, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.16052724800858, &#34;count&#34;: 1, &#34;min&#34;: 62.16052724800858, &#34;max&#34;: 62.16052724800858}}} #metrics {&#34;StartTime&#34;: 1655370520.092909, &#34;EndTime&#34;: 1655370520.0929692, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.23718501072304, &#34;count&#34;: 1, &#34;min&#34;: 62.23718501072304, &#34;max&#34;: 62.23718501072304}}} [06/16/2022 09:08:40 INFO 139637139089216] #quality_metric: host=algo-1, epoch=8, validation mse_objective &lt;loss&gt;=13.891092337814031 [06/16/2022 09:08:40 INFO 139637139089216] #early_stopping_criteria_metric: host=algo-1, epoch=8, criteria=mse_objective, value=13.375494863472733 [06/16/2022 09:08:40 INFO 139637139089216] Epoch 8: Loss has not improved for 0 epochs. [06/16/2022 09:08:40 INFO 139637139089216] Saving model for epoch: 8 [06/16/2022 09:08:40 INFO 139637139089216] Saved checkpoint to &#34;/tmp/tmpnn4ry60m/mx-mod-0000.params&#34; [06/16/2022 09:08:40 INFO 139637139089216] #progress_metric: host=algo-1, completed 60.0 % of epochs #metrics {&#34;StartTime&#34;: 1655370519.5388684, &#34;EndTime&#34;: 1655370520.10428, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 8, &#34;Meta&#34;: &#34;training_data_iter&#34;}, &#34;Metrics&#34;: {&#34;Total Records Seen&#34;: {&#34;sum&#34;: 4580.0, &#34;count&#34;: 1, &#34;min&#34;: 4580, &#34;max&#34;: 4580}, &#34;Total Batches Seen&#34;: {&#34;sum&#34;: 161.0, &#34;count&#34;: 1, &#34;min&#34;: 161, &#34;max&#34;: 161}, &#34;Max Records Seen Between Resets&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Max Batches Seen Between Resets&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}, &#34;Reset Count&#34;: {&#34;sum&#34;: 11.0, &#34;count&#34;: 1, &#34;min&#34;: 11, &#34;max&#34;: 11}, &#34;Number of Records Since Last Reset&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Number of Batches Since Last Reset&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}}} [06/16/2022 09:08:40 INFO 139637139089216] #throughput_metric: host=algo-1, train throughput=804.4782822583467 records/second #metrics {&#34;StartTime&#34;: 1655370520.549621, &#34;EndTime&#34;: 1655370520.5497212, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.29013213634490964, &#34;count&#34;: 1, &#34;min&#34;: 0.29013213634490964, &#34;max&#34;: 0.29013213634490964}}} #metrics {&#34;StartTime&#34;: 1655370520.5502884, &#34;EndTime&#34;: 1655370520.5503304, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2928758223851522, &#34;count&#34;: 1, &#34;min&#34;: 0.2928758223851522, &#34;max&#34;: 0.2928758223851522}}} #metrics {&#34;StartTime&#34;: 1655370520.550614, &#34;EndTime&#34;: 1655370520.5506585, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2929053433736165, &#34;count&#34;: 1, &#34;min&#34;: 0.2929053433736165, &#34;max&#34;: 0.2929053433736165}}} #metrics {&#34;StartTime&#34;: 1655370520.5509858, &#34;EndTime&#34;: 1655370520.5510325, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.28909748130374485, &#34;count&#34;: 1, &#34;min&#34;: 0.28909748130374485, &#34;max&#34;: 0.28909748130374485}}} #metrics {&#34;StartTime&#34;: 1655370520.5513449, &#34;EndTime&#34;: 1655370520.5513887, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30040884017944336, &#34;count&#34;: 1, &#34;min&#34;: 0.30040884017944336, &#34;max&#34;: 0.30040884017944336}}} #metrics {&#34;StartTime&#34;: 1655370520.5516994, &#34;EndTime&#34;: 1655370520.551744, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2986269039577908, &#34;count&#34;: 1, &#34;min&#34;: 0.2986269039577908, &#34;max&#34;: 0.2986269039577908}}} #metrics {&#34;StartTime&#34;: 1655370520.5520782, &#34;EndTime&#34;: 1655370520.5521278, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2992264938354492, &#34;count&#34;: 1, &#34;min&#34;: 0.2992264938354492, &#34;max&#34;: 0.2992264938354492}}} #metrics {&#34;StartTime&#34;: 1655370520.5524397, &#34;EndTime&#34;: 1655370520.5524619, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.29840770721435544, &#34;count&#34;: 1, &#34;min&#34;: 0.29840770721435544, &#34;max&#34;: 0.29840770721435544}}} #metrics {&#34;StartTime&#34;: 1655370520.552744, &#34;EndTime&#34;: 1655370520.5527666, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.28933507495456273, &#34;count&#34;: 1, &#34;min&#34;: 0.28933507495456273, &#34;max&#34;: 0.28933507495456273}}} #metrics {&#34;StartTime&#34;: 1655370520.5530725, &#34;EndTime&#34;: 1655370520.5530975, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2842833508385552, &#34;count&#34;: 1, &#34;min&#34;: 0.2842833508385552, &#34;max&#34;: 0.2842833508385552}}} #metrics {&#34;StartTime&#34;: 1655370520.553432, &#34;EndTime&#34;: 1655370520.5534549, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.29578848520914713, &#34;count&#34;: 1, &#34;min&#34;: 0.29578848520914713, &#34;max&#34;: 0.29578848520914713}}} #metrics {&#34;StartTime&#34;: 1655370520.5537934, &#34;EndTime&#34;: 1655370520.5538404, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2905331203672621, &#34;count&#34;: 1, &#34;min&#34;: 0.2905331203672621, &#34;max&#34;: 0.2905331203672621}}} #metrics {&#34;StartTime&#34;: 1655370520.5541644, &#34;EndTime&#34;: 1655370520.5542152, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2965577051374647, &#34;count&#34;: 1, &#34;min&#34;: 0.2965577051374647, &#34;max&#34;: 0.2965577051374647}}} #metrics {&#34;StartTime&#34;: 1655370520.5545385, &#34;EndTime&#34;: 1655370520.554582, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3103733253479004, &#34;count&#34;: 1, &#34;min&#34;: 0.3103733253479004, &#34;max&#34;: 0.3103733253479004}}} #metrics {&#34;StartTime&#34;: 1655370520.5548851, &#34;EndTime&#34;: 1655370520.5549314, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2948703628116184, &#34;count&#34;: 1, &#34;min&#34;: 0.2948703628116184, &#34;max&#34;: 0.2948703628116184}}} #metrics {&#34;StartTime&#34;: 1655370520.5552418, &#34;EndTime&#34;: 1655370520.5552843, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2986698521508111, &#34;count&#34;: 1, &#34;min&#34;: 0.2986698521508111, &#34;max&#34;: 0.2986698521508111}}} #metrics {&#34;StartTime&#34;: 1655370520.5555859, &#34;EndTime&#34;: 1655370520.555631, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.38023031128777396, &#34;count&#34;: 1, &#34;min&#34;: 0.38023031128777396, &#34;max&#34;: 0.38023031128777396}}} #metrics {&#34;StartTime&#34;: 1655370520.5559533, &#34;EndTime&#34;: 1655370520.5560029, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3807586415608724, &#34;count&#34;: 1, &#34;min&#34;: 0.3807586415608724, &#34;max&#34;: 0.3807586415608724}}} #metrics {&#34;StartTime&#34;: 1655370520.556318, &#34;EndTime&#34;: 1655370520.5563595, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.37992733743455676, &#34;count&#34;: 1, &#34;min&#34;: 0.37992733743455676, &#34;max&#34;: 0.37992733743455676}}} #metrics {&#34;StartTime&#34;: 1655370520.556667, &#34;EndTime&#34;: 1655370520.556708, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3801351261138916, &#34;count&#34;: 1, &#34;min&#34;: 0.3801351261138916, &#34;max&#34;: 0.3801351261138916}}} #metrics {&#34;StartTime&#34;: 1655370520.5570157, &#34;EndTime&#34;: 1655370520.5570614, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4010053952534994, &#34;count&#34;: 1, &#34;min&#34;: 0.4010053952534994, &#34;max&#34;: 0.4010053952534994}}} #metrics {&#34;StartTime&#34;: 1655370520.557363, &#34;EndTime&#34;: 1655370520.5574064, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4145890680948893, &#34;count&#34;: 1, &#34;min&#34;: 0.4145890680948893, &#34;max&#34;: 0.4145890680948893}}} #metrics {&#34;StartTime&#34;: 1655370520.5577104, &#34;EndTime&#34;: 1655370520.5577602, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4057956960466173, &#34;count&#34;: 1, &#34;min&#34;: 0.4057956960466173, &#34;max&#34;: 0.4057956960466173}}} #metrics {&#34;StartTime&#34;: 1655370520.558101, &#34;EndTime&#34;: 1655370520.5581424, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4078213649325901, &#34;count&#34;: 1, &#34;min&#34;: 0.4078213649325901, &#34;max&#34;: 0.4078213649325901}}} #metrics {&#34;StartTime&#34;: 1655370520.5584185, &#34;EndTime&#34;: 1655370520.5584414, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9560589875115288, &#34;count&#34;: 1, &#34;min&#34;: 0.9560589875115288, &#34;max&#34;: 0.9560589875115288}}} #metrics {&#34;StartTime&#34;: 1655370520.558794, &#34;EndTime&#34;: 1655370520.5588386, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9563559150695801, &#34;count&#34;: 1, &#34;min&#34;: 0.9563559150695801, &#34;max&#34;: 0.9563559150695801}}} #metrics {&#34;StartTime&#34;: 1655370520.559153, &#34;EndTime&#34;: 1655370520.5591962, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9556234571668837, &#34;count&#34;: 1, &#34;min&#34;: 0.9556234571668837, &#34;max&#34;: 0.9556234571668837}}} #metrics {&#34;StartTime&#34;: 1655370520.5594993, &#34;EndTime&#34;: 1655370520.5595422, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9561930656433105, &#34;count&#34;: 1, &#34;min&#34;: 0.9561930656433105, &#34;max&#34;: 0.9561930656433105}}} #metrics {&#34;StartTime&#34;: 1655370520.5598738, &#34;EndTime&#34;: 1655370520.5599148, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9691415850321452, &#34;count&#34;: 1, &#34;min&#34;: 0.9691415850321452, &#34;max&#34;: 0.9691415850321452}}} #metrics {&#34;StartTime&#34;: 1655370520.5601826, &#34;EndTime&#34;: 1655370520.560206, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9761156145731608, &#34;count&#34;: 1, &#34;min&#34;: 0.9761156145731608, &#34;max&#34;: 0.9761156145731608}}} #metrics {&#34;StartTime&#34;: 1655370520.5605092, &#34;EndTime&#34;: 1655370520.5605328, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9731467416551378, &#34;count&#34;: 1, &#34;min&#34;: 0.9731467416551378, &#34;max&#34;: 0.9731467416551378}}} #metrics {&#34;StartTime&#34;: 1655370520.5608394, &#34;EndTime&#34;: 1655370520.5608828, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9760680919223361, &#34;count&#34;: 1, &#34;min&#34;: 0.9760680919223361, &#34;max&#34;: 0.9760680919223361}}} [06/16/2022 09:08:40 INFO 139637139089216] #quality_metric: host=algo-1, epoch=9, train mse_objective &lt;loss&gt;=0.29013213634490964 #metrics {&#34;StartTime&#34;: 1655370520.6441615, &#34;EndTime&#34;: 1655370520.6442583, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.868024040670957, &#34;count&#34;: 1, &#34;min&#34;: 13.868024040670957, &#34;max&#34;: 13.868024040670957}}} #metrics {&#34;StartTime&#34;: 1655370520.6448584, &#34;EndTime&#34;: 1655370520.6448953, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.04374246036305, &#34;count&#34;: 1, &#34;min&#34;: 14.04374246036305, &#34;max&#34;: 14.04374246036305}}} #metrics {&#34;StartTime&#34;: 1655370520.6451862, &#34;EndTime&#34;: 1655370520.6452343, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.290261062921262, &#34;count&#34;: 1, &#34;min&#34;: 13.290261062921262, &#34;max&#34;: 13.290261062921262}}} #metrics {&#34;StartTime&#34;: 1655370520.6455963, &#34;EndTime&#34;: 1655370520.6456602, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.421795414943322, &#34;count&#34;: 1, &#34;min&#34;: 13.421795414943322, &#34;max&#34;: 13.421795414943322}}} #metrics {&#34;StartTime&#34;: 1655370520.6459756, &#34;EndTime&#34;: 1655370520.64602, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.578905292585784, &#34;count&#34;: 1, &#34;min&#34;: 14.578905292585784, &#34;max&#34;: 14.578905292585784}}} #metrics {&#34;StartTime&#34;: 1655370520.6463363, &#34;EndTime&#34;: 1655370520.64638, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.51547660079657, &#34;count&#34;: 1, &#34;min&#34;: 14.51547660079657, &#34;max&#34;: 14.51547660079657}}} #metrics {&#34;StartTime&#34;: 1655370520.6467245, &#34;EndTime&#34;: 1655370520.64677, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.553095798866421, &#34;count&#34;: 1, &#34;min&#34;: 14.553095798866421, &#34;max&#34;: 14.553095798866421}}} #metrics {&#34;StartTime&#34;: 1655370520.6470294, &#34;EndTime&#34;: 1655370520.6470678, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.528086344401041, &#34;count&#34;: 1, &#34;min&#34;: 14.528086344401041, &#34;max&#34;: 14.528086344401041}}} #metrics {&#34;StartTime&#34;: 1655370520.6473835, &#34;EndTime&#34;: 1655370520.6474307, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.390285716337317, &#34;count&#34;: 1, &#34;min&#34;: 13.390285716337317, &#34;max&#34;: 13.390285716337317}}} #metrics {&#34;StartTime&#34;: 1655370520.6477358, &#34;EndTime&#34;: 1655370520.6477783, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.705990660424327, &#34;count&#34;: 1, &#34;min&#34;: 13.705990660424327, &#34;max&#34;: 13.705990660424327}}} #metrics {&#34;StartTime&#34;: 1655370520.648081, &#34;EndTime&#34;: 1655370520.6481264, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.814183253867954, &#34;count&#34;: 1, &#34;min&#34;: 13.814183253867954, &#34;max&#34;: 13.814183253867954}}} #metrics {&#34;StartTime&#34;: 1655370520.6484518, &#34;EndTime&#34;: 1655370520.6485014, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.45258705288756, &#34;count&#34;: 1, &#34;min&#34;: 13.45258705288756, &#34;max&#34;: 13.45258705288756}}} #metrics {&#34;StartTime&#34;: 1655370520.6488178, &#34;EndTime&#34;: 1655370520.6488643, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.407178691789216, &#34;count&#34;: 1, &#34;min&#34;: 14.407178691789216, &#34;max&#34;: 14.407178691789216}}} #metrics {&#34;StartTime&#34;: 1655370520.649163, &#34;EndTime&#34;: 1655370520.6492069, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 15.006446987974877, &#34;count&#34;: 1, &#34;min&#34;: 15.006446987974877, &#34;max&#34;: 15.006446987974877}}} #metrics {&#34;StartTime&#34;: 1655370520.6495147, &#34;EndTime&#34;: 1655370520.6495585, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.442462397556678, &#34;count&#34;: 1, &#34;min&#34;: 14.442462397556678, &#34;max&#34;: 14.442462397556678}}} #metrics {&#34;StartTime&#34;: 1655370520.6498666, &#34;EndTime&#34;: 1655370520.6499114, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.486004997702207, &#34;count&#34;: 1, &#34;min&#34;: 14.486004997702207, &#34;max&#34;: 14.486004997702207}}} #metrics {&#34;StartTime&#34;: 1655370520.6502216, &#34;EndTime&#34;: 1655370520.6502666, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.534964168772976, &#34;count&#34;: 1, &#34;min&#34;: 17.534964168772976, &#34;max&#34;: 17.534964168772976}}} #metrics {&#34;StartTime&#34;: 1655370520.650586, &#34;EndTime&#34;: 1655370520.6506255, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.617788277420342, &#34;count&#34;: 1, &#34;min&#34;: 17.617788277420342, &#34;max&#34;: 17.617788277420342}}} #metrics {&#34;StartTime&#34;: 1655370520.650935, &#34;EndTime&#34;: 1655370520.6509607, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.542332667930452, &#34;count&#34;: 1, &#34;min&#34;: 17.542332667930452, &#34;max&#34;: 17.542332667930452}}} #metrics {&#34;StartTime&#34;: 1655370520.6512735, &#34;EndTime&#34;: 1655370520.6513178, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.554816750919116, &#34;count&#34;: 1, &#34;min&#34;: 17.554816750919116, &#34;max&#34;: 17.554816750919116}}} #metrics {&#34;StartTime&#34;: 1655370520.6516218, &#34;EndTime&#34;: 1655370520.6516662, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.379646450865504, &#34;count&#34;: 1, &#34;min&#34;: 19.379646450865504, &#34;max&#34;: 19.379646450865504}}} #metrics {&#34;StartTime&#34;: 1655370520.6519647, &#34;EndTime&#34;: 1655370520.6520078, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.02353683172488, &#34;count&#34;: 1, &#34;min&#34;: 19.02353683172488, &#34;max&#34;: 19.02353683172488}}} #metrics {&#34;StartTime&#34;: 1655370520.652338, &#34;EndTime&#34;: 1655370520.6523826, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.319062476064644, &#34;count&#34;: 1, &#34;min&#34;: 19.319062476064644, &#34;max&#34;: 19.319062476064644}}} #metrics {&#34;StartTime&#34;: 1655370520.6526477, &#34;EndTime&#34;: 1655370520.652687, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.223100250842524, &#34;count&#34;: 1, &#34;min&#34;: 19.223100250842524, &#34;max&#34;: 19.223100250842524}}} #metrics {&#34;StartTime&#34;: 1655370520.6530235, &#34;EndTime&#34;: 1655370520.6530693, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.12922459022671, &#34;count&#34;: 1, &#34;min&#34;: 61.12922459022671, &#34;max&#34;: 61.12922459022671}}} #metrics {&#34;StartTime&#34;: 1655370520.6533709, &#34;EndTime&#34;: 1655370520.653412, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 60.997905656403184, &#34;count&#34;: 1, &#34;min&#34;: 60.997905656403184, &#34;max&#34;: 60.997905656403184}}} #metrics {&#34;StartTime&#34;: 1655370520.6537628, &#34;EndTime&#34;: 1655370520.6538067, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.44938869102329, &#34;count&#34;: 1, &#34;min&#34;: 61.44938869102329, &#34;max&#34;: 61.44938869102329}}} #metrics {&#34;StartTime&#34;: 1655370520.6541333, &#34;EndTime&#34;: 1655370520.6541798, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.21539665670956, &#34;count&#34;: 1, &#34;min&#34;: 61.21539665670956, &#34;max&#34;: 61.21539665670956}}} #metrics {&#34;StartTime&#34;: 1655370520.6544425, &#34;EndTime&#34;: 1655370520.6544812, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.395761527267155, &#34;count&#34;: 1, &#34;min&#34;: 61.395761527267155, &#34;max&#34;: 61.395761527267155}}} #metrics {&#34;StartTime&#34;: 1655370520.6548033, &#34;EndTime&#34;: 1655370520.65485, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.79503197763481, &#34;count&#34;: 1, &#34;min&#34;: 61.79503197763481, &#34;max&#34;: 61.79503197763481}}} #metrics {&#34;StartTime&#34;: 1655370520.6551595, &#34;EndTime&#34;: 1655370520.6552022, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.01028502221201, &#34;count&#34;: 1, &#34;min&#34;: 62.01028502221201, &#34;max&#34;: 62.01028502221201}}} #metrics {&#34;StartTime&#34;: 1655370520.6555095, &#34;EndTime&#34;: 1655370520.6555557, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.290199429381126, &#34;count&#34;: 1, &#34;min&#34;: 62.290199429381126, &#34;max&#34;: 62.290199429381126}}} [06/16/2022 09:08:40 INFO 139637139089216] #quality_metric: host=algo-1, epoch=9, validation mse_objective &lt;loss&gt;=13.868024040670957 [06/16/2022 09:08:40 INFO 139637139089216] #early_stopping_criteria_metric: host=algo-1, epoch=9, criteria=mse_objective, value=13.290261062921262 [06/16/2022 09:08:40 INFO 139637139089216] Epoch 9: Loss has not improved for 0 epochs. [06/16/2022 09:08:40 INFO 139637139089216] Saving model for epoch: 9 [06/16/2022 09:08:40 INFO 139637139089216] Saved checkpoint to &#34;/tmp/tmpa9qtnet0/mx-mod-0000.params&#34; [06/16/2022 09:08:40 INFO 139637139089216] #progress_metric: host=algo-1, completed 66.66666666666667 % of epochs #metrics {&#34;StartTime&#34;: 1655370520.104647, &#34;EndTime&#34;: 1655370520.6660225, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 9, &#34;Meta&#34;: &#34;training_data_iter&#34;}, &#34;Metrics&#34;: {&#34;Total Records Seen&#34;: {&#34;sum&#34;: 5035.0, &#34;count&#34;: 1, &#34;min&#34;: 5035, &#34;max&#34;: 5035}, &#34;Total Batches Seen&#34;: {&#34;sum&#34;: 177.0, &#34;count&#34;: 1, &#34;min&#34;: 177, &#34;max&#34;: 177}, &#34;Max Records Seen Between Resets&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Max Batches Seen Between Resets&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}, &#34;Reset Count&#34;: {&#34;sum&#34;: 12.0, &#34;count&#34;: 1, &#34;min&#34;: 12, &#34;max&#34;: 12}, &#34;Number of Records Since Last Reset&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Number of Batches Since Last Reset&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}}} [06/16/2022 09:08:40 INFO 139637139089216] #throughput_metric: host=algo-1, train throughput=809.7957188454478 records/second #metrics {&#34;StartTime&#34;: 1655370521.1553307, &#34;EndTime&#34;: 1655370521.155433, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2872430700725979, &#34;count&#34;: 1, &#34;min&#34;: 0.2872430700725979, &#34;max&#34;: 0.2872430700725979}}} #metrics {&#34;StartTime&#34;: 1655370521.1560113, &#34;EndTime&#34;: 1655370521.1560647, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.28935033798217774, &#34;count&#34;: 1, &#34;min&#34;: 0.28935033798217774, &#34;max&#34;: 0.28935033798217774}}} #metrics {&#34;StartTime&#34;: 1655370521.1564155, &#34;EndTime&#34;: 1655370521.1564596, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.28860703733232285, &#34;count&#34;: 1, &#34;min&#34;: 0.28860703733232285, &#34;max&#34;: 0.28860703733232285}}} #metrics {&#34;StartTime&#34;: 1655370521.1567824, &#34;EndTime&#34;: 1655370521.1568294, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2859999407662286, &#34;count&#34;: 1, &#34;min&#34;: 0.2859999407662286, &#34;max&#34;: 0.2859999407662286}}} #metrics {&#34;StartTime&#34;: 1655370521.1571589, &#34;EndTime&#34;: 1655370521.1572099, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30112812254163956, &#34;count&#34;: 1, &#34;min&#34;: 0.30112812254163956, &#34;max&#34;: 0.30112812254163956}}} #metrics {&#34;StartTime&#34;: 1655370521.157527, &#34;EndTime&#34;: 1655370521.157572, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3003709740108914, &#34;count&#34;: 1, &#34;min&#34;: 0.3003709740108914, &#34;max&#34;: 0.3003709740108914}}} #metrics {&#34;StartTime&#34;: 1655370521.1578062, &#34;EndTime&#34;: 1655370521.1578603, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.29976721551683216, &#34;count&#34;: 1, &#34;min&#34;: 0.29976721551683216, &#34;max&#34;: 0.29976721551683216}}} #metrics {&#34;StartTime&#34;: 1655370521.157921, &#34;EndTime&#34;: 1655370521.1580422, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3003331512875027, &#34;count&#34;: 1, &#34;min&#34;: 0.3003331512875027, &#34;max&#34;: 0.3003331512875027}}} #metrics {&#34;StartTime&#34;: 1655370521.1580873, &#34;EndTime&#34;: 1655370521.1580994, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.28624652438693576, &#34;count&#34;: 1, &#34;min&#34;: 0.28624652438693576, &#34;max&#34;: 0.28624652438693576}}} #metrics {&#34;StartTime&#34;: 1655370521.1581507, &#34;EndTime&#34;: 1655370521.1581624, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2824632830089993, &#34;count&#34;: 1, &#34;min&#34;: 0.2824632830089993, &#34;max&#34;: 0.2824632830089993}}} #metrics {&#34;StartTime&#34;: 1655370521.158344, &#34;EndTime&#34;: 1655370521.1583612, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2913515912161933, &#34;count&#34;: 1, &#34;min&#34;: 0.2913515912161933, &#34;max&#34;: 0.2913515912161933}}} #metrics {&#34;StartTime&#34;: 1655370521.1584191, &#34;EndTime&#34;: 1655370521.158431, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2867222547531128, &#34;count&#34;: 1, &#34;min&#34;: 0.2867222547531128, &#34;max&#34;: 0.2867222547531128}}} #metrics {&#34;StartTime&#34;: 1655370521.158595, &#34;EndTime&#34;: 1655370521.1586115, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2964517784118652, &#34;count&#34;: 1, &#34;min&#34;: 0.2964517784118652, &#34;max&#34;: 0.2964517784118652}}} #metrics {&#34;StartTime&#34;: 1655370521.1586666, &#34;EndTime&#34;: 1655370521.1586788, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3138303894466824, &#34;count&#34;: 1, &#34;min&#34;: 0.3138303894466824, &#34;max&#34;: 0.3138303894466824}}} #metrics {&#34;StartTime&#34;: 1655370521.158846, &#34;EndTime&#34;: 1655370521.1588624, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2951772732204861, &#34;count&#34;: 1, &#34;min&#34;: 0.2951772732204861, &#34;max&#34;: 0.2951772732204861}}} #metrics {&#34;StartTime&#34;: 1655370521.158917, &#34;EndTime&#34;: 1655370521.1589293, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30056966675652397, &#34;count&#34;: 1, &#34;min&#34;: 0.30056966675652397, &#34;max&#34;: 0.30056966675652397}}} #metrics {&#34;StartTime&#34;: 1655370521.158965, &#34;EndTime&#34;: 1655370521.158974, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.37995175891452365, &#34;count&#34;: 1, &#34;min&#34;: 0.37995175891452365, &#34;max&#34;: 0.37995175891452365}}} #metrics {&#34;StartTime&#34;: 1655370521.1590054, &#34;EndTime&#34;: 1655370521.159015, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.38024831877814397, &#34;count&#34;: 1, &#34;min&#34;: 0.38024831877814397, &#34;max&#34;: 0.38024831877814397}}} #metrics {&#34;StartTime&#34;: 1655370521.1590466, &#34;EndTime&#34;: 1655370521.159056, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3797888882954915, &#34;count&#34;: 1, &#34;min&#34;: 0.3797888882954915, &#34;max&#34;: 0.3797888882954915}}} #metrics {&#34;StartTime&#34;: 1655370521.1590874, &#34;EndTime&#34;: 1655370521.1590958, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3799847994910346, &#34;count&#34;: 1, &#34;min&#34;: 0.3799847994910346, &#34;max&#34;: 0.3799847994910346}}} #metrics {&#34;StartTime&#34;: 1655370521.1591284, &#34;EndTime&#34;: 1655370521.159137, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.40136776606241864, &#34;count&#34;: 1, &#34;min&#34;: 0.40136776606241864, &#34;max&#34;: 0.40136776606241864}}} #metrics {&#34;StartTime&#34;: 1655370521.1591702, &#34;EndTime&#34;: 1655370521.1591794, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.41631723827785916, &#34;count&#34;: 1, &#34;min&#34;: 0.41631723827785916, &#34;max&#34;: 0.41631723827785916}}} #metrics {&#34;StartTime&#34;: 1655370521.1595278, &#34;EndTime&#34;: 1655370521.1595488, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4060533693101671, &#34;count&#34;: 1, &#34;min&#34;: 0.4060533693101671, &#34;max&#34;: 0.4060533693101671}}} #metrics {&#34;StartTime&#34;: 1655370521.1596036, &#34;EndTime&#34;: 1655370521.159616, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.40915524694654676, &#34;count&#34;: 1, &#34;min&#34;: 0.40915524694654676, &#34;max&#34;: 0.40915524694654676}}} #metrics {&#34;StartTime&#34;: 1655370521.1596544, &#34;EndTime&#34;: 1655370521.1596653, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9560007158915201, &#34;count&#34;: 1, &#34;min&#34;: 0.9560007158915201, &#34;max&#34;: 0.9560007158915201}}} #metrics {&#34;StartTime&#34;: 1655370521.159742, &#34;EndTime&#34;: 1655370521.1597555, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9564177046881782, &#34;count&#34;: 1, &#34;min&#34;: 0.9564177046881782, &#34;max&#34;: 0.9564177046881782}}} #metrics {&#34;StartTime&#34;: 1655370521.159797, &#34;EndTime&#34;: 1655370521.159842, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9556455760531956, &#34;count&#34;: 1, &#34;min&#34;: 0.9556455760531956, &#34;max&#34;: 0.9556455760531956}}} #metrics {&#34;StartTime&#34;: 1655370521.1598837, &#34;EndTime&#34;: 1655370521.1598947, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9562226380242241, &#34;count&#34;: 1, &#34;min&#34;: 0.9562226380242241, &#34;max&#34;: 0.9562226380242241}}} #metrics {&#34;StartTime&#34;: 1655370521.159933, &#34;EndTime&#34;: 1655370521.1599882, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9693977695041233, &#34;count&#34;: 1, &#34;min&#34;: 0.9693977695041233, &#34;max&#34;: 0.9693977695041233}}} #metrics {&#34;StartTime&#34;: 1655370521.160031, &#34;EndTime&#34;: 1655370521.1600428, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9774056010776095, &#34;count&#34;: 1, &#34;min&#34;: 0.9774056010776095, &#34;max&#34;: 0.9774056010776095}}} #metrics {&#34;StartTime&#34;: 1655370521.1601214, &#34;EndTime&#34;: 1655370521.160135, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9729545444912381, &#34;count&#34;: 1, &#34;min&#34;: 0.9729545444912381, &#34;max&#34;: 0.9729545444912381}}} #metrics {&#34;StartTime&#34;: 1655370521.1601768, &#34;EndTime&#34;: 1655370521.1601887, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9772756746080187, &#34;count&#34;: 1, &#34;min&#34;: 0.9772756746080187, &#34;max&#34;: 0.9772756746080187}}} [06/16/2022 09:08:41 INFO 139637139089216] #quality_metric: host=algo-1, epoch=10, train mse_objective &lt;loss&gt;=0.2872430700725979 #metrics {&#34;StartTime&#34;: 1655370521.2678201, &#34;EndTime&#34;: 1655370521.2679038, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.885411281211704, &#34;count&#34;: 1, &#34;min&#34;: 13.885411281211704, &#34;max&#34;: 13.885411281211704}}} #metrics {&#34;StartTime&#34;: 1655370521.2680554, &#34;EndTime&#34;: 1655370521.2680728, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.020633473115808, &#34;count&#34;: 1, &#34;min&#34;: 14.020633473115808, &#34;max&#34;: 14.020633473115808}}} #metrics {&#34;StartTime&#34;: 1655370521.2681167, &#34;EndTime&#34;: 1655370521.268127, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.289335961435356, &#34;count&#34;: 1, &#34;min&#34;: 13.289335961435356, &#34;max&#34;: 13.289335961435356}}} #metrics {&#34;StartTime&#34;: 1655370521.2681606, &#34;EndTime&#34;: 1655370521.2681696, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.498685948988971, &#34;count&#34;: 1, &#34;min&#34;: 13.498685948988971, &#34;max&#34;: 13.498685948988971}}} #metrics {&#34;StartTime&#34;: 1655370521.2682428, &#34;EndTime&#34;: 1655370521.268255, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.589147231158089, &#34;count&#34;: 1, &#34;min&#34;: 14.589147231158089, &#34;max&#34;: 14.589147231158089}}} #metrics {&#34;StartTime&#34;: 1655370521.2682908, &#34;EndTime&#34;: 1655370521.2682998, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.571391984528185, &#34;count&#34;: 1, &#34;min&#34;: 14.571391984528185, &#34;max&#34;: 14.571391984528185}}} #metrics {&#34;StartTime&#34;: 1655370521.2683313, &#34;EndTime&#34;: 1655370521.26834, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.57652671664369, &#34;count&#34;: 1, &#34;min&#34;: 14.57652671664369, &#34;max&#34;: 14.57652671664369}}} #metrics {&#34;StartTime&#34;: 1655370521.2683728, &#34;EndTime&#34;: 1655370521.2683816, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.593636307061887, &#34;count&#34;: 1, &#34;min&#34;: 14.593636307061887, &#34;max&#34;: 14.593636307061887}}} #metrics {&#34;StartTime&#34;: 1655370521.2684186, &#34;EndTime&#34;: 1655370521.268428, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.429443359375, &#34;count&#34;: 1, &#34;min&#34;: 13.429443359375, &#34;max&#34;: 13.429443359375}}} #metrics {&#34;StartTime&#34;: 1655370521.2684615, &#34;EndTime&#34;: 1655370521.2684712, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.770306157130822, &#34;count&#34;: 1, &#34;min&#34;: 13.770306157130822, &#34;max&#34;: 13.770306157130822}}} #metrics {&#34;StartTime&#34;: 1655370521.2685473, &#34;EndTime&#34;: 1655370521.2685602, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.774360207950368, &#34;count&#34;: 1, &#34;min&#34;: 13.774360207950368, &#34;max&#34;: 13.774360207950368}}} #metrics {&#34;StartTime&#34;: 1655370521.268598, &#34;EndTime&#34;: 1655370521.2686093, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.472411510991115, &#34;count&#34;: 1, &#34;min&#34;: 13.472411510991115, &#34;max&#34;: 13.472411510991115}}} #metrics {&#34;StartTime&#34;: 1655370521.268646, &#34;EndTime&#34;: 1655370521.2686553, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.376153684129902, &#34;count&#34;: 1, &#34;min&#34;: 14.376153684129902, &#34;max&#34;: 14.376153684129902}}} #metrics {&#34;StartTime&#34;: 1655370521.26869, &#34;EndTime&#34;: 1655370521.2687001, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 15.254106708601409, &#34;count&#34;: 1, &#34;min&#34;: 15.254106708601409, &#34;max&#34;: 15.254106708601409}}} #metrics {&#34;StartTime&#34;: 1655370521.2687356, &#34;EndTime&#34;: 1655370521.2687457, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.410895852481618, &#34;count&#34;: 1, &#34;min&#34;: 14.410895852481618, &#34;max&#34;: 14.410895852481618}}} #metrics {&#34;StartTime&#34;: 1655370521.268781, &#34;EndTime&#34;: 1655370521.2687902, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.556536506204043, &#34;count&#34;: 1, &#34;min&#34;: 14.556536506204043, &#34;max&#34;: 14.556536506204043}}} #metrics {&#34;StartTime&#34;: 1655370521.2688277, &#34;EndTime&#34;: 1655370521.268838, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.5369873046875, &#34;count&#34;: 1, &#34;min&#34;: 17.5369873046875, &#34;max&#34;: 17.5369873046875}}} #metrics {&#34;StartTime&#34;: 1655370521.2688732, &#34;EndTime&#34;: 1655370521.268883, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.582763073491115, &#34;count&#34;: 1, &#34;min&#34;: 17.582763073491115, &#34;max&#34;: 17.582763073491115}}} #metrics {&#34;StartTime&#34;: 1655370521.2689197, &#34;EndTime&#34;: 1655370521.2689302, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.53965130974265, &#34;count&#34;: 1, &#34;min&#34;: 17.53965130974265, &#34;max&#34;: 17.53965130974265}}} #metrics {&#34;StartTime&#34;: 1655370521.2689676, &#34;EndTime&#34;: 1655370521.2689779, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.54274076573989, &#34;count&#34;: 1, &#34;min&#34;: 17.54274076573989, &#34;max&#34;: 17.54274076573989}}} #metrics {&#34;StartTime&#34;: 1655370521.2690144, &#34;EndTime&#34;: 1655370521.2690246, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.389045266544116, &#34;count&#34;: 1, &#34;min&#34;: 19.389045266544116, &#34;max&#34;: 19.389045266544116}}} #metrics {&#34;StartTime&#34;: 1655370521.269063, &#34;EndTime&#34;: 1655370521.2690732, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.010003781786153, &#34;count&#34;: 1, &#34;min&#34;: 19.010003781786153, &#34;max&#34;: 19.010003781786153}}} #metrics {&#34;StartTime&#34;: 1655370521.269109, &#34;EndTime&#34;: 1655370521.2691188, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.310861026539524, &#34;count&#34;: 1, &#34;min&#34;: 19.310861026539524, &#34;max&#34;: 19.310861026539524}}} #metrics {&#34;StartTime&#34;: 1655370521.2691545, &#34;EndTime&#34;: 1655370521.2691648, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.197841868681067, &#34;count&#34;: 1, &#34;min&#34;: 19.197841868681067, &#34;max&#34;: 19.197841868681067}}} #metrics {&#34;StartTime&#34;: 1655370521.2692018, &#34;EndTime&#34;: 1655370521.2692115, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.11454743030024, &#34;count&#34;: 1, &#34;min&#34;: 61.11454743030024, &#34;max&#34;: 61.11454743030024}}} #metrics {&#34;StartTime&#34;: 1655370521.2692473, &#34;EndTime&#34;: 1655370521.2692573, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 60.98857326133579, &#34;count&#34;: 1, &#34;min&#34;: 60.98857326133579, &#34;max&#34;: 60.98857326133579}}} #metrics {&#34;StartTime&#34;: 1655370521.2692924, &#34;EndTime&#34;: 1655370521.2693026, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.43591069240196, &#34;count&#34;: 1, &#34;min&#34;: 61.43591069240196, &#34;max&#34;: 61.43591069240196}}} #metrics {&#34;StartTime&#34;: 1655370521.269337, &#34;EndTime&#34;: 1655370521.2693465, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.2046688304228, &#34;count&#34;: 1, &#34;min&#34;: 61.2046688304228, &#34;max&#34;: 61.2046688304228}}} #metrics {&#34;StartTime&#34;: 1655370521.2693815, &#34;EndTime&#34;: 1655370521.2693925, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.63593845741421, &#34;count&#34;: 1, &#34;min&#34;: 61.63593845741421, &#34;max&#34;: 61.63593845741421}}} #metrics {&#34;StartTime&#34;: 1655370521.269429, &#34;EndTime&#34;: 1655370521.269439, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.98420505897671, &#34;count&#34;: 1, &#34;min&#34;: 61.98420505897671, &#34;max&#34;: 61.98420505897671}}} #metrics {&#34;StartTime&#34;: 1655370521.269476, &#34;EndTime&#34;: 1655370521.2694857, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.02894263174019, &#34;count&#34;: 1, &#34;min&#34;: 62.02894263174019, &#34;max&#34;: 62.02894263174019}}} #metrics {&#34;StartTime&#34;: 1655370521.2695224, &#34;EndTime&#34;: 1655370521.2695951, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.67916331571691, &#34;count&#34;: 1, &#34;min&#34;: 62.67916331571691, &#34;max&#34;: 62.67916331571691}}} [06/16/2022 09:08:41 INFO 139637139089216] #quality_metric: host=algo-1, epoch=10, validation mse_objective &lt;loss&gt;=13.885411281211704 [06/16/2022 09:08:41 INFO 139637139089216] #early_stopping_criteria_metric: host=algo-1, epoch=10, criteria=mse_objective, value=13.289335961435356 [06/16/2022 09:08:41 INFO 139637139089216] Saving model for epoch: 10 [06/16/2022 09:08:41 INFO 139637139089216] Saved checkpoint to &#34;/tmp/tmpq9ykqpbv/mx-mod-0000.params&#34; [06/16/2022 09:08:41 INFO 139637139089216] #progress_metric: host=algo-1, completed 73.33333333333333 % of epochs #metrics {&#34;StartTime&#34;: 1655370520.6668208, &#34;EndTime&#34;: 1655370521.279281, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 10, &#34;Meta&#34;: &#34;training_data_iter&#34;}, &#34;Metrics&#34;: {&#34;Total Records Seen&#34;: {&#34;sum&#34;: 5490.0, &#34;count&#34;: 1, &#34;min&#34;: 5490, &#34;max&#34;: 5490}, &#34;Total Batches Seen&#34;: {&#34;sum&#34;: 193.0, &#34;count&#34;: 1, &#34;min&#34;: 193, &#34;max&#34;: 193}, &#34;Max Records Seen Between Resets&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Max Batches Seen Between Resets&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}, &#34;Reset Count&#34;: {&#34;sum&#34;: 13.0, &#34;count&#34;: 1, &#34;min&#34;: 13, &#34;max&#34;: 13}, &#34;Number of Records Since Last Reset&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Number of Batches Since Last Reset&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}}} [06/16/2022 09:08:41 INFO 139637139089216] #throughput_metric: host=algo-1, train throughput=742.7218980968515 records/second #metrics {&#34;StartTime&#34;: 1655370521.838044, &#34;EndTime&#34;: 1655370521.8381288, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.28510828071170385, &#34;count&#34;: 1, &#34;min&#34;: 0.28510828071170385, &#34;max&#34;: 0.28510828071170385}}} #metrics {&#34;StartTime&#34;: 1655370521.8382256, &#34;EndTime&#34;: 1655370521.83824, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2867154428693983, &#34;count&#34;: 1, &#34;min&#34;: 0.2867154428693983, &#34;max&#34;: 0.2867154428693983}}} #metrics {&#34;StartTime&#34;: 1655370521.8382823, &#34;EndTime&#34;: 1655370521.8383014, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.28554254319932726, &#34;count&#34;: 1, &#34;min&#34;: 0.28554254319932726, &#34;max&#34;: 0.28554254319932726}}} #metrics {&#34;StartTime&#34;: 1655370521.838337, &#34;EndTime&#34;: 1655370521.8383467, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2838771841261122, &#34;count&#34;: 1, &#34;min&#34;: 0.2838771841261122, &#34;max&#34;: 0.2838771841261122}}} #metrics {&#34;StartTime&#34;: 1655370521.838379, &#34;EndTime&#34;: 1655370521.8383877, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3009424262576633, &#34;count&#34;: 1, &#34;min&#34;: 0.3009424262576633, &#34;max&#34;: 0.3009424262576633}}} #metrics {&#34;StartTime&#34;: 1655370521.8384206, &#34;EndTime&#34;: 1655370521.8384306, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3021854559580485, &#34;count&#34;: 1, &#34;min&#34;: 0.3021854559580485, &#34;max&#34;: 0.3021854559580485}}} #metrics {&#34;StartTime&#34;: 1655370521.8384652, &#34;EndTime&#34;: 1655370521.838474, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.29982299592759876, &#34;count&#34;: 1, &#34;min&#34;: 0.29982299592759876, &#34;max&#34;: 0.29982299592759876}}} #metrics {&#34;StartTime&#34;: 1655370521.8385057, &#34;EndTime&#34;: 1655370521.8385148, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3022594918145074, &#34;count&#34;: 1, &#34;min&#34;: 0.3022594918145074, &#34;max&#34;: 0.3022594918145074}}} #metrics {&#34;StartTime&#34;: 1655370521.8385477, &#34;EndTime&#34;: 1655370521.8385568, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2839886787202623, &#34;count&#34;: 1, &#34;min&#34;: 0.2839886787202623, &#34;max&#34;: 0.2839886787202623}}} #metrics {&#34;StartTime&#34;: 1655370521.8385937, &#34;EndTime&#34;: 1655370521.8386028, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.28109286732143823, &#34;count&#34;: 1, &#34;min&#34;: 0.28109286732143823, &#34;max&#34;: 0.28109286732143823}}} #metrics {&#34;StartTime&#34;: 1655370521.83868, &#34;EndTime&#34;: 1655370521.8386939, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2881507539749146, &#34;count&#34;: 1, &#34;min&#34;: 0.2881507539749146, &#34;max&#34;: 0.2881507539749146}}} #metrics {&#34;StartTime&#34;: 1655370521.8387344, &#34;EndTime&#34;: 1655370521.8387449, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2841576279534234, &#34;count&#34;: 1, &#34;min&#34;: 0.2841576279534234, &#34;max&#34;: 0.2841576279534234}}} #metrics {&#34;StartTime&#34;: 1655370521.8387816, &#34;EndTime&#34;: 1655370521.8387928, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2963763893975152, &#34;count&#34;: 1, &#34;min&#34;: 0.2963763893975152, &#34;max&#34;: 0.2963763893975152}}} #metrics {&#34;StartTime&#34;: 1655370521.8388288, &#34;EndTime&#34;: 1655370521.8388386, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.31561148643493653, &#34;count&#34;: 1, &#34;min&#34;: 0.31561148643493653, &#34;max&#34;: 0.31561148643493653}}} #metrics {&#34;StartTime&#34;: 1655370521.8388734, &#34;EndTime&#34;: 1655370521.8388822, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.29526398870680065, &#34;count&#34;: 1, &#34;min&#34;: 0.29526398870680065, &#34;max&#34;: 0.29526398870680065}}} #metrics {&#34;StartTime&#34;: 1655370521.8389163, &#34;EndTime&#34;: 1655370521.8389266, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30245128101772734, &#34;count&#34;: 1, &#34;min&#34;: 0.30245128101772734, &#34;max&#34;: 0.30245128101772734}}} #metrics {&#34;StartTime&#34;: 1655370521.8389623, &#34;EndTime&#34;: 1655370521.8389711, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3798140949673123, &#34;count&#34;: 1, &#34;min&#34;: 0.3798140949673123, &#34;max&#34;: 0.3798140949673123}}} #metrics {&#34;StartTime&#34;: 1655370521.8390067, &#34;EndTime&#34;: 1655370521.8390164, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.38002708541022406, &#34;count&#34;: 1, &#34;min&#34;: 0.38002708541022406, &#34;max&#34;: 0.38002708541022406}}} #metrics {&#34;StartTime&#34;: 1655370521.8390517, &#34;EndTime&#34;: 1655370521.839062, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.37976446893480087, &#34;count&#34;: 1, &#34;min&#34;: 0.37976446893480087, &#34;max&#34;: 0.37976446893480087}}} #metrics {&#34;StartTime&#34;: 1655370521.8391478, &#34;EndTime&#34;: 1655370521.8391607, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3799731847974989, &#34;count&#34;: 1, &#34;min&#34;: 0.3799731847974989, &#34;max&#34;: 0.3799731847974989}}} #metrics {&#34;StartTime&#34;: 1655370521.8391998, &#34;EndTime&#34;: 1655370521.8392105, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.401339987648858, &#34;count&#34;: 1, &#34;min&#34;: 0.401339987648858, &#34;max&#34;: 0.401339987648858}}} #metrics {&#34;StartTime&#34;: 1655370521.8392463, &#34;EndTime&#34;: 1655370521.839256, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4178485446506076, &#34;count&#34;: 1, &#34;min&#34;: 0.4178485446506076, &#34;max&#34;: 0.4178485446506076}}} #metrics {&#34;StartTime&#34;: 1655370521.8392937, &#34;EndTime&#34;: 1655370521.8393037, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.40586137029859753, &#34;count&#34;: 1, &#34;min&#34;: 0.40586137029859753, &#34;max&#34;: 0.40586137029859753}}} #metrics {&#34;StartTime&#34;: 1655370521.8393395, &#34;EndTime&#34;: 1655370521.8393495, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4103736029730903, &#34;count&#34;: 1, &#34;min&#34;: 0.4103736029730903, &#34;max&#34;: 0.4103736029730903}}} #metrics {&#34;StartTime&#34;: 1655370521.8393855, &#34;EndTime&#34;: 1655370521.839395, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9560335731506348, &#34;count&#34;: 1, &#34;min&#34;: 0.9560335731506348, &#34;max&#34;: 0.9560335731506348}}} #metrics {&#34;StartTime&#34;: 1655370521.839433, &#34;EndTime&#34;: 1655370521.839443, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9564363055759006, &#34;count&#34;: 1, &#34;min&#34;: 0.9564363055759006, &#34;max&#34;: 0.9564363055759006}}} #metrics {&#34;StartTime&#34;: 1655370521.8394783, &#34;EndTime&#34;: 1655370521.8394887, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9556554137335883, &#34;count&#34;: 1, &#34;min&#34;: 0.9556554137335883, &#34;max&#34;: 0.9556554137335883}}} #metrics {&#34;StartTime&#34;: 1655370521.8395252, &#34;EndTime&#34;: 1655370521.8395355, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9562869029574924, &#34;count&#34;: 1, &#34;min&#34;: 0.9562869029574924, &#34;max&#34;: 0.9562869029574924}}} #metrics {&#34;StartTime&#34;: 1655370521.8395717, &#34;EndTime&#34;: 1655370521.8395817, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.969590695699056, &#34;count&#34;: 1, &#34;min&#34;: 0.969590695699056, &#34;max&#34;: 0.969590695699056}}} #metrics {&#34;StartTime&#34;: 1655370521.8396208, &#34;EndTime&#34;: 1655370521.839631, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9784575674268935, &#34;count&#34;: 1, &#34;min&#34;: 0.9784575674268935, &#34;max&#34;: 0.9784575674268935}}} #metrics {&#34;StartTime&#34;: 1655370521.8396654, &#34;EndTime&#34;: 1655370521.8396754, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9730250146653917, &#34;count&#34;: 1, &#34;min&#34;: 0.9730250146653917, &#34;max&#34;: 0.9730250146653917}}} #metrics {&#34;StartTime&#34;: 1655370521.8397114, &#34;EndTime&#34;: 1655370521.8397214, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.978685728708903, &#34;count&#34;: 1, &#34;min&#34;: 0.978685728708903, &#34;max&#34;: 0.978685728708903}}} [06/16/2022 09:08:41 INFO 139637139089216] #quality_metric: host=algo-1, epoch=11, train mse_objective &lt;loss&gt;=0.28510828071170385 #metrics {&#34;StartTime&#34;: 1655370521.908292, &#34;EndTime&#34;: 1655370521.908372, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.917296764897365, &#34;count&#34;: 1, &#34;min&#34;: 13.917296764897365, &#34;max&#34;: 13.917296764897365}}} #metrics {&#34;StartTime&#34;: 1655370521.9084628, &#34;EndTime&#34;: 1655370521.9084768, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.026091332529106, &#34;count&#34;: 1, &#34;min&#34;: 14.026091332529106, &#34;max&#34;: 14.026091332529106}}} #metrics {&#34;StartTime&#34;: 1655370521.9085178, &#34;EndTime&#34;: 1655370521.908528, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.328656963273591, &#34;count&#34;: 1, &#34;min&#34;: 13.328656963273591, &#34;max&#34;: 13.328656963273591}}} #metrics {&#34;StartTime&#34;: 1655370521.9085612, &#34;EndTime&#34;: 1655370521.9085703, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.589917949601716, &#34;count&#34;: 1, &#34;min&#34;: 13.589917949601716, &#34;max&#34;: 13.589917949601716}}} #metrics {&#34;StartTime&#34;: 1655370521.9086041, &#34;EndTime&#34;: 1655370521.9086134, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.57923500210631, &#34;count&#34;: 1, &#34;min&#34;: 14.57923500210631, &#34;max&#34;: 14.57923500210631}}} #metrics {&#34;StartTime&#34;: 1655370521.9086478, &#34;EndTime&#34;: 1655370521.9086573, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.614668303844976, &#34;count&#34;: 1, &#34;min&#34;: 14.614668303844976, &#34;max&#34;: 14.614668303844976}}} #metrics {&#34;StartTime&#34;: 1655370521.9086905, &#34;EndTime&#34;: 1655370521.9086993, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.565996357038909, &#34;count&#34;: 1, &#34;min&#34;: 14.565996357038909, &#34;max&#34;: 14.565996357038909}}} #metrics {&#34;StartTime&#34;: 1655370521.9087322, &#34;EndTime&#34;: 1655370521.9087415, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.629148595473346, &#34;count&#34;: 1, &#34;min&#34;: 14.629148595473346, &#34;max&#34;: 14.629148595473346}}} #metrics {&#34;StartTime&#34;: 1655370521.9087734, &#34;EndTime&#34;: 1655370521.9087832, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.480956432866115, &#34;count&#34;: 1, &#34;min&#34;: 13.480956432866115, &#34;max&#34;: 13.480956432866115}}} #metrics {&#34;StartTime&#34;: 1655370521.9088173, &#34;EndTime&#34;: 1655370521.9088268, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.820599724264707, &#34;count&#34;: 1, &#34;min&#34;: 13.820599724264707, &#34;max&#34;: 13.820599724264707}}} #metrics {&#34;StartTime&#34;: 1655370521.9088614, &#34;EndTime&#34;: 1655370521.9088717, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.776865042892156, &#34;count&#34;: 1, &#34;min&#34;: 13.776865042892156, &#34;max&#34;: 13.776865042892156}}} #metrics {&#34;StartTime&#34;: 1655370521.9089086, &#34;EndTime&#34;: 1655370521.9089189, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.53057741651348, &#34;count&#34;: 1, &#34;min&#34;: 13.53057741651348, &#34;max&#34;: 13.53057741651348}}} #metrics {&#34;StartTime&#34;: 1655370521.9089546, &#34;EndTime&#34;: 1655370521.9089653, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.385308957567402, &#34;count&#34;: 1, &#34;min&#34;: 14.385308957567402, &#34;max&#34;: 14.385308957567402}}} #metrics {&#34;StartTime&#34;: 1655370521.909004, &#34;EndTime&#34;: 1655370521.9090133, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 15.259306066176471, &#34;count&#34;: 1, &#34;min&#34;: 15.259306066176471, &#34;max&#34;: 15.259306066176471}}} #metrics {&#34;StartTime&#34;: 1655370521.9090486, &#34;EndTime&#34;: 1655370521.909058, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.419347426470589, &#34;count&#34;: 1, &#34;min&#34;: 14.419347426470589, &#34;max&#34;: 14.419347426470589}}} #metrics {&#34;StartTime&#34;: 1655370521.9090927, &#34;EndTime&#34;: 1655370521.909103, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.603858498965993, &#34;count&#34;: 1, &#34;min&#34;: 14.603858498965993, &#34;max&#34;: 14.603858498965993}}} #metrics {&#34;StartTime&#34;: 1655370521.909139, &#34;EndTime&#34;: 1655370521.909148, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.537115358838847, &#34;count&#34;: 1, &#34;min&#34;: 17.537115358838847, &#34;max&#34;: 17.537115358838847}}} #metrics {&#34;StartTime&#34;: 1655370521.9091842, &#34;EndTime&#34;: 1655370521.9091947, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.564494413488053, &#34;count&#34;: 1, &#34;min&#34;: 17.564494413488053, &#34;max&#34;: 17.564494413488053}}} #metrics {&#34;StartTime&#34;: 1655370521.9092293, &#34;EndTime&#34;: 1655370521.9092398, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.540641635071996, &#34;count&#34;: 1, &#34;min&#34;: 17.540641635071996, &#34;max&#34;: 17.540641635071996}}} #metrics {&#34;StartTime&#34;: 1655370521.9092762, &#34;EndTime&#34;: 1655370521.9092863, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.54013659907322, &#34;count&#34;: 1, &#34;min&#34;: 17.54013659907322, &#34;max&#34;: 17.54013659907322}}} #metrics {&#34;StartTime&#34;: 1655370521.9093225, &#34;EndTime&#34;: 1655370521.9093328, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.40724271886489, &#34;count&#34;: 1, &#34;min&#34;: 19.40724271886489, &#34;max&#34;: 19.40724271886489}}} #metrics {&#34;StartTime&#34;: 1655370521.909369, &#34;EndTime&#34;: 1655370521.9093795, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.024035883884803, &#34;count&#34;: 1, &#34;min&#34;: 19.024035883884803, &#34;max&#34;: 19.024035883884803}}} #metrics {&#34;StartTime&#34;: 1655370521.9094188, &#34;EndTime&#34;: 1655370521.9094286, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.3359375, &#34;count&#34;: 1, &#34;min&#34;: 19.3359375, &#34;max&#34;: 19.3359375}}} #metrics {&#34;StartTime&#34;: 1655370521.9094682, &#34;EndTime&#34;: 1655370521.9094777, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.152896656709558, &#34;count&#34;: 1, &#34;min&#34;: 19.152896656709558, &#34;max&#34;: 19.152896656709558}}} #metrics {&#34;StartTime&#34;: 1655370521.9095163, &#34;EndTime&#34;: 1655370521.909527, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.09646187576593, &#34;count&#34;: 1, &#34;min&#34;: 61.09646187576593, &#34;max&#34;: 61.09646187576593}}} #metrics {&#34;StartTime&#34;: 1655370521.909563, &#34;EndTime&#34;: 1655370521.9095724, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 60.98802035462622, &#34;count&#34;: 1, &#34;min&#34;: 60.98802035462622, &#34;max&#34;: 60.98802035462622}}} #metrics {&#34;StartTime&#34;: 1655370521.909608, &#34;EndTime&#34;: 1655370521.9096174, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.4263365502451, &#34;count&#34;: 1, &#34;min&#34;: 61.4263365502451, &#34;max&#34;: 61.4263365502451}}} #metrics {&#34;StartTime&#34;: 1655370521.9096725, &#34;EndTime&#34;: 1655370521.9096832, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.19737572763481, &#34;count&#34;: 1, &#34;min&#34;: 61.19737572763481, &#34;max&#34;: 61.19737572763481}}} #metrics {&#34;StartTime&#34;: 1655370521.9097197, &#34;EndTime&#34;: 1655370521.909729, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.81381644454657, &#34;count&#34;: 1, &#34;min&#34;: 61.81381644454657, &#34;max&#34;: 61.81381644454657}}} #metrics {&#34;StartTime&#34;: 1655370521.909764, &#34;EndTime&#34;: 1655370521.9097743, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.171288583792894, &#34;count&#34;: 1, &#34;min&#34;: 62.171288583792894, &#34;max&#34;: 62.171288583792894}}} #metrics {&#34;StartTime&#34;: 1655370521.9098115, &#34;EndTime&#34;: 1655370521.9098217, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.923943014705884, &#34;count&#34;: 1, &#34;min&#34;: 61.923943014705884, &#34;max&#34;: 61.923943014705884}}} #metrics {&#34;StartTime&#34;: 1655370521.9098577, &#34;EndTime&#34;: 1655370521.909868, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.848561006433826, &#34;count&#34;: 1, &#34;min&#34;: 62.848561006433826, &#34;max&#34;: 62.848561006433826}}} [06/16/2022 09:08:41 INFO 139637139089216] #quality_metric: host=algo-1, epoch=11, validation mse_objective &lt;loss&gt;=13.917296764897365 [06/16/2022 09:08:41 INFO 139637139089216] #early_stopping_criteria_metric: host=algo-1, epoch=11, criteria=mse_objective, value=13.328656963273591 [06/16/2022 09:08:41 INFO 139637139089216] Saving model for epoch: 11 [06/16/2022 09:08:41 INFO 139637139089216] Saved checkpoint to &#34;/tmp/tmpvut95rhe/mx-mod-0000.params&#34; [06/16/2022 09:08:41 INFO 139637139089216] #progress_metric: host=algo-1, completed 80.0 % of epochs #metrics {&#34;StartTime&#34;: 1655370521.2795324, &#34;EndTime&#34;: 1655370521.933144, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 11, &#34;Meta&#34;: &#34;training_data_iter&#34;}, &#34;Metrics&#34;: {&#34;Total Records Seen&#34;: {&#34;sum&#34;: 5945.0, &#34;count&#34;: 1, &#34;min&#34;: 5945, &#34;max&#34;: 5945}, &#34;Total Batches Seen&#34;: {&#34;sum&#34;: 209.0, &#34;count&#34;: 1, &#34;min&#34;: 209, &#34;max&#34;: 209}, &#34;Max Records Seen Between Resets&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Max Batches Seen Between Resets&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}, &#34;Reset Count&#34;: {&#34;sum&#34;: 14.0, &#34;count&#34;: 1, &#34;min&#34;: 14, &#34;max&#34;: 14}, &#34;Number of Records Since Last Reset&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Number of Batches Since Last Reset&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}}} [06/16/2022 09:08:41 INFO 139637139089216] #throughput_metric: host=algo-1, train throughput=695.6503887226684 records/second #metrics {&#34;StartTime&#34;: 1655370522.744168, &#34;EndTime&#34;: 1655370522.744255, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.28346955882178415, &#34;count&#34;: 1, &#34;min&#34;: 0.28346955882178415, &#34;max&#34;: 0.28346955882178415}}} #metrics {&#34;StartTime&#34;: 1655370522.7448397, &#34;EndTime&#34;: 1655370522.7448685, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.28467477321624757, &#34;count&#34;: 1, &#34;min&#34;: 0.28467477321624757, &#34;max&#34;: 0.28467477321624757}}} #metrics {&#34;StartTime&#34;: 1655370522.7452223, &#34;EndTime&#34;: 1655370522.7452695, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2833048539691501, &#34;count&#34;: 1, &#34;min&#34;: 0.2833048539691501, &#34;max&#34;: 0.2833048539691501}}} #metrics {&#34;StartTime&#34;: 1655370522.745621, &#34;EndTime&#34;: 1655370522.7456656, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.28234869215223524, &#34;count&#34;: 1, &#34;min&#34;: 0.28234869215223524, &#34;max&#34;: 0.28234869215223524}}} #metrics {&#34;StartTime&#34;: 1655370522.7460055, &#34;EndTime&#34;: 1655370522.7460508, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3009205606248644, &#34;count&#34;: 1, &#34;min&#34;: 0.3009205606248644, &#34;max&#34;: 0.3009205606248644}}} #metrics {&#34;StartTime&#34;: 1655370522.7463856, &#34;EndTime&#34;: 1655370522.7464309, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3041692638397217, &#34;count&#34;: 1, &#34;min&#34;: 0.3041692638397217, &#34;max&#34;: 0.3041692638397217}}} #metrics {&#34;StartTime&#34;: 1655370522.746754, &#34;EndTime&#34;: 1655370522.7467983, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2999672328101264, &#34;count&#34;: 1, &#34;min&#34;: 0.2999672328101264, &#34;max&#34;: 0.2999672328101264}}} #metrics {&#34;StartTime&#34;: 1655370522.7471278, &#34;EndTime&#34;: 1655370522.7471726, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30425420337253145, &#34;count&#34;: 1, &#34;min&#34;: 0.30425420337253145, &#34;max&#34;: 0.30425420337253145}}} #metrics {&#34;StartTime&#34;: 1655370522.747513, &#34;EndTime&#34;: 1655370522.7475533, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.28228312651316323, &#34;count&#34;: 1, &#34;min&#34;: 0.28228312651316323, &#34;max&#34;: 0.28228312651316323}}} #metrics {&#34;StartTime&#34;: 1655370522.7478898, &#34;EndTime&#34;: 1655370522.7479336, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2800179158316718, &#34;count&#34;: 1, &#34;min&#34;: 0.2800179158316718, &#34;max&#34;: 0.2800179158316718}}} #metrics {&#34;StartTime&#34;: 1655370522.7482476, &#34;EndTime&#34;: 1655370522.7482934, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2857889880074395, &#34;count&#34;: 1, &#34;min&#34;: 0.2857889880074395, &#34;max&#34;: 0.2857889880074395}}} #metrics {&#34;StartTime&#34;: 1655370522.7486112, &#34;EndTime&#34;: 1655370522.7486553, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.28236386722988555, &#34;count&#34;: 1, &#34;min&#34;: 0.28236386722988555, &#34;max&#34;: 0.28236386722988555}}} #metrics {&#34;StartTime&#34;: 1655370522.748978, &#34;EndTime&#34;: 1655370522.7490282, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2964678043789334, &#34;count&#34;: 1, &#34;min&#34;: 0.2964678043789334, &#34;max&#34;: 0.2964678043789334}}} #metrics {&#34;StartTime&#34;: 1655370522.7493563, &#34;EndTime&#34;: 1655370522.7494028, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.31731542693244086, &#34;count&#34;: 1, &#34;min&#34;: 0.31731542693244086, &#34;max&#34;: 0.31731542693244086}}} #metrics {&#34;StartTime&#34;: 1655370522.7497418, &#34;EndTime&#34;: 1655370522.7497861, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2955373117658827, &#34;count&#34;: 1, &#34;min&#34;: 0.2955373117658827, &#34;max&#34;: 0.2955373117658827}}} #metrics {&#34;StartTime&#34;: 1655370522.7501106, &#34;EndTime&#34;: 1655370522.7501576, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30444920751783583, &#34;count&#34;: 1, &#34;min&#34;: 0.30444920751783583, &#34;max&#34;: 0.30444920751783583}}} #metrics {&#34;StartTime&#34;: 1655370522.750486, &#34;EndTime&#34;: 1655370522.750531, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3797358269161648, &#34;count&#34;: 1, &#34;min&#34;: 0.3797358269161648, &#34;max&#34;: 0.3797358269161648}}} #metrics {&#34;StartTime&#34;: 1655370522.7508702, &#34;EndTime&#34;: 1655370522.7509215, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3799650086296929, &#34;count&#34;: 1, &#34;min&#34;: 0.3799650086296929, &#34;max&#34;: 0.3799650086296929}}} #metrics {&#34;StartTime&#34;: 1655370522.7512617, &#34;EndTime&#34;: 1655370522.7513063, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3797464413113064, &#34;count&#34;: 1, &#34;min&#34;: 0.3797464413113064, &#34;max&#34;: 0.3797464413113064}}} #metrics {&#34;StartTime&#34;: 1655370522.7516298, &#34;EndTime&#34;: 1655370522.7516758, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3799987326727973, &#34;count&#34;: 1, &#34;min&#34;: 0.3799987326727973, &#34;max&#34;: 0.3799987326727973}}} #metrics {&#34;StartTime&#34;: 1655370522.7519953, &#34;EndTime&#34;: 1655370522.7520382, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.40152807659573025, &#34;count&#34;: 1, &#34;min&#34;: 0.40152807659573025, &#34;max&#34;: 0.40152807659573025}}} #metrics {&#34;StartTime&#34;: 1655370522.752357, &#34;EndTime&#34;: 1655370522.752402, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4192352294921875, &#34;count&#34;: 1, &#34;min&#34;: 0.4192352294921875, &#34;max&#34;: 0.4192352294921875}}} #metrics {&#34;StartTime&#34;: 1655370522.7527537, &#34;EndTime&#34;: 1655370522.7527943, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4058685482872857, &#34;count&#34;: 1, &#34;min&#34;: 0.4058685482872857, &#34;max&#34;: 0.4058685482872857}}} #metrics {&#34;StartTime&#34;: 1655370522.7531254, &#34;EndTime&#34;: 1655370522.7531717, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.41155166731940374, &#34;count&#34;: 1, &#34;min&#34;: 0.41155166731940374, &#34;max&#34;: 0.41155166731940374}}} #metrics {&#34;StartTime&#34;: 1655370522.7535152, &#34;EndTime&#34;: 1655370522.7535622, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9560334947374132, &#34;count&#34;: 1, &#34;min&#34;: 0.9560334947374132, &#34;max&#34;: 0.9560334947374132}}} #metrics {&#34;StartTime&#34;: 1655370522.7538936, &#34;EndTime&#34;: 1655370522.753939, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9564646212259929, &#34;count&#34;: 1, &#34;min&#34;: 0.9564646212259929, &#34;max&#34;: 0.9564646212259929}}} #metrics {&#34;StartTime&#34;: 1655370522.7542644, &#34;EndTime&#34;: 1655370522.754341, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9556433169047037, &#34;count&#34;: 1, &#34;min&#34;: 0.9556433169047037, &#34;max&#34;: 0.9556433169047037}}} #metrics {&#34;StartTime&#34;: 1655370522.7547088, &#34;EndTime&#34;: 1655370522.754759, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9563207986619737, &#34;count&#34;: 1, &#34;min&#34;: 0.9563207986619737, &#34;max&#34;: 0.9563207986619737}}} #metrics {&#34;StartTime&#34;: 1655370522.7550771, &#34;EndTime&#34;: 1655370522.7551222, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9697733688354492, &#34;count&#34;: 1, &#34;min&#34;: 0.9697733688354492, &#34;max&#34;: 0.9697733688354492}}} #metrics {&#34;StartTime&#34;: 1655370522.7554772, &#34;EndTime&#34;: 1655370522.755521, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9795195134480794, &#34;count&#34;: 1, &#34;min&#34;: 0.9795195134480794, &#34;max&#34;: 0.9795195134480794}}} #metrics {&#34;StartTime&#34;: 1655370522.75584, &#34;EndTime&#34;: 1655370522.7558835, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9728144900004069, &#34;count&#34;: 1, &#34;min&#34;: 0.9728144900004069, &#34;max&#34;: 0.9728144900004069}}} #metrics {&#34;StartTime&#34;: 1655370522.7562275, &#34;EndTime&#34;: 1655370522.75627, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9797180620829264, &#34;count&#34;: 1, &#34;min&#34;: 0.9797180620829264, &#34;max&#34;: 0.9797180620829264}}} [06/16/2022 09:08:42 INFO 139637139089216] #quality_metric: host=algo-1, epoch=12, train mse_objective &lt;loss&gt;=0.28346955882178415 #metrics {&#34;StartTime&#34;: 1655370522.8490186, &#34;EndTime&#34;: 1655370522.8491027, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.952870686848959, &#34;count&#34;: 1, &#34;min&#34;: 13.952870686848959, &#34;max&#34;: 13.952870686848959}}} #metrics {&#34;StartTime&#34;: 1655370522.8497438, &#34;EndTime&#34;: 1655370522.8497763, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.041442272709865, &#34;count&#34;: 1, &#34;min&#34;: 14.041442272709865, &#34;max&#34;: 14.041442272709865}}} #metrics {&#34;StartTime&#34;: 1655370522.8501592, &#34;EndTime&#34;: 1655370522.850207, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.387701296338848, &#34;count&#34;: 1, &#34;min&#34;: 13.387701296338848, &#34;max&#34;: 13.387701296338848}}} #metrics {&#34;StartTime&#34;: 1655370522.850549, &#34;EndTime&#34;: 1655370522.850595, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.677093505859375, &#34;count&#34;: 1, &#34;min&#34;: 13.677093505859375, &#34;max&#34;: 13.677093505859375}}} #metrics {&#34;StartTime&#34;: 1655370522.8509276, &#34;EndTime&#34;: 1655370522.8509762, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.571622362323836, &#34;count&#34;: 1, &#34;min&#34;: 14.571622362323836, &#34;max&#34;: 14.571622362323836}}} #metrics {&#34;StartTime&#34;: 1655370522.851315, &#34;EndTime&#34;: 1655370522.8513618, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.684067670036764, &#34;count&#34;: 1, &#34;min&#34;: 14.684067670036764, &#34;max&#34;: 14.684067670036764}}} #metrics {&#34;StartTime&#34;: 1655370522.8516748, &#34;EndTime&#34;: 1655370522.8517213, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.562893138212317, &#34;count&#34;: 1, &#34;min&#34;: 14.562893138212317, &#34;max&#34;: 14.562893138212317}}} #metrics {&#34;StartTime&#34;: 1655370522.8520532, &#34;EndTime&#34;: 1655370522.8520968, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.698338527305454, &#34;count&#34;: 1, &#34;min&#34;: 14.698338527305454, &#34;max&#34;: 14.698338527305454}}} #metrics {&#34;StartTime&#34;: 1655370522.8524196, &#34;EndTime&#34;: 1655370522.8524632, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.534875607958027, &#34;count&#34;: 1, &#34;min&#34;: 13.534875607958027, &#34;max&#34;: 13.534875607958027}}} #metrics {&#34;StartTime&#34;: 1655370522.852819, &#34;EndTime&#34;: 1655370522.8528612, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.85780424230239, &#34;count&#34;: 1, &#34;min&#34;: 13.85780424230239, &#34;max&#34;: 13.85780424230239}}} #metrics {&#34;StartTime&#34;: 1655370522.853187, &#34;EndTime&#34;: 1655370522.8532326, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.80215035232843, &#34;count&#34;: 1, &#34;min&#34;: 13.80215035232843, &#34;max&#34;: 13.80215035232843}}} #metrics {&#34;StartTime&#34;: 1655370522.8535538, &#34;EndTime&#34;: 1655370522.8535995, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.598674699371935, &#34;count&#34;: 1, &#34;min&#34;: 13.598674699371935, &#34;max&#34;: 13.598674699371935}}} #metrics {&#34;StartTime&#34;: 1655370522.8544183, &#34;EndTime&#34;: 1655370522.8544478, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.3717788995481, &#34;count&#34;: 1, &#34;min&#34;: 14.3717788995481, &#34;max&#34;: 14.3717788995481}}} #metrics {&#34;StartTime&#34;: 1655370522.8547835, &#34;EndTime&#34;: 1655370522.8548334, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 15.38017661898744, &#34;count&#34;: 1, &#34;min&#34;: 15.38017661898744, &#34;max&#34;: 15.38017661898744}}} #metrics {&#34;StartTime&#34;: 1655370522.855156, &#34;EndTime&#34;: 1655370522.855203, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.405591179342832, &#34;count&#34;: 1, &#34;min&#34;: 14.405591179342832, &#34;max&#34;: 14.405591179342832}}} #metrics {&#34;StartTime&#34;: 1655370522.8555677, &#34;EndTime&#34;: 1655370522.8556118, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.681273815678615, &#34;count&#34;: 1, &#34;min&#34;: 14.681273815678615, &#34;max&#34;: 14.681273815678615}}} #metrics {&#34;StartTime&#34;: 1655370522.8559337, &#34;EndTime&#34;: 1655370522.8559775, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.541234633501837, &#34;count&#34;: 1, &#34;min&#34;: 17.541234633501837, &#34;max&#34;: 17.541234633501837}}} #metrics {&#34;StartTime&#34;: 1655370522.8563218, &#34;EndTime&#34;: 1655370522.8563619, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.555366067325366, &#34;count&#34;: 1, &#34;min&#34;: 17.555366067325366, &#34;max&#34;: 17.555366067325366}}} #metrics {&#34;StartTime&#34;: 1655370522.856702, &#34;EndTime&#34;: 1655370522.8567526, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.542622285730697, &#34;count&#34;: 1, &#34;min&#34;: 17.542622285730697, &#34;max&#34;: 17.542622285730697}}} #metrics {&#34;StartTime&#34;: 1655370522.8570683, &#34;EndTime&#34;: 1655370522.857115, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.537333170572918, &#34;count&#34;: 1, &#34;min&#34;: 17.537333170572918, &#34;max&#34;: 17.537333170572918}}} #metrics {&#34;StartTime&#34;: 1655370522.8574288, &#34;EndTime&#34;: 1655370522.8574727, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.39291082643995, &#34;count&#34;: 1, &#34;min&#34;: 19.39291082643995, &#34;max&#34;: 19.39291082643995}}} #metrics {&#34;StartTime&#34;: 1655370522.85785, &#34;EndTime&#34;: 1655370522.8579016, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.05091588637408, &#34;count&#34;: 1, &#34;min&#34;: 19.05091588637408, &#34;max&#34;: 19.05091588637408}}} #metrics {&#34;StartTime&#34;: 1655370522.8582387, &#34;EndTime&#34;: 1655370522.8582804, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.31894519282322, &#34;count&#34;: 1, &#34;min&#34;: 19.31894519282322, &#34;max&#34;: 19.31894519282322}}} #metrics {&#34;StartTime&#34;: 1655370522.8586054, &#34;EndTime&#34;: 1655370522.8586512, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.117049871706495, &#34;count&#34;: 1, &#34;min&#34;: 19.117049871706495, &#34;max&#34;: 19.117049871706495}}} #metrics {&#34;StartTime&#34;: 1655370522.859465, &#34;EndTime&#34;: 1655370522.8595097, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.08127249923407, &#34;count&#34;: 1, &#34;min&#34;: 61.08127249923407, &#34;max&#34;: 61.08127249923407}}} #metrics {&#34;StartTime&#34;: 1655370522.8598611, &#34;EndTime&#34;: 1655370522.8599038, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 60.99089259727329, &#34;count&#34;: 1, &#34;min&#34;: 60.99089259727329, &#34;max&#34;: 60.99089259727329}}} #metrics {&#34;StartTime&#34;: 1655370522.860268, &#34;EndTime&#34;: 1655370522.8603106, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.41563744638481, &#34;count&#34;: 1, &#34;min&#34;: 61.41563744638481, &#34;max&#34;: 61.41563744638481}}} #metrics {&#34;StartTime&#34;: 1655370522.860659, &#34;EndTime&#34;: 1655370522.8607047, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.1947691674326, &#34;count&#34;: 1, &#34;min&#34;: 61.1947691674326, &#34;max&#34;: 61.1947691674326}}} #metrics {&#34;StartTime&#34;: 1655370522.8610227, &#34;EndTime&#34;: 1655370522.8610675, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.909871419270836, &#34;count&#34;: 1, &#34;min&#34;: 61.909871419270836, &#34;max&#34;: 61.909871419270836}}} #metrics {&#34;StartTime&#34;: 1655370522.8613935, &#34;EndTime&#34;: 1655370522.8614388, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.42349063648897, &#34;count&#34;: 1, &#34;min&#34;: 62.42349063648897, &#34;max&#34;: 62.42349063648897}}} #metrics {&#34;StartTime&#34;: 1655370522.8617764, &#34;EndTime&#34;: 1655370522.8618243, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.9844970703125, &#34;count&#34;: 1, &#34;min&#34;: 61.9844970703125, &#34;max&#34;: 61.9844970703125}}} #metrics {&#34;StartTime&#34;: 1655370522.862132, &#34;EndTime&#34;: 1655370522.862174, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.90167356004902, &#34;count&#34;: 1, &#34;min&#34;: 62.90167356004902, &#34;max&#34;: 62.90167356004902}}} [06/16/2022 09:08:42 INFO 139637139089216] #quality_metric: host=algo-1, epoch=12, validation mse_objective &lt;loss&gt;=13.952870686848959 [06/16/2022 09:08:42 INFO 139637139089216] #early_stopping_criteria_metric: host=algo-1, epoch=12, criteria=mse_objective, value=13.387701296338848 [06/16/2022 09:08:42 INFO 139637139089216] Saving model for epoch: 12 [06/16/2022 09:08:42 INFO 139637139089216] Saved checkpoint to &#34;/tmp/tmp34kxp_1v/mx-mod-0000.params&#34; [06/16/2022 09:08:42 INFO 139637139089216] #progress_metric: host=algo-1, completed 86.66666666666667 % of epochs #metrics {&#34;StartTime&#34;: 1655370521.9340303, &#34;EndTime&#34;: 1655370522.8723795, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 12, &#34;Meta&#34;: &#34;training_data_iter&#34;}, &#34;Metrics&#34;: {&#34;Total Records Seen&#34;: {&#34;sum&#34;: 6400.0, &#34;count&#34;: 1, &#34;min&#34;: 6400, &#34;max&#34;: 6400}, &#34;Total Batches Seen&#34;: {&#34;sum&#34;: 225.0, &#34;count&#34;: 1, &#34;min&#34;: 225, &#34;max&#34;: 225}, &#34;Max Records Seen Between Resets&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Max Batches Seen Between Resets&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}, &#34;Reset Count&#34;: {&#34;sum&#34;: 15.0, &#34;count&#34;: 1, &#34;min&#34;: 15, &#34;max&#34;: 15}, &#34;Number of Records Since Last Reset&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Number of Batches Since Last Reset&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}}} [06/16/2022 09:08:42 INFO 139637139089216] #throughput_metric: host=algo-1, train throughput=484.65986594961834 records/second #metrics {&#34;StartTime&#34;: 1655370523.309051, &#34;EndTime&#34;: 1655370523.3091316, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2821601152420044, &#34;count&#34;: 1, &#34;min&#34;: 0.2821601152420044, &#34;max&#34;: 0.2821601152420044}}} #metrics {&#34;StartTime&#34;: 1655370523.3092227, &#34;EndTime&#34;: 1655370523.3095124, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.28304072274102104, &#34;count&#34;: 1, &#34;min&#34;: 0.28304072274102104, &#34;max&#34;: 0.28304072274102104}}} #metrics {&#34;StartTime&#34;: 1655370523.3096056, &#34;EndTime&#34;: 1655370523.3098087, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.281617022090488, &#34;count&#34;: 1, &#34;min&#34;: 0.281617022090488, &#34;max&#34;: 0.281617022090488}}} #metrics {&#34;StartTime&#34;: 1655370523.3098888, &#34;EndTime&#34;: 1655370523.3099086, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.28118344730801054, &#34;count&#34;: 1, &#34;min&#34;: 0.28118344730801054, &#34;max&#34;: 0.28118344730801054}}} #metrics {&#34;StartTime&#34;: 1655370523.3099592, &#34;EndTime&#34;: 1655370523.3099697, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.300567405488756, &#34;count&#34;: 1, &#34;min&#34;: 0.300567405488756, &#34;max&#34;: 0.300567405488756}}} #metrics {&#34;StartTime&#34;: 1655370523.3100107, &#34;EndTime&#34;: 1655370523.3100214, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3058248339758979, &#34;count&#34;: 1, &#34;min&#34;: 0.3058248339758979, &#34;max&#34;: 0.3058248339758979}}} #metrics {&#34;StartTime&#34;: 1655370523.3100584, &#34;EndTime&#34;: 1655370523.3100681, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2997693220774333, &#34;count&#34;: 1, &#34;min&#34;: 0.2997693220774333, &#34;max&#34;: 0.2997693220774333}}} #metrics {&#34;StartTime&#34;: 1655370523.310537, &#34;EndTime&#34;: 1655370523.3105624, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3058673487769233, &#34;count&#34;: 1, &#34;min&#34;: 0.3058673487769233, &#34;max&#34;: 0.3058673487769233}}} #metrics {&#34;StartTime&#34;: 1655370523.3107874, &#34;EndTime&#34;: 1655370523.3108113, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2809463940726386, &#34;count&#34;: 1, &#34;min&#34;: 0.2809463940726386, &#34;max&#34;: 0.2809463940726386}}} #metrics {&#34;StartTime&#34;: 1655370523.3108766, &#34;EndTime&#34;: 1655370523.3110325, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2791322316063775, &#34;count&#34;: 1, &#34;min&#34;: 0.2791322316063775, &#34;max&#34;: 0.2791322316063775}}} #metrics {&#34;StartTime&#34;: 1655370523.311095, &#34;EndTime&#34;: 1655370523.3112502, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2839924584494697, &#34;count&#34;: 1, &#34;min&#34;: 0.2839924584494697, &#34;max&#34;: 0.2839924584494697}}} #metrics {&#34;StartTime&#34;: 1655370523.311314, &#34;EndTime&#34;: 1655370523.3115017, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.28104520426856144, &#34;count&#34;: 1, &#34;min&#34;: 0.28104520426856144, &#34;max&#34;: 0.28104520426856144}}} #metrics {&#34;StartTime&#34;: 1655370523.3115702, &#34;EndTime&#34;: 1655370523.3117313, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2964671399858263, &#34;count&#34;: 1, &#34;min&#34;: 0.2964671399858263, &#34;max&#34;: 0.2964671399858263}}} #metrics {&#34;StartTime&#34;: 1655370523.3117957, &#34;EndTime&#34;: 1655370523.3119678, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3177454800075955, &#34;count&#34;: 1, &#34;min&#34;: 0.3177454800075955, &#34;max&#34;: 0.3177454800075955}}} #metrics {&#34;StartTime&#34;: 1655370523.3121834, &#34;EndTime&#34;: 1655370523.3122082, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.2956636407640245, &#34;count&#34;: 1, &#34;min&#34;: 0.2956636407640245, &#34;max&#34;: 0.2956636407640245}}} #metrics {&#34;StartTime&#34;: 1655370523.3122656, &#34;EndTime&#34;: 1655370523.3124177, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.30609261088901096, &#34;count&#34;: 1, &#34;min&#34;: 0.30609261088901096, &#34;max&#34;: 0.30609261088901096}}} #metrics {&#34;StartTime&#34;: 1655370523.3124983, &#34;EndTime&#34;: 1655370523.3125155, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.37971383624606664, &#34;count&#34;: 1, &#34;min&#34;: 0.37971383624606664, &#34;max&#34;: 0.37971383624606664}}} #metrics {&#34;StartTime&#34;: 1655370523.3125606, &#34;EndTime&#34;: 1655370523.3125713, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.379945953157213, &#34;count&#34;: 1, &#34;min&#34;: 0.379945953157213, &#34;max&#34;: 0.379945953157213}}} #metrics {&#34;StartTime&#34;: 1655370523.3126125, &#34;EndTime&#34;: 1655370523.3126242, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3797486570146349, &#34;count&#34;: 1, &#34;min&#34;: 0.3797486570146349, &#34;max&#34;: 0.3797486570146349}}} #metrics {&#34;StartTime&#34;: 1655370523.3126614, &#34;EndTime&#34;: 1655370523.312672, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.3800104278988308, &#34;count&#34;: 1, &#34;min&#34;: 0.3800104278988308, &#34;max&#34;: 0.3800104278988308}}} #metrics {&#34;StartTime&#34;: 1655370523.3129928, &#34;EndTime&#34;: 1655370523.313109, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.40137112193637425, &#34;count&#34;: 1, &#34;min&#34;: 0.40137112193637425, &#34;max&#34;: 0.40137112193637425}}} #metrics {&#34;StartTime&#34;: 1655370523.3131723, &#34;EndTime&#34;: 1655370523.3131878, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4200400214725071, &#34;count&#34;: 1, &#34;min&#34;: 0.4200400214725071, &#34;max&#34;: 0.4200400214725071}}} #metrics {&#34;StartTime&#34;: 1655370523.3135312, &#34;EndTime&#34;: 1655370523.3135555, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4054637707604302, &#34;count&#34;: 1, &#34;min&#34;: 0.4054637707604302, &#34;max&#34;: 0.4054637707604302}}} #metrics {&#34;StartTime&#34;: 1655370523.313618, &#34;EndTime&#34;: 1655370523.3137882, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.4123759068383111, &#34;count&#34;: 1, &#34;min&#34;: 0.4123759068383111, &#34;max&#34;: 0.4123759068383111}}} #metrics {&#34;StartTime&#34;: 1655370523.3138504, &#34;EndTime&#34;: 1655370523.314, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9560433302985297, &#34;count&#34;: 1, &#34;min&#34;: 0.9560433302985297, &#34;max&#34;: 0.9560433302985297}}} #metrics {&#34;StartTime&#34;: 1655370523.3140628, &#34;EndTime&#34;: 1655370523.3142183, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9565350829230415, &#34;count&#34;: 1, &#34;min&#34;: 0.9565350829230415, &#34;max&#34;: 0.9565350829230415}}} #metrics {&#34;StartTime&#34;: 1655370523.3144197, &#34;EndTime&#34;: 1655370523.314445, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9556538920932346, &#34;count&#34;: 1, &#34;min&#34;: 0.9556538920932346, &#34;max&#34;: 0.9556538920932346}}} #metrics {&#34;StartTime&#34;: 1655370523.3145072, &#34;EndTime&#34;: 1655370523.3146608, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.956398237016466, &#34;count&#34;: 1, &#34;min&#34;: 0.956398237016466, &#34;max&#34;: 0.956398237016466}}} #metrics {&#34;StartTime&#34;: 1655370523.3147306, &#34;EndTime&#34;: 1655370523.3148854, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9698759672376844, &#34;count&#34;: 1, &#34;min&#34;: 0.9698759672376844, &#34;max&#34;: 0.9698759672376844}}} #metrics {&#34;StartTime&#34;: 1655370523.314955, &#34;EndTime&#34;: 1655370523.315116, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9803103680080838, &#34;count&#34;: 1, &#34;min&#34;: 0.9803103680080838, &#34;max&#34;: 0.9803103680080838}}} #metrics {&#34;StartTime&#34;: 1655370523.3151734, &#34;EndTime&#34;: 1655370523.315281, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9728766738043891, &#34;count&#34;: 1, &#34;min&#34;: 0.9728766738043891, &#34;max&#34;: 0.9728766738043891}}} #metrics {&#34;StartTime&#34;: 1655370523.3153994, &#34;EndTime&#34;: 1655370523.3154194, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;train_mse_objective&#34;: {&#34;sum&#34;: 0.9802551205952962, &#34;count&#34;: 1, &#34;min&#34;: 0.9802551205952962, &#34;max&#34;: 0.9802551205952962}}} [06/16/2022 09:08:43 INFO 139637139089216] #quality_metric: host=algo-1, epoch=13, train mse_objective &lt;loss&gt;=0.2821601152420044 #metrics {&#34;StartTime&#34;: 1655370523.3924441, &#34;EndTime&#34;: 1655370523.3924918, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 0}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.984094357958027, &#34;count&#34;: 1, &#34;min&#34;: 13.984094357958027, &#34;max&#34;: 13.984094357958027}}} #metrics {&#34;StartTime&#34;: 1655370523.392559, &#34;EndTime&#34;: 1655370523.3928542, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 1}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.058059991574755, &#34;count&#34;: 1, &#34;min&#34;: 14.058059991574755, &#34;max&#34;: 14.058059991574755}}} #metrics {&#34;StartTime&#34;: 1655370523.3929365, &#34;EndTime&#34;: 1655370523.3929565, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 2}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.450200697954964, &#34;count&#34;: 1, &#34;min&#34;: 13.450200697954964, &#34;max&#34;: 13.450200697954964}}} #metrics {&#34;StartTime&#34;: 1655370523.3931112, &#34;EndTime&#34;: 1655370523.3931284, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 3}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.753068512561274, &#34;count&#34;: 1, &#34;min&#34;: 13.753068512561274, &#34;max&#34;: 13.753068512561274}}} #metrics {&#34;StartTime&#34;: 1655370523.3931706, &#34;EndTime&#34;: 1655370523.3931825, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 4}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.57038490445006, &#34;count&#34;: 1, &#34;min&#34;: 14.57038490445006, &#34;max&#34;: 14.57038490445006}}} #metrics {&#34;StartTime&#34;: 1655370523.3932192, &#34;EndTime&#34;: 1655370523.3932292, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 5}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.710671219171262, &#34;count&#34;: 1, &#34;min&#34;: 14.710671219171262, &#34;max&#34;: 14.710671219171262}}} #metrics {&#34;StartTime&#34;: 1655370523.3934238, &#34;EndTime&#34;: 1655370523.393508, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 6}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.560327866498161, &#34;count&#34;: 1, &#34;min&#34;: 14.560327866498161, &#34;max&#34;: 14.560327866498161}}} #metrics {&#34;StartTime&#34;: 1655370523.3935683, &#34;EndTime&#34;: 1655370523.3936737, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 7}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.722483317057291, &#34;count&#34;: 1, &#34;min&#34;: 14.722483317057291, &#34;max&#34;: 14.722483317057291}}} #metrics {&#34;StartTime&#34;: 1655370523.3937376, &#34;EndTime&#34;: 1655370523.3938339, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 8}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.583147235945159, &#34;count&#34;: 1, &#34;min&#34;: 13.583147235945159, &#34;max&#34;: 13.583147235945159}}} #metrics {&#34;StartTime&#34;: 1655370523.3938925, &#34;EndTime&#34;: 1655370523.393978, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 9}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.884679457720589, &#34;count&#34;: 1, &#34;min&#34;: 13.884679457720589, &#34;max&#34;: 13.884679457720589}}} #metrics {&#34;StartTime&#34;: 1655370523.3940337, &#34;EndTime&#34;: 1655370523.3941226, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 10}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.834466672411152, &#34;count&#34;: 1, &#34;min&#34;: 13.834466672411152, &#34;max&#34;: 13.834466672411152}}} #metrics {&#34;StartTime&#34;: 1655370523.3941772, &#34;EndTime&#34;: 1655370523.3942597, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 11}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 13.663813572303921, &#34;count&#34;: 1, &#34;min&#34;: 13.663813572303921, &#34;max&#34;: 13.663813572303921}}} #metrics {&#34;StartTime&#34;: 1655370523.39432, &#34;EndTime&#34;: 1655370523.3944044, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 12}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.375144808900123, &#34;count&#34;: 1, &#34;min&#34;: 14.375144808900123, &#34;max&#34;: 14.375144808900123}}} #metrics {&#34;StartTime&#34;: 1655370523.3944583, &#34;EndTime&#34;: 1655370523.3945484, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 13}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 15.29500923904718, &#34;count&#34;: 1, &#34;min&#34;: 15.29500923904718, &#34;max&#34;: 15.29500923904718}}} #metrics {&#34;StartTime&#34;: 1655370523.394603, &#34;EndTime&#34;: 1655370523.3946898, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 14}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.410013236251531, &#34;count&#34;: 1, &#34;min&#34;: 14.410013236251531, &#34;max&#34;: 14.410013236251531}}} #metrics {&#34;StartTime&#34;: 1655370523.3947444, &#34;EndTime&#34;: 1655370523.3948307, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 15}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 14.709462483723959, &#34;count&#34;: 1, &#34;min&#34;: 14.709462483723959, &#34;max&#34;: 14.709462483723959}}} #metrics {&#34;StartTime&#34;: 1655370523.3948846, &#34;EndTime&#34;: 1655370523.3949654, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 16}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.542254279641543, &#34;count&#34;: 1, &#34;min&#34;: 17.542254279641543, &#34;max&#34;: 17.542254279641543}}} #metrics {&#34;StartTime&#34;: 1655370523.3950195, &#34;EndTime&#34;: 1655370523.3951025, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 17}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.55028638652727, &#34;count&#34;: 1, &#34;min&#34;: 17.55028638652727, &#34;max&#34;: 17.55028638652727}}} #metrics {&#34;StartTime&#34;: 1655370523.3951552, &#34;EndTime&#34;: 1655370523.395237, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 18}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.541951497395832, &#34;count&#34;: 1, &#34;min&#34;: 17.541951497395832, &#34;max&#34;: 17.541951497395832}}} #metrics {&#34;StartTime&#34;: 1655370523.3952894, &#34;EndTime&#34;: 1655370523.3953753, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 19}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 17.536605535768995, &#34;count&#34;: 1, &#34;min&#34;: 17.536605535768995, &#34;max&#34;: 17.536605535768995}}} #metrics {&#34;StartTime&#34;: 1655370523.3954294, &#34;EndTime&#34;: 1655370523.3955135, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 20}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.408946317784928, &#34;count&#34;: 1, &#34;min&#34;: 19.408946317784928, &#34;max&#34;: 19.408946317784928}}} #metrics {&#34;StartTime&#34;: 1655370523.3955681, &#34;EndTime&#34;: 1655370523.3956475, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 21}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.091358259612438, &#34;count&#34;: 1, &#34;min&#34;: 19.091358259612438, &#34;max&#34;: 19.091358259612438}}} #metrics {&#34;StartTime&#34;: 1655370523.3956995, &#34;EndTime&#34;: 1655370523.3957841, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 22}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.353264303768384, &#34;count&#34;: 1, &#34;min&#34;: 19.353264303768384, &#34;max&#34;: 19.353264303768384}}} #metrics {&#34;StartTime&#34;: 1655370523.3958383, &#34;EndTime&#34;: 1655370523.395919, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 23}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 19.1146635167739, &#34;count&#34;: 1, &#34;min&#34;: 19.1146635167739, &#34;max&#34;: 19.1146635167739}}} #metrics {&#34;StartTime&#34;: 1655370523.3959725, &#34;EndTime&#34;: 1655370523.3960545, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 24}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.06577914368872, &#34;count&#34;: 1, &#34;min&#34;: 61.06577914368872, &#34;max&#34;: 61.06577914368872}}} #metrics {&#34;StartTime&#34;: 1655370523.3961074, &#34;EndTime&#34;: 1655370523.3962035, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 25}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 60.98327158011642, &#34;count&#34;: 1, &#34;min&#34;: 60.98327158011642, &#34;max&#34;: 60.98327158011642}}} #metrics {&#34;StartTime&#34;: 1655370523.396259, &#34;EndTime&#34;: 1655370523.3963413, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 26}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.40277458639706, &#34;count&#34;: 1, &#34;min&#34;: 61.40277458639706, &#34;max&#34;: 61.40277458639706}}} #metrics {&#34;StartTime&#34;: 1655370523.3963935, &#34;EndTime&#34;: 1655370523.3964741, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 27}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.177875593596816, &#34;count&#34;: 1, &#34;min&#34;: 61.177875593596816, &#34;max&#34;: 61.177875593596816}}} #metrics {&#34;StartTime&#34;: 1655370523.3965266, &#34;EndTime&#34;: 1655370523.396607, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 28}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.88241277956495, &#34;count&#34;: 1, &#34;min&#34;: 61.88241277956495, &#34;max&#34;: 61.88241277956495}}} #metrics {&#34;StartTime&#34;: 1655370523.3966587, &#34;EndTime&#34;: 1655370523.3967392, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 29}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.56720329733456, &#34;count&#34;: 1, &#34;min&#34;: 62.56720329733456, &#34;max&#34;: 62.56720329733456}}} #metrics {&#34;StartTime&#34;: 1655370523.396791, &#34;EndTime&#34;: 1655370523.3968744, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 30}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 61.944091796875, &#34;count&#34;: 1, &#34;min&#34;: 61.944091796875, &#34;max&#34;: 61.944091796875}}} #metrics {&#34;StartTime&#34;: 1655370523.396927, &#34;EndTime&#34;: 1655370523.397007, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;model&#34;: 31}, &#34;Metrics&#34;: {&#34;validation_mse_objective&#34;: {&#34;sum&#34;: 62.8279478783701, &#34;count&#34;: 1, &#34;min&#34;: 62.8279478783701, &#34;max&#34;: 62.8279478783701}}} [06/16/2022 09:08:43 INFO 139637139089216] #quality_metric: host=algo-1, epoch=13, validation mse_objective &lt;loss&gt;=13.984094357958027 [06/16/2022 09:08:43 INFO 139637139089216] #early_stopping_criteria_metric: host=algo-1, epoch=13, criteria=mse_objective, value=13.450200697954964 [06/16/2022 09:08:43 INFO 139637139089216] Saving model for epoch: 13 [06/16/2022 09:08:43 INFO 139637139089216] Saved checkpoint to &#34;/tmp/tmp5rbsw0hu/mx-mod-0000.params&#34; [06/16/2022 09:08:43 INFO 139637139089216] Early stop condition met. Stopping training. [06/16/2022 09:08:43 INFO 139637139089216] #progress_metric: host=algo-1, completed 100 % epochs #metrics {&#34;StartTime&#34;: 1655370522.8730938, &#34;EndTime&#34;: 1655370523.4062154, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;, &#34;epoch&#34;: 13, &#34;Meta&#34;: &#34;training_data_iter&#34;}, &#34;Metrics&#34;: {&#34;Total Records Seen&#34;: {&#34;sum&#34;: 6855.0, &#34;count&#34;: 1, &#34;min&#34;: 6855, &#34;max&#34;: 6855}, &#34;Total Batches Seen&#34;: {&#34;sum&#34;: 241.0, &#34;count&#34;: 1, &#34;min&#34;: 241, &#34;max&#34;: 241}, &#34;Max Records Seen Between Resets&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Max Batches Seen Between Resets&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}, &#34;Reset Count&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}, &#34;Number of Records Since Last Reset&#34;: {&#34;sum&#34;: 455.0, &#34;count&#34;: 1, &#34;min&#34;: 455, &#34;max&#34;: 455}, &#34;Number of Batches Since Last Reset&#34;: {&#34;sum&#34;: 16.0, &#34;count&#34;: 1, &#34;min&#34;: 16, &#34;max&#34;: 16}}} [06/16/2022 09:08:43 INFO 139637139089216] #throughput_metric: host=algo-1, train throughput=853.2635130392172 records/second [06/16/2022 09:08:43 WARNING 139637139089216] wait_for_all_workers will not sync workers since the kv store is not running distributed [06/16/2022 09:08:43 WARNING 139637139089216] wait_for_all_workers will not sync workers since the kv store is not running distributed [06/16/2022 09:08:43 INFO 139637139089216] #early_stopping_criteria_metric: host=algo-1, epoch=13, criteria=mse_objective, value=13.450200697954964 [06/16/2022 09:08:43 INFO 139637139089216] #validation_score (algo-1) : (&#39;mse_objective&#39;, 12.65540687710631) [06/16/2022 09:08:43 INFO 139637139089216] #validation_score (algo-1) : (&#39;mse&#39;, 12.65540687710631) [06/16/2022 09:08:43 INFO 139637139089216] #validation_score (algo-1) : (&#39;absolute_loss&#39;, 2.5288796518363204) [06/16/2022 09:08:43 INFO 139637139089216] #validation_score (algo-1) : (&#39;rmse&#39;, 3.557443868440697) [06/16/2022 09:08:43 INFO 139637139089216] #validation_score (algo-1) : (&#39;r2&#39;, 0.7973001536506935) [06/16/2022 09:08:43 INFO 139637139089216] #validation_score (algo-1) : (&#39;mae&#39;, 2.5288796144373276) [06/16/2022 09:08:43 INFO 139637139089216] #quality_metric: host=algo-1, validation mse_objective &lt;loss&gt;=12.65540687710631 [06/16/2022 09:08:43 INFO 139637139089216] #quality_metric: host=algo-1, validation mse &lt;loss&gt;=12.65540687710631 [06/16/2022 09:08:43 INFO 139637139089216] #quality_metric: host=algo-1, validation absolute_loss &lt;loss&gt;=2.5288796518363204 [06/16/2022 09:08:43 INFO 139637139089216] #quality_metric: host=algo-1, validation rmse &lt;loss&gt;=3.557443868440697 [06/16/2022 09:08:43 INFO 139637139089216] #quality_metric: host=algo-1, validation r2 &lt;loss&gt;=0.7973001536506935 [06/16/2022 09:08:43 INFO 139637139089216] #quality_metric: host=algo-1, validation mae &lt;loss&gt;=2.5288796144373276 [06/16/2022 09:08:43 INFO 139637139089216] Best model found for hyperparameters: {&#34;optimizer&#34;: &#34;adam&#34;, &#34;learning_rate&#34;: 0.1, &#34;wd&#34;: 0.0001, &#34;l1&#34;: 0.0, &#34;lr_scheduler_step&#34;: 10, &#34;lr_scheduler_factor&#34;: 0.99, &#34;lr_scheduler_minimum_lr&#34;: 1e-05} [06/16/2022 09:08:43 INFO 139637139089216] Saved checkpoint to &#34;/tmp/tmpx0uodk2f/mx-mod-0000.params&#34; [06/16/2022 09:08:43 INFO 139637139089216] Test data is not provided. #metrics {&#34;StartTime&#34;: 1655370513.1834052, &#34;EndTime&#34;: 1655370523.4940147, &#34;Dimensions&#34;: {&#34;Algorithm&#34;: &#34;Linear Learner&#34;, &#34;Host&#34;: &#34;algo-1&#34;, &#34;Operation&#34;: &#34;training&#34;}, &#34;Metrics&#34;: {&#34;initialize.time&#34;: {&#34;sum&#34;: 421.53167724609375, &#34;count&#34;: 1, &#34;min&#34;: 421.53167724609375, &#34;max&#34;: 421.53167724609375}, &#34;epochs&#34;: {&#34;sum&#34;: 15.0, &#34;count&#34;: 1, &#34;min&#34;: 15, &#34;max&#34;: 15}, &#34;check_early_stopping.time&#34;: {&#34;sum&#34;: 10.078668594360352, &#34;count&#34;: 15, &#34;min&#34;: 0.22172927856445312, &#34;max&#34;: 1.9156932830810547}, &#34;update.time&#34;: {&#34;sum&#34;: 9697.13544845581, &#34;count&#34;: 14, &#34;min&#34;: 527.1692276000977, &#34;max&#34;: 934.0167045593262}, &#34;finalize.time&#34;: {&#34;sum&#34;: 78.68623733520508, &#34;count&#34;: 1, &#34;min&#34;: 78.68623733520508, &#34;max&#34;: 78.68623733520508}, &#34;setuptime&#34;: {&#34;sum&#34;: 43.54071617126465, &#34;count&#34;: 1, &#34;min&#34;: 43.54071617126465, &#34;max&#34;: 43.54071617126465}, &#34;totaltime&#34;: {&#34;sum&#34;: 10659.562349319458, &#34;count&#34;: 1, &#34;min&#34;: 10659.562349319458, &#34;max&#34;: 10659.562349319458}}} 2022-06-16 09:08:50 Uploading - Uploading generated training model 2022-06-16 09:09:24 Completed - Training job completed Training seconds: 132 Billable seconds: 132 . . Great! The training is done but it has also generated a lot of logs in the output. These logs are also available in the CloudWatch and if you want you can disable them from the fit function using logs=&#39;None&#39; parameter. Let&#39;s try to analyze what information is presented in these logs. . The first part of these logs is related to infrastructure provisioning, downloading the training container, the data, and starting the training run. . 2022-06-15 10:19:35 Starting - Starting the training job... 2022-06-15 10:19:51 Starting - Preparing the instances for trainingProfilerReport-1655288375: InProgress ...... 2022-06-15 10:21:04 Downloading - Downloading input data...... 2022-06-15 10:21:50 Training - Downloading the training image... 2022-06-15 10:22:35 Training - Training image download completed. Training in progress... . The second part of these logs is related to training performance metrics. The majority of the logs belong to this part. . [06/15/2022 10:22:53 INFO 140271381174080] #validation_score (algo-1) : (&#39;mse_objective&#39;, 12.65540687710631) [06/15/2022 10:22:53 INFO 140271381174080] #validation_score (algo-1) : (&#39;mse&#39;, 12.65540687710631) [06/15/2022 10:22:53 INFO 140271381174080] #validation_score (algo-1) : (&#39;absolute_loss&#39;, 2.5288796518363204) [06/15/2022 10:22:53 INFO 140271381174080] #validation_score (algo-1) : (&#39;rmse&#39;, 3.557443868440697) [06/15/2022 10:22:53 INFO 140271381174080] #validation_score (algo-1) : (&#39;r2&#39;, 0.7973001536506935) [06/15/2022 10:22:53 INFO 140271381174080] #validation_score (algo-1) : (&#39;mae&#39;, 2.5288796144373276) . The third and final part is telling us about the Training job status and billable time in seconds. . 2022-06-16 09:08:50 Uploading - Uploading generated training model 2022-06-16 09:09:24 Completed - Training job completed Training seconds: 132 Billable seconds: 132 . In our case, the billable seconds are 132. This billable time is for the ml.m5.large instance that we have configured for our training run. To find the billable amount we first need to find the price rate for our selected machine. For this go to the SageMaker pricing link https://aws.amazon.com/sagemaker/pricing/ and select On Demand Pricing. From the given tabs click on the Training tab. Select your region US East (N. Virginia). This will show you the pricing of different training instances. This page can also be used to find the available training instance types in your region. From this page, we can find that the price for our instance type ml.m5.large is $0.115. This price rate is per hour so we need to convert our billable time to per hour before multiplying it. i.e. (132/3600) * 0.115 = $0.0042 which is less than a penny for our training job. . We can also use Estimator object to get important information related to our training run. . # to get the output path where the trained model artifacts are stored ll_estimator.output_path . &#39;s3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/output/&#39; . # to get the training job name ll_estimator.latest_training_job.job_name . &#39;linear-learner-2022-06-16-09-04-57-576&#39; . # to get the trained model location ll_estimator.model_data . &#39;s3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/output/linear-learner-2022-06-16-09-04-57-576/output/model.tar.gz&#39; . # to get all the configuration information for our training run ll_estimator.latest_training_job.describe() . {&#39;TrainingJobName&#39;: &#39;linear-learner-2022-06-16-09-04-57-576&#39;, &#39;TrainingJobArn&#39;: &#39;arn:aws:sagemaker:us-east-1:801598032724:training-job/linear-learner-2022-06-16-09-04-57-576&#39;, &#39;ModelArtifacts&#39;: {&#39;S3ModelArtifacts&#39;: &#39;s3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/output/linear-learner-2022-06-16-09-04-57-576/output/model.tar.gz&#39;}, &#39;TrainingJobStatus&#39;: &#39;Completed&#39;, &#39;SecondaryStatus&#39;: &#39;Completed&#39;, &#39;HyperParameters&#39;: {&#39;mini_batch_size&#39;: &#39;30&#39;, &#39;predictor_type&#39;: &#39;regressor&#39;}, &#39;AlgorithmSpecification&#39;: {&#39;TrainingImage&#39;: &#39;382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:1&#39;, &#39;TrainingInputMode&#39;: &#39;File&#39;, &#39;MetricDefinitions&#39;: [{&#39;Name&#39;: &#39;train:progress&#39;, &#39;Regex&#39;: &#39;#progress_metric: host= S+, completed ( S+) %&#39;}, {&#39;Name&#39;: &#39;validation:mae&#39;, &#39;Regex&#39;: &#39;#quality_metric: host= S+, validation mae &lt;loss&gt;=( S+)&#39;}, {&#39;Name&#39;: &#39;train:objective_loss&#39;, &#39;Regex&#39;: &#39;#quality_metric: host= S+, epoch= S+, train S+_objective &lt;loss&gt;=( S+)&#39;}, {&#39;Name&#39;: &#39;test:objective_loss&#39;, &#39;Regex&#39;: &#39;#quality_metric: host= S+, test S+_objective &lt;loss&gt;=( S+)&#39;}, {&#39;Name&#39;: &#39;test:mse&#39;, &#39;Regex&#39;: &#39;#quality_metric: host= S+, test mse &lt;loss&gt;=( S+)&#39;}, {&#39;Name&#39;: &#39;validation:objective_loss&#39;, &#39;Regex&#39;: &#39;#quality_metric: host= S+, epoch= S+, validation S+_objective &lt;loss&gt;=( S+)&#39;}, {&#39;Name&#39;: &#39;validation:objective_loss:final&#39;, &#39;Regex&#39;: &#39;#quality_metric: host= S+, validation S+_objective &lt;loss&gt;=( S+)&#39;}, {&#39;Name&#39;: &#39;validation:rmse&#39;, &#39;Regex&#39;: &#39;#quality_metric: host= S+, validation rmse &lt;loss&gt;=( S+)&#39;}, {&#39;Name&#39;: &#39;test:absolute_loss&#39;, &#39;Regex&#39;: &#39;#quality_metric: host= S+, test absolute_loss &lt;loss&gt;=( S+)&#39;}, {&#39;Name&#39;: &#39;validation:mse&#39;, &#39;Regex&#39;: &#39;#quality_metric: host= S+, validation mse &lt;loss&gt;=( S+)&#39;}, {&#39;Name&#39;: &#39;train:mse&#39;, &#39;Regex&#39;: &#39;#quality_metric: host= S+, train mse &lt;loss&gt;=( S+)&#39;}, {&#39;Name&#39;: &#39;train:absolute_loss&#39;, &#39;Regex&#39;: &#39;#quality_metric: host= S+, train absolute_loss &lt;loss&gt;=( S+)&#39;}, {&#39;Name&#39;: &#39;train:objective_loss:final&#39;, &#39;Regex&#39;: &#39;#quality_metric: host= S+, train S+_objective &lt;loss&gt;=( S+)&#39;}, {&#39;Name&#39;: &#39;validation:r2&#39;, &#39;Regex&#39;: &#39;#quality_metric: host= S+, validation r2 &lt;loss&gt;=( S+)&#39;}, {&#39;Name&#39;: &#39;train:throughput&#39;, &#39;Regex&#39;: &#39;#throughput_metric: host= S+, train throughput=( S+) records/second&#39;}, {&#39;Name&#39;: &#39;validation:absolute_loss&#39;, &#39;Regex&#39;: &#39;#quality_metric: host= S+, validation absolute_loss &lt;loss&gt;=( S+)&#39;}], &#39;EnableSageMakerMetricsTimeSeries&#39;: False}, &#39;RoleArn&#39;: &#39;arn:aws:iam::801598032724:role/service-role/AmazonSageMaker-ExecutionRole-20220516T161743&#39;, &#39;InputDataConfig&#39;: [{&#39;ChannelName&#39;: &#39;train&#39;, &#39;DataSource&#39;: {&#39;S3DataSource&#39;: {&#39;S3DataType&#39;: &#39;S3Prefix&#39;, &#39;S3Uri&#39;: &#39;s3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/input/training/training_data.csv&#39;, &#39;S3DataDistributionType&#39;: &#39;FullyReplicated&#39;}}, &#39;ContentType&#39;: &#39;text/csv&#39;, &#39;CompressionType&#39;: &#39;None&#39;, &#39;RecordWrapperType&#39;: &#39;None&#39;}, {&#39;ChannelName&#39;: &#39;validation&#39;, &#39;DataSource&#39;: {&#39;S3DataSource&#39;: {&#39;S3DataType&#39;: &#39;S3Prefix&#39;, &#39;S3Uri&#39;: &#39;s3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/input/validation_data/validation_data.csv&#39;, &#39;S3DataDistributionType&#39;: &#39;FullyReplicated&#39;}}, &#39;ContentType&#39;: &#39;text/csv&#39;, &#39;CompressionType&#39;: &#39;None&#39;, &#39;RecordWrapperType&#39;: &#39;None&#39;}], &#39;OutputDataConfig&#39;: {&#39;KmsKeyId&#39;: &#39;&#39;, &#39;S3OutputPath&#39;: &#39;s3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/output/&#39;}, &#39;ResourceConfig&#39;: {&#39;InstanceType&#39;: &#39;ml.m5.large&#39;, &#39;InstanceCount&#39;: 1, &#39;VolumeSizeInGB&#39;: 30}, &#39;StoppingCondition&#39;: {&#39;MaxRuntimeInSeconds&#39;: 86400}, &#39;CreationTime&#39;: datetime.datetime(2022, 6, 16, 9, 4, 57, 790000, tzinfo=tzlocal()), &#39;TrainingStartTime&#39;: datetime.datetime(2022, 6, 16, 9, 6, 48, 358000, tzinfo=tzlocal()), &#39;TrainingEndTime&#39;: datetime.datetime(2022, 6, 16, 9, 9, 0, 598000, tzinfo=tzlocal()), &#39;LastModifiedTime&#39;: datetime.datetime(2022, 6, 16, 9, 9, 45, 67000, tzinfo=tzlocal()), &#39;SecondaryStatusTransitions&#39;: [{&#39;Status&#39;: &#39;Starting&#39;, &#39;StartTime&#39;: datetime.datetime(2022, 6, 16, 9, 4, 57, 790000, tzinfo=tzlocal()), &#39;EndTime&#39;: datetime.datetime(2022, 6, 16, 9, 6, 48, 358000, tzinfo=tzlocal()), &#39;StatusMessage&#39;: &#39;Preparing the instances for training&#39;}, {&#39;Status&#39;: &#39;Downloading&#39;, &#39;StartTime&#39;: datetime.datetime(2022, 6, 16, 9, 6, 48, 358000, tzinfo=tzlocal()), &#39;EndTime&#39;: datetime.datetime(2022, 6, 16, 9, 7, 44, 44000, tzinfo=tzlocal()), &#39;StatusMessage&#39;: &#39;Downloading input data&#39;}, {&#39;Status&#39;: &#39;Training&#39;, &#39;StartTime&#39;: datetime.datetime(2022, 6, 16, 9, 7, 44, 44000, tzinfo=tzlocal()), &#39;EndTime&#39;: datetime.datetime(2022, 6, 16, 9, 8, 50, 28000, tzinfo=tzlocal()), &#39;StatusMessage&#39;: &#39;Training image download completed. Training in progress.&#39;}, {&#39;Status&#39;: &#39;Uploading&#39;, &#39;StartTime&#39;: datetime.datetime(2022, 6, 16, 9, 8, 50, 28000, tzinfo=tzlocal()), &#39;EndTime&#39;: datetime.datetime(2022, 6, 16, 9, 9, 0, 598000, tzinfo=tzlocal()), &#39;StatusMessage&#39;: &#39;Uploading generated training model&#39;}, {&#39;Status&#39;: &#39;Completed&#39;, &#39;StartTime&#39;: datetime.datetime(2022, 6, 16, 9, 9, 0, 598000, tzinfo=tzlocal()), &#39;EndTime&#39;: datetime.datetime(2022, 6, 16, 9, 9, 0, 598000, tzinfo=tzlocal()), &#39;StatusMessage&#39;: &#39;Training job completed&#39;}], &#39;FinalMetricDataList&#39;: [{&#39;MetricName&#39;: &#39;train:progress&#39;, &#39;Value&#39;: 100.0, &#39;Timestamp&#39;: datetime.datetime(2022, 6, 16, 9, 8, 43, tzinfo=tzlocal())}, {&#39;MetricName&#39;: &#39;validation:mae&#39;, &#39;Value&#39;: 2.5288796424865723, &#39;Timestamp&#39;: datetime.datetime(2022, 6, 16, 9, 8, 43, tzinfo=tzlocal())}, {&#39;MetricName&#39;: &#39;train:objective_loss&#39;, &#39;Value&#39;: 0.28216010332107544, &#39;Timestamp&#39;: datetime.datetime(2022, 6, 16, 9, 8, 43, tzinfo=tzlocal())}, {&#39;MetricName&#39;: &#39;validation:objective_loss&#39;, &#39;Value&#39;: 13.984094619750977, &#39;Timestamp&#39;: datetime.datetime(2022, 6, 16, 9, 8, 43, tzinfo=tzlocal())}, {&#39;MetricName&#39;: &#39;validation:objective_loss:final&#39;, &#39;Value&#39;: 12.655406951904297, &#39;Timestamp&#39;: datetime.datetime(2022, 6, 16, 9, 8, 43, tzinfo=tzlocal())}, {&#39;MetricName&#39;: &#39;validation:rmse&#39;, &#39;Value&#39;: 3.557443857192993, &#39;Timestamp&#39;: datetime.datetime(2022, 6, 16, 9, 8, 43, tzinfo=tzlocal())}, {&#39;MetricName&#39;: &#39;validation:mse&#39;, &#39;Value&#39;: 12.655406951904297, &#39;Timestamp&#39;: datetime.datetime(2022, 6, 16, 9, 8, 43, tzinfo=tzlocal())}, {&#39;MetricName&#39;: &#39;validation:r2&#39;, &#39;Value&#39;: 0.7973001599311829, &#39;Timestamp&#39;: datetime.datetime(2022, 6, 16, 9, 8, 43, tzinfo=tzlocal())}, {&#39;MetricName&#39;: &#39;train:throughput&#39;, &#39;Value&#39;: 853.2634887695312, &#39;Timestamp&#39;: datetime.datetime(2022, 6, 16, 9, 8, 43, tzinfo=tzlocal())}, {&#39;MetricName&#39;: &#39;validation:absolute_loss&#39;, &#39;Value&#39;: 2.5288796424865723, &#39;Timestamp&#39;: datetime.datetime(2022, 6, 16, 9, 8, 43, tzinfo=tzlocal())}], &#39;EnableNetworkIsolation&#39;: False, &#39;EnableInterContainerTrafficEncryption&#39;: False, &#39;EnableManagedSpotTraining&#39;: False, &#39;TrainingTimeInSeconds&#39;: 132, &#39;BillableTimeInSeconds&#39;: 132, &#39;ProfilerConfig&#39;: {&#39;S3OutputPath&#39;: &#39;s3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/output/&#39;, &#39;ProfilingIntervalInMilliseconds&#39;: 500}, &#39;ProfilerRuleConfigurations&#39;: [{&#39;RuleConfigurationName&#39;: &#39;ProfilerReport-1655370297&#39;, &#39;RuleEvaluatorImage&#39;: &#39;503895931360.dkr.ecr.us-east-1.amazonaws.com/sagemaker-debugger-rules:latest&#39;, &#39;VolumeSizeInGB&#39;: 0, &#39;RuleParameters&#39;: {&#39;rule_to_invoke&#39;: &#39;ProfilerReport&#39;}}], &#39;ProfilerRuleEvaluationStatuses&#39;: [{&#39;RuleConfigurationName&#39;: &#39;ProfilerReport-1655370297&#39;, &#39;RuleEvaluationJobArn&#39;: &#39;arn:aws:sagemaker:us-east-1:801598032724:processing-job/linear-learner-2022-06-16--profilerreport-1655370297-d239ec9a&#39;, &#39;RuleEvaluationStatus&#39;: &#39;NoIssuesFound&#39;, &#39;LastModifiedTime&#39;: datetime.datetime(2022, 6, 16, 9, 9, 45, 61000, tzinfo=tzlocal())}], &#39;ProfilingStatus&#39;: &#39;Enabled&#39;, &#39;ResponseMetadata&#39;: {&#39;RequestId&#39;: &#39;ae027c8b-2090-40eb-b904-75509ee9f3e5&#39;, &#39;HTTPStatusCode&#39;: 200, &#39;HTTPHeaders&#39;: {&#39;x-amzn-requestid&#39;: &#39;ae027c8b-2090-40eb-b904-75509ee9f3e5&#39;, &#39;content-type&#39;: &#39;application/x-amz-json-1.1&#39;, &#39;content-length&#39;: &#39;6459&#39;, &#39;date&#39;: &#39;Thu, 16 Jun 2022 09:10:08 GMT&#39;}, &#39;RetryAttempts&#39;: 0}} . . Training logs can also be viewed on AWS CloudWatch under LogGroup /aws/sagemaker/TrainingJobs and Log Stream same as the training job name. . Deploy Model and Make Predictions . Our model is trained and now we can deploy it to AWS SageMaker endpoint with a single call. These endpoints are fully managed and support autoscaling. . In the deployment call we are required to provide . instance type this is the machine type on which we want to deploy our model | instance count number of instances we want to provision for our machine. If more than one instance is provisioned then SageMaker EndPoint will automatically load balance between them | endpoint_name a unique identifier for your endpoint. If it is not provided then it will default to your training job name. | . It is important to note that endpoints are scoped to an individual AWS account, and are not public. The URL does not contain the account ID, but SageMaker determines the account ID from the authentication token that is supplied by the caller. We will discuss in a separate post on how to use AWS Lambda and API Gateway to set up and deploy your inference endpoint for public access. . Read more about deployment from the documentation sagemaker.estimator.Estimator.deploy . # define the endpoint. can be any unique string endpoint_name = ll_estimator.latest_training_job.job_name print(endpoint_name) . linear-learner-2022-06-16-09-04-57-576 . # deploy the model ll_predictor = ll_estimator.deploy( initial_instance_count=1, instance_type=&#39;ml.t2.medium&#39;, endpoint_name=endpoint_name ) . -! . This call will take a minute or so to spin up the required machine and create the endpoint. Once it is ready it will be available in SageMaker Resources / Endpoints. . . # a test sample to get live inference from our model test_sample = &#39;0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98&#39; . Similar to data input channels that tell the Estimator object on how to read the data from S3 bucket, we need to tell our predictor object on how to receive (encode) input and return (decode) data during inference. For this we will use sagemaker.serializers.CSVSerializer object to serialize data of various formats to a CSV-formatted string. . from sagemaker.serializers import CSVSerializer from sagemaker.deserializers import CSVDeserializer ll_predictor.serializer = CSVSerializer() ll_predictor.deserializer = CSVDeserializer() . Note that we can also pass these Serializers in the deploy method call like this . ll_predictor = ll_estimator.deploy( initial_instance_count=1, instance_type=&#39;ml.t2.medium&#39;, endpoint_name=endpoint_name, serializer = CSVSerializer(), deserializer = CSVDeserializer() ) . # call the endpoint for live inference ll_predictor.predict(test_sample) . [[&#39;29.98671531677246&#39;]] . The prediction output tells us that this house will cost $29986. . When we called the estimator.deploy() it has returned us sagemaker.Predictor object. If we want to call endpoint from a separate code then we can use the same &#39;Predictor&#39; object to call it. You can read more about this class here sagemaker.predictor.Predictor . from sagemaker.predictor import Predictor ll_predictor_2 = Predictor( endpoint_name, session, # If not specified, then one is created using the default AWS configuration chain CSVSerializer(), CSVDeserializer() ) . ll_predictor_2.predict(test_sample) . [[&#39;29.98671531677246&#39;]] . Clean Up . Remember that the inference machine will remain in use until you take it down yourself. This is unlike training where provisioning and decommissioning of machines are done automatically. We have used ml.t2.medium for hosting our model which costs around $0.056 per hour. This is not much but still, do clean up your environment once you are done experimenting to avoid unnecessary bills. . You can remove the endpoint with a single call. . # ll_predictor_2 will work too ll_predictor.delete_endpoint() . Summary . In this post we have learned about the SageMaker workflow, and how to train and deploy machine learning models with a handful of APIs from SageMaker SDK. .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/06/08/sagemaker-training-overview.html",
            "relUrl": "/aws/ml/sagemaker/2022/06/08/sagemaker-training-overview.html",
            "date": " • Jun 8, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "storemagic - Don't lose your variables in Jupyter Notebook",
            "content": ". Enviornment . !jupyter --version . Selected Jupyter core packages... IPython : 7.29.0 ipykernel : 6.4.1 ipywidgets : 7.6.5 jupyter_client : 6.1.12 jupyter_core : 4.8.1 jupyter_server : 1.4.1 jupyterlab : 3.2.1 nbclient : 0.5.3 nbconvert : 6.1.0 nbformat : 5.1.3 notebook : 6.4.5 qtconsole : 5.1.1 traitlets : 5.1.0 . About . This post is about a very important (and highly underrated) Jupyter notebook extension &quot;storemagic&quot;. We will see how we can use it to store our important data, and recover safely even when the unforgivable happens. . Introduction . All Jupyter Notebook lovers remember their petrified moments when they are struck by a [dark terminal] force, their heart skips a beat, and they stare blanky at the screen. When they know that the unforgivable has happened but they still timidly move the cursor to the last cell and press Enter with the faintest hope that it might be still alive. But fate has chosen something else for us, and we poor people are left cursing ourselves as to why we did not save our precious work! . I have been to those despair valleys (many times) till I stumbled upon &#39;storemagic&#39;. It seems like archaic magic as so few people know or use it, but I can guarantee once you get used to it you will barely write more than five lines in your notebooks without using it. So let&#39;s see how it works. . Store a variable . Use %store magic for lightweight persistence. It stores variables, aliases and macros in IPython’s database. Let&#39;s create a variable and then store it using this magic. . # create a variable var_hello = &quot;hello world!&quot; # store this variable %store var_hello . Stored &#39;var_hello&#39; (str) . Now I am going to intentionally restart the kernel. We can check that our created variable is now gone from the memory. . var_hello . NameError Traceback (most recent call last) ~ AppData Local Temp/ipykernel_12100/520558475.py in &lt;module&gt; -&gt; 1 var_hello NameError: name &#39;var_hello&#39; is not defined . But no worries. We have it stored safely using our magic. So let&#39;s get it back. . # get the variable back from store %store -r var_hello . Okay, we have our variable back and (with a sigh of relief) we can use it again. . var_hello . &#39;hello world!&#39; . Let&#39;s create a few more variables and do some more magic with them. . # create variables var_foo = [1,2,3,4] var_bar = {&#39;a&#39;:var_hello} # store multiple variables %store var_foo var_bar . Stored &#39;var_foo&#39; (list) Stored &#39;var_bar&#39; (dict) . Check all stored varaibles . We can check all the varaibles stored using the following magic command. . %store . Stored variables and their in-db values: var_bar -&gt; {&#39;a&#39;: &#39;hello world!&#39;} var_foo -&gt; [1, 2, 3, 4] var_hello -&gt; &#39;hello world!&#39; . Remove a variable from store . To remove a variable from our storage is also straight forward. Put its name after %store -d flag . # remove &#39;var_hello&#39; %store -d var_hello . &#39;var_hello&#39; is now gone. Forever ... . # check the remaining variables in store %store . Stored variables and their in-db values: var_bar -&gt; {&#39;a&#39;: &#39;hello world!&#39;} var_foo -&gt; [1, 2, 3, 4] . Remove all variables from store . If you want to remove all the varaibles from store and start clean then use -z flag . # remove all variables %store -z . Kaboom! all variables are gone. . # check store %store . Stored variables and their in-db values: . Reference . Check the official IPython storemagic documentation here: https://ipython.readthedocs.io/en/stable/config/extensions/storemagic.html .",
            "url": "https://hassaanbinaslam.github.io/myblog/jupyter/python/2022/05/30/storemagic-jupyter-notebook.html",
            "relUrl": "/jupyter/python/2022/05/30/storemagic-jupyter-notebook.html",
            "date": " • May 30, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Data Preparation with SageMaker Data Wrangler (Part 5)",
            "content": ". Enviornment . This notebook is prepared with Amazon SageMaker Studio using Python 3 (Data Science) Kernel and ml.t3.medium instance. . About . This is a detailed guide on using AWS SageMaker Data Wrangler service to prepare data for machine learning models. SageMaker Data Wrangler is a multipurpose tool with which you can . import data from multiple sources | explore data with visualizations | apply transformations | export data for ml training | . This guide is divided into five parts . Part 1: Prepare synthetic data and place it on multiple sources | Part 2: Import data from multiple sources using Data Wrangler | Part 3: Explore data with Data Wrangler visualizations | Part 4: Preprocess data using Data Wrangler | Part 5: Export data for ML training (You are here) | . Part 5: Export data for ML training . It is important to note that the transformations we have used are not applied to the data yet. These transformations need to be executed to get the final transformed data. When we export Data Wrangler flow it generates the code that when executed will perform the data transformations. Data Wrangler supports four export methods: Save to S3, Pipeline, Python Code, and Feature Store. In this post, we will see how to export data to S3 as this is the most common use case. . Open the customer-churn-p4.flow file from part-4. On the last step click the plus sign and select Export to &gt; Amazon S3 (via Jupyter Notebook) . . Sagemaker Data Wrangler will auto-generate a Jupyter notebook that will contain all the required code to transform and put data on the S3 bucket. . . You may review the code and make any changes otherwise run it as it is till point (Optional)Next Steps. This is the first time SageMaker will process the data and place the output on S3 bucket. SageMaker may take a couple of minutes to execute all the cells. It is important to note that this notebook will initiate a container running on a separate machine to do all the processing. The machine specs are defined in the notebook as . # Processing Job Instance count and instance type. instance_count = 2 instance_type = &quot;ml.m5.4xlarge&quot; . Once execution is complete you see the output message containing the S3 bucket location where the final output is stored. . . The optional part of this notebook also contains code to generate xgboost model on the transformed data. To execute these steps make the following changes in the notebook. . Change the flag to run the optional steps. . run_optional_steps = True . Next, update the xgboost hyperparameters to train a binary classification model (customer churn or not?). . hyperparameters = { &quot;max_depth&quot;:&quot;5&quot;, &quot;objective&quot;: &quot;binary:logistic&quot;, &quot;num_round&quot;: &quot;10&quot;, } . Execute the optional steps. Again note that these steps will initiate a container running on a separate machine (&quot;ml.m5.2xlarge&quot;) to do the training work. The training job will take a few minutes to complete and once it is done trained model will be available on the S3 bucket for inference use. This autogenerated notebook customer-churn-p4.ipynb is available on GitHub here. . Summary . In this last post of the series, we used SageMaker Data Wrangler to auto-generate code to preprocess the data and store the final output on S3 bucket. We also used the same notebook to train an xgboost model on the processed data. .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/26/aws-sagemaker-wrangler-p5.html",
            "relUrl": "/aws/ml/sagemaker/2022/05/26/aws-sagemaker-wrangler-p5.html",
            "date": " • May 26, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Data Preparation with SageMaker Data Wrangler (Part 4)",
            "content": ". Enviornment . This notebook is prepared with Amazon SageMaker Studio using Python 3 (Data Science) Kernel and ml.t3.medium instance. . About . This is a detailed guide on using AWS SageMaker Data Wrangler service to prepare data for machine learning models. SageMaker Data Wrangler is a multipurpose tool with which you can . import data from multiple sources | explore data with visualizations | apply transformations | export data for ml training | . This guide is divided into five parts . Part 1: Prepare synthetic data and place it on multiple sources | Part 2: Import data from multiple sources using Data Wrangler | Part 3: Explore data with Data Wrangler visualizations | Part 4: Preprocess data using Data Wrangler (You are here) | Part 5: Export data for ML training | . Part 4: Preprocess data using Data Wrangler . We will continue from where we left in part-3. Open customer-churn.flow file in AWS SageMaker Data Wrangler console. Once opened our flow will look like this . . We will add the following transformations to our code. . Remove redundant columns | Remove features with low predictive power | Transform feature values to correct format | Encode categorical features | Move the target label to the start | . Remove redundant columns . When we made joins between tables (see part-2) it resulted in some redundant columns CustomerID_* . We will remove them first. For this click on plus sign beside 2nd Join, and select Add Transform. From the next transform UI clink Add Step and then search for transformer Manage Column. Inside Manage Columns transformer select . Transform = Drop Column | Columns to drop = CustomerID_0, CustomerID_1 | . Click preview and Add. . Remove features with low predictive power . In part-3 we used Quick Model to get the predictive power of features. When we analyze features with low importance we find that Phone is one such feature that does not hold much information for the model. For a model, a phone number is just some random collection of numbers and does not hold any meaning. There are other features with low importance too but they still hold some information for the model. So let&#39;s drop Phone. The steps will be same as in the last part. . Transform feature values to correct format . Churn? is our target label but its value has an extra &#39;.&#39; at the end. If we remove that symbol then it can easily be converted to a Boolean type. So let&#39;s do that. From the transformers list this time choose Format String and select . Transform = Remove Symbols | Input Columns = Churn? | Symbols = . | . Click Preview and Add. . . Now that the data is in the correct format (True/False) we can apply another transformer on it to convert it to Boolean feature. So select PARSE COLUMN AS TYPE transformer and configure . Column = Churn? | From = String | To = Boolean | . Click Preview and then Add. . Encode categorical features . At this point we have only two columns with String datatype: State and Area Code. If we look at the Area Code it has high variance and little feature importance. It is better to drop this feature. So Add another transformer and drop Area Code. For State we will apply one-hot encoding. So for this select transformer Encode Categorical and configure . Transform = One-hot encode | Input Columns = State | Output style = Columns | . Leave the rest of the options as default. Click Preview and Add. . . Move the target label to the start . SageMaker requires that the target label should be the first column in the dataset. So add another transformer Manage columns and configure . Transform = Move column | Move Type = Move to start | Column to move = Churn? | . . Evaluate model performance . We have done some key transformations. We can use Quick Model again to analyze the model performance at this point. We have done a similar analysis in part-3 so let&#39;s do it again and compare the results. From the last transformation step, click plus sign and choose Add Analysis . . We can see from the results that these transformations have a positive impact on the model performance and the F1 score has moved up from 0.841 to 0.861. . Summary . In this post we have seen how we can apply a transformation to our data and can use Quick Model to quickly analyze the model performance. customer-churn-p4.flow file used in this post can be found on the GitHub here. In the next post, we will discuss how to export data from Data Wrangler to different destinations. .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/25/aws-sagemaker-wrangler-p4.html",
            "relUrl": "/aws/ml/sagemaker/2022/05/25/aws-sagemaker-wrangler-p4.html",
            "date": " • May 25, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Data Preparation with SageMaker Data Wrangler (Part 3)",
            "content": ". Enviornment . This notebook is prepared with Amazon SageMaker Studio using Python 3 (Data Science) Kernel and ml.t3.medium instance. . About . This is a detailed guide on using AWS SageMaker Data Wrangler service to prepare data for machine learning models. SageMaker Data Wrangler is a multipurpose tool with which you can . import data from multiple sources | explore data with visualizations | apply transformations | export data for ml training | . This guide is divided into five parts . Part 1: Prepare synthetic data and place it on multiple sources | Part 2: Import data from multiple sources using Data Wrangler | Part 3: Explore data with Data Wrangler visualizations (You are here) | Part 4: Preprocess data using Data Wrangler | Part 5: Export data for ML training | . Part 3: Explore data with Data Wrangler visualizations . In this post, we will use SageMaker Data Wrangler to create some visualizations for exploratory data analysis (EDA). Open the customer-churn.flow from part-2. It is also available on GitHub here. We will create a histogram to explore the frequency distribution of daily calls. Once the flow process is open on the Data Flow UI it will look like this . . Click on the 2nd join plus sign and select &#39;Add Analysis&#39;. From the next analysis UI select . Analysis Type = Histogram | Analysis Name = call_minutes_churn | X_axis = day_min | Facet by = Churn? | . Click Preview and Data Wrangler will create the following histogram . . From this histogram you can see that customers whose calls duration are 4 minutes or less are more likely to stay, and customer having call duration longer than 4 minutes are more likely to churn. Save the flow to return back to main Data Flow UI. . Preview ML model performance using Quick Model . Quick Model is another great feature of SageMaker wrangler with which we can quickly train a Random Forrest Classification model and analyze the importance of features. For this again click on the plus sign against the 2nd Join, and select Add Analysis. Then from the Analysis UI select . Analysis Type = Quick Model | Analysis Name = Quick model | Label = Churn? | . Label is our target identifier. Click preview. Data Wrangler will take around a minute to train the model, and will provide a chart with feature importances. . . From this feature importance chart, we can see that the day_mins and night_charge features have the highest importance. It also shows that the model has achieved F1 score of 0.841 on the test data. We can take this model as a baseline and work on the important features and model tuning to improve its performance. Click Save to return to the main Data Flow UI. . Summary . In this post, we saw that we can quickly create visualizations from Data Wrangler to do our EDA work. There are many other built-in analysis reports available (check Data Leakage and Data Quality reports) that can quickly provide a very detailed analysis of the data. The customer-churn.flow file is available on GitHub here. In the next post, we will perform some preprocessing and transformations to make our data ready for ML training. .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/24/aws-sagemaker-wrangler-p3.html",
            "relUrl": "/aws/ml/sagemaker/2022/05/24/aws-sagemaker-wrangler-p3.html",
            "date": " • May 24, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Data Preparation with SageMaker Data Wrangler (Part 2)",
            "content": ". Enviornment . This notebook is prepared with Amazon SageMaker Studio using Python 3 (Data Science) Kernel and ml.t3.medium instance. . About . This is a detailed guide on using AWS SageMaker Data Wrangler service to prepare data for machine learning models. SageMaker Data Wrangler is a multipurpose tool with which you can . import data from multiple sources | explore data with visualizations | apply transformations | export data for ml training | . This guide is divided into five parts . Part 1: Prepare synthetic data and place it on multiple sources | Part 2: Import data from multiple sources using Data Wrangler (You are here) | Part 3: Explore data with Data Wrangler visualizations | Part 4: Preprocess data using Data Wrangler | Part 5: Export data for ML training | . Part 2: Import data from multiple sources using Data Wrangler . In this post, we will create SageMaker Data Wrangler Flow pipeline to import data from multiple sources. Once data is imported, we will then add a step to join the data into a single dataset that can be used for training ML models. . Launch SageMaker Data Wrangler Flow . Create a new Data Wrangler flow by clicking on the main menu tabs File &gt; New &gt; Data Wrangler Flow. . . Once launched SageMaker may take a minute to initialize a new flow. The reason for this is SageMaker will launch a separate machine in the background ml.m5.4xlarge with 16vCPU and 64 GiB memory for processing flow files. A flow file is a JSON file that just captures all the steps performed from the Flow UI console. When you execute the flow, the Flow engine parses this file and performs all the steps. Once a new flow file is available, rename it to customer-churn.flow. . . Import data from sources . First, we will create a flow to import data (created in the part-1 post) from S3 bucket. For this from the flow UI click on Amazon S3 bucket. From the next window select the bucket name S3://sagemaker-us-east-1-801598032724. In your case, it could be different where you have stored the data. From the UI select the filename &quot;telco_churn_customer_info.csv&quot; and click Import . . Once the data is imported repeat the steps for the filename &quot;telco_churn_account_info.csv&quot;. If you are not seeing the &quot;import from S3 bucket&quot; option on the UI then check the flow UI and click on the &#39;Import&#39; tab option. Once both files are imported, your Data Flow tab will look similar to this . . Now that we have imported data from S3, we can now work on importing data from the Athena database. For this from the Flow UI Import tab click on Amazon Athena option. From the next UI select AwsDataCatalog Data catalog option. For Databases drop down select telco_db and in the query pane write the below query. . select * from telco_churn_utility . You can also preview the data by clicking on the table preview option. Once satisfied with the results click &#39;Import&#39;. When asked about the database name write telco_churn_utility . . At this point, you will find all three tables imported in Data Flow UI. Against each table, a plus sign (+) will appear that you can use to add any transformations you want to apply on each table. . . for telco_churn_customer_info click on the plus sign and then select &#39;Edit&#39; to change data types. . . We will add the following transformations . Change Area Code from Long to String | Click Preview | Then click Apply | . . Similarly for telco_churn_account_info.csv edit data types as . Change Account Length to Long | Change Int&#39;l Plan and VMail Plan to Bool | Click Preview and then click Apply | . For telco_churn_utility.csv edit data types as . Change custserv_calls to Long | Click Preview and then click Apply | . At this point, we have imported the data from all three sources and have also properly transformed their column types. . Joining Tables . Now we will join all three tables to get a full dataset. For this from the Flow UI Data flow click on the plus sign next to customer_info data type and this time select &#39;Join&#39;. From the new window select account_info as the right dataset and click Configure . . From the next screen select . Join Type = Full Outer | Columns Left = CustomerID | Columns Right = CustomerID | Click Preview and then Add | . . A new join step will appear on the Data Flow UI. Click on the plus sign next to it and repeat the steps for utility table . . Join Type = Full Outer | Columns Left = CustomerID_0 | Columns Right = CustomerID | Click Preview and then Add | . . Summary . At this point, we have all the tables joined together. The customer-churn.flow created is available on the GitHub here. In the next post, we will clean duplicate columns and create some visualizations to analyze the data. .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/23/aws-sagemaker-wrangler-p2.html",
            "relUrl": "/aws/ml/sagemaker/2022/05/23/aws-sagemaker-wrangler-p2.html",
            "date": " • May 23, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Collecting metrics and logs from on-premises servers with the CloudWatch agent",
            "content": ". About . This post is a detailed guide on using AWS CloudWatch Agent to collect logs and metrics from on-premises Ubuntu server. . The CloudWatch agent is open-source tool under the MIT license, and is hosted on GitHub amazon-cloudwatch-agent | With this agent you can collect more system-level metrics from Amazon EC2 instances or onprem servers across operating systems. You can retrieve custom metrics from your applications or services using the StatsD and collectd protocols. StatsD is supported on both Linux servers and servers running Windows Server. collectd is supported only on Linux servers | For the list of metrics that can be collected by CloudWatch agent follow this link metrics-collected-by-CloudWatch-agent | . Environment Details . For on-premises Ubuntu server, we will use an EC2 machine with Ubuntu OS. Enable Auto-assign public IP and keep all the default settings. Once the instance is in a running state use SSH Key to connect to it. . If you are using Windows OS and while connecting to Ubuntu machine you are getting &quot;Permissions for &#39;ssh-key.pem&#39; are too open.&quot; then take help from this post to resolve it windows-ssh-permissions-for-private-key-are-too-open . . Once you are successfully connected to EC2 Ubuntu machine you will get the following message on the terminal. . CloudWatch Agent Installation and Configuration Steps . Create IAM roles and users for use with CloudWatch agent . Access to AWS resources requires permissions. You create an IAM role, an IAM user, or both to grant permissions that the CloudWatch agent needs to write metrics to CloudWatch. . If you&#39;re going to use the agent on Amazon EC2 instances, you should create an IAM role. | f you&#39;re going to use the agent on on-premises servers, you should create an IAM user. | . Since we want to use EC2 machine as an on-premises machine so we will create an IAM user. . To create the IAM user necessary for the CloudWatch agent to run on on-premises servers follow these steps . Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. | In the navigation pane on the left, choose Users and then Add users. | Enter the user name for the new user. | Select Access key - Programmatic access and choose Next: Permissions. | Choose Attach existing policies directly. | In the list of policies, select the check box next to CloudWatchAgentServerPolicy. If necessary, use the search box to find the policy. | Choose Next: Tags. | Optionally create tags for the new IAM user, and then choose Next:Review. | Confirm that the correct policy is listed, and choose Create user. | Next to the name of the new user, choose Show. Copy the access key and secret key to a file so that you can use them when installing the agent. Choose Close. | Install and configure AWS CLI on Ubuntu server . Connect to the Ubuntu server using any SSH client. We need to first download and install AWS CLI. Follow the below commands to download and install it. For installing AWS CLI on macOS and Windows take help from this post awscli-getting-started-install . 1. Download AWS CLI package . curl &quot;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip&quot; -o &quot;awscliv2.zip&quot; . 2. Install UNZIP package . sudo apt install unzip . 3. Unzip AWSCLI Package . unzip awscliv2.zip . 4. Install AWS CLI . sudo ./aws/install . 5. Verify AWS CLI Installation . aws --version . . 6. Configure AWS CLI . Make sure that you use AmazonCloudWatchAgent profile name as this is used by the OnPremise case by default. For more details, you may take help from this post install-CloudWatch-Agent-commandline-fleet . aws configure --profile AmazonCloudWatchAgent . . 7. Verify credentials in User home directory . cat /home/ubuntu/.aws/credentials . Install and run the CloudWatch agent on Ubuntu server . 1. Download the agent . The following download link is for Ubuntu. For any other OS you can take help from this post for downloaded agent download-cloudwatch-agent-commandline . wget https://s3.amazonaws.com/amazoncloudwatch-agent/ubuntu/amd64/latest/amazon-cloudwatch-agent.deb . 2. Install the agent . sudo dpkg -i -E ./amazon-cloudwatch-agent.deb . 3. Prepare agent configuration file . Prepare agent configuration file. This config file will be provided to the agent in the run command. One such sample is provided below. For more details on this config file you may take help from this link create-cloudwatch-agent-configuration-file. Note the path of this config file (agent config) as we will need it in later commands. . // config-cloudwatchagent.json { &quot;agent&quot;: { &quot;metrics_collection_interval&quot;: 10, &quot;logfile&quot;: &quot;/opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log&quot;, &quot;run_as_user&quot;: &quot;ubuntu&quot;, &quot;debug&quot;: false }, &quot;metrics&quot;: { &quot;namespace&quot;: &quot;myblog/cloudwatchagent/demo&quot;, &quot;metrics_collected&quot;: { &quot;cpu&quot;: { &quot;resources&quot;: [ &quot;*&quot; ], &quot;measurement&quot;: [ {&quot;name&quot;: &quot;cpu_usage_idle&quot;, &quot;rename&quot;: &quot;CPU_USAGE_IDLE&quot;, &quot;unit&quot;: &quot;Percent&quot;}, {&quot;name&quot;: &quot;cpu_usage_nice&quot;, &quot;unit&quot;: &quot;Percent&quot;}, &quot;cpu_usage_guest&quot;, &quot;cpu_usage_active&quot; ], &quot;totalcpu&quot;: true, &quot;metrics_collection_interval&quot;: 10 }, &quot;disk&quot;: { &quot;resources&quot;: [ &quot;/&quot;, &quot;/tmp&quot; ], &quot;measurement&quot;: [ {&quot;name&quot;: &quot;free&quot;, &quot;rename&quot;: &quot;DISK_FREE&quot;, &quot;unit&quot;: &quot;Gigabytes&quot;}, &quot;total&quot;, &quot;used&quot; ], &quot;ignore_file_system_types&quot;: [ &quot;sysfs&quot;, &quot;devtmpfs&quot; ], &quot;metrics_collection_interval&quot;: 60 }, &quot;diskio&quot;: { &quot;resources&quot;: [ &quot;*&quot; ], &quot;measurement&quot;: [ &quot;reads&quot;, &quot;writes&quot;, &quot;read_time&quot;, &quot;write_time&quot;, &quot;io_time&quot; ], &quot;metrics_collection_interval&quot;: 60 }, &quot;swap&quot;: { &quot;measurement&quot;: [ &quot;swap_used&quot;, &quot;swap_free&quot;, &quot;swap_used_percent&quot; ] }, &quot;mem&quot;: { &quot;measurement&quot;: [ &quot;mem_used&quot;, &quot;mem_cached&quot;, &quot;mem_total&quot; ], &quot;metrics_collection_interval&quot;: 1 }, &quot;net&quot;: { &quot;resources&quot;: [ &quot;eth0&quot; ], &quot;measurement&quot;: [ &quot;bytes_sent&quot;, &quot;bytes_recv&quot;, &quot;drop_in&quot;, &quot;drop_out&quot; ] }, &quot;netstat&quot;: { &quot;measurement&quot;: [ &quot;tcp_established&quot;, &quot;tcp_syn_sent&quot;, &quot;tcp_close&quot; ], &quot;metrics_collection_interval&quot;: 60 }, &quot;processes&quot;: { &quot;measurement&quot;: [ &quot;running&quot;, &quot;sleeping&quot;, &quot;dead&quot; ] } }, &quot;force_flush_interval&quot; : 30 }, &quot;logs&quot;: { &quot;logs_collected&quot;: { &quot;files&quot;: { &quot;collect_list&quot;: [ { &quot;file_path&quot;: &quot;/opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log&quot;, &quot;log_group_name&quot;: &quot;myblog/onprem/ubuntu/amazon-cloudwatch-agent&quot;, &quot;log_stream_name&quot;: &quot;myblog-cloudwatchagent-demo.log&quot;, &quot;timezone&quot;: &quot;UTC&quot; } ] } }, &quot;log_stream_name&quot;: &quot;my_log_stream_name&quot;, &quot;force_flush_interval&quot; : 15 } } . Some important parts of this config file . logfile . &quot;logfile&quot;: &quot;/opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log&quot; . CloudWatch agent log file location on on-premise server is specified by this tag. After running the agent you can check this log file for any exception messages. . log_group_name . &quot;log_group_name&quot;: &quot;myblog/onprem/ubuntu/amazon-cloudwatch-agent&quot; . An on-premise logfile is also uploaded to CloudWatch under log-group-name specified by this tag. . log_stream_name . &quot;log_stream_name&quot;: &quot;myblog-cloudwatchagent-demo.log&quot; . Log stream name of the CloudWatch where logfile log steam will be uploaded. . namespace . &quot;namespace&quot;: &quot;myblog/cloudwatchagent/demo&quot; . On CloudWatch console you find the uploaded metrics under the custom namespace specified by this tag. In our case, it is &quot;myblog/cloudwatchagent/demo&quot; . 4. Update shared configuration file . From the config file . Uncomment the [credentails] tag | Update shared_credentails_profile name. This is the profile name with which we have configured our AWS CLI &#39;AmazonCloudWatchAgent&#39;. If you have used any other name then use that name here. | Update shared_credentials_file path. This is the path for AWS user credentails file created by AWS CLI. &#39;/home/username/.aws/credentials&#39; and in our case it is /home/ubuntu/.aws/credentials | Configuration file is located at /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml. For more details on this shared configuration file follow this link CloudWatch-Agent-profile-instance-first . sudo vim /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml . . 5. Start the agent . sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m onPremise -s -c file:/home/ubuntu/config-cloudwatchagent.json . Make sure that you provide the correct path to the JSON config file. In our case, it is file:/home/ubuntu/config-cloudwatchagent.json. For more details check this link start-CloudWatch-Agent-on-premise-SSM-onprem . . 6. Check agent status . sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a status . If the agent is running you will get status : running otherwise you will get status : stopped . . 7. Check agent logs . The agent generates a log while it runs. This log includes troubleshooting information. This log is the amazon-cloudwatch-agent.log file. This file is located in /opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log on Linux servers. This is the same logfile path we also defined in the JSON config file. If you are using multiple agents on the machine then you can give them separate log file paths using their JSON configurations. . sudo tail -f /var/log/amazon/amazon-cloudwatch-agent/amazon-cloudwatch-agent.log . Check the logs if there is an exception message or not. . . Please note that both the log files are the same. It could be that agent is keeping multiple copies for internal processing. . /opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log . or . /var/log/amazon/amazon-cloudwatch-agent/amazon-cloudwatch-agent.log . Check the agent logs on AWS CloudWatch console . Agent logs are also uploaded to CloudWatch console under log group and stream that we mentioned in JSON config file. In our case it is . &quot;log_group_name&quot;: &quot;myblog/onprem/ubuntu/amazon-cloudwatch-agent&quot; &quot;log_stream_name&quot;: &quot;myblog-cloudwatchagent-demo.log&quot; . . Check the machine metrics on CloudWatch console . Now finally we can check the metrics uploaded by the agent on CloudWatch console under CloudWatch &gt; Metrics &gt; ALL metrics &gt; Custom namespaces . The name of the metrics namespace is the same as what we defined in our JSON config file . &quot;metrics&quot;: { &quot;namespace&quot;: &quot;myblog/cloudwatchagent/demo&quot; . . Common scenarios with the CloudWatch agent . For more trouble shooting scenerios follow these link . troubleshooting-CloudWatch-Agent | CloudWatch-Agent-common-scenarios | . To stop the CloudWatch agent locally using the command line . On a Linux server, enter the following . sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a stop . I updated my agent configuration but don&#8217;t see the new metrics or logs in the CloudWatch console . If you update your CloudWatch agent configuration file, the next time that you start the agent, you need to use the fetch-config option. For example, if you stored the updated file on the local computer, enter the following command. Replace &lt;configuration-file-path&gt; with the actual config file path. . sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -s -m ec2 -c file:&lt;configuration-file-path&gt; .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/cloudwatch/2022/05/21/cloudwatch-agent-onprem.html",
            "relUrl": "/aws/cloudwatch/2022/05/21/cloudwatch-agent-onprem.html",
            "date": " • May 21, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Data Preparation with SageMaker Data Wrangler (Part 1)",
            "content": ". Enviornment . This notebook is prepared with Amazon SageMaker Studio using Python 3 (Data Science) Kernel and ml.t3.medium instance. . About . This is a detailed guide on using AWS SageMaker Data Wrangler service to prepare data for machine learning models. SageMaker Data Wrangler is a multipurpose tool with which you can . import data from multiple sources | explore data with visualizations | apply transformations | export data for ml training | . This guide is divided into five parts . Part 1: Prepare synthetic data and place it on multiple sources (You are here) | Part 2: Import data from multiple sources using Data Wrangler | Part 3: Explore data with Data Wrangler visualizations | Part 4: Preprocess data using Data Wrangler | Part 5: Export data for ML training | . Credits . Getting Started with Amazon SageMaker Studio book by Michael Hsieh. Michael Hsieh is a senior AI/machine learning (ML) solutions architect at Amazon Web Services. He creates and evangelizes for ML solutions centered around Amazon SageMaker. He also works with enterprise customers to advance their ML journeys. . Part 1: Prepare synthetic data and place it on multiple sources . Let&#39;s prepare some dataset and place it on the S3 bucket and AWS Glue tables. Then we will use Data Wrangler to pull and join data from these two sources. The idea is to simulate some real project challenges where data is not coming from a single source but is distributed in multiple stores, and is in different formats. It is usually the preprocessing pipeline job to get data from these sources and join and preprocess it. . Data . Mobile operators have historical records on which customers ultimately ended up churning and which continued using the service. We can use this historical information to construct an ML model of one mobile operator’s churn using a process called training. After training the model, we can pass the profile information of an arbitrary customer (the same profile information that we used to train the model) to the model, and have the model predict whether this customer is going to churn. Of course, we expect the model to make mistakes. After all, predicting the future is a tricky business! But we’ll learn how to deal with prediction errors. . The dataset we use is publicly available and was mentioned in the book Discovering Knowledge in Data by Daniel T. Larose. It is attributed by the author to the University of California Irvine Repository of Machine Learning Datasets (Jafari-Marandi, R., Denton, J., Idris, A., Smith, B. K., &amp; Keramati, A. (2020). . Preparation . # install aws data wrangler package # restart kernel after installation # more on this package later in the notebook. !pip install -q awswrangler . WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv WARNING: You are using pip version 22.0.4; however, version 22.1 is available. You should consider upgrading via the &#39;/opt/conda/bin/python -m pip install --upgrade pip&#39; command. . import pandas as pd import sagemaker sess = sagemaker.Session() prefix = &#39;myblog/demo-customer-churn&#39; . !aws s3 cp s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt ./ . download: s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt to ./churn.txt . df = pd.read_csv(&quot;./churn.txt&quot;) # make &#39;CustomerID&#39; column from the index df[&#39;CustomerID&#39;]=df.index pd.set_option(&quot;display.max_columns&quot;, 500) df.head(10) . State Account Length Area Code Phone Int&#39;l Plan VMail Plan VMail Message Day Mins Day Calls Day Charge Eve Mins Eve Calls Eve Charge Night Mins Night Calls Night Charge Intl Mins Intl Calls Intl Charge CustServ Calls Churn? CustomerID . 0 PA | 163 | 806 | 403-2562 | no | yes | 300 | 8.162204 | 3 | 7.579174 | 3.933035 | 4 | 6.508639 | 4.065759 | 100 | 5.111624 | 4.928160 | 6 | 5.673203 | 3 | True. | 0 | . 1 SC | 15 | 836 | 158-8416 | yes | no | 0 | 10.018993 | 4 | 4.226289 | 2.325005 | 0 | 9.972592 | 7.141040 | 200 | 6.436188 | 3.221748 | 6 | 2.559749 | 8 | False. | 1 | . 2 MO | 131 | 777 | 896-6253 | no | yes | 300 | 4.708490 | 3 | 4.768160 | 4.537466 | 3 | 4.566715 | 5.363235 | 100 | 5.142451 | 7.139023 | 2 | 6.254157 | 4 | False. | 2 | . 3 WY | 75 | 878 | 817-5729 | yes | yes | 700 | 1.268734 | 3 | 2.567642 | 2.528748 | 5 | 2.333624 | 3.773586 | 450 | 3.814413 | 2.245779 | 6 | 1.080692 | 6 | False. | 3 | . 4 WY | 146 | 878 | 450-4942 | yes | no | 0 | 2.696177 | 3 | 5.908916 | 6.015337 | 3 | 3.670408 | 3.751673 | 250 | 2.796812 | 6.905545 | 4 | 7.134343 | 6 | True. | 4 | . 5 VA | 83 | 866 | 454-9110 | no | no | 0 | 3.634776 | 7 | 4.804892 | 6.051944 | 5 | 5.278437 | 2.937880 | 300 | 4.817958 | 4.948816 | 4 | 5.135323 | 5 | False. | 5 | . 6 IN | 140 | 737 | 331-5751 | yes | no | 0 | 3.229420 | 4 | 3.165082 | 2.440153 | 8 | 0.264543 | 2.352274 | 300 | 3.869176 | 5.393439 | 4 | 1.784765 | 4 | False. | 6 | . 7 LA | 54 | 766 | 871-3612 | no | no | 0 | 0.567920 | 6 | 1.950098 | 4.507027 | 0 | 4.473086 | 0.688785 | 400 | 6.132137 | 5.012747 | 5 | 0.417421 | 8 | False. | 7 | . 8 MO | 195 | 777 | 249-5723 | yes | no | 0 | 5.811116 | 6 | 4.331065 | 8.104126 | 2 | 4.475034 | 4.208352 | 250 | 5.974575 | 4.750153 | 7 | 3.320311 | 7 | True. | 8 | . 9 AL | 104 | 657 | 767-7682 | yes | no | 0 | 2.714430 | 7 | 5.138669 | 8.529944 | 6 | 3.321121 | 2.342177 | 300 | 4.328966 | 3.433554 | 5 | 5.677058 | 4 | False. | 9 | . df.shape . (5000, 22) . By modern standards, it’s a relatively small dataset, with only 5,000 records, where each record uses 21 attributes to describe the profile of a customer of an unknown US mobile operator. The attributes are: . State: the US state in which the customer resides, indicated by a two-letter abbreviation; for example, OH or NJ | Account Length: the number of days that this account has been active | Area Code: the three-digit area code of the corresponding customer’s phone number | Phone: the remaining seven-digit phone number | Int’l Plan: whether the customer has an international calling plan: yes/no | VMail Plan: whether the customer has a voice mail feature: yes/no | VMail Message: the average number of voice mail messages per month | Day Mins: the total number of calling minutes used during the day | Day Calls: the total number of calls placed during the day | Day Charge: the billed cost of daytime calls | Eve Mins, Eve Calls, Eve Charge: the billed cost for calls placed during the evening | Night Mins, Night Calls, Night Charge: the billed cost for calls placed during nighttime | Intl Mins, Intl Calls, Intl Charge: the billed cost for international calls | CustServ Calls: the number of calls placed to Customer Service | Churn?: whether the customer left the service: true/false | . The last attribute, Churn?, is known as the target attribute: the attribute that we want the ML model to predict. Because the target attribute is binary, our model will be performing binary prediction, also known as binary classification. . We have our dataset. Now we will split this dataset into three subsets . customer: customer data, and place it as a CSV file on the S3 bucket | account: accounts data, and place it as CSV on the same S3 bucket | utility: utility data, and place it as Glue tables | . customer_columns = [&#39;CustomerID&#39;, &#39;State&#39;, &#39;Area Code&#39;, &#39;Phone&#39;] account_columns = [&#39;CustomerID&#39;, &#39;Account Length&#39;, &quot;Int&#39;l Plan&quot;, &#39;VMail Plan&#39;, &#39;Churn?&#39;] utility_columns = [&#39;CustomerID&#39;, &#39;VMail Message&#39;, &#39;Day Mins&#39;, &#39;Day Calls&#39;, &#39;Day Charge&#39;, &#39;Eve Mins&#39;, &#39;Eve Calls&#39;, &#39;Eve Charge&#39;, &#39;Night Mins&#39;, &#39;Night Calls&#39;, &#39;Night Charge&#39;, &#39;Intl Mins&#39;, &#39;Intl Calls&#39;, &#39;Intl Charge&#39;, &#39;CustServ Calls&#39;] . We will use the default bucket associated with our SageMaker session. You may use any other bucket with proper access permissions. . bucket = sess.default_bucket() bucket . &#39;sagemaker-us-east-1-801598032724&#39; . Next, we will use AWS Data Wrangler Python package (awswrangler) to create an AWS Glue database. . awswrangler is an open source Python library maintained by AWS team, as is defined as . An AWS Professional Service open source python initiative that extends the power of Pandas library to AWS connecting DataFrames and AWS data related services. Easy integration with Athena, Glue, Redshift, Timestream, OpenSearch, Neptune, QuickSight, Chime, CloudWatchLogs, DynamoDB, EMR, SecretManager, PostgreSQL, MySQL, SQLServer and S3 (Parquet, CSV, JSON and EXCEL). . You may read more about this library here . Documentation:https://aws-data-wrangler.readthedocs.io/en/stable/what.html* Github repo: https://github.com/awslabs/aws-data-wrangler | . Please note that AWS SageMaker session needs some additional AWS Glue permissions to create a database. If you get an error while creating a Glue database in following steps then add those permissions. . Error: AccessDeniedException: An error occurred (AccessDeniedException) when calling the GetDatabase operation: User: arn:aws:sts::801598032724:assumed-role/AmazonSageMaker-ExecutionRole-20220516T161743/SageMaker is not authorized to perform: glue:GetDatabase on resource: arn:aws:glue:us-east-1:801598032724:database/telco_db because no identity-based policy allows the glue:GetDatabase action . Fix: Go to your SageMaker Execution Role and add permission AWSGlueConsoleFullAccess . # define the Glue DB name db_name = &#39;telco_db&#39; . import awswrangler as wr # get all the existing Glue db list databases = wr.catalog.databases() # print existing db names print(&quot;*** existing databases *** n&quot;) print(databases) # if our db does not exist then create it if db_name not in databases.values: wr.catalog.create_database(db_name, description = &#39;Demo DB for telco churn dataset&#39;) print(&quot; n*** existing + new databases *** n&quot;) print(wr.catalog.databases()) else: print(f&quot;Database {db_name} already exists&quot;) . *** existing databases *** Database Description 0 sagemaker_data_wrangler 1 sagemaker_processing *** existing + new databases *** Database Description 0 sagemaker_data_wrangler 1 sagemaker_processing 2 telco_db Demo DB for telco churn dataset . # in case you want to delete a database using this notebook # wr.catalog.delete_database(db_name) . Similarly you can go to AWS Glue console to see that the new database has been created. . . Now we will place the three data subsets into their respective locations. . suffix = [&#39;customer_info&#39;, &#39;account_info&#39;, &#39;utility&#39;] for i, columns in enumerate([customer_columns, account_columns, utility_columns]): # get the data subset df_tmp = df[columns] # prepare filename and output path fname = &#39;telco_churn_%s&#39; % suffix[i] outputpath = f&#39;s3://{bucket}/{prefix}/data/{fname}&#39; print(f&quot; n*** working on {suffix[i]}***&quot;) print(f&quot;filename: {fname}&quot;) print(f&quot;output path: {outputpath}&quot;) if i &gt; 1: # for utility wr.s3.to_csv( df=df_tmp, path=outputpath, dataset=True, database=db_name, # Athena/Glue database table=fname, # Athena/Glue table index=False, mode=&#39;overwrite&#39;) else: # for customer and account wr.s3.to_csv( df=df_tmp, path=f&#39;{outputpath}.csv&#39;, index=False) . *** working on customer_info*** filename: telco_churn_customer_info output path: s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_customer_info *** working on account_info*** filename: telco_churn_account_info output path: s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_account_info *** working on utility*** filename: telco_churn_utility output path: s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_utility . We can verify the uploaded data from the S3 bucket. . . Similarly, from Glue console we can verify that the utility table has been created. . . If you want to remain within the notebook and do the verification then that can also be done. . # list s3 objects wr.s3.list_objects(&#39;s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/&#39;) . [&#39;s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_account_info.csv&#39;, &#39;s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_customer_info.csv&#39;, &#39;s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_utility/b4003acdf33e48ce989401e92146923c.csv&#39;] . # list glue catalog tables wr.catalog.tables() . Database Table Description TableType Columns Partitions . 0 telco_db | telco_churn_utility | | EXTERNAL_TABLE | customerid, vmail_message, day_mins, day_calls... | | . Summary . At this point we have our dataset ready in AWS S3 and Glue, and in the next part we will use AWS SageMaker Data Wrangler to import and join this data. .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/17/aws-sagemaker-wrangler-p1.html",
            "relUrl": "/aws/ml/sagemaker/2022/05/17/aws-sagemaker-wrangler-p1.html",
            "date": " • May 17, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "AWS Machine Learning Certification Notes (MLS-C01)",
            "content": ". About . This post is a compilation of important notes and references for AWS Machine Learning Certification MLSC01. . Notes . Amazon Comprehend . Amazon Comprehend is a natural-language processing (NLP) service that uses machine learning to uncover valuable insights and connections in text. . References . https://aws.amazon.com/comprehend/ . Amazon Rekognition . Amazon Rekognition offers pre-trained and customizable computer vision (CV) capabilities to extract information and insights from your images and videos. . References . https://aws.amazon.com/rekognition/ . Amazon Polly . Amazon Polly is a service that turns text into lifelike speech, allowing you to create applications that talk, and build entirely new categories of speech-enabled products. . References . https://aws.amazon.com/polly/ . Amazon Lex . Amazon Lex is a fully managed artificial intelligence (AI) service with advanced natural language models to design, build, test, and deploy conversational interfaces in applications (chat bots). . References . https://aws.amazon.com/lex/ . Amazon Transcribe . Amazon Transcribe is an automatic speech recognition service that makes it easy to add speech to text capabilities to any application. Transcribe’s features enable you to ingest audio input, produce easy to read and review transcripts, improve accuracy with customization, and filter content to ensure customer privacy. . References . https://aws.amazon.com/transcribe/ . Latent Dirichlet Allocation (LDA) Algorithm . It is a topic modeling technique to generate abstract topics based on word frequency from a set of documents | It is similar to unsupervised classification of documents | It is useful for automatically organizing, summerizing, understanding and searching large electronic archives. It can help in discovering hidden themes in the collection | classifying document into dicoverable themes | organize/summerize/search the documents | . | . References . https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2 . Multinomial Logistic Regression Algorithm . Multinomial Logistic Regression is an extension of logistic regression (supervised) that allows more than two discrete outcomes (multiclass). . References . https://en.wikipedia.org/wiki/Multinomial_logistic_regression . Factorization Machines Algorithm . The Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation. . References . https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html . Sequence to Sequence (seq2seq) Algorithm . Amazon SageMaker Sequence to Sequence is a supervised learning algorithm where the input is a sequence of tokens (for example, text, audio) and the output generated is another sequence of tokens. Example applications include . machine translation (input a sentence from one language and predict what that sentence would be in another language) | text summarization (input a longer string of words and predict a shorter string of words that is a summary) | speech-to-text (audio clips converted into output sentences in tokens) | . Problems in this domain have been successfully modeled with deep neural networks that show a significant performance boost over previous methodologies. Amazon SageMaker seq2seq uses Recurrent Neural Networks (RNNs) and Convolutional Neural Network (CNN) models with attention as encoder-decoder architectures. . References . https://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq.html . Term frequency-inverse document frequency Algorithm . TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. . This is done by multiplying two metrics: how many times a word appears in a document (frequency), and the inverse document frequency of the word across a set of documents. . The term frequency of a word in a document. There are several ways of calculating this frequency, with the simplest being a raw count of instances a word appears in a document. | The inverse document frequency of the word across a set of documents. This means, how common or rare a word is in the entire document set. The closer it is to 0, the more common a word is. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm. So, if the word is very common and appears in many documents, this number will approach 0. Otherwise, it will approach 1 | . It has many uses, most importantly in automated text analysis, and is very useful for scoring words in machine learning algorithms for Natural Language Processing (NLP). . References . https://monkeylearn.com/blog/what-is-tf-idf . BlazingText Algorithm . The Amazon SageMaker BlazingText algorithm provides highly optimized implementations of the Word2vec and text classification algorithms. The Word2vec algorithm is useful for many downstream natural language processing (NLP) tasks, such as sentiment analysis, named entity recognition, machine translation, etc. Text classification is an important task for applications that perform web searches, information retrieval, ranking, and document classification. . The Word2vec algorithm maps words to high-quality distributed vectors. The resulting vector representation of a word is called a word embedding. Words that are semantically similar correspond to vectors that are close together. That way, word embeddings capture the semantic relationships between words. . References . https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html . Amazon SageMaker Batch Transform . Use batch transform when you need to do the following: . Preprocess datasets to remove noise or bias that interferes with training or inference from your dataset. | Get inferences from large datasets. | Run inference when you don&#39;t need a persistent endpoint. | Associate input records with inferences to assist the interpretation of results. | . References . https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html . Amazon SageMaker Real-time inference / Hosting Services . Real-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. You can deploy your model to SageMaker hosting services and get an endpoint that can be used for inference. These endpoints are fully managed and support autoscaling. . References . https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html . Amazon SageMaker Inference Pipeline . An inference pipeline is a Amazon SageMaker model that is composed of a linear sequence of two to fifteen containers that process requests for inferences on data. You use an inference pipeline to define and deploy any combination of pretrained SageMaker built-in algorithms and your own custom algorithms packaged in Docker containers. You can use an inference pipeline to combine preprocessing, predictions, and post-processing data science tasks. Inference pipelines are fully managed. . Within an inference pipeline model, SageMaker handles invocations as a sequence of HTTP requests. The first container in the pipeline handles the initial request, then the intermediate response is sent as a request to the second container, and so on, for each container in the pipeline. SageMaker returns the final response to the client. . When you deploy the pipeline model, SageMaker installs and runs all of the containers on each Amazon Elastic Compute Cloud (Amazon EC2) instance in the endpoint or transform job. Feature processing and inferences run with low latency because the containers are co-located on the same EC2 instances. . References . https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html . Amazon SageMaker Neo . Amazon SageMaker Neo automatically optimizes machine learning models for inference on cloud instances and edge devices to run faster with no loss in accuracy. You start with a machine learning model already built with DarkNet, Keras, MXNet, PyTorch, TensorFlow, TensorFlow-Lite, ONNX, or XGBoost and trained in Amazon SageMaker or anywhere else. Then you choose your target hardware platform, which can be a SageMaker hosting instance or an edge device based on processors from Ambarella, Apple, ARM, Intel, MediaTek, Nvidia, NXP, Qualcomm, RockChip, Texas Instruments, or Xilinx. With a single click, SageMaker Neo optimizes the trained model and compiles it into an executable. The compiler uses a machine learning model to apply the performance optimizations that extract the best available performance for your model on the cloud instance or edge device. You then deploy the model as a SageMaker endpoint or on supported edge devices and start making predictions. . References . https://aws.amazon.com/sagemaker/neo/ . LSTM / Long Short-Term Memory . LSTM is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can process not only single data points (such as images), but also entire sequences of data (such as speech or video). . LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTM is applicable to tasks such as anomaly detection in network traffic or IDSs (intrusion detection systems) . References . https://en.wikipedia.org/wiki/Long_short-term_memory . Semantic Segmentation Algorithm . The SageMaker semantic segmentation algorithm provides a fine-grained, pixel-level approach to developing computer vision applications. It tags every pixel in an image with a class label from a predefined set of classes. Tagging is fundamental for understanding scenes, which is critical to an increasing number of computer vision applications, such as self-driving vehicles, medical imaging diagnostics, and robot sensing. . For comparison, the SageMaker Image Classification Algorithm is a supervised learning algorithm that analyzes only whole images, classifying them into one of multiple output categories. The Object Detection Algorithm is a supervised learning algorithm that detects and classifies all instances of an object in an image. It indicates the location and scale of each object in the image with a rectangular bounding box. . Because the semantic segmentation algorithm classifies every pixel in an image, it also provides information about the shapes of the objects contained in the image. The segmentation output is represented as a grayscale image, called a segmentation mask. A segmentation mask is a grayscale image with the same shape as the input image. . The SageMaker semantic segmentation algorithm is built using the MXNet Gluon framework and the Gluon CV toolkit. . References . https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html | . Accuracy . Accuracy measures the fraction of correct predictions. The range is 0 to 1. . Accuracy = (TP + TN) / (TP + FP + TN + FN) . Precision . Precision measures the fraction of actual positives among those examples that are predicted as positive. The range is 0 to 1. This formula tells us that the larger value of FP (False Positives), the lower the Precision. . Precision = TP / (TP + FP) . For maximun precision there should be no FP. FP are also called Type 1 error. . Recall . The Recall measures the fraction of actual positives that are predicted as positive. The range is 0 to 1. This formula tells us that the larger value of FN (False Negatives), the lower the Recall. . Recall = TP / (TP + FN) . For maximun recall there should be no FN. FN are also called Type 2 error. . Note: Precision and Recall are inversely proportional to eachother. . References . https://towardsdatascience.com/model-evaluation-i-precision-and-recall-166ddb257c7b . L1 regularization . L1 regularization, also known as L1 norm or Lasso (in regression problems), combats overfitting by shrinking the parameters towards 0. This makes some features obsolete. . It’s a form of feature selection, because when we assign a feature with a 0 weight, we’re multiplying the feature values by 0 which returns 0, eradicating the significance of that feature. If the input features of our model have weights closer to 0, our L1 norm would be sparse. A selection of the input features would have weights equal to zero, and the rest would be non-zero. . L2 regularization . L2 regularization, or the L2 norm, or Ridge (in regression problems), combats overfitting by forcing weights to be small, but not making them exactly 0. This regularization returns a non-sparse solution since the weights will be non-zero (although some may be close to 0). A major snag to consider when using L2 regularization is that it’s not robust to outliers. . References . https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization . K-means . K-means is a clustering algorithm that tries to partition a set of points into K sets (clusters) such that the points in each cluster tend to be near each other. It is unsupervised because the points have no external classification. . K-nearest neighbors . K-nearest neighbors is a classification (or regression) algorithm that in order to determine the classification of a point, combines the classification of the K nearest points. It is supervised because you are trying to classify a point based on the known classification of other points. . Courses . Exam Readiness: AWS Certified Machine Learning - Specialty . https://explore.skillbuilder.aws/learn/course/27/play/54/exam-readiness-aws-certified-machine-learning-specialty . This is overall a very good short course that can help you identify your strengths and weaknesses in each exam domain so you know where to focus when studying for the exam. . ACloudGuru AWS Certified Machine Learning - Specialty 2020 . https://acloudguru.com/course/aws-certified-machine-learning-specialty . This is a detailed course on the topics covered in the exam. But this course lacks on &quot;Modeling&quot; domain and hands-on labs. Besides taking this course you should have a good knowledge and working experience in data science and machine learning domain. I already have AI/ML background so it was not an issue for me. Some people have recommended taking Machine Learning, Data Science and Deep Learning with Python from Frank Kane on Udemy if you don&#39;t have an ML background but I am not sure about it&#39;s worth. . Practice Projects . Besides preparing for the exam you should do some projects to build good hands-on knowledge. For this you can use How-To Guides from AWS Getting Started Resource Center (Link Here). Some of my favorite projects are . Build, train, deploy, and monitor a machine learning model with Amazon SageMaker Studio | Optimizing and Scaling Machine Learning Training with Managed Spot Training for Amazon SageMaker | . Practice Dumps . For exam practice tests I have used Jon Bonso Udemy course AWS Certified Machine Learning Specialty Practice Exams . Other Tips . About a week before your exam date start checking Reddit Communities. From time to time people post about their achievements and experiences on taking the exam. People also mention the services or topics that they were asked about during the exam. Keep a close eye on such posts and try to find any topic that you have not covered before. .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/ml/2022/05/14/aws-ml-cert-notes.html",
            "relUrl": "/aws/ml/2022/05/14/aws-ml-cert-notes.html",
            "date": " • May 14, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Kaggle - Blue Book for Bulldozers",
            "content": ". About . This notebook explores and builds a model for a bulldozer auction prices dataset from the Kaggle competition. . Environment Details . from platform import python_version import sklearn, numpy, matplotlib, pandas print(&quot;python==&quot; + python_version()) print(&quot;sklearn==&quot; + sklearn.__version__) print(&quot;numpy==&quot; + numpy.__version__) print(&quot;pandas==&quot; + pandas.__version__) print(&quot;matplotlib==&quot; + matplotlib.__version__) . . python==3.8.8 sklearn==1.0.2 numpy==1.20.1 pandas==1.2.3 matplotlib==3.5.1 . # Notebook settings import pandas as pd # display all dataframe columns pd.set_option(&#39;display.max_columns&#39;, None) . Prepare the dataset . Download the dataset files . Train.zip and extract Train.csv. This is our training dataset. | Valid.csv. This is our validation dataset. | Test.csv. This is our test dataset. | . This dataset can be downloaded from the Kaggle competition site, and extracted files should be placed under folder ./datasets/2022-04-35-bluebook-for-bulldozers/. These files are made available with this notebook in the GitHub repository and can be downloaded from there too. If you are using Git then it will not download them from the remote server as they exceed 50MB limit (read more here). For working with large files Git needs an extra extension to work with them called git-lfs. . Follow the steps from Git-LFS site to install it on the system. To install it directly from the notebook (running on Linux) use these commands . # download and install git-lfs. Uncomment them as execute. # !curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash # !sudo yum install git-lfs -y # !git lfs install . Once git-lfs is installed use the below command to download large files from the remote server (GitHub). . # uncomment them and execute # git lfs fetch --all # git lfs checkout . Take an initial look at the training data . Load the training data and look for the following information . column names | column data types | how much data is missing? | sample data elements | . # load the training dataset dataset_path = &#39;datasets/2022-04-35-bluebook-for-bulldozers/&#39; df_raw = pd.read_csv(dataset_path+&#39;Train.csv&#39;, low_memory=False) df = df_raw.copy() . # print training dataset summary information df_info = pd.DataFrame() df_info[&#39;sample&#39;] = df.iloc[0] df_info[&#39;data_type&#39;] = df.dtypes df_info[&#39;percent_missing&#39;] = 100*df.isnull().sum() / len(df) print(f&quot;Total features: {len(df.columns)}&quot;) df_info.sort_values(&#39;percent_missing&#39;) . sample data_type percent_missing . SalesID 1139246 | int64 | 0.000000 | . state Alabama | object | 0.000000 | . fiProductClassDesc Wheel Loader - 110.0 to 120.0 Horsepower | object | 0.000000 | . fiBaseModel 521 | object | 0.000000 | . fiModelDesc 521D | object | 0.000000 | . ProductGroup WL | object | 0.000000 | . saledate 11/16/2006 0:00 | object | 0.000000 | . datasource 121 | int64 | 0.000000 | . ModelID 3157 | int64 | 0.000000 | . MachineID 999089 | int64 | 0.000000 | . SalePrice 66000 | int64 | 0.000000 | . YearMade 2004 | int64 | 0.000000 | . ProductGroupDesc Wheel Loader | object | 0.000000 | . Enclosure EROPS w AC | object | 0.081022 | . auctioneerID 3.0 | float64 | 5.019882 | . Hydraulics 2 Valve | object | 20.082269 | . fiSecondaryDesc D | object | 34.201558 | . Coupler None or Unspecified | object | 46.662013 | . Forks None or Unspecified | object | 52.115425 | . ProductSize NaN | object | 52.545964 | . Transmission NaN | object | 54.320972 | . Ride_Control None or Unspecified | object | 62.952696 | . MachineHoursCurrentMeter 68.0 | float64 | 64.408850 | . Drive_System NaN | object | 73.982923 | . Ripper NaN | object | 74.038766 | . Undercarriage_Pad_Width NaN | object | 75.102026 | . Thumb NaN | object | 75.247616 | . Stick_Length NaN | object | 75.265067 | . Pattern_Changer NaN | object | 75.265067 | . Grouser_Type NaN | object | 75.281271 | . Track_Type NaN | object | 75.281271 | . Tire_Size None or Unspecified | object | 76.386912 | . Travel_Controls NaN | object | 80.097476 | . Blade_Type NaN | object | 80.097725 | . Turbocharged NaN | object | 80.271985 | . Stick NaN | object | 80.271985 | . Pad_Type NaN | object | 80.271985 | . Backhoe_Mounting NaN | object | 80.387161 | . fiModelDescriptor NaN | object | 82.070676 | . UsageBand Low | object | 82.639078 | . Differential_Type Standard | object | 82.695918 | . Steering_Controls Conventional | object | 82.706388 | . fiModelSeries NaN | object | 85.812901 | . Coupler_System NaN | object | 89.165971 | . Grouser_Tracks NaN | object | 89.189903 | . Hydraulics_Flow NaN | object | 89.189903 | . Scarifier NaN | object | 93.710190 | . Pushblock NaN | object | 93.712932 | . Engine_Horsepower NaN | object | 93.712932 | . Enclosure_Type NaN | object | 93.712932 | . Blade_Width NaN | object | 93.712932 | . Blade_Extension NaN | object | 93.712932 | . Tip_Control NaN | object | 93.712932 | . # print some unique values against each feature def sniff(df, rows=7): &quot;&quot;&quot; For each column return a set of unique values &quot;&quot;&quot; data = {} for col in df.columns: data[col] = df[col].unique()[:rows] return pd.DataFrame.from_dict(data, orient=&#39;index&#39;).T . sniff(df) . SalesID SalePrice MachineID ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter UsageBand saledate fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Enclosure Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Hydraulics_Flow Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls . 0 1139246 | 66000 | 999089 | 3157 | 121 | 3.0 | 2004 | 68.0 | Low | 11/16/2006 0:00 | 521D | 521 | D | NaN | NaN | NaN | Wheel Loader - 110.0 to 120.0 Horsepower | Alabama | WL | Wheel Loader | NaN | EROPS w AC | None or Unspecified | NaN | None or Unspecified | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 2 Valve | NaN | NaN | NaN | NaN | None or Unspecified | None or Unspecified | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Standard | Conventional | . 1 1139248 | 57000 | 117657 | 77 | 132 | 1.0 | 1996 | 4640.0 | High | 3/26/2004 0:00 | 950FII | 950 | F | II | LC | Medium | Wheel Loader - 150.0 to 175.0 Horsepower | North Carolina | SSL | Skid Steer Loaders | Four Wheel Drive | OROPS | NaN | None or Unspecified | NaN | Extended | Powershuttle | None or Unspecified | Yes | None or Unspecified | None or Unspecified | No | Auxiliary | None or Unspecified | None or Unspecified | Yes | Sideshift &amp; Tip | 23.5 | NaN | None or Unspecified | None or Unspecified | Standard | Steel | None or Unspecified | None or Unspecified | None or Unspecified | None or Unspecified | Double | None or Unspecified | PAT | None or Unspecified | NaN | NaN | . 2 1139249 | 10000 | 434808 | 7009 | 136 | 2.0 | 2001 | 2838.0 | Medium | 2/26/2004 0:00 | 226 | 226 | NaN | -6E | 6 | Small | Skid Steer Loader - 1351.0 to 1601.0 Lb Operat... | New York | TEX | Track Excavators | Two Wheel Drive | EROPS | Yes | Reversible | No | Standard | Standard | Yes | None or Unspecified | 12&#39; | Low Profile | Variable | NaN | Yes | Yes | None or Unspecified | None or Unspecified | NaN | Manual | Yes | Yes | High Flow | Rubber | 16 inch | 11&#39; 0&quot; | Hydraulic | Yes | Triple | Yes | None or Unspecified | Differential Steer | Limited Slip | Command Control | . 3 1139251 | 38500 | 1026470 | 332 | 149 | 11.0 | 2007 | 3486.0 | NaN | 5/19/2011 0:00 | PC120-6E | PC120 | G | LC | L | Large / Medium | Hydraulic Excavator, Track - 12.0 to 14.0 Metr... | Texas | BL | Backhoe Loaders | No | NaN | None | Street | Yes | None | Powershift | None | None | 14&#39; | High Profile | None | Standard | None | Single Shank | None | Tip | 13&quot; | Hydraulic | None | None | None or Unspecified | None | 32 inch | 15&#39; 9&quot; | Manual | No | Single | None | Semi U | Lever | No Spin | Four Wheel Standard | . 4 1139253 | 11000 | 1057373 | 17311 | 172 | 4.0 | 1993 | 722.0 | None | 7/23/2009 0:00 | S175 | S175 | E | -5 | LT | Mini | Skid Steer Loader - 1601.0 to 1751.0 Lb Operat... | Arizona | TTT | Track Type Tractors | All Wheel Drive | EROPS AC | None | Grouser | None | None | None or Unspecified | None | None | 13&#39; | None | None | Base + 1 Function | None | Multi Shank | None | None | 26.5 | None | None | None | None | None | 28 inch | 10&#39; 2&quot; | None | None | None | None | VPAT | Finger Tip | Locking | Wheel | . 5 1139255 | 26500 | 1001274 | 4605 | None | 7.0 | 2008 | 508.0 | None | 12/18/2008 0:00 | 310G | 310 | HAG | III | CR | Large | Backhoe Loader - 14.0 to 15.0 Ft Standard Digg... | Florida | MG | Motor Graders | None | NO ROPS | None | None | None | None | Hydrostatic | None | None | 16&#39; | None | None | Base + 3 Function | None | None | None | None | 29.5 | None | None | None | None | None | 30 inch | 10&#39; 6&quot; | None | None | None | None | Straight | 2 Pedal | None | No | . 6 1139256 | 21000 | 772701 | 1937 | None | 99.0 | 1000 | 11540.0 | None | 8/26/2004 0:00 | 790ELC | 790 | B | -1 | SB | Compact | Hydraulic Excavator, Track - 21.0 to 24.0 Metr... | Illinois | None | None | None | None or Unspecified | None | None | None | None | Autoshift | None | None | &lt;12&#39; | None | None | 4 Valve | None | None | None | None | 14&quot; | None | None | None | None | None | 22 inch | 9&#39; 10&quot; | None | None | None | None | Angle | Pedal | None | None | . From this first look at the data, we can see that . data is of three types numeric | string | datetime | . | some columns have missing data up to 94% e.g. Tip_Control | missing data is represented as NaN | None or unspecified | . | some columns&#39; data types need to be corrected for example SaleID, MachineID are represented as integers but they are categorical nominal features meaning each value is discrete and has no relation among them | UsageBand is of type string but is a categorical ordinal feature meaning their values cannot be measured but have some order between them | Tire_size, Stick_length are actual measurements and need to be converted to appropriate units | . | . Baseline Model . It is a good idea to create a baseline model early in the data science project as it can help to establish a baseline for . time it takes to train a model if the baseline model is taking too much time then we may use a smaller set of the training data for further steps | . | feature importances it can help us establish a relationship between features and the target | help us remove features that have no relationship with the target sooner | . | model performance we can take this model performance as a baseline, and compare it to see how much cleanup and feature engineering steps improve the model performance | . | . For the baseline model, we would have to rely on numerical features as they don&#39;t require any preprocessing and can be readily used. Some numerical features have too much missing data so we have to be selective here. . # filter columns that are not string along with their percentage of missing data numerical_features = df_info.loc[df_info.data_type != &#39;object&#39;].sort_values(&#39;percent_missing&#39;) numerical_features . sample data_type percent_missing . SalesID 1139246 | int64 | 0.000000 | . SalePrice 66000 | int64 | 0.000000 | . MachineID 999089 | int64 | 0.000000 | . ModelID 3157 | int64 | 0.000000 | . datasource 121 | int64 | 0.000000 | . YearMade 2004 | int64 | 0.000000 | . auctioneerID 3.0 | float64 | 5.019882 | . MachineHoursCurrentMeter 68.0 | float64 | 64.408850 | . From these numerical features MachineHoursCurrentMeter has around 64% missing data. Let&#39;s keep this feature as well for our baseline model. . # establish target and baseline features target = &#39;SalePrice&#39; # this is the feature we are trying to predict baseline_features = list(numerical_features.index) baseline_features.remove(target) # remove target feature form input variables baseline_features . [&#39;SalesID&#39;, &#39;MachineID&#39;, &#39;ModelID&#39;, &#39;datasource&#39;, &#39;YearMade&#39;, &#39;auctioneerID&#39;, &#39;MachineHoursCurrentMeter&#39;] . We have established our target and features, and can now train our baseline model. We will use only RandomForrest for this dataset. We have 7 features to learn from so let&#39;s start with n_estimators=70 . from sklearn.ensemble import RandomForestRegressor X, y = df[baseline_features], df[target] X = X.fillna(0) # replace missing numerical values with 0 rf = RandomForestRegressor(n_estimators=70, oob_score=True, n_jobs=-1, verbose=1) rf.fit(X, y) oob_score = rf.oob_score_ oob_score . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 1.2min [Parallel(n_jobs=-1)]: Done 70 out of 70 | elapsed: 1.9min finished . 0.7901663917842495 . Besides the OOB score, we can also interpret our model by looking into the trained model trees&#39; depth and leaves. If OOB score is same but our trees are smaller with fewer nodes then that model is better and more generalized. Larger trees make the model more complex and less generalized. For this let&#39;s create two more functions. . import numpy as np def rf_n_leaves(rf): &quot;&quot;&quot; return the total number of nodes in all the trees of the forrest. &quot;&quot;&quot; return sum(est.tree_.n_leaves for est in rf.estimators_) def rf_m_depth(rf): &quot;&quot;&quot; return a median height of all the trees of the forrest. &quot;&quot;&quot; return np.median([est.tree_.max_depth for est in rf.estimators_]) . # print model oob_score, number of forrest leaves and median height n_leaves = rf_n_leaves(rf) m_depth = rf_m_depth(rf) print(f&quot;OOB scrore = {oob_score: .3f} nTree leaves = {n_leaves: ,d} nMedian depth = {m_depth}&quot;) . OOB scrore = 0.790 Tree leaves = 16,209,726 Median depth = 53.5 . Our baseline model has an OOB score of just around 79% which is not bad as a starter. Now let&#39;s also plot the feature importance for this model. . def plot_feature_importance(feature_importance, feature_names, figsize=(7,7)): &quot;&quot;&quot; plot the feature importances in a bar graph along with feature names. &quot;&quot;&quot; fimp = pd.Series(feature_importance, feature_names) fimp.nlargest(len(fimp)).plot(figsize=figsize, kind=&#39;barh&#39;).invert_yaxis() . feature_importance = rf.feature_importances_ feature_names = X.columns plot_feature_importance(feature_importance, feature_names) . From this feature importance plot, we can see that . ModelID is the highest predictor of SalePrice. This could be because vehicles belonging to a certain ModelID category could have their SalePrice in the same range. | SalesID and MachineID are coming up next as important features. This is not a good signal as both these features are unique for sale transactions and machine identification. MachineID also has inconsistencies as noted in this kaggle discussion. A model using these features will not be generalized. It would be better if we remove these features altogether otherwise they can affect the model&#39;s performance. | YearMade comes next which also makes sense as older vehicles will have less price compared to the new ones. | . Cleaning up . In this section we will remove unimportanct features and fix the data types of remaining features. . Remove ID columns . As noted in last section we noted that following ID features can be removed from the dataset. . SalesID | MachineID | . del df[&#39;SalesID&#39;] del df[&#39;MachineID&#39;] . Fix data types and data issues . Let&#39;s visit each feature from our dataset and check whether we need to fix the data type. Use df_info created in the last section to verify the data types of each feature. . Numerical features . Let&#39;s first visit the numerical feature. . auctioneerID . It has the datatype as float64 but this feature is actually categorical nominal as each ID is discrete and has no relation between them. It should be of type str. So let&#39;s fix that. . df[&#39;auctioneerID&#39;] = df[&#39;auctioneerID&#39;].astype(str) . Datetime feature . Let&#39;s visit DateTime features and correct their data type . saledate . &#39;saledate&#39; is a DateTime feature. So let&#39;s correct its data type. . df[&#39;saledate&#39;] = pd.to_datetime(df[&#39;saledate&#39;]) . Categorical features . Let&#39;s now visit the categorical features. . For categorical features, there is no better way than printing the unique values for each column and spend some time analyzing the values. Analyze . if the feature has some missing values | if there are any missing values but are represented by some other value like &#39;Unspecified&#39;, &#39;None or Unspecified&#39; | keep a separate sheet with all the features and make notes for each feature like there are no further actions required. The feature is good for use | need to replace missing values | any other observations | etc. | . | . Transform missing values . After visiting all the features we have found that missing values are represented in multiple ways like . Unspecified | None or Unspecified | None | #NAME? | &quot;&quot; | . So we would transform and replace all these values with np.nan so they all represent the same thing. . # before transformation. # let&#39;s use this feature to verify results. df[&#39;Hydraulics&#39;].unique() . array([&#39;2 Valve&#39;, &#39;Auxiliary&#39;, nan, &#39;Standard&#39;, &#39;Base + 1 Function&#39;, &#39;Base + 3 Function&#39;, &#39;4 Valve&#39;, &#39;3 Valve&#39;, &#39;Base + 2 Function&#39;, &#39;Base + 4 Function&#39;, &#39;None or Unspecified&#39;, &#39;Base + 5 Function&#39;, &#39;Base + 6 Function&#39;], dtype=object) . def normalize_str_values(df): &quot;&quot;&quot; normalize dataframe str values * transform case to lowercase * replace missing values with np.nan &quot;&quot;&quot; for col in df.columns: if df[col].dtype == object: print(f&quot;normalize column: {col}&quot;) df[col] = df[col].str.lower() df[col] = df[col].fillna(np.nan) df[col] = df[col].replace(&#39;unspecified&#39;, np.nan) df[col] = df[col].replace(&#39;none or unspecified&#39;, np.nan) df[col] = df[col].replace(&#39;none&#39;, np.nan) df[col] = df[col].replace(&#39;#name?&#39;, np.nan) df[col] = df[col].replace(&#39;&#39;, np.nan) normalize_str_values(df) . normalize column: auctioneerID normalize column: UsageBand normalize column: fiModelDesc normalize column: fiBaseModel normalize column: fiSecondaryDesc normalize column: fiModelSeries normalize column: fiModelDescriptor normalize column: ProductSize normalize column: fiProductClassDesc normalize column: state normalize column: ProductGroup normalize column: ProductGroupDesc normalize column: Drive_System normalize column: Enclosure normalize column: Forks normalize column: Pad_Type normalize column: Ride_Control normalize column: Stick normalize column: Transmission normalize column: Turbocharged normalize column: Blade_Extension normalize column: Blade_Width normalize column: Enclosure_Type normalize column: Engine_Horsepower normalize column: Hydraulics normalize column: Pushblock normalize column: Ripper normalize column: Scarifier normalize column: Tip_Control normalize column: Tire_Size normalize column: Coupler normalize column: Coupler_System normalize column: Grouser_Tracks normalize column: Hydraulics_Flow normalize column: Track_Type normalize column: Undercarriage_Pad_Width normalize column: Stick_Length normalize column: Thumb normalize column: Pattern_Changer normalize column: Grouser_Type normalize column: Backhoe_Mounting normalize column: Blade_Type normalize column: Travel_Controls normalize column: Differential_Type normalize column: Steering_Controls . . # after transformation. # remember that transformation is applied to all string type columns. We are using just one column to verify the results. df[&#39;Hydraulics&#39;].unique() . array([&#39;2 valve&#39;, &#39;auxiliary&#39;, nan, &#39;standard&#39;, &#39;base + 1 function&#39;, &#39;base + 3 function&#39;, &#39;4 valve&#39;, &#39;3 valve&#39;, &#39;base + 2 function&#39;, &#39;base + 4 function&#39;, &#39;base + 5 function&#39;, &#39;base + 6 function&#39;], dtype=object) . Transform measurements . Some features are represented as a string but actually they are numerical measurement values. For example . Tire_Size has the size in inches with a symbol attached &quot; | Undercarriage_Pad_Width has the size in inches with the unit attached inch | Blade_Width has the size in cm with a symbol attached &#39;. It also has values less the 12cm represented as &lt;12&#39; | Stick_Length has values in both feet and inches. We can simply convert them from 19 &#39;8&quot; to 19.8 | After the above transformations, their data types should be converted to numeric | . let&#39;s apply these changes to our dataset. . # before transformation for col in [&#39;Tire_Size&#39;, &#39;Undercarriage_Pad_Width&#39;, &#39;Blade_Width&#39;, &#39;Stick_Length&#39;]: print(f&quot;**{col}**: &quot;, df[col].unique()) . **Tire_Size**: [nan &#39;23.5&#39; &#39;13&#34;&#39; &#39;26.5&#39; &#39;29.5&#39; &#39;14&#34;&#39; &#39;20.5&#39; &#39;17.5&#34;&#39; &#39;15.5&#34;&#39; &#39;20.5&#34;&#39; &#39;17.5&#39; &#39;7.0&#34;&#39; &#39;15.5&#39; &#39;23.5&#34;&#39; &#39;10&#34;&#39; &#39;23.1&#34;&#39; &#39;10 inch&#39;] **Undercarriage_Pad_Width**: [nan &#39;16 inch&#39; &#39;32 inch&#39; &#39;28 inch&#39; &#39;30 inch&#39; &#39;22 inch&#39; &#39;24 inch&#39; &#39;18 inch&#39; &#39;36 inch&#39; &#39;20 inch&#39; &#39;27 inch&#39; &#39;15 inch&#39; &#39;26 inch&#39; &#39;34 inch&#39; &#39;33 inch&#39; &#39;14 inch&#39; &#39;31 inch&#39; &#39;25 inch&#39; &#39;31.5 inch&#39;] **Blade_Width**: [nan &#34;12&#39;&#34; &#34;14&#39;&#34; &#34;13&#39;&#34; &#34;16&#39;&#34; &#34;&lt;12&#39;&#34;] **Stick_Length**: [nan &#39;11 &#39; 0&#34;&#39; &#39;15 &#39; 9&#34;&#39; &#39;10 &#39; 2&#34;&#39; &#39;10 &#39; 6&#34;&#39; &#39;9 &#39; 10&#34;&#39; &#39;10 &#39; 10&#34;&#39; &#39;9 &#39; 6&#34;&#39; &#39;9 &#39; 7&#34;&#39; &#39;12 &#39; 8&#34;&#39; &#39;8 &#39; 2&#34;&#39; &#39;8 &#39; 6&#34;&#39; &#39;9 &#39; 8&#34;&#39; &#39;12 &#39; 10&#34;&#39; &#39;11 &#39; 10&#34;&#39; &#39;8 &#39; 10&#34;&#39; &#39;8 &#39; 4&#34;&#39; &#39;12 &#39; 4&#34;&#39; &#39;9 &#39; 5&#34;&#39; &#39;6 &#39; 3&#34;&#39; &#39;14 &#39; 1&#34;&#39; &#39;13 &#39; 7&#34;&#39; &#39;13 &#39; 10&#34;&#39; &#39;13 &#39; 9&#34;&#39; &#39;7 &#39; 10&#34;&#39; &#39;15 &#39; 4&#34;&#39; &#39;9 &#39; 2&#34;&#39; &#39;24 &#39; 3&#34;&#39; &#39;19 &#39; 8&#34;&#39;] . df[&#39;Stick_Length&#39;] = df[&#39;Stick_Length&#39;].replace(r&quot;&#39; &quot;, &quot;.&quot;, regex=True) for col in [&#39;Tire_Size&#39;, &#39;Undercarriage_Pad_Width&#39;, &#39;Blade_Width&#39;, &#39;Stick_Length&#39;]: df[col] = df[col].str.extract(r&#39;([0-9.]*)&#39;, expand=True) df[col] = df[col].replace(&#39;&#39;, np.nan) df[col] = pd.to_numeric(df[col]) . # after transformation for col in [&#39;Tire_Size&#39;, &#39;Undercarriage_Pad_Width&#39;, &#39;Blade_Width&#39;, &#39;Stick_Length&#39;]: print(f&quot;**{col}**: &quot;, df[col].unique()) . **Tire_Size**: [ nan 23.5 13. 26.5 29.5 14. 20.5 17.5 15.5 7. 10. 23.1] **Undercarriage_Pad_Width**: [ nan 16. 32. 28. 30. 22. 24. 18. 36. 20. 27. 15. 26. 34. 33. 14. 31. 25. 31.5] **Blade_Width**: [nan 12. 14. 13. 16.] **Stick_Length**: [ nan 11. 15.9 10.2 10.6 9.1 10.1 9.6 9.7 12.8 8.2 8.6 9.8 12.1 11.1 8.1 8.4 12.4 9.5 6.3 14.1 13.7 13.1 13.9 7.1 15.4 9.2 24.3 19.8] . Dealing with missing data . Replace missing numeric values . For numerical features, we will follow the following approach to replace missing values . For a column x create a new column x_na where x_na[i] is marked as True if x[i] is missing | Replace the missing values in the x column with a median value | . def fix_missing_num(df, colname): &quot;&quot;&quot; replace missing values with * median value * flag the missing value in a separate *_na column &quot;&quot;&quot; df[colname+&#39;_na&#39;] = pd.isnull(df[colname]) df[colname].fillna(df[colname].median(), inplace=True) . YearMade . &quot;YearMade&quot; doesn&#39;t show any missing values but if we look closely at the data we will find that some instances have the value &quot;1000&quot;. The year 1000 is very unlikely for any vehicle to be made in and we can consider these instances as missing values. Let&#39;s do that . # befor transformation df.plot.scatter(&#39;YearMade&#39;, &#39;SalePrice&#39;) . &lt;AxesSubplot:xlabel=&#39;YearMade&#39;, ylabel=&#39;SalePrice&#39;&gt; . df.loc[df.YearMade==1000, &#39;YearMade&#39;] = np.nan . # after transformation df.plot.scatter(&#39;YearMade&#39;, &#39;SalePrice&#39;) . &lt;AxesSubplot:xlabel=&#39;YearMade&#39;, ylabel=&#39;SalePrice&#39;&gt; . The plot now shows a more clear relationship between &#39;YearMade&#39; and &#39;SalePrice&#39;. But the spike in the year 1920 is still concerning. Most probably it is also a recording error when the manufacturing year was not known then it was assigned some lowest available value in the system (similar to the year 1000). Let&#39;s take this assumption that manufacturing years before 1950 are unknown and should be assigned np.nan . df.loc[df.YearMade&lt;1950, &#39;YearMade&#39;] = np.nan . # after transformation df.plot.scatter(&#39;YearMade&#39;, &#39;SalePrice&#39;) . &lt;AxesSubplot:xlabel=&#39;YearMade&#39;, ylabel=&#39;SalePrice&#39;&gt; . Let&#39;s also replace the missing values with the function created above. . fix_missing_num(df, &#39;YearMade&#39;) . MachineHoursCurrentMeter . The next numerical feature that comes is MachineHoursCurrentMeter. This feature tells us the number of hours a machine has been in use when it was brought to the auction. So older machines are much more likely to have more hours on them as compared to newer machines. There should be a correlation between machine hours and the vehicle in use period (a period between manufacturing and auction). To verify this relationship we first need to find the period in years between manufacturing and auction. We have the &#39;YearMade&#39; that tells us when the vehicle was made. We have the &#39;saledate&#39; which is a DateTime string object but we can use it to find the &#39;YearSold&#39;. . df[&#39;YearSold&#39;] = df[&#39;saledate&#39;].dt.year . # verify that we have correct data df[[&#39;saledate&#39;, &#39;YearSold&#39;]].head() . saledate YearSold . 0 2006-11-16 | 2006 | . 1 2004-03-26 | 2004 | . 2 2004-02-26 | 2004 | . 3 2011-05-19 | 2011 | . 4 2009-07-23 | 2009 | . Now we can use &#39;YearMade&#39; and &#39;YearSold&#39; to find the number of years the vehicle remained in use. Let&#39;s call this new column &#39;YearsInUse&#39; . df[&#39;YearsInUse&#39;] = df[&#39;YearSold&#39;] - df[&#39;YearMade&#39;] . # verify the results df[[&#39;YearsInUse&#39;, &#39;YearSold&#39;, &#39;YearMade&#39;]].head() . YearsInUse YearSold YearMade . 0 2.0 | 2006 | 2004.0 | . 1 8.0 | 2004 | 1996.0 | . 2 3.0 | 2004 | 2001.0 | . 3 10.0 | 2011 | 2001.0 | . 4 2.0 | 2009 | 2007.0 | . A sold year cannot be less than a manufacturing year. So let&#39;s verify data integrity as well. . df.loc[df.YearsInUse&lt;0, [&#39;YearsInUse&#39;, &#39;saledate&#39;, &#39;YearSold&#39;, &#39;YearMade&#39;]].head() . YearsInUse saledate YearSold YearMade . 24007 -2.0 | 1994-02-11 | 1994 | 1996.0 | . 24009 -1.0 | 1995-04-18 | 1995 | 1996.0 | . 24015 -2.0 | 1994-09-20 | 1994 | 1996.0 | . 24029 -1.0 | 1995-04-28 | 1995 | 1996.0 | . 24064 -1.0 | 1995-04-28 | 1995 | 1996.0 | . YearInUse cannot have a negative value and this shows that either &#39;YearMade&#39; or &#39;saledate&#39; is incorrect. We can assume that error can be with &#39;YearMade&#39; as this is an auction dataset and &#39;saledate&#39; will be more reliable. For entries where &#39;YearMade&#39; is greater than &#39;YearSold&#39; we can replace &#39;YearMade&#39; with &#39;YearSold&#39; (better to have &#39;YearsInUse&#39; equal to zero than negative). . df.loc[df.YearMade&gt;df.YearSold, &#39;YearMade&#39;] = df.YearSold . Let&#39;s recalculate the &#39;YearsInUse&#39; with corrected data. . df[&#39;YearsInUse&#39;] = df[&#39;YearSold&#39;] - df[&#39;YearMade&#39;] . Let&#39;s verify that the data is consistent and all vehicles have &#39;YearMade&#39; less than their &#39;YearSold&#39; . df.loc[df.YearsInUse&lt;0, [&#39;YearsInUse&#39;, &#39;saledate&#39;, &#39;YearSold&#39;, &#39;YearMade&#39;]].head() . YearsInUse saledate YearSold YearMade . We can now plot the relationship between &#39;YearsInUse&#39; and &#39;MachineHoursCurrentMeter&#39; . df.plot.scatter(&#39;YearsInUse&#39;, &#39;MachineHoursCurrentMeter&#39;) . &lt;AxesSubplot:xlabel=&#39;YearsInUse&#39;, ylabel=&#39;MachineHoursCurrentMeter&#39;&gt; . This plot shows that there is some relation between a vehicle being in use and its meter hours. As the &#39;YearsInUse&#39; value increases we also see an increase in meter hours, but after around 15 &#39;YearsInUse&#39; the relationship does not hold on and meter hours start dropping to zero. It means that MachineHoursCurrentMeter data has inconsistencies as many vehicles remained in use for multiple years but they also have zero meter readings. This is very unrealistic and vehicles will not be sitting idle for many years till their auction. It could be that the meter reading for them was not known and 0 could have been used for the &#39;Unspecified or Unknown&#39; value. . Let&#39;s take this assumption and transform &#39;MachineHoursCurrentMeter&#39; to correctly represent that . df.loc[df.MachineHoursCurrentMeter==0, &#39;MachineHoursCurrentMeter&#39;] = np.nan . Also apply our missing values fix on this feature . fix_missing_num(df, &#39;MachineHoursCurrentMeter&#39;) . Tire_Size . The next numerical feature is &#39;Tire_Size&#39;. We can plot the distribution of tire sizes to find any outliers. . df[&#39;Tire_Size&#39;].hist() . &lt;AxesSubplot:&gt; . # print tire sizes np.sort(df[&#39;Tire_Size&#39;].unique()) . array([ 7. , 10. , 13. , 14. , 15.5, 17.5, 20.5, 23.1, 23.5, 26.5, 29.5, nan]) . The plot does not show any outliers and data seems consistant, so we can apply our missing values fix on this feature. . fix_missing_num(df, &#39;Tire_Size&#39;) . Stick_Length . The Next numerical feature is &#39;Stick_Lenght&#39;. Let&#39;s plot the distribution to check for any outliers. . np.sort(df[&#39;Stick_Length&#39;].unique()) . array([ 6.3, 7.1, 8.1, 8.2, 8.4, 8.6, 9.1, 9.2, 9.5, 9.6, 9.7, 9.8, 10.1, 10.2, 10.6, 11. , 11.1, 12.1, 12.4, 12.8, 13.1, 13.7, 13.9, 14.1, 15.4, 15.9, 19.8, 24.3, nan]) . df[&#39;Stick_Length&#39;].plot.hist() . &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . The above plot shows a normal distribution and no outliers. So we can apply our missing values fix on this feature. . fix_missing_num(df, &#39;Stick_Length&#39;) . Undercarriage_Pad_Width . Next numerical feature is &#39;Undercarriage_Pad_Width&#39;. Let&#39;s follow the same steps for this feature. . np.sort(df[&#39;Undercarriage_Pad_Width&#39;].unique()) . array([14. , 15. , 16. , 18. , 20. , 22. , 24. , 25. , 26. , 27. , 28. , 30. , 31. , 31.5, 32. , 33. , 34. , 36. , nan]) . df[&#39;Undercarriage_Pad_Width&#39;].plot.hist() . &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . The distribution for this feature looks fine, and we can apply missing values fix on it. . fix_missing_num(df, &#39;Undercarriage_Pad_Width&#39;) . Blade_Width . Next numerical feature in &#39;Blade_Width&#39;. Following the same steps as before. . np.sort(df[&#39;Blade_Width&#39;].unique()) . array([12., 13., 14., 16., nan]) . df[&#39;Blade_Width&#39;].plot.hist() . &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . Apply the fix on this feature. . fix_missing_num(df, &#39;Blade_Width&#39;) . Replace missing categorical values . encoding and checking the importance We will now replace missing values for categorical features in the following way. . We will label encode them. We will treat them as ordinal features and assign them a numeric value | Missing values will automatically be assigned a value, and that will be 0 | . Some important discussion points on treating nominal categorical features as ordinal and then encoding them. A more prevalent approach is to one hot encode (OHE) them. The drawback of OHE approach is that it makes the decision trees very unbalanced if the dataset has multiple categorical features with high variance. So instead of applying OHE to all features, we will do it in a two-step approach. First, we will label encode them and train a model on them. After that, we will check their feature importance, and if a feature comes up as an important with a low variance then we will use OHE for it. Otherwise we will leave them with label encoding. . More can be read about categorical features encoding from these references . The Mechanics of Machine Learning by Terence Parr and Jeremy Howard section 6.2 | Getting Deeper into Categorical Encodings for Machine Learning | One-Hot Encoding is making your Tree-Based Ensembles worse, here’s why? | . Let&#39;s create some functions to encode our categorical features. . from pandas.api.types import is_categorical_dtype, is_string_dtype def df_string_to_cat(df): for col in df.columns: if is_string_dtype(df[col]): print(f&quot;label encoding applied on {col}&quot;) df[col] = df[col].astype(&#39;category&#39;).cat.as_ordered() def df_cat_to_catcode(df): for col in df.columns: if is_categorical_dtype(df[col]): df[col] = df[col].cat.codes + 1 . Please note that Pandas represents np.nan with category code &quot;-1&quot;, and so adding &quot;1&quot; in function df_cat_to_catcode shifts np.nan to 0 and all category codes to be 1 and above. . # before transformation df.head(5).T.head(10) . 0 1 2 3 4 . SalePrice 66000 | 57000 | 10000 | 38500 | 11000 | . ModelID 3157 | 77 | 7009 | 332 | 17311 | . datasource 121 | 121 | 121 | 121 | 121 | . auctioneerID 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | . YearMade 2004.0 | 1996.0 | 2001.0 | 2001.0 | 2007.0 | . MachineHoursCurrentMeter 68.0 | 4640.0 | 2838.0 | 3486.0 | 722.0 | . UsageBand low | low | high | high | medium | . saledate 2006-11-16 00:00:00 | 2004-03-26 00:00:00 | 2004-02-26 00:00:00 | 2011-05-19 00:00:00 | 2009-07-23 00:00:00 | . fiModelDesc 521d | 950fii | 226 | pc120-6e | s175 | . fiBaseModel 521 | 950 | 226 | pc120 | s175 | . # apply the cat transformation df_string_to_cat(df) df_cat_to_catcode(df) . label encoding applied on auctioneerID label encoding applied on UsageBand label encoding applied on fiModelDesc label encoding applied on fiBaseModel label encoding applied on fiSecondaryDesc label encoding applied on fiModelSeries label encoding applied on fiModelDescriptor label encoding applied on ProductSize label encoding applied on fiProductClassDesc label encoding applied on state label encoding applied on ProductGroup label encoding applied on ProductGroupDesc label encoding applied on Drive_System label encoding applied on Enclosure label encoding applied on Forks label encoding applied on Pad_Type label encoding applied on Ride_Control label encoding applied on Stick label encoding applied on Transmission label encoding applied on Turbocharged label encoding applied on Blade_Extension label encoding applied on Enclosure_Type label encoding applied on Engine_Horsepower label encoding applied on Hydraulics label encoding applied on Pushblock label encoding applied on Ripper label encoding applied on Scarifier label encoding applied on Tip_Control label encoding applied on Coupler label encoding applied on Coupler_System label encoding applied on Grouser_Tracks label encoding applied on Hydraulics_Flow label encoding applied on Track_Type label encoding applied on Thumb label encoding applied on Pattern_Changer label encoding applied on Grouser_Type label encoding applied on Backhoe_Mounting label encoding applied on Blade_Type label encoding applied on Travel_Controls label encoding applied on Differential_Type label encoding applied on Steering_Controls . . # after transformation df.head(5).T.head(10) . 0 1 2 3 4 . SalePrice 66000 | 57000 | 10000 | 38500 | 11000 | . ModelID 3157 | 77 | 7009 | 332 | 17311 | . datasource 121 | 121 | 121 | 121 | 121 | . auctioneerID 23 | 23 | 23 | 23 | 23 | . YearMade 2004.0 | 1996.0 | 2001.0 | 2001.0 | 2007.0 | . MachineHoursCurrentMeter 68.0 | 4640.0 | 2838.0 | 3486.0 | 722.0 | . UsageBand 2 | 2 | 1 | 1 | 3 | . saledate 2006-11-16 00:00:00 | 2004-03-26 00:00:00 | 2004-02-26 00:00:00 | 2011-05-19 00:00:00 | 2009-07-23 00:00:00 | . fiModelDesc 950 | 1725 | 331 | 3674 | 4208 | . fiBaseModel 296 | 527 | 110 | 1375 | 1529 | . Preprocessed dataset . At this point, all our numerical and categorical features have been preprocessed. There should be no missing values, and all categorical features should have been encoded. Only DateTime columns are remaining to be processed and we will do that in the next section. . Let&#39;s verify the data using summary information. . df_info = pd.DataFrame() df_info[&#39;sample&#39;] = df.iloc[0] df_info[&#39;data_type&#39;] = df.dtypes df_info[&#39;percent_missing&#39;] = 100*df.isnull().sum() / len(df) print(f&quot;Total features: {len(df.columns)}&quot;) df_info.sort_values(&#39;percent_missing&#39;) . Total features: 59 . sample data_type percent_missing . SalePrice 66000 | int64 | 0.0 | . Pushblock 0 | int8 | 0.0 | . Ripper 0 | int8 | 0.0 | . Scarifier 0 | int8 | 0.0 | . Tip_Control 0 | int8 | 0.0 | . Tire_Size 20.5 | float64 | 0.0 | . Coupler 0 | int8 | 0.0 | . Coupler_System 0 | int8 | 0.0 | . Grouser_Tracks 0 | int8 | 0.0 | . Hydraulics_Flow 0 | int8 | 0.0 | . Track_Type 0 | int8 | 0.0 | . Undercarriage_Pad_Width 28.0 | float64 | 0.0 | . Stick_Length 9.7 | float64 | 0.0 | . Hydraulics 1 | int8 | 0.0 | . Thumb 0 | int8 | 0.0 | . Grouser_Type 0 | int8 | 0.0 | . Backhoe_Mounting 0 | int8 | 0.0 | . Blade_Type 0 | int8 | 0.0 | . Travel_Controls 0 | int8 | 0.0 | . Differential_Type 4 | int8 | 0.0 | . Steering_Controls 2 | int8 | 0.0 | . YearMade_na False | bool | 0.0 | . YearSold 2006 | int64 | 0.0 | . YearsInUse 2.0 | float64 | 0.0 | . MachineHoursCurrentMeter_na False | bool | 0.0 | . Tire_Size_na True | bool | 0.0 | . Stick_Length_na True | bool | 0.0 | . Pattern_Changer 0 | int8 | 0.0 | . Undercarriage_Pad_Width_na True | bool | 0.0 | . Engine_Horsepower 0 | int8 | 0.0 | . Blade_Width 14.0 | float64 | 0.0 | . ModelID 3157 | int64 | 0.0 | . datasource 121 | int64 | 0.0 | . auctioneerID 23 | int8 | 0.0 | . YearMade 2004.0 | float64 | 0.0 | . MachineHoursCurrentMeter 68.0 | float64 | 0.0 | . UsageBand 2 | int8 | 0.0 | . saledate 2006-11-16 00:00:00 | datetime64[ns] | 0.0 | . fiModelDesc 950 | int16 | 0.0 | . fiBaseModel 296 | int16 | 0.0 | . fiSecondaryDesc 40 | int16 | 0.0 | . fiModelSeries 0 | int8 | 0.0 | . fiModelDescriptor 0 | int16 | 0.0 | . Enclosure_Type 0 | int8 | 0.0 | . ProductSize 0 | int8 | 0.0 | . state 1 | int8 | 0.0 | . ProductGroup 6 | int8 | 0.0 | . ProductGroupDesc 6 | int8 | 0.0 | . Drive_System 0 | int8 | 0.0 | . Enclosure 3 | int8 | 0.0 | . Forks 0 | int8 | 0.0 | . Pad_Type 0 | int8 | 0.0 | . Ride_Control 0 | int8 | 0.0 | . Stick 0 | int8 | 0.0 | . Transmission 0 | int8 | 0.0 | . Turbocharged 0 | int8 | 0.0 | . Blade_Extension 0 | int8 | 0.0 | . fiProductClassDesc 59 | int8 | 0.0 | . Blade_Width_na True | bool | 0.0 | . Let&#39;s retrain our base model one more time but this time with all the features except datetime columns to see where we stand in our OOB score. Below is a utility function created to quickly iterate over model training. . def train_and_plot_model(df, target=&#39;SalePrice&#39;, drop_features=[], n_estimators=70, plot=True, verbose=1): &quot;&quot;&quot; A utility function to train a RandomForrest model on the provided data, and plot the feature importances. Parameters - df: pandas.DataFrame input dataset to be used for training target: str target feature. this is the feature we are trying to predict drop_features: list any features to be dropped before training. Default is empty list. n_estimators: int number of estimators to be used for model training. Default is 50. &quot;&quot;&quot; # target = &#39;SalePrice&#39; # this is the feature we are trying to predict features = list(df.columns) # remove target feature and other specified features form the input variables features.remove(target) for f in drop_features: features.remove(f) X, y = df[features], df[target] rf = RandomForestRegressor(n_estimators, oob_score=True, n_jobs=-1, verbose=verbose) rf.fit(X, y) oob_score = rf.oob_score_ # get trained model leaves and depth n_leaves = rf_n_leaves(rf) m_depth = rf_m_depth(rf) # print trained model info print(f&quot;OOB scrore = {oob_score: .3f} nTree leaves = {n_leaves: ,d} nMedian depth = {m_depth}&quot;) # plot trained model feature importance feature_importance = rf.feature_importances_ if plot: plot_feature_importance(feature_importance, features, (10,15)) # return trained model, feature names, and their importances return (rf, features, feature_importance, oob_score) . # keeping n_estimators same as previous base model i.e. 70 (rf, feature_names, feature_importance, oob_pre) = train_and_plot_model(df, drop_features=[&#39;saledate&#39;], n_estimators=70) . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 2.7min [Parallel(n_jobs=-1)]: Done 70 out of 70 | elapsed: 4.2min finished . OOB scrore = 0.904 Tree leaves = 14,660,873 Median depth = 45.0 . This is a big improvement in our model performance. Our base model had 0.790 OOB score and now we are at 0.904. Our features count has also increased from 7 to 59, so we can take one more shot at it by increasing the estomators count (n_estimators). Let&#39;s use 150 trees this time (double that last time) to see how much effect it can have on model performance. . (rf, feature_names, feature_importance, oob_pre) = train_and_plot_model(df, drop_features=[&#39;saledate&#39;], n_estimators=150, plot=False) . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 2.8min [Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 9.2min finished . OOB scrore = 0.906 Tree leaves = 31,408,663 Median depth = 46.0 . Though there is only a slight increase in model performance but it took us significantly more time to train the model. So we will keep our estimators low and revisit them during the tuning phase. With &quot;70&quot; estimators our model performance is . OOB scrore = 0.904 Tree leaves = 14,660,873 Median depth = 45.0 . At this point, our features have correct data types and their missing values are properly adjusted. We can now focus on some feature engineering aspects. Before moving further let&#39;s also save our dataset till this point so if we make an error we can restart from this checkpoint. . # store preprocessed data as a check point for this state df.to_pickle(dataset_path+&#39;preprocessed.pkl&#39;) . We have used pickle format to preserve data types for saved data. . # load preprocessed data (optional step) # df = pd.read_pickle(dataset_path+&#39;preprocessed.pkl&#39;) # (rf, feature_names, feature_importance, oob_pre) = train_and_plot_model(df, drop_features=[&#39;saledate&#39;], n_estimators=70) . Feature Engineering . For feature engineering, we will give priority to important features. For this let us again analyze the preprocessed dataset starting from important features to see what can be done against each feature. . # sort the dataframe with important features at the start temp = pd.Series(feature_importance, feature_names) cols = temp.nlargest(len(temp)).index sniff(df[cols], 10) . ProductSize YearsInUse fiBaseModel fiSecondaryDesc YearMade fiProductClassDesc ModelID YearMade_na fiModelDesc Hydraulics_Flow YearSold state Enclosure Hydraulics auctioneerID fiModelSeries fiModelDescriptor MachineHoursCurrentMeter Transmission Pushblock Engine_Horsepower Ripper Drive_System datasource Blade_Type Stick_Length UsageBand Coupler Tire_Size Tire_Size_na Undercarriage_Pad_Width Thumb Grouser_Type Travel_Controls Track_Type Stick_Length_na Forks MachineHoursCurrentMeter_na Ride_Control Undercarriage_Pad_Width_na Tip_Control Differential_Type Pattern_Changer ProductGroup ProductGroupDesc Scarifier Enclosure_Type Stick Steering_Controls Blade_Width Blade_Width_na Pad_Type Blade_Extension Turbocharged Grouser_Tracks Coupler_System Backhoe_Mounting . 0 0 | 2.0 | 296 | 40 | 2004.0 | 59 | 3157 | False | 950 | 0 | 2006 | 1 | 3 | 1 | 23 | 0 | 0 | 68.0 | 0 | 0 | 0 | 0 | 0 | 121 | 0 | 9.7 | 2 | 0 | 20.5 | True | 28.0 | 0 | 0 | 0 | 0 | True | 0 | False | 0 | True | 0 | 4 | 0 | 6 | 6 | 0 | 0 | 0 | 2 | 14.0 | True | 0 | 0 | 0 | 0 | 0 | 0 | . 1 4 | 8.0 | 527 | 54 | 1996.0 | 62 | 77 | True | 1725 | 2 | 2004 | 33 | 5 | 4 | 2 | 97 | 65 | 4640.0 | 5 | 1 | 1 | 3 | 2 | 132 | 5 | 11.0 | 1 | 2 | 23.5 | False | 16.0 | 1 | 1 | 3 | 2 | False | 1 | True | 1 | False | 1 | 0 | 2 | 3 | 3 | 1 | 2 | 1 | 0 | 12.0 | False | 2 | 1 | 1 | 1 | 1 | 1 | . 2 6.0 | 3.0 | 110.0 | 0.0 | 2001.0 | 39.0 | 7009.0 | NaN | 331.0 | 1.0 | 2011.0 | 32.0 | 1.0 | 0.0 | 13.0 | 44.0 | 20.0 | 2838.0 | 6.0 | NaN | 2.0 | 2.0 | 4.0 | 136.0 | 6.0 | 15.9 | 3.0 | 1.0 | 13.0 | NaN | 32.0 | 2.0 | 3.0 | 5.0 | 1.0 | NaN | NaN | NaN | 2.0 | NaN | 2.0 | 1.0 | 1.0 | 4.0 | 4.0 | NaN | 1.0 | 2.0 | 1.0 | 13.0 | NaN | 3.0 | NaN | NaN | NaN | NaN | NaN | . 3 3.0 | 10.0 | 1375.0 | 56.0 | 2007.0 | 8.0 | 332.0 | NaN | 3674.0 | NaN | 2009.0 | 44.0 | 0.0 | 11.0 | 4.0 | 102.0 | 64.0 | 3486.0 | 4.0 | NaN | NaN | 1.0 | 3.0 | 149.0 | 9.0 | 10.2 | 0.0 | NaN | 26.5 | NaN | 30.0 | NaN | 2.0 | 4.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 3.0 | NaN | 1.0 | 1.0 | NaN | NaN | NaN | 3.0 | 16.0 | NaN | 1.0 | NaN | NaN | NaN | NaN | NaN | . 4 5.0 | 4.0 | 1529.0 | 47.0 | 1993.0 | 40.0 | 17311.0 | NaN | 4208.0 | NaN | 2008.0 | 3.0 | 2.0 | 5.0 | 24.0 | 33.0 | 83.0 | 722.0 | 3.0 | NaN | NaN | NaN | 1.0 | 172.0 | 7.0 | 10.6 | NaN | NaN | 29.5 | NaN | 22.0 | NaN | NaN | 2.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 2.0 | NaN | 5.0 | 5.0 | NaN | NaN | NaN | 5.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 5 2.0 | 11.0 | 175.0 | 61.0 | 2008.0 | 2.0 | 4605.0 | NaN | 493.0 | NaN | 2005.0 | 9.0 | 4.0 | 7.0 | 27.0 | 98.0 | 33.0 | 508.0 | 1.0 | NaN | NaN | NaN | NaN | NaN | 1.0 | 9.1 | NaN | NaN | 14.0 | NaN | 24.0 | NaN | NaN | 6.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 2.0 | 2.0 | NaN | NaN | NaN | 4.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 6 1.0 | 1.0 | 419.0 | 20.0 | 1998.0 | 14.0 | 1937.0 | NaN | 1453.0 | NaN | 2007.0 | 13.0 | NaN | 3.0 | 30.0 | 2.0 | 100.0 | 11540.0 | 2.0 | NaN | NaN | NaN | NaN | NaN | 4.0 | 10.1 | NaN | NaN | 17.5 | NaN | 18.0 | NaN | NaN | 1.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 7 NaN | 7.0 | 243.0 | 105.0 | 1999.0 | 17.0 | 3539.0 | NaN | 740.0 | NaN | 2010.0 | 37.0 | NaN | 2.0 | 26.0 | 73.0 | 128.0 | 4883.0 | NaN | NaN | NaN | NaN | NaN | NaN | 8.0 | 9.6 | NaN | NaN | 15.5 | NaN | 36.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 8 NaN | 5.0 | 250.0 | 133.0 | 2003.0 | 68.0 | 36003.0 | NaN | 779.0 | NaN | 2000.0 | 35.0 | NaN | 6.0 | 25.0 | 13.0 | 71.0 | 302.0 | NaN | NaN | NaN | NaN | NaN | NaN | 3.0 | 12.8 | NaN | NaN | 7.0 | NaN | 20.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 9 NaN | 14.0 | 540.0 | 129.0 | 1991.0 | 51.0 | 3883.0 | NaN | 1771.0 | NaN | 2002.0 | 4.0 | NaN | 8.0 | 11.0 | 54.0 | 122.0 | 20700.0 | NaN | NaN | NaN | NaN | NaN | NaN | 2.0 | 8.2 | NaN | NaN | 10.0 | NaN | 27.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . The above table is sorted based on the importance of each feature. Features at the start have more importance. So let&#39;s visit each feature to see if any feature engineering (FE) can be used to gain more insights from the data. . ProductSize, YearsInUse: These features have numbers. Not a candidate for FE. | fiBaseModel: It is label encoded. Let&#39;s visit this column&#39;s original values to see if any more features can be generated from it. | . df_raw[&#39;fiBaseModel&#39;].unique()[:50] . array([&#39;521D&#39;, &#39;950FII&#39;, &#39;226&#39;, &#39;PC120-6E&#39;, &#39;S175&#39;, &#39;310G&#39;, &#39;790ELC&#39;, &#39;416D&#39;, &#39;430HAG&#39;, &#39;988B&#39;, &#39;D31E&#39;, &#39;PC200LC6&#39;, &#39;420D&#39;, &#39;214E&#39;, &#39;310E&#39;, &#39;334&#39;, &#39;45NX&#39;, &#39;302.5&#39;, &#39;580SUPER K&#39;, &#39;JS260&#39;, &#39;120G&#39;, &#39;966FII&#39;, &#39;EX550STD&#39;, &#39;685B&#39;, &#39;345BL&#39;, &#39;330BL&#39;, &#39;873&#39;, &#39;WA250&#39;, &#39;750BLT&#39;, &#39;303CR&#39;, &#39;95ZII&#39;, &#39;416&#39;, &#39;303.5&#39;, &#39;CTL60&#39;, &#39;140G&#39;, &#39;307CSB&#39;, &#39;EC210LC&#39;, &#39;MF650&#39;, &#39;RC30&#39;, &#39;EX120-5&#39;, &#39;70XT&#39;, &#39;772A&#39;, &#39;160HNA&#39;, &#39;216&#39;, &#39;304CR&#39;, &#39;D3CIIIXL&#39;, &#39;236&#39;, &#39;120C&#39;, &#39;PC228&#39;, &#39;SK160LC&#39;], dtype=object) . fiBaseModel: original values look very random and do not give much information. There are two other columns in the importance list &#39;fiModelDesc&#39;, and &#39;fiSecondaryDesc&#39; and from their name they look related to &#39;fiBaseModel&#39;. So let&#39;s analyze them together. | . df_raw[[&#39;fiBaseModel&#39;, &#39;fiModelDesc&#39;, &#39;fiSecondaryDesc&#39;]].head(10) . fiBaseModel fiModelDesc fiSecondaryDesc . 0 521 | 521D | D | . 1 950 | 950FII | F | . 2 226 | 226 | NaN | . 3 PC120 | PC120-6E | NaN | . 4 S175 | S175 | NaN | . 5 310 | 310G | G | . 6 790 | 790ELC | E | . 7 416 | 416D | D | . 8 430 | 430HAG | HAG | . 9 988 | 988B | B | . fiBaseModel, fiModelDesc, fiSecondaryDesc: From the above table all these three features are very much related but their values are very random and do not give us much information. So let&#39;s leave them as it is. | YearMade: It has numbers. Not a candidate for FE. | fiProductClassDesc. This feature is also encode so let&#39;s visit it&#39;s original values. | . df_raw[&#39;fiProductClassDesc&#39;].unique()[:15] . array([&#39;Wheel Loader - 110.0 to 120.0 Horsepower&#39;, &#39;Wheel Loader - 150.0 to 175.0 Horsepower&#39;, &#39;Skid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity&#39;, &#39;Hydraulic Excavator, Track - 12.0 to 14.0 Metric Tons&#39;, &#39;Skid Steer Loader - 1601.0 to 1751.0 Lb Operating Capacity&#39;, &#39;Backhoe Loader - 14.0 to 15.0 Ft Standard Digging Depth&#39;, &#39;Hydraulic Excavator, Track - 21.0 to 24.0 Metric Tons&#39;, &#39;Hydraulic Excavator, Track - 3.0 to 4.0 Metric Tons&#39;, &#39;Wheel Loader - 350.0 to 500.0 Horsepower&#39;, &#39;Track Type Tractor, Dozer - 20.0 to 75.0 Horsepower&#39;, &#39;Hydraulic Excavator, Track - 19.0 to 21.0 Metric Tons&#39;, &#39;Hydraulic Excavator, Track - 4.0 to 5.0 Metric Tons&#39;, &#39;Hydraulic Excavator, Track - 2.0 to 3.0 Metric Tons&#39;, &#39;Hydraulic Excavator, Track - 24.0 to 28.0 Metric Tons&#39;, &#39;Motorgrader - 45.0 to 130.0 Horsepower&#39;], dtype=object) . fiProductClassDesc: This feature has text strings and it is also showing that they are not randow but has some pattern in then. They seems to be a good candidate for FE. We will do that in next section. | ModelID, YearSold: These features have numbers. Not a candidate for FE. | Hydraulics_Flow: It is encoded so let&#39;s visit original values first. | . df_raw[&#39;Hydraulics_Flow&#39;].unique() . array([nan, &#39;Standard&#39;, &#39;High Flow&#39;, &#39;None or Unspecified&#39;], dtype=object) . Hydraulics_Flow: We label encoded it and it came up as an important feature. Its values are showing low variance so it is a better candidate for one-hot encoding. We will do that in the next section. | state: It is encoded so let&#39;s visit its original values. | . print(f&quot;total unique values: {len(df_raw[&#39;state&#39;].unique())}&quot;) df_raw[&#39;state&#39;].unique()[:15] . total unique values: 53 . array([&#39;Alabama&#39;, &#39;North Carolina&#39;, &#39;New York&#39;, &#39;Texas&#39;, &#39;Arizona&#39;, &#39;Florida&#39;, &#39;Illinois&#39;, &#39;Oregon&#39;, &#39;Ohio&#39;, &#39;Arkansas&#39;, &#39;Wisconsin&#39;, &#39;Kansas&#39;, &#39;Nevada&#39;, &#39;Iowa&#39;, &#39;Maine&#39;], dtype=object) . state: By looking at original values we can see that it is a categorical nominal feature. It can be one hot encoded but since it has high variance (53 unique values) it is better to keep it as label encoded. So leave this feature as it is. | Enclosure: It is encoded. So let&#39;s check original values | . print(f&quot;total unique values: {len(df_raw[&#39;Enclosure&#39;].unique())}&quot;) df_raw[&#39;Enclosure&#39;].unique() . total unique values: 7 . array([&#39;EROPS w AC&#39;, &#39;OROPS&#39;, &#39;EROPS&#39;, nan, &#39;EROPS AC&#39;, &#39;NO ROPS&#39;, &#39;None or Unspecified&#39;], dtype=object) . Enclosure: Original values show that it is a categorical feature with good importance and low variance, so it is also suitable for OHE. | Hydraulics: It is also encoded. So let&#39;s check original values | . print(f&quot;total unique values: {len(df_raw[&#39;Hydraulics&#39;].unique())}&quot;) df_raw[&#39;Hydraulics&#39;].unique() . total unique values: 13 . array([&#39;2 Valve&#39;, &#39;Auxiliary&#39;, nan, &#39;Standard&#39;, &#39;Base + 1 Function&#39;, &#39;Base + 3 Function&#39;, &#39;4 Valve&#39;, &#39;3 Valve&#39;, &#39;Base + 2 Function&#39;, &#39;Base + 4 Function&#39;, &#39;None or Unspecified&#39;, &#39;Base + 5 Function&#39;, &#39;Base + 6 Function&#39;], dtype=object) . Hydraulics: Now this feature is again categorical, does not have high variance but also has low importance. We can consider it for OHE but since it is coming at the lower end feature importance, it will not have much impact on model performace. So we can skip it for OHE. | For the remaining features, importance is not significant enough to be considered for any FE. We can keep them as it is. | saledate: We did not use this feature in our last model training. But from the feature importance we can see that features that contain any date information are showing significant importance. So we should also include this feature in our next model. | . To summarize this section, the features that are suitable for any FE are . fiProductClassDesc | Hydraulics_Flow | Enclosure | saledate | . fiProductClassDesc . Let&#39;s check the original values for this feature one more time. . df_raw[&#39;fiProductClassDesc&#39;].head() . 0 Wheel Loader - 110.0 to 120.0 Horsepower 1 Wheel Loader - 150.0 to 175.0 Horsepower 2 Skid Steer Loader - 1351.0 to 1601.0 Lb Operat... 3 Hydraulic Excavator, Track - 12.0 to 14.0 Metr... 4 Skid Steer Loader - 1601.0 to 1751.0 Lb Operat... Name: fiProductClassDesc, dtype: object . Though this feature is named &#39;ProductClassDesc&#39; but by looking at its value we can see that besides class description there is also information on class specification. If we take the first value then . &#39;Wheel Loader&#39; -&gt; this is the class description | &#39;110.0 to 120.0 Horsepower&#39; -&gt; this is class specification | . and even in the class specification we have . 110 -&gt; spec lower limit | 120 -&gt; spec upper limit | &#39;Horsepower&#39; -&gt; spec unit | . Use this information to create new columns . # split the class description df_split = df_raw.fiProductClassDesc.str.split(&#39; - &#39;,expand=True).values . # on 0 index we have class description df_split[:,0] . array([&#39;Wheel Loader&#39;, &#39;Wheel Loader&#39;, &#39;Skid Steer Loader&#39;, ..., &#39;Hydraulic Excavator, Track&#39;, &#39;Hydraulic Excavator, Track&#39;, &#39;Hydraulic Excavator, Track&#39;], dtype=object) . # on 1 index we have class specification df_split[:,1] . array([&#39;110.0 to 120.0 Horsepower&#39;, &#39;150.0 to 175.0 Horsepower&#39;, &#39;1351.0 to 1601.0 Lb Operating Capacity&#39;, ..., &#39;3.0 to 4.0 Metric Tons&#39;, &#39;2.0 to 3.0 Metric Tons&#39;, &#39;2.0 to 3.0 Metric Tons&#39;], dtype=object) . # let&#39;s create two new columns for this df[&#39;fiProductClassDesc&#39;] = df_split[:,0] df[&#39;fiProductClassSpec&#39;] = df_split[:,1] . # split class spec further to get limits and units pattern = r&#39;([0-9. +]*)(?: to ([0-9. +]*)| +) ([a-zA-Z ]*)&#39; df_split = df[&#39;fiProductClassSpec&#39;].str.extract(pattern, expand=True).values df_split = pd.DataFrame(df_split, columns=[&#39;fiProductClassSpec_lower&#39;, &#39;fiProductClassSpec_upper&#39;, &#39;fiProductClassSpec_units&#39;]) df_split.head() . fiProductClassSpec_lower fiProductClassSpec_upper fiProductClassSpec_units . 0 110.0 | 120.0 | Horsepower | . 1 150.0 | 175.0 | Horsepower | . 2 1351.0 | 1601.0 | Lb Operating Capacity | . 3 12.0 | 14.0 | Metric Tons | . 4 1601.0 | 1751.0 | Lb Operating Capacity | . # merge new columns to our dataset df = pd.concat([df, df_split], axis=1) del df[&#39;fiProductClassSpec&#39;] # class spec is no more required. we have it&#39;s sub-features df.head() . SalePrice ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter UsageBand saledate fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Enclosure Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Hydraulics_Flow Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls YearMade_na YearSold YearsInUse MachineHoursCurrentMeter_na Tire_Size_na Stick_Length_na Undercarriage_Pad_Width_na Blade_Width_na fiProductClassSpec_lower fiProductClassSpec_upper fiProductClassSpec_units . 0 66000 | 3157 | 121 | 23 | 2004.0 | 68.0 | 2 | 2006-11-16 | 950 | 296 | 40 | 0 | 0 | 0 | Wheel Loader | 1 | 6 | 6 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2006 | 2.0 | False | True | True | True | True | 110.0 | 120.0 | Horsepower | . 1 57000 | 77 | 121 | 23 | 1996.0 | 4640.0 | 2 | 2004-03-26 | 1725 | 527 | 54 | 97 | 0 | 4 | Wheel Loader | 33 | 6 | 6 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 23.5 | 0 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2004 | 8.0 | False | False | True | True | True | 150.0 | 175.0 | Horsepower | . 2 10000 | 7009 | 121 | 23 | 2001.0 | 2838.0 | 1 | 2004-02-26 | 331 | 110 | 0 | 0 | 0 | 0 | Skid Steer Loader | 32 | 3 | 3 | 0 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 2 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2004 | 3.0 | False | True | True | True | True | 1351.0 | 1601.0 | Lb Operating Capacity | . 3 38500 | 332 | 121 | 23 | 2001.0 | 3486.0 | 1 | 2011-05-19 | 3674 | 1375 | 0 | 44 | 0 | 6 | Hydraulic Excavator, Track | 44 | 4 | 4 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2011 | 10.0 | False | True | True | True | True | 12.0 | 14.0 | Metric Tons | . 4 11000 | 17311 | 121 | 23 | 2007.0 | 722.0 | 3 | 2009-07-23 | 4208 | 1529 | 0 | 0 | 0 | 0 | Skid Steer Loader | 32 | 3 | 3 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 2 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2009 | 2.0 | False | True | True | True | True | 1601.0 | 1751.0 | Lb Operating Capacity | . # convert to numerical features df[&#39;fiProductClassSpec_lower&#39;] = pd.to_numeric(df[&#39;fiProductClassSpec_lower&#39;]) df[&#39;fiProductClassSpec_upper&#39;] = pd.to_numeric(df[&#39;fiProductClassSpec_upper&#39;]) # apply fix for numerical features fix_missing_num(df, &#39;fiProductClassSpec_lower&#39;) fix_missing_num(df, &#39;fiProductClassSpec_upper&#39;) # apply fix for categorical features df_string_to_cat(df) df_cat_to_catcode(df) . label encoding applied on fiProductClassDesc label encoding applied on fiProductClassSpec_units . (rf, feature_names, feature_importance, oob_hydralics) = train_and_plot_model(df, drop_features=[&#39;saledate&#39;]) . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 1.6min [Parallel(n_jobs=-1)]: Done 70 out of 70 | elapsed: 2.6min finished . OOB scrore = 0.905 Tree leaves = 14,650,747 Median depth = 47.0 . There is only a slight increase in OOB score but if we check the feature importance plot both fiProductClassSpec_upper and fiProductClassSpec_lower are showing high importance. We can take this as a positive signal for good features. . Hydralics_Flow . We need to apply one hot encoding (OHE) to this feature. Let&#39;s start by checking unique values for Hydraulics_Flow. . df[&#39;Hydraulics_Flow&#39;].value_counts() . 0 357788 2 42784 1 553 Name: Hydraulics_Flow, dtype: int64 . We have encoded this feature in the preprocessing section. Although we can use this encoded feature for one-hot encoding but we don&#39;t have original labels at this point. It would be better if we use original labels for OHE so that the dummy columns created as a result of that also have proper names with labels. Using encoded dummy column names makes them difficult to understand and follow. let&#39;s use the original dataframe to check the unique values. . df_raw[&#39;Hydraulics_Flow&#39;].value_counts(dropna=False) . NaN 357763 Standard 42784 High Flow 553 None or Unspecified 25 Name: Hydraulics_Flow, dtype: int64 . Before applying OHE we need to preprocess &#39;None or Unspecified&#39; as they repsent the same as np.nan. So let&#39;s do that. . # get the original values df[&#39;Hydraulics_Flow&#39;] = df_raw[&#39;Hydraulics_Flow&#39;] df[&#39;Hydraulics_Flow&#39;] = df[&#39;Hydraulics_Flow&#39;].replace(&#39;None or Unspecified&#39;, np.nan) df[&#39;Hydraulics_Flow&#39;].value_counts(dropna=False) . NaN 357788 Standard 42784 High Flow 553 Name: Hydraulics_Flow, dtype: int64 . Let&#39;s check the first few rows of this column. We will use them to verify our final result. . df[&#39;Hydraulics_Flow&#39;].head() . 0 NaN 1 NaN 2 Standard 3 NaN 4 Standard Name: Hydraulics_Flow, dtype: object . Notice that in the first five rows there are &#39;Standard&#39; values at row index 2 and 4, and the remaining are &#39;NaN&#39; values. We will OHE them in the next step and compare the results to ensure encoding is properly working. . from sklearn.preprocessing import OneHotEncoder onehot_encoder = OneHotEncoder() onehot_output = onehot_encoder.fit_transform(df[[&#39;Hydraulics_Flow&#39;]]) # check the output print(onehot_output[:5].toarray()) . [[0. 0. 1.] [0. 0. 1.] [0. 1. 0.] [0. 0. 1.] [0. 1. 0.]] . These are same five rows but this time encoded with one-hot values. From the position of &#39;1&#39; appearing in different columns we can deduce that first column is for label &#39;High Flow&#39; and second is for &#39;Standard&#39; and third is for &#39;NaN&#39;. It would be easier for us to track these dummy columns if we have proper names on them. So let&#39;s do that. . We can get the dummy column names by calling get_feature_names_out() on our encoder. . onehot_encoder.get_feature_names_out() . array([&#39;Hydraulics_Flow_High Flow&#39;, &#39;Hydraulics_Flow_Standard&#39;, &#39;Hydraulics_Flow_nan&#39;], dtype=object) . To create a dataframe of these dummy variables. . df_onehot = pd.DataFrame(onehot_output.toarray(), columns=onehot_encoder.get_feature_names_out()) df_onehot.head() . Hydraulics_Flow_High Flow Hydraulics_Flow_Standard Hydraulics_Flow_nan . 0 0.0 | 0.0 | 1.0 | . 1 0.0 | 0.0 | 1.0 | . 2 0.0 | 1.0 | 0.0 | . 3 0.0 | 0.0 | 1.0 | . 4 0.0 | 1.0 | 0.0 | . At this point Hydraulics_Flow is OHE so we can drop the original column from the dataset and add these encoded columns. . del df[&#39;Hydraulics_Flow&#39;] df = pd.concat([df, df_onehot], axis=1) # concat dataframes column wise df.head() . SalePrice ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter UsageBand saledate fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Enclosure Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls YearMade_na YearSold YearsInUse MachineHoursCurrentMeter_na Tire_Size_na Stick_Length_na Undercarriage_Pad_Width_na Blade_Width_na fiProductClassSpec_lower fiProductClassSpec_upper fiProductClassSpec_units fiProductClassSpec_lower_na fiProductClassSpec_upper_na Hydraulics_Flow_High Flow Hydraulics_Flow_Standard Hydraulics_Flow_nan . 0 66000 | 3157 | 121 | 23 | 2004.0 | 68.0 | 2 | 2006-11-16 | 950 | 296 | 40 | 0 | 0 | 0 | 6 | 1 | 6 | 6 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2006 | 2.0 | False | True | True | True | True | 110.0 | 120.0 | 2 | False | False | 0.0 | 0.0 | 1.0 | . 1 57000 | 77 | 121 | 23 | 1996.0 | 4640.0 | 2 | 2004-03-26 | 1725 | 527 | 54 | 97 | 0 | 4 | 6 | 33 | 6 | 6 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 23.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2004 | 8.0 | False | False | True | True | True | 150.0 | 175.0 | 2 | False | False | 0.0 | 0.0 | 1.0 | . 2 10000 | 7009 | 121 | 23 | 2001.0 | 2838.0 | 1 | 2004-02-26 | 331 | 110 | 0 | 0 | 0 | 0 | 4 | 32 | 3 | 3 | 0 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2004 | 3.0 | False | True | True | True | True | 1351.0 | 1601.0 | 3 | False | False | 0.0 | 1.0 | 0.0 | . 3 38500 | 332 | 121 | 23 | 2001.0 | 3486.0 | 1 | 2011-05-19 | 3674 | 1375 | 0 | 44 | 0 | 6 | 2 | 44 | 4 | 4 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2011 | 10.0 | False | True | True | True | True | 12.0 | 14.0 | 4 | False | False | 0.0 | 0.0 | 1.0 | . 4 11000 | 17311 | 121 | 23 | 2007.0 | 722.0 | 3 | 2009-07-23 | 4208 | 1529 | 0 | 0 | 0 | 0 | 4 | 32 | 3 | 3 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2009 | 2.0 | False | True | True | True | True | 1601.0 | 1751.0 | 3 | False | False | 0.0 | 1.0 | 0.0 | . Let&#39;s retain our model to check if there is any affect on model performance. . (rf, feature_names, feature_importance, oob_hydralics) = train_and_plot_model(df, drop_features=[&#39;saledate&#39;]) . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 1.6min [Parallel(n_jobs=-1)]: Done 70 out of 70 | elapsed: 2.6min finished . OOB scrore = 0.905 Tree leaves = 14,650,376 Median depth = 48.0 . There is no effect on the model performance but one feature &#39;Hydraulics_Flow_nan&#39; is showing some importance on the plot. The remaining features (&#39;Hydraulics_Flow_High Flow&#39; and &#39;Hydraulics_Flow_Standard&#39;) do not affect the model&#39;s performance. If it was not of &#39;Hydraulics_Flow_nan&#39; importance we could have skipped OHE for &#39;Hydraulics_Flow&#39;. . Enclosure . Next feature is Enclosure, and we will follow the same steps as for last feature to one-hot encode it. . # check value counts df_raw[&#39;Enclosure&#39;].value_counts(dropna=False) . OROPS 173932 EROPS 139026 EROPS w AC 87820 NaN 325 EROPS AC 17 NO ROPS 3 None or Unspecified 2 Name: Enclosure, dtype: int64 . Here ROPS is an abbreviation for Roll Over Protection System and there are multiple variants of this standard . OROPS = Open ROPS | EROPS = Enclosed ROPS | EROPS AC = Enclosed ROPS with Air Conditioning | EROPS w AC = Enclosed ROPS with Air Conditioning. Same as &#39;EROPS AC&#39; | NO ROPS = No ROPS. Same as &#39;NaN&#39; or &#39;None or Unspecified&#39; | . You can read more about ROPS standards here . http://www.miningrops.com.au/ropsintro.html | https://www.youtube.com/watch?v=LZ40O1My8E4&amp;ab_channel=MissouriEarthMovers | . Using this information we can also preprocess this feature to make its values more consistent. . # get the original values df[&#39;Enclosure&#39;] = df_raw[&#39;Enclosure&#39;] # change &#39;None or Unspecified&#39; and &#39;NO ROPS&#39; to np.nan df[&#39;Enclosure&#39;] = df[&#39;Enclosure&#39;].replace(&#39;None or Unspecified&#39;, np.nan) df[&#39;Enclosure&#39;] = df[&#39;Enclosure&#39;].replace(&#39;NO ROPS&#39;, np.nan) # change &#39;EROPS w AC&#39; to &#39;EROPS AC&#39; df[&#39;Enclosure&#39;] = df[&#39;Enclosure&#39;].replace(&#39;EROPS w AC&#39;, &#39;EROPS AC&#39;) df[&#39;Enclosure&#39;].value_counts(dropna=False) . OROPS 173932 EROPS 139026 EROPS AC 87837 NaN 330 Name: Enclosure, dtype: int64 . # before OHE df.head() . SalePrice ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter UsageBand saledate fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Enclosure Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls YearMade_na YearSold YearsInUse MachineHoursCurrentMeter_na Tire_Size_na Stick_Length_na Undercarriage_Pad_Width_na Blade_Width_na fiProductClassSpec_lower fiProductClassSpec_upper fiProductClassSpec_units fiProductClassSpec_lower_na fiProductClassSpec_upper_na Hydraulics_Flow_High Flow Hydraulics_Flow_Standard Hydraulics_Flow_nan . 0 66000 | 3157 | 121 | 23 | 2004.0 | 68.0 | 2 | 2006-11-16 | 950 | 296 | 40 | 0 | 0 | 0 | 6 | 1 | 6 | 6 | 0 | EROPS AC | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2006 | 2.0 | False | True | True | True | True | 110.0 | 120.0 | 2 | False | False | 0.0 | 0.0 | 1.0 | . 1 57000 | 77 | 121 | 23 | 1996.0 | 4640.0 | 2 | 2004-03-26 | 1725 | 527 | 54 | 97 | 0 | 4 | 6 | 33 | 6 | 6 | 0 | EROPS AC | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 23.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2004 | 8.0 | False | False | True | True | True | 150.0 | 175.0 | 2 | False | False | 0.0 | 0.0 | 1.0 | . 2 10000 | 7009 | 121 | 23 | 2001.0 | 2838.0 | 1 | 2004-02-26 | 331 | 110 | 0 | 0 | 0 | 0 | 4 | 32 | 3 | 3 | 0 | OROPS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2004 | 3.0 | False | True | True | True | True | 1351.0 | 1601.0 | 3 | False | False | 0.0 | 1.0 | 0.0 | . 3 38500 | 332 | 121 | 23 | 2001.0 | 3486.0 | 1 | 2011-05-19 | 3674 | 1375 | 0 | 44 | 0 | 6 | 2 | 44 | 4 | 4 | 0 | EROPS AC | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2011 | 10.0 | False | True | True | True | True | 12.0 | 14.0 | 4 | False | False | 0.0 | 0.0 | 1.0 | . 4 11000 | 17311 | 121 | 23 | 2007.0 | 722.0 | 3 | 2009-07-23 | 4208 | 1529 | 0 | 0 | 0 | 0 | 4 | 32 | 3 | 3 | 0 | EROPS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2009 | 2.0 | False | True | True | True | True | 1601.0 | 1751.0 | 3 | False | False | 0.0 | 1.0 | 0.0 | . # one hot encode &#39;Enclosure&#39; onehot_encoder = OneHotEncoder() onehot_output = onehot_encoder.fit_transform(df[[&#39;Enclosure&#39;]]) df_onehot = pd.DataFrame(onehot_output.toarray(), columns=onehot_encoder.get_feature_names_out()) df_onehot.head() . Enclosure_EROPS Enclosure_EROPS AC Enclosure_OROPS Enclosure_nan . 0 0.0 | 1.0 | 0.0 | 0.0 | . 1 0.0 | 1.0 | 0.0 | 0.0 | . 2 0.0 | 0.0 | 1.0 | 0.0 | . 3 0.0 | 1.0 | 0.0 | 0.0 | . 4 1.0 | 0.0 | 0.0 | 0.0 | . # drop original column del df[&#39;Enclosure&#39;] # add dummy columns to the dataframe df = pd.concat([df, df_onehot], axis=1) # concat dataframes column wise # after OHE df.head() . SalePrice ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter UsageBand saledate fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls YearMade_na YearSold YearsInUse MachineHoursCurrentMeter_na Tire_Size_na Stick_Length_na Undercarriage_Pad_Width_na Blade_Width_na fiProductClassSpec_lower fiProductClassSpec_upper fiProductClassSpec_units fiProductClassSpec_lower_na fiProductClassSpec_upper_na Hydraulics_Flow_High Flow Hydraulics_Flow_Standard Hydraulics_Flow_nan Enclosure_EROPS Enclosure_EROPS AC Enclosure_OROPS Enclosure_nan . 0 66000 | 3157 | 121 | 23 | 2004.0 | 68.0 | 2 | 2006-11-16 | 950 | 296 | 40 | 0 | 0 | 0 | 6 | 1 | 6 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2006 | 2.0 | False | True | True | True | True | 110.0 | 120.0 | 2 | False | False | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 1 57000 | 77 | 121 | 23 | 1996.0 | 4640.0 | 2 | 2004-03-26 | 1725 | 527 | 54 | 97 | 0 | 4 | 6 | 33 | 6 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 23.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2004 | 8.0 | False | False | True | True | True | 150.0 | 175.0 | 2 | False | False | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 2 10000 | 7009 | 121 | 23 | 2001.0 | 2838.0 | 1 | 2004-02-26 | 331 | 110 | 0 | 0 | 0 | 0 | 4 | 32 | 3 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2004 | 3.0 | False | True | True | True | True | 1351.0 | 1601.0 | 3 | False | False | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | . 3 38500 | 332 | 121 | 23 | 2001.0 | 3486.0 | 1 | 2011-05-19 | 3674 | 1375 | 0 | 44 | 0 | 6 | 2 | 44 | 4 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2011 | 10.0 | False | True | True | True | True | 12.0 | 14.0 | 4 | False | False | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 4 11000 | 17311 | 121 | 23 | 2007.0 | 722.0 | 3 | 2009-07-23 | 4208 | 1529 | 0 | 0 | 0 | 0 | 4 | 32 | 3 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2009 | 2.0 | False | True | True | True | True | 1601.0 | 1751.0 | 3 | False | False | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | . Let&#39;s retain our model to check if there is any affect on model performance. . (rf, feature_names, feature_importance, oob_enclosure) = train_and_plot_model(df, drop_features=[&#39;saledate&#39;]) . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 2.0min [Parallel(n_jobs=-1)]: Done 70 out of 70 | elapsed: 3.6min finished . OOB scrore = 0.902 Tree leaves = 14,668,686 Median depth = 45.0 . There is a slight decrease in model performance but one new feature &#39;Enclosure_EROPS AC&#39; is showing very high on the importance plot. . saledate . We have already created &#39;yearsold&#39; feature. We can more consequent features from &#39;saledate&#39;. . df[&quot;salemonth&quot;] = df[&#39;saledate&#39;].dt.month df[&quot;saleday&quot;] = df[&#39;saledate&#39;].dt.day df[&quot;saledayofweek&quot;] = df[&#39;saledate&#39;].dt.dayofweek df[&quot;saledayofyear&quot;] = df[&#39;saledate&#39;].dt.dayofyear # we can drop the orignal del df[&#39;saledate&#39;] . df.head() . SalePrice ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter UsageBand fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls YearMade_na YearSold YearsInUse MachineHoursCurrentMeter_na Tire_Size_na Stick_Length_na Undercarriage_Pad_Width_na Blade_Width_na fiProductClassSpec_lower fiProductClassSpec_upper fiProductClassSpec_units fiProductClassSpec_lower_na fiProductClassSpec_upper_na Hydraulics_Flow_High Flow Hydraulics_Flow_Standard Hydraulics_Flow_nan Enclosure_EROPS Enclosure_EROPS AC Enclosure_OROPS Enclosure_nan salemonth saleday saledayofweek saledayofyear . 0 66000 | 3157 | 121 | 23 | 2004.0 | 68.0 | 2 | 950 | 296 | 40 | 0 | 0 | 0 | 6 | 1 | 6 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2006 | 2.0 | False | True | True | True | True | 110.0 | 120.0 | 2 | False | False | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 11 | 16 | 3 | 320 | . 1 57000 | 77 | 121 | 23 | 1996.0 | 4640.0 | 2 | 1725 | 527 | 54 | 97 | 0 | 4 | 6 | 33 | 6 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 23.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2004 | 8.0 | False | False | True | True | True | 150.0 | 175.0 | 2 | False | False | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 3 | 26 | 4 | 86 | . 2 10000 | 7009 | 121 | 23 | 2001.0 | 2838.0 | 1 | 331 | 110 | 0 | 0 | 0 | 0 | 4 | 32 | 3 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2004 | 3.0 | False | True | True | True | True | 1351.0 | 1601.0 | 3 | False | False | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 2 | 26 | 3 | 57 | . 3 38500 | 332 | 121 | 23 | 2001.0 | 3486.0 | 1 | 3674 | 1375 | 0 | 44 | 0 | 6 | 2 | 44 | 4 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2011 | 10.0 | False | True | True | True | True | 12.0 | 14.0 | 4 | False | False | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 5 | 19 | 3 | 139 | . 4 11000 | 17311 | 121 | 23 | 2007.0 | 722.0 | 3 | 4208 | 1529 | 0 | 0 | 0 | 0 | 4 | 32 | 3 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2009 | 2.0 | False | True | True | True | True | 1601.0 | 1751.0 | 3 | False | False | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 7 | 23 | 3 | 204 | . (rf, feature_names, feature_importance, oob_date) = train_and_plot_model(df) . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 2.4min [Parallel(n_jobs=-1)]: Done 70 out of 70 | elapsed: 3.9min finished . OOB scrore = 0.907 Tree leaves = 14,493,456 Median depth = 45.0 . There is an increase in model performance and multiple newly created date features are showing good importance on the plot. .",
            "url": "https://hassaanbinaslam.github.io/myblog/ml/2022/04/25/bluebook-for-bulldozers.html",
            "relUrl": "/ml/2022/04/25/bluebook-for-bulldozers.html",
            "date": " • Apr 25, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "Docker - Send Container Logs to AWS CloudWatch",
            "content": ". About . This post is about configuring docker container to send application logs to Amazon CloudWatch. Logs entries can be retrieved from AWS Management Console. . Environment Details . Python = 3.8.x | Docker version = 20.10.7 | OS = Amazon Linux 2 | . iamadmin:~/environment $ docker version Client: Version: 20.10.7 API version: 1.41 Go version: go1.15.14 Git commit: f0df350 Built: Wed Nov 17 03:05:36 2021 OS/Arch: linux/amd64 Context: default Experimental: true Server: Engine: Version: 20.10.7 API version: 1.41 (minimum version 1.12) Go version: go1.15.14 Git commit: b0f5bc3 Built: Wed Nov 17 03:06:14 2021 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.4.6 GitCommit: d71fcd7d8303cbf684402823e425e9dd2e99285d runc: Version: 1.0.0 GitCommit: 84113eef6fc27af1b01b3181f31bbaf708715301 docker-init: Version: 0.19.0 GitCommit: de40ad0 iamadmin:~/environment $ cat /etc/os-release NAME=&quot;Amazon Linux&quot; VERSION=&quot;2&quot; ID=&quot;amzn&quot; ID_LIKE=&quot;centos rhel fedora&quot; VERSION_ID=&quot;2&quot; PRETTY_NAME=&quot;Amazon Linux 2&quot; ANSI_COLOR=&quot;0;33&quot; CPE_NAME=&quot;cpe:2.3:o:amazon:amazon_linux:2&quot; HOME_URL=&quot;https://amazonlinux.com/&quot; . Sample Application . Let us create a simple hello world application that will print &quot;hello world&quot; message to stdout. After each message the application sleeps for 5 seconds, and keeps on doing this for 5 mins (300 sec). After this the program exists. . Project structure of this application is . app/ └── src/ └── hello.py . Where . app/ is the project root folder | src/ folder contain the python application code | src/hello.py is the main application | . Code files are provided below . # app/src/hello.py from datetime import datetime import time def main(): # run for about 5 min: 300 sec for i in range(60): now = datetime.now() dt_string = now.strftime(&quot;%d/%m/%Y %H:%M:%S&quot;) # prepare message msg = f&quot;hello world at {dt_string}&quot; # put message to stdout and logs print(msg) # sleep for some seconds time.sleep(5) if __name__ == &quot;__main__&quot;: main() . When I run the hello.py file I get the output on the termial with hello world messages like this. . . Dockerize the application . Let&#39;s put it inside a docker container. For this let&#39;s create a Dockerfile and place it in app/ folder. . # app/Dockerfile FROM python:3.8-slim-buster # set the working directory in the container WORKDIR /app # copy the content of the local src directory to the working directory COPY src/ . # command to run on container start CMD [ &quot;python3&quot;, &quot;-u&quot;, &quot;./hello.py&quot;] . We can build our docker image by running the command from terminal at folder app/ . docker build --tag python-docker . . Output of this command will look like this . We can check the created docker image using command from terminal . docker images . Output of this command will look like this . So our docker image is ready, we can now run it using command . docker run python-docker . After running this command you will see the application logs on the terminal. . Get AWS Credentials . Now that we have our sample application and it&#39;s docker container ready, we can work on pushing the docker logs to AWS CloudWatch. For this we need access credentials to AWS account where we want our logs to be available. We will create a separate account in AWS with CloudWatch access and use it&#39;s credentials with docker daemon. Our steps will be . Create IAM policy with CloudWatch access | Create IAM group with that policy | Create IAM user and add that to this group | . Create IAM Policy . From AWS Console go to IAM Console | Select Policies, and click &#39;Create Policy&#39; | From Create Policy window, select Service = CloudWatch Logs | Actions = CreateLogStream, GetLogRecord, DescribeLogGroups, DescribeLogStreams, GetLogEvents, CreateLogGroup, PutLogEvents | Resources = All | . | . After giving required permissions, policy summary will be like . { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Sid&quot;: &quot;DockerContainerLogs&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;logs:CreateLogStream&quot;, &quot;logs:GetLogRecord&quot;, &quot;logs:DescribeLogGroups&quot;, &quot;logs:DescribeLogStreams&quot;, &quot;logs:GetLogEvents&quot;, &quot;logs:CreateLogGroup&quot;, &quot;logs:PutLogEvents&quot; ], &quot;Resource&quot;: &quot;*&quot; } ] } . Create IAM Group and User . From IAM console create a new IAM group and give it some appropriate name &#39;docker-logs-group&#39; | Attach the above created policy to that group | From the console create a new IAM user with &quot;Access key - Programmatic access&quot;. Give it some appropriate name &#39;docker-logs-user&#39; | Store access key ID and secret access key | Add the user to the group created in last step | . Configure AWS credentials for docker daemon . To configure docker daemon to use AWS access credentials, execute command from the terminal sudo systemctl edit docker. A new window will open for text to edit, and add the following lines to it. Replace my-aws-access-key and my-secret-access-key with your access keys. . [Service] Environment=&quot;AWS_ACCESS_KEY_ID=my-aws-access-key&quot; Environment=&quot;AWS_SECRET_ACCESS_KEY=my-secret-access-key&quot; . This command will update the credentials in file /etc/systemd/system/docker.service.d/override.conf. Verify it using command . $ cat /etc/systemd/system/docker.service.d/override.conf [Service] Environment=&quot;AWS_ACCESS_KEY_ID=AKIA3VIXXJNKPUSIOR3Y&quot; Environment=&quot;AWS_SECRET_ACCESS_KEY=XhjlKVkZm1XdXedjgBcfLVM3FBU6zkGU&quot; . After making changes to Docker daemon we need to restart it. For this . Flush the change with command sudo systemctl daemon-reload | Restart the docker daemon with command sudo systemctl restart docker | . Run docker container with awslogs driver . We can now run the docker image with awslogs driver using command . docker run --log-driver=awslogs --log-opt awslogs-region=us-east-1 --log-opt awslogs-group=myLogGroup --log-opt awslogs-create-group=true python-docker . log-driver configures the driver to be used for logs. Default driver is &#39;json-file&#39; and awslogs is for CloudWatch | awslogs-region specifies the region for AWS CloudWatch logs | awslogs-group specifies the log group for CloudWatch | awslogs-create-group specifes that if provided log group does not exists on CloudWatch then create one | . . Verify Logs from CloudWatch . Go to CloudWatch console and select Log Groups and then myLogGroup. You will find the logs generated by docker container. . . All the code used for this post can be obtained from the GitHub repository hassaanbinaslam/2022-04-11-docker-logs-cloudwatch . Project code files | Project zip file | . Error Messages . If docker daemon is not able to find AWS credentails then it will generate an error message similar to pasted below . docker: Error response from daemon: failed to initialize logging driver: failed to create Cloudwatch log stream: NoCredentialProviders: no valid providers in chain. Deprecated. For verbose messaging see aws.Config.CredentialsChainVerboseErrors. . If you get this message then you need to recheck the credentails passed to docker daemon. . One thing I noticed is that on Windows there is no way to pass AWS credentials to docker daemon. People have reported similar issues with docker running on MAC OS. Refer to below link for this discussion . https://github.com/docker/for-win/issues/9684 | . Other method to provide AWS credentials to docker daemon . Docker documentation mentions that AWS credentails can also be set . By configuring the environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. I have tried this approach but docker daemon is not able to pick AWS credentials from environment variables | By using AWS credentials file ~/.aws/credentials. I have also tried this approach and it does not work either | . Important References . Docker configuring logging drivers | Amazon CloudWatch Logs logging driver | https://transang.me/configure-docker-to-send-log-to-aws/ | .",
            "url": "https://hassaanbinaslam.github.io/myblog/docker/python/aws/cloudwatch/2022/04/11/docker-logs-cloudwatch.html",
            "relUrl": "/docker/python/aws/cloudwatch/2022/04/11/docker-logs-cloudwatch.html",
            "date": " • Apr 11, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "Machine Learning Nomenclature",
            "content": ". About . This post is a collection of commonly used machine learning (ML) terminologies. . Dataset . The data we use in ML is usually defined as dataset, and datasets are a collection of data. The dataset contains the features and target to predict. . It has other names . data | input data | train and test data | . Instance . An instance is a row in the dataset. . Is has other names . row | observation | sample | (data) point | . Feature . Feature is a column in the dataset. It is used as an input used for prediction or classification. Features are commonly represented by x variable. . It has other names . column | attribute | (input) variable | . Features are of two types . Categorical or qualitative | Numerical or quantitative | . Target . It is the information a machine learning algorithm learns to predict. Target is commonly represented by y variable. . It has other names . label | output | . Labeled Data . A data that has both the feature and target attributes defined . Unlabeled Data . A data that has the features defined but has no target attribute. . Categorical Feature . A feature that is not measureable and has discrete set of values like gender, family retionships, movie categories etc. We commonly use bar charts and pie graphs for categorical features. . It has other names . qualitative feature | . Categorical features are of two types . Nominal | Ordinal | . Nominal feature . Nominal (categorical) feature is one that can not be measured and has no order assgined to it e.g. eye colors, gender etc. . Ordinal feature . Ordinal (categorical) feature is one that can not be measured but has some order assgined to it like movie ratings, military ranks etc. . Numerical feature . Numerical features are those that can be measured or counted and have some ascending or descending order assigned to them. . It has other names . Continious feature | Quantitative feature | . Numerical features can be of two types . Discrete | Continous | . Discrete feature . Discrete (numerical) feature is one that has specified values and are usually counted e.g. number of facebook like, number of tickets sold etc. . Continous feature . Continous (numerical) feature is one that can have any value assigned to it, and is usually measured e.g. temperature, wind speed etc. . Data ├── Categorical / Qualitative │ ├── Nominal │ └── Ordinal └── Numerical / Quantitative ├── Discrete └── Continous . Classification . If the target feature is categorical then the ML task is called classification. . Regression . If the target feature is numerical then the ML task is called regression. . Positive class . In binary classification the output class is usually labelled as positive or negative. The positive class is the thing we are testing for. For example, positive class for an email classifier is &#39;spam&#39;, and positive class for a medical test can be &#39;tumor&#39;. . Negative class . Negative class is the opposite to positive class. For example, negative class for an email classifier is &#39;not spam&#39;, and a negative class for a medical test can be &#39;not tumor&#39;. . True positive (TP) . Model correctly predicted the positve class . True negative (TN) . Model correctly predicted the negative class . False positive (FP) . Model incorrectly predicted the positive class. Actual class is negative. It has other names . Type I error | . False negative (FN) . Model incorrectly predicted the negative class. Actual class is positive. It has other names . Type II error | . Accuracy . Accuracy = (TP + TN) / (TP + TN + FP + FN) . Precision . It tells how accurate the positive predictions are. . Precision = TP / (TP + FP) . It is a good metric when cost of false positives is high. . True Positive Rate (TPR) . TPR = TP / (TP + FN) . It is the probability that an actual positive class will test positive. . It has other names . Recall | Sensitivity | . True positive is the y-axis in an ROC curve. It is a good metric when cost of false negatives is high. . False Positive Rate (FPR) . FPR = FP / (FP + TN) . It has other names . 1 - specificity | . It is x-axis on ROC curve. . ROC Curve . Receiver Operating Characteristic (ROC) is a curve of TPR vs FPR at different classification thresholds. . True Negative Rate (TNR) . TNR = TN / (TN + FP) . It is the probability of a negative class to test negative. . It has other names . Specificity | .",
            "url": "https://hassaanbinaslam.github.io/myblog/ml/2022/03/31/ml-nomenclature.html",
            "relUrl": "/ml/2022/03/31/ml-nomenclature.html",
            "date": " • Mar 31, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "AWS EFS Sync to S3 Using DataSync",
            "content": ". About . This post is to document all the steps required to synchronize AWS EFS with an S3 bucket using DataSync service. The flow of information is from S3 to EFS and not vice versa. . We will discuss two approaches to trigger the datasync service . Trigger-based. Whenever there is a new file uploaded in S3 bucket | Schedule-based. A trigger will run datasync at scheduled intervals | . Once a datasync service is invoked it will take care of syncing files from S3 bucket to EFS. . Environment Details . Python = 3.9.x | . Steps for trigger based approach . . Create an S3 bucket . Let&#39;s first create an S3 bucket that will contain our data, and this is the bucket we would like to be in sync with EFS. I am naming the bucket as mydata-202203. You may name it as you please. Choose a region of your choice and leave the rest of the settings as defaults. . Create an EFS . From EFS console give name as mydata-efs. I am using default VPC for this post. Use Regional availability settings. Click Create. Once file system is created, click on Access points and create an access point for this efs to be mounted in other service. For access point use following settings . name = mydata-ap | root dir path = / | POSIX User POSIX UID = 1000 | Group ID = 1000 | . | Root directory creation permissions Owner user id = 1000 | Owner group id = 1000 | POSIX permissions = 777 | . | . Click Create. . Note on EFS security group settings . In the last section, I have used a default VPC security group (sg) while creating EFS. Default sg allows traffic for all protocols and all ports, both inbound and outbound. But if you are using a custom security group then make sure that you have an inbound rule for . Type = NFS | Protocol = TCP | Port range = 2049 | . Otherwise, you will not be able to access EFS using NFS clients, and if you find an error similar to the below then it means you need to check the security group settings. . . Create DataSync service task . configure source location = create a new location location type = Amazon S3 | region = us-east-1 | s3 bucket = mydata-202203 | s3 storage class = standard | folder = [leave empty] | IAM role = click on auto generate | . | configure destination location = create a new location location type = EFS | region = us-east-1 | efs file system = mydata-efs | mount path = /efs | subnet = us-east-1a | security group = default | . | configure settings task name = mydata-datasync | task execution configuration verify data = verify only the data transferred | set bandwidth limit = use available | . | data transfer configuration data to scan = entire source location | transfer mode = transfer only the data that has changed | uncheck &quot;keep deleted files&quot; | check &quot;overwrite files&quot; | . | schedule frequency = not scheduled | . | task logging cloudwatch log group = autogenerate | . | . | . Click &quot;next&quot;. Review and Launch. . Test DataSync Service . Let&#39;s test datasync service by manually starting it. If S3 bucket is empty then datasync will throw an exception as below . . This is not an issue. Just place some files (test1.txt in my case) in the bucket and start the datasync service again. If it executes successfully then you will get a message as Execution Status = Success . DataSync can work without Internet Gateway or VPC Endpoint . One thing I noticed is that DataSync service can work even without the presence of an internet gateway or S3 service endpoint. EFS is VPC bound and S3 is global but DataSync can still communicate with both of them. This was different for Lambda. Once Lambda is configured for a VPC then it is not able to access S3 without an internet gateway or VPC endpoint. . Verify EFS by mounting it to the EC2 machine . In the last section, we ran DataSync and it successfully copied files from S3 to EFS. So let&#39;s verify our files from EFS by mounting it to an EC2 instance. . Create an EC2 machine . AMI = Amazon Linux 2 AMI (HVM) - Kernel 5.10, SSD Volume Type | Intance type = t2.micro (free tier) | Instance details Network = default VPC | Auto-assign Public IP = Enable | . | Review and Lanunch &gt; Launch &gt; Proceed without key pair. | . Once the instance is up and running, click on it and connect using EC2 instance connect option. Create a dir &#39;efs&#39; using the command . mkdir efs . In a separate tab open EFS, and click on the file system we have created. Click Attach. From &quot;Mount via DNS&quot; copy command for NFS client. paste that in EC2 bash terminal . sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-01acd308743098251.efs.us-east-1.amazonaws.com:/ efs . Once successfully mounted, verify that the file &#39;test1.txt&#39; exists in EFS. . . Create Lambda function to trigger DataSync task . Now let&#39;s create a lambda function that will trigger the datasync task. This function will itself be triggered by an S3 event notification whenever a file is uploaded or deleted. . Create a lambda function as | name = datasync-trigger-s3 | runtime = Python 3.9 | . Leave the rest of the settings as default, update the code as below, and deploy. . In the code, we are first filtering the object key for which the event is generated. Then we trigger the datasync task and pass the object key as a filter string. With the filter key provided datasync job will only sync provided object from S3 to EFS. . import json import boto3 import os DataSync_task_arn = &#39;arn:aws:datasync:us-east-1:801598032724:task/task-0c04a4a15668b6b8a&#39; DataSync = boto3.client(&#39;datasync&#39;) def lambda_handler(event, context): objectKey = &#39;&#39; try: objectKey = event[&quot;Records&quot;][0][&quot;s3&quot;][&quot;object&quot;][&quot;key&quot;] except KeyError: raise KeyError(&quot;Received invalid event - unable to locate Object key to upload.&quot;, event) response = DataSync.start_task_execution( TaskArn=DataSync_task_arn, OverrideOptions={ &#39;OverwriteMode&#39; : &#39;ALWAYS&#39;, &#39;PreserveDeletedFiles&#39; : &#39;REMOVE&#39;, }, Includes=[ { &#39;FilterType&#39;: &#39;SIMPLE_PATTERN&#39;, &#39;Value&#39;: &#39;/&#39; + os.path.basename(objectKey) } ] ) print(f&quot;response= {response}&quot;) return { &#39;response&#39; : response } . Add policy AWSDataSyncFullAccess to this lambda function role otherwise it will not be able to trigger datasync task. . Configure S3 bucket event notifications . Our lambda function is ready. Now we can enable S3 bucket event notifications as put the lambda function as a target. For this from S3 bucket Properties &gt; Event notifications &gt; Create event notifications . event name = object-put-delete | event type = s3:ObjectCreated:Put, and s3:ObjectRemoved:Delete | destination = lambda function (datasync-trigger-s3) | . Click Save changes . Test DataSync task through S3 events trigger . Now let&#39;s test our trigger by placing a new file in S3 bucket. In my case it is &#39;test2.txt&#39;. Once file is successfully uploaded we can check the EC2 instance to verify the file presence. . . We can also verify that the datasync job was triggered from lambda CloudWatch logs. . response= {&#39;TaskExecutionArn&#39;: &#39;arn:aws:datasync:us-east-1:801598032724:task/task-0c04a4a15668b6b8a/execution/exec-020e456f670ca2419&#39;, &#39;ResponseMetadata&#39;: {&#39;RequestId&#39;: &#39;c8166ce4-ef14-415c-beff-09cc7720f4a3&#39;, &#39;HTTPStatusCode&#39;: 200, &#39;HTTPHeaders&#39;: {&#39;date&#39;: &#39;Wed, 30 Mar 2022 13:27:45 GMT&#39;, &#39;content-type&#39;: &#39;application/x-amz-json-1.1&#39;, &#39;content-length&#39;: &#39;123&#39;, &#39;connection&#39;: &#39;keep-alive&#39;, &#39;x-amzn-requestid&#39;: &#39;c8166ce4-ef14-415c-beff-09cc7720f4a3&#39;}, &#39;RetryAttempts&#39;: 0}} . In the logs we have task execution id exec-020e456f670ca2419 , and we can use that to verify task&#39;s status from datasync console. . . Steps for scheduled based approach . We have seen in the last section that we can use S3 event notifications to trigger datasync tasks. Now we will discuss a schedule-based trigger for datasync task. This can be done in two ways . While creating a datasync task we can define a frequency for it to follow. But the limitation on this is that it can not be lower than a 1 hour window. | If we want to schedule a datasync task on a smaller than 1 hour window then we can use AWS EventBridge (previously CloudWatch Events) to trigger a lambda function that can inturn invoke a datasync task. In the coming section, we will follow this approach. | . . Create a lambda function . Let&#39;s create a new lambda function with the following code. This lambda will invoke the datasync task. Add permissions to this lambda AWSDataSyncFullAccess . function name = datasync-trigger-scheduled | runtime = Python 3.9 | . import json import boto3 import os DataSync_task_arn = &#39;arn:aws:datasync:us-east-1:801598032724:task/task-0c04a4a15668b6b8a&#39; DataSync = boto3.client(&#39;datasync&#39;) def lambda_handler(event, context): response = DataSync.start_task_execution( TaskArn=DataSync_task_arn, OverrideOptions={ &#39;OverwriteMode&#39; : &#39;ALWAYS&#39;, &#39;PreserveDeletedFiles&#39; : &#39;REMOVE&#39;, } ) print(f&quot;response= {response}&quot;) return { &#39;response&#39; : response } . Create EventBridge event . Go to EventBridge Events &gt; Rules &gt; select Create Rule . Define rule details name = datasync-trigger | event bus = default | rule type = scheduled | . | Define schedule Sample event = {} | Schedule Pattern Rate expression = 5 min | . | . | Select Targets target = Lambda function | function = datasync-trigger-scheduled | . | . Click Next and Create Rule . EventBridge will automatically add a policy statement to lambda function (datasync-trigger-scheduled) allowing it to trigger lambda. You can verify the policy from lambda Configurations &gt; Permissions &gt; Resource based policy. If no resource policy exists then you need to manually add a policy to allow EventBridge to invoke it. For this click on Resource based policy &gt; Policy statements &gt; Add permissions. . policy statement = AWS service | service = EventBridge | statement id = eventbridge-1000 (or any unique id) | principal = events.amazonaws.com | source ARN = arn:aws:events:us-east-1:801598032724:rule/datasync-trigger (arn for eventbridge event) | . Verify event and datasync task execution . We have configured eventbridge to fire an event after every 5 min we can verify it from eventbrige monitoring tab and its cloudwatch logs. | Lambda function invocations can be verified from its cloudwatch logs | Datasync task execution status can be verified from its history tab and cloudwatch logs. | .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/lambda/efs/s3/synchonization/datasync/2022/03/29/efs-s3-datasync.html",
            "relUrl": "/aws/lambda/efs/s3/synchonization/datasync/2022/03/29/efs-s3-datasync.html",
            "date": " • Mar 29, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "AWS EFS Sync to S3 Using Lambda",
            "content": ". About . This post documents all the steps required to synchronize AWS EFS with an S3 bucket using a lambda function. The flow of information is from S3 to EFS and not vice versa. . The approach is whenever a new file is uploaded or deleted from the S3 bucket, it will create an event notification. This event will trigger a lambda function that has the efs file system mounted to it. The Lambda function will then synchronize the files from S3 to EFS. . . Environment Details . Python = 3.9.x | . Steps . Create an S3 bucket . Let&#39;s first create an S3 bucket that will contain our data, and this is the bucket we would like to be in sync with EFS. I am naming the bucket mydata-202203. You may name it as you please. Choose a region of your choice and leave the rest of the settings as defaults. . Create a Lambda function . Now create a lambda function that will receive event notifications from the S3 bucket, and sync files on efs. I am naming it mydata-sync and our runtime will be Python 3.9. Keep the rest of the settings as default, and create the function. . Create S3 event notifications . From the bucket, mydata-sync go to Properties. Scroll down to Event notifications and click create. Give any name to the event. I am calling it object-sync. From the provided event types select . s3:ObjectCreated:Put | s3:ObjectRemoved:Delete | . From the section Destination select Lambda Function, and from the list choose the lambda function name we created in the last section mydata-sync . Click Save Changes . Test S3 notifications . Let&#39;s now test if S3 event notifications are being received by our lambda function. For this update lambda function code and simply print the event received. After updating the lambda function, make sure to deploy it. . import json def lambda_handler(event, context): print(event) return { &#39;statusCode&#39;: 200, &#39;body&#39;: json.dumps(&#39;Hello from Lambda!&#39;) } . Now upload some files in our S3 bucket, and it should trigger our lambda function. For testing, I have uploaded an empty test1.txt file in our bucket. Once successfully uploaded I check the Lambda function logs to see if any event is received. For this go to lambda function mydata-sync &gt; Monitor &gt; Logs &gt; View logs in CloudWatch. For the CloudWatch console view the latest log stream. Below is the event I have received in the logs . {&#39;Records&#39;: [{&#39;eventVersion&#39;: &#39;2.1&#39;, &#39;eventSource&#39;: &#39;aws:s3&#39;, &#39;awsRegion&#39;: &#39;us-east-1&#39;, &#39;eventTime&#39;: &#39;2022-03-28T16:08:00.896Z&#39;, &#39;eventName&#39;: &#39;ObjectCreated:Put&#39;, &#39;userIdentity&#39;: {&#39;principalId&#39;: &#39;AWS:AIDA3VIXXJNKIVU6P5NY3&#39;}, &#39;requestParameters&#39;: {&#39;sourceIPAddress&#39;: &#39;202.163.113.76&#39;}, &#39;responseElements&#39;: {&#39;x-amz-request-id&#39;: &#39;39MD61ZS00SNK2RT&#39;, &#39;x-amz-id-2&#39;: &#39;U+zPUWOrfzTuVi7kbaBONLHoJXKqUICsVqyKBg4yPKYbUV7pQLGc4Z5A2fSIVvDFtSJHC6v29EDJoXhypWsj2wXanUu8YrLocr3z+yK1qoo=&#39;}, &#39;s3&#39;: {&#39;s3SchemaVersion&#39;: &#39;1.0&#39;, &#39;configurationId&#39;: &#39;object-sync&#39;, &#39;bucket&#39;: {&#39;name&#39;: &#39;mydata-202203&#39;, &#39;ownerIdentity&#39;: {&#39;principalId&#39;: &#39;AYAQOSFZ1VPK&#39;}, &#39;arn&#39;: &#39;arn:aws:s3:::mydata-202203&#39;}, &#39;object&#39;: {&#39;key&#39;: &#39;test1.txt&#39;, &#39;size&#39;: 0, &#39;eTag&#39;: &#39;d41d8cd98f00b204e9800998ecf8427e&#39;, &#39;sequencer&#39;: &#39;006241DD60D67A4556&#39;}}}]} . let&#39;s load this event in a dictionary and find some important parameters . event = {&#39;Records&#39;: [{&#39;eventVersion&#39;: &#39;2.1&#39;, &#39;eventSource&#39;: &#39;aws:s3&#39;, &#39;awsRegion&#39;: &#39;us-east-1&#39;, &#39;eventTime&#39;: &#39;2022-03-28T16:08:00.896Z&#39;, &#39;eventName&#39;: &#39;ObjectCreated:Put&#39;, &#39;userIdentity&#39;: {&#39;principalId&#39;: &#39;AWS:AIDA3VIXXJNKIVU6P5NY3&#39;}, &#39;requestParameters&#39;: {&#39;sourceIPAddress&#39;: &#39;202.163.113.76&#39;}, &#39;responseElements&#39;: {&#39;x-amz-request-id&#39;: &#39;39MD61ZS00SNK2RT&#39;, &#39;x-amz-id-2&#39;: &#39;U+zPUWOrfzTuVi7kbaBONLHoJXKqUICsVqyKBg4yPKYbUV7pQLGc4Z5A2fSIVvDFtSJHC6v29EDJoXhypWsj2wXanUu8YrLocr3z+yK1qoo=&#39;}, &#39;s3&#39;: {&#39;s3SchemaVersion&#39;: &#39;1.0&#39;, &#39;configurationId&#39;: &#39;object-sync&#39;, &#39;bucket&#39;: {&#39;name&#39;: &#39;mydata-202203&#39;, &#39;ownerIdentity&#39;: {&#39;principalId&#39;: &#39;AYAQOSFZ1VPK&#39;}, &#39;arn&#39;: &#39;arn:aws:s3:::mydata-202203&#39;}, &#39;object&#39;: {&#39;key&#39;: &#39;test1.txt&#39;, &#39;size&#39;: 0, &#39;eTag&#39;: &#39;d41d8cd98f00b204e9800998ecf8427e&#39;, &#39;sequencer&#39;: &#39;006241DD60D67A4556&#39;}}}]} event . {&#39;Records&#39;: [{&#39;eventVersion&#39;: &#39;2.1&#39;, &#39;eventSource&#39;: &#39;aws:s3&#39;, &#39;awsRegion&#39;: &#39;us-east-1&#39;, &#39;eventTime&#39;: &#39;2022-03-28T16:08:00.896Z&#39;, &#39;eventName&#39;: &#39;ObjectCreated:Put&#39;, &#39;userIdentity&#39;: {&#39;principalId&#39;: &#39;AWS:AIDA3VIXXJNKIVU6P5NY3&#39;}, &#39;requestParameters&#39;: {&#39;sourceIPAddress&#39;: &#39;202.163.113.76&#39;}, &#39;responseElements&#39;: {&#39;x-amz-request-id&#39;: &#39;39MD61ZS00SNK2RT&#39;, &#39;x-amz-id-2&#39;: &#39;U+zPUWOrfzTuVi7kbaBONLHoJXKqUICsVqyKBg4yPKYbUV7pQLGc4Z5A2fSIVvDFtSJHC6v29EDJoXhypWsj2wXanUu8YrLocr3z+yK1qoo=&#39;}, &#39;s3&#39;: {&#39;s3SchemaVersion&#39;: &#39;1.0&#39;, &#39;configurationId&#39;: &#39;object-sync&#39;, &#39;bucket&#39;: {&#39;name&#39;: &#39;mydata-202203&#39;, &#39;ownerIdentity&#39;: {&#39;principalId&#39;: &#39;AYAQOSFZ1VPK&#39;}, &#39;arn&#39;: &#39;arn:aws:s3:::mydata-202203&#39;}, &#39;object&#39;: {&#39;key&#39;: &#39;test1.txt&#39;, &#39;size&#39;: 0, &#39;eTag&#39;: &#39;d41d8cd98f00b204e9800998ecf8427e&#39;, &#39;sequencer&#39;: &#39;006241DD60D67A4556&#39;}}}]} . # event name event[&#39;Records&#39;][0][&#39;eventName&#39;] . &#39;ObjectCreated:Put&#39; . # bucket name event[&#39;Records&#39;][0][&#39;s3&#39;][&#39;bucket&#39;][&#39;name&#39;] . &#39;mydata-202203&#39; . # uploaded object key event[&#39;Records&#39;][0][&#39;s3&#39;][&#39;object&#39;][&#39;key&#39;] . &#39;test1.txt&#39; . Alright, we have seen that we are receiving notifications from S3 bucket so let&#39;s now move on to the next section. . Create an EFS . From EFS console give name as mydata-efs. I am using default VPC for this post. Use Regional availability settings. Click Create. Once file system is created, click on Access points and create an access point for this efs to be mounted in other service. For access point use following settings . name = mydata-ap | root dir path = /efs | POSIX User POSIX UID = 1000 | Group ID = 1000 | . | Root directory creation permissions Owner user id = 1000 | Owner group id = 1000 | POSIX permissions = 777 | . | . Click Create. . Here I have used the root dir path as /efs this means that from this access point my access will be limited to folder /efs. If you want to provide full access to all folders then set to root path to /. . Note on EFS security group settings . In the last section, I have used a default VPC security group (sg) while creating EFS. default sg allows traffic for all protocols and all ports, both for inbound and outbound traffic. But if you are using a custom security group then make sure that you have an inbound rule for . Type = NFS | Protocol = TCP | Port range = 2049 | . Otherwise, you will not be able to access EFS using NFS clients. . Mount EFS to Lambda Function . To mount an EFS to the Lambda function requires some additional steps. . First add permissions to Lambda function. From lambda function &gt; Configurations &gt; Permissions &gt; Execution role. Click on the execution role to open it in IAM concole. For the selected role attach an additional policy AmazonElasticFileSystemFullAccess. . Second, add the lambda to a VPC group in which efs was created. We have created efs in default VPC so let&#39;s add lambda to it. For this from lambda Configurations &gt; VPC click edit. For the next pane select default VPC, all subnets, default VPC security group, and click save. . Now we can add EFS to lambda. Go to lambda Configurations &gt; File Systems &gt; Add file system. Select the file system mydata-efs and associated access point mydata-ap and local mount point as /mnt/efs. The local mount point is the mounted directory from where we can access our EFS from inside the lambda environment. Click Save . Check EFS mount point from Lambda . Let&#39;s verify from lambda that EFS has been mounted and can we access it. So update the lambda code as below and deploy it. . import json import os def lambda_handler(event, context): mount_path = &#39;/mnt/efs&#39; if os.path.exists(mount_path): print(f&quot;{mount_path} exists&quot;) print(os.listdir(&#39;/mnt/efs&#39;)) print(event) return { &#39;statusCode&#39;: 200, &#39;body&#39;: json.dumps(&#39;Hello from Lambda!&#39;) } . Now test this code using a test event S3 Put. For this go to lambda Test &gt; Create new event &gt; Template (s3-put). &#39;S3 Put&#39; test event is similar to the one we saw in the last section. We can use this request template to simulate the event received from S3 bucket. Once the test is successfully executed, check the log output. . START RequestId: 2e307a14-f373-46d5-b763-594d5f406ae6 Version: $LATEST /mnt/efs exists [] {&#39;Records&#39;: [{&#39;eventVersion&#39;: &#39;2.0&#39;, &#39;eventSource&#39;: &#39;aws:s3&#39;, &#39;awsRegion&#39;: &#39;us-east-1&#39;, &#39;eventTime&#39;: &#39;1970-01-01T00:00:00.000Z&#39;, &#39;eventName&#39;: &#39;ObjectCreated:Put&#39;, &#39;userIdentity&#39;: {&#39;principalId&#39;: &#39;EXAMPLE&#39;}, &#39;requestParameters&#39;: {&#39;sourceIPAddress&#39;: &#39;127.0.0.1&#39;}, &#39;responseElements&#39;: {&#39;x-amz-request-id&#39;: &#39;EXAMPLE123456789&#39;, &#39;x-amz-id-2&#39;: &#39;EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH&#39;}, &#39;s3&#39;: {&#39;s3SchemaVersion&#39;: &#39;1.0&#39;, &#39;configurationId&#39;: &#39;testConfigRule&#39;, &#39;bucket&#39;: {&#39;name&#39;: &#39;example-bucket&#39;, &#39;ownerIdentity&#39;: {&#39;principalId&#39;: &#39;EXAMPLE&#39;}, &#39;arn&#39;: &#39;arn:aws:s3:::example-bucket&#39;}, &#39;object&#39;: {&#39;key&#39;: &#39;test%2Fkey&#39;, &#39;size&#39;: 1024, &#39;eTag&#39;: &#39;0123456789abcdef0123456789abcdef&#39;, &#39;sequencer&#39;: &#39;0A1B2C3D4E5F678901&#39;}}}]} END RequestId: 2e307a14-f373-46d5-b763-594d5f406ae6 REPORT RequestId: 2e307a14-f373-46d5-b763-594d5f406ae6 Duration: 7.02 ms Billed Duration: 8 ms Memory Size: 128 MB Max Memory Used: 37 MB Init Duration: 93.81 ms . From the logs we can see that the mounted EFS directory exists /mnt/efs but currently the folder is empty. . Configure VPC endpoint for S3 . Till now we have configured S3 notifications to trigger a lambda function and also mounted EFS to it. Our next step is to process the event received in lambda, and download the file from S3 to EFS. But since our lambda function is configured for a VPC we cannot connect to S3 from it. Even though we can still receive S3 event notification, when we try to connect to S3 to download any file we will get a timeout error. To fix this we will create a VPC endpoint for S3 bucket. . For this go to VPC console &gt; Endpoints &gt; Create endpoint, and set the following . name = mydata-ep | service category = aws services | services = com.amazonaws.us-east-1.s3 (Gateway) | vpc = default | route table = default (main route table) | policy = full access | . Click Create endpoint . Configure S3 permissions for Lambda . For lambda to be able to connect to S3 we also need to give it proper permissions. For this go to Lambda &gt; Configurations &gt; Permissions &gt; Execution Role &gt; click on role name. From the IAM Role console select add permissions, and then select AmazonS3FullAccess . Process S3 event notifications . Our lambda and EFS are ready and we can now process S3 events. Update the lambda code as below to process S3 events. It will download and delete from EFS to keep it in sync with S3 bucket. . import json import boto3 import os s3 = boto3.client(&quot;s3&quot;) def lambda_handler(event, context): event_name = event[&quot;Records&quot;][0][&quot;eventName&quot;] bucket_name = event[&quot;Records&quot;][0][&quot;s3&quot;][&quot;bucket&quot;][&quot;name&quot;] object_key = event[&quot;Records&quot;][0][&quot;s3&quot;][&quot;object&quot;][&quot;key&quot;] efs_file_name = &quot;/mnt/efs/&quot; + object_key # S3 put if event_name == &quot;ObjectCreated:Put&quot;: s3.download_file(bucket_name, object_key, efs_file_name) print(f&quot;file downloaded: {efs_file_name}&quot;) # S3 delete if event_name == &quot;ObjectRemoved:Delete&quot;: # check if file exists on efs if os.path.exists(efs_file_name): os.remove(efs_file_name) print(f&quot;file deleted: {efs_file_name}&quot;) return {&quot;statusCode&quot;: 200, &quot;body&quot;: json.dumps(event)} . We can test this code using the S3-put test event we used last time. Modify the event for bucket name and object key as below. . { &quot;Records&quot;: [ { &quot;eventVersion&quot;: &quot;2.0&quot;, &quot;eventSource&quot;: &quot;aws:s3&quot;, &quot;awsRegion&quot;: &quot;us-east-1&quot;, &quot;eventTime&quot;: &quot;1970-01-01T00:00:00.000Z&quot;, &quot;eventName&quot;: &quot;ObjectCreated:Put&quot;, &quot;userIdentity&quot;: { &quot;principalId&quot;: &quot;EXAMPLE&quot; }, &quot;requestParameters&quot;: { &quot;sourceIPAddress&quot;: &quot;127.0.0.1&quot; }, &quot;responseElements&quot;: { &quot;x-amz-request-id&quot;: &quot;EXAMPLE123456789&quot;, &quot;x-amz-id-2&quot;: &quot;EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH&quot; }, &quot;s3&quot;: { &quot;s3SchemaVersion&quot;: &quot;1.0&quot;, &quot;configurationId&quot;: &quot;testConfigRule&quot;, &quot;bucket&quot;: { &quot;name&quot;: &quot;mydata-202203&quot;, &quot;ownerIdentity&quot;: { &quot;principalId&quot;: &quot;EXAMPLE&quot; }, &quot;arn&quot;: &quot;arn:aws:s3:::example-bucket&quot; }, &quot;object&quot;: { &quot;key&quot;: &quot;test1.txt&quot;, &quot;size&quot;: 1024, &quot;eTag&quot;: &quot;0123456789abcdef0123456789abcdef&quot;, &quot;sequencer&quot;: &quot;0A1B2C3D4E5F678901&quot; } } } ] } . Click test. From the output logs, we can see that our code was able to download the file from S3 bucket and write it on EFS. . START RequestId: 7e9c0dc2-f970-426e-8372-e59b07f5536c Version: $LATEST file downloaded: /mnt/efs/test1.txt END RequestId: 7e9c0dc2-f970-426e-8372-e59b07f5536c REPORT RequestId: 7e9c0dc2-f970-426e-8372-e59b07f5536c Duration: 370.00 ms Billed Duration: 371 ms Memory Size: 128 MB Max Memory Used: 72 MB Init Duration: 367.68 ms . Note that if you get any permission errors then it could be due to the mounting path errors. Please do check the access point path and lambda mount path. . Verify file on EFS . We can verify files on EFS by directly mounting them to an EC2 machine and verifying from there. So let&#39;s do that. . Create an EC2 machine . AMI = Amazon Linux 2 AMI (HVM) - Kernel 5.10, SSD Volume Type | Intance type = t2.micro (free tier) | Instance details Network = default VPC | Auto-assign Public IP = Enable | . | Review and Lanunch &gt; Launch &gt; Proceed without key pair. | . Once the instance is up and running, click on it and connect using EC2 instance connect option. Create a dir &#39;efs&#39; using the command . mkdir efs . In a separate tab open EFS, and click on the file system we have created. Click Attach. From &quot;Mount via DNS&quot; copy command for NFS client. paste that in EC2 bash terminal . sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-0c9526e2f48ece247.efs.us-east-1.amazonaws.com:/ efs . Once successfully mounted verify that the file &#39;test1.txt&#39; exists in EFS. We can also delete the file from S3 and similarly verify from EFS that the file has been removed. . Summary . A summary of all the steps . Create an S3 bucket | Create a Lambda function | Create event notifications on the S3 bucket to trigger the lambda function | Create an EFS file system and its access point. Check the security group setting for inbound rules for NFS traffic | Add EFS and S3 permissions to lambda | Add lambda to VPC | Create VPC endpoint for S3 bucket | Update lambda code to process event notifications | Use EC2 to mount EFS and verify the files | .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/lambda/efs/s3/synchonization/2022/03/28/efs-s3-sync-lambda.html",
            "relUrl": "/aws/lambda/efs/s3/synchonization/2022/03/28/efs-s3-sync-lambda.html",
            "date": " • Mar 28, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "AWS Lambda - Test and Debug Locally in Visual Studio Code",
            "content": ". About . This post is about running, and debugging AWS Lambda function locally from Visual Studio Code environment and it extensions AWS Toolkit. . Environment Details . Python = 3.8.x | WSL version = 2 | WSL Ubuntu version = 20.04 | Docker Engine version = 20.10.12 | Docker Desktop version = 4.4.4 | Host OS = Windows 10 | VSCode version = 1.65 | SAM CLI version = 1.40.1 | . Sample Application . For this post we will use a simple hello world application as our focus is on debugging. We will use AWS SAM CLI to create our application. You can follow the steps provided in tutorial AWS SAM Developer Guide&gt;Getting started with AWS SAM to create this application. . From the provided link (SAM Developer Guide): This application implements a basic API backend. It consists of an Amazon API Gateway endpoint and an AWS Lambda function. When you send a GET request to the API Gateway endpoint, the Lambda function is invoked. This function returns a hello world message. . The following diagram shows the components of this application: . . To initialize a serverless app use command . sam init . Complete the SAM initialization setup steps . PS C: MyWorkspace gitrepos 2022-03-16-lambda-debug&gt; sam init You can preselect a particular runtime or package type when using the `sam init` experience. Call `sam init --help` to learn more. Which template source would you like to use? 1 - AWS Quick Start Templates 2 - Custom Template Location Choice: 1 Choose an AWS Quick Start application template 1 - Hello World Example 2 - Multi-step workflow 3 - Serverless API 4 - Scheduled task 5 - Standalone function 6 - Data processing 7 - Infrastructure event management 8 - Machine Learning Template: 1 Use the most popular runtime and package type? (Python and zip) [y/N]: y Project name [sam-app]: Cloning from https://github.com/aws/aws-sam-cli-app-templates (process may take a moment) -- Generating application: -- Name: sam-app Runtime: python3.9 Architectures: x86_64 Dependency Manager: pip Application Template: hello-world Output Directory: . Next steps can be found in the README file at ./sam-app/README.md Commands you can use next ========================= [*] Create pipeline: cd sam-app &amp;&amp; sam pipeline init --bootstrap [*] Test Function in the Cloud: sam sync --stack-name {stack-name} --watch . Once the application is initialized the project structure will look like this . sam-app/ ├── README.md ├── events/ │ └── event.json ├── hello_world/ │ ├── __init__.py │ ├── app.py #Contains your AWS Lambda handler logic. │ └── requirements.txt #Contains any Python dependencies the application requires, used for sam build ├── template.yaml #Contains the AWS SAM template defining your application&#39;s AWS resources. └── tests/ └── unit/ ├── __init__.py └── test_handler.py . There are three especially important files: . template.yaml: Contains the AWS SAM template that defines your application&#39;s AWS resources. | hello_world/app.py: Contains your actual Lambda handler logic. | hello_world/requirements.txt: Contains any Python dependencies that the application requires, and is used for sam build. | . Follow the instructions from the tutorial to build, test, and deploy the application. . All the code used for this post can be obtained from the GitHub repository hassaanbinaslam/2022-03-16-lambda-debug . Project code files | Project zip file | . SAM Project Directory . SAM CLI After project initialization from sam init make sure that you move to project root folder | Project root folder is the one that contain template.yaml defining application AWS resources. In this app case project root folder is sam-app/ | All the subsequest commands including project sam build, sam deploy, invoke and test lambda should be done from project root folder | . | VSCode When you open the project make sure that your project root directory is pointing to sam-app/ folder as shown in image below | . | . . Run Lambda Locally . To invoke lambda function locally use SAM CLI command . sam local invoke . PS C: MyWorkspace gitrepos 2022-03-16-lambda-debug sam-app&gt; sam local invoke Invoking app.lambda_handler (python3.9) Skip pulling image and use local one: public.ecr.aws/sam/emulation-python3.9:rapid-1.40.1-x86_64. Mounting C: MyWorkspace gitrepos 2022-03-16-lambda-debug sam-app hello_world as /var/task:ro,delegated inside runtime container START RequestId: 1d4e45de-38be-4b91-8e9f-4d3c7da8788a Version: $LATEST END RequestId: 1d4e45de-38be-4b91-8e9f-4d3c7da8788a REPORT RequestId: 1d4e45de-38be-4b91-8e9f-4d3c7da8788a Init Duration: 1.40 ms Duration: 990.84 ms Billed Duration: 991 ms Memory Size: 128 MB Max Memory Used: 128 MB {&quot;statusCode&quot;: 200, &quot;body&quot;: &quot;{ &quot;message &quot;: &quot;hello world &quot;}&quot;} . If you have multiple lambda functions in the app, you can invoke a specific lambda function by using it&#39;s name in invoke command as . sam local invoke &quot;HelloWorldFunction&quot; . Run API Gateway Locally . You can run API Gateway locally to test HTTP request response functionality using command . sam local start-api . This command will start a local instance of API Gateway and provide you with a URL that you can use to send a request using CURL commmand . PS C: MyWorkspace gitrepos 2022-03-16-lambda-debug sam-app&gt; sam local start-api Mounting HelloWorldFunction at http://127.0.0.1:3000/hello [GET] You can now browse to the above endpoints to invoke your functions. You do not need to restart/reload SAM CLI while working on your functions, changes will be reflected instantly/automatically. You only need to restart SAM CLI if you update your AWS SAM template 2022-03-17 11:41:55 * Running on http://127.0.0.1:3000/ (Press CTRL+C to quit) . From terminal output you can find tha HelloWorldFunction is mounted at http://127.0.0.1:3000/hello. From another terminal we can call this URL . PS C: MyWorkspace gitrepos 2022-03-16-lambda-debug sam-app&gt; curl http://127.0.0.1:3000/hello StatusCode : 200 StatusDescription : OK Content : {&quot;message&quot;: &quot;hello world&quot;} RawContent : HTTP/1.0 200 OK Content-Length: 26 Content-Type: application/json Date: Thu, 17 Mar 2022 06:43:08 GMT Server: Werkzeug/1.0.1 Python/3.8.8 {&quot;message&quot;: &quot;hello world&quot;} Forms : {} Headers : {[Content-Length, 26], [Content-Type, application/json], [Date, Thu, 17 Mar 2022 06:43:08 GMT], [Server, Werkzeug/1.0.1 Python/3.8.8]} Images : {} InputFields : {} Links : {} ParsedHtml : mshtml.HTMLDocumentClass RawContentLength : 26 . Debug Lambda Application Locally . To debug a lambda function we have following options . Option 1: Debug through SAM template . From VSCode open template.yaml, and go to the resources section of the template that defines serverless resources. Click on the lambda function resource, which in our case is HelloWorldFunction. A tooltip will appear over it saying AWS: Add Debug Configuration. Click it as shown below. . . This will create a new folder in the project with debug launch configuration launch.json. . . Let&#39;s add a breakpoint in our lambda handler code hello_world/app.py, and start debugging by clicking the green &quot;play&quot; button in the RUN view. When the debugging sessions starts, the DEBUG CONSOLE panel shows debugging output and displays any values returned by the Lambda function. . . Option 2: Debug Lambda Directly from Code . From VSCode open lambda handler code sam-app/hello_world/app.py. A tooltip will appear above the lambda_handler function with options . AWS: Add Debug Configuration | AWS: Edit Debug Configuration | . . Click on AWS: Add Debug Configuration and it will show two further options . template.yaml:HelloWorldFunction (to debug only the lambda function) | template.yaml:HelloWorldFunction (API Event: HelloWorld) (to debug lambda function along with API gateway) | . . Let&#39;s select API option this time. It will again create a launch configuration, and now we can debug our code. Click on the green &quot;play&quot; button again to start the debug session with request request coming from API Gateway to Lambda function. . You can also edit the debug config visually by selecting the AWS: Edit Debug Configuration, and a side pane will appear from where we can easily edit and update our debug configuration. . . All the code used for this post can be obtained from the GitHub repository hassaanbinaslam/2022-03-16-lambda-debug . Project code files | Project zip file | .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/lambda/docker/2022/03/16/lambda-debug.html",
            "relUrl": "/aws/lambda/docker/2022/03/16/lambda-debug.html",
            "date": " • Mar 16, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "Docker - Debugging Python Application",
            "content": ". About . This post is about debugging a Python application running on a Docker container inside WSL2 linux environment. Highlight of this post is Visual Studio Code environment and it extensions Remote Containers. . Environment Details . Python = 3.8.x | WSL version = 2 | WSL Ubuntu version = 20.04 | Docker Engine version = 20.10.12 | Docker Desktop version = 4.4.4 | Host OS = Windows 10 | VSCode version = 1.65 | . Sample Application . For this post I will use a a simple hello world application that will print &quot;hello world&quot; messages to stdout, and also logs them in a logfilelog. After each message the application sleeps for 5 seconds, and keeps on doing this for 5 mins (300 sec). After this the program exists. . This application was created as part of the previous blog post Docker - Accessing Python Application Logs. It is a very simple application, and you can find all the code in GitHub repository snapshots-docker-post-11032022 . Project code files | Project zip file | . Project structure of this application is . app/ ├── src/ │ ├── commons/ │ │ └── logger.py │ └── hello.py └── Dockerfile . Where . app/ is the project root folder | src/ folder contain the python application code | src/commons/logger.py is the logging module | src/hello.py is the main application | app/Dockerfile is the Docker image build file | . When I run the src/hello.py file from my local machine (Windows 10) I get the output on the termial with hello world messages like this. . . A &quot;logfile.log&quot; will also appear in the project directory containing the logged messages. . . ├── app/ │ ├── src/ │ │ ├── commons/ │ │ │ └── logger.py │ │ └── hello.py │ └── Dockerfile └── **logfile.log** . Contents of &quot;logfile.log&quot; file will look like this . INFO 2022-03-11 13:01:56,451 - hello world at 11/03/2022 13:01:56 INFO 2022-03-11 13:02:01,464 - hello world at 11/03/2022 13:02:01 INFO 2022-03-11 13:02:06,466 - hello world at 11/03/2022 13:02:06 INFO 2022-03-11 13:02:11,480 - hello world at 11/03/2022 13:02:11 INFO 2022-03-11 13:02:16,496 - hello world at 11/03/2022 13:02:16 . We can build our docker image and run it using commands . docker build --tag python-docker . docker run --name helloworld python-docker . Output on the terminal will be like this . Notice the difference in the print message when the application was is locally, and from the docker container. . Local (Win10) message = hello world at 14/03/2022 18:04:02 from Windows | Docker container message = hello world at 14/03/2022 13:12:14 from Linux | . Debug Docker Application . To debug the application from inside the docker container we will use VSCode extention Visual Studio Code Remote - Containers. From the extension docs . The Visual Studio Code Remote - Containers extension lets you use a Docker container as a full-featured development environment. It allows you to open any folder inside (or mounted into) a container and take advantage of Visual Studio Code&#39;s full feature set. . Once this extension is installed a new icon ( Remote Window ) will appear at the bottom left corner of the VSCode window. Once clicked on the icon, a dropped down will appear as shown below. From this drop down choose option Reopen in Container . . Now it is important to understand that Visual Studio Code Remote - Containers extension let&#39;s you use a Docker container as a full-featured development environment. It allows you to open any folder or repository inside a container and take advantage of Visual Studio Code&#39;s full feature set including code debugging, linting, formatting, intellisense, and other tooling. VSCode also provides its own prebuild docker images with all the necessary tools installed into them. Or we can we instruct the VSCode to create a new development container using our docker file. You can find a list of prebuild docker images here: microsoft-vscode-devcontainers . VSCode uses a configuration file called &quot;devcontainer.json&quot; to store instructions on how to create and attach to a development container. You can read more about this config file here: devcontainerjson-reference . Now let&#39;s create a new docker development environment using our Dockerfile. . Open VSCode Commands Palette (F1 or CTL+Shift+P on Win10) | Select &quot;Remote-Containers: Add Development Container Configuration Files&quot; option from dropdown | Then select &quot;from Docker&quot; since we want our development container environment same as defined in the Dockerfile | . If this option is not available, and the drop down is showing other options like in the image below, then VSCode is unable to find a Dockerfile associated with the project. . . Notice my project dir in the image below. The root folder of my project snapshots-docker-post-11032022 does not contain any Dockerfile. . . VSCode remote extension assumes that there is a Docker file at the root of the project directory. My project root contain app/ folder and inside this folder Dockerfile is located. When we select &quot;Remote-Containers: Add Development Container Configuration Files&quot; option docker extension checks the project root folder for a Dockerfile. It could not find one in my project and that is why it removed &#39;From Dockerfile&#39; option from the dropdown. Let&#39;s correctly open the project with app/ as the root folder (or place the Dockerfile at the project root folder). After correcting this mistake, my project in VSCode looks like this . . Now use the extension one more time to create a development container. This time you will find the option &quot;From Dockerfile&quot; in the dropdown menu . . Once this option is selected, VSCode will add a folder &quot;.devcontainer&quot; in the project root containing instructions on how to build and launch the development container. Then it will run those instruction to launch a container and connect to it. VSCode terminal will show the logs of all the commands used in launching that container, and at bottom left of VSCode window it will show the name of the container to which it is currently connected. . . Note that at this point we are actually working from inside a container. But to actually develop and debug the code from this container you will be required to install more extensions to it. If we had used a VSCode prebuild image then all the required extensions will be automatically available. To install required extension we can use VSCode extensions tab. . . Python extension will be required to work with Python code. So let&#39;s intall in our working container. You can also copy the names of installed extensions and paste them in the &quot;.devcontainer&quot; config file as shown below . // Add the IDs of extensions you want installed when the container is created. &quot;extensions&quot;: [ &quot;ms-python.python&quot;, &quot;ms-python.vscode-pylance&quot; ] . This way when next time we use this config file to launch a new dev container, all these extensions will be automatically installed for us. To customise the config file you can take help from this template provided by VSCode team python-3/.devcontainer . Installation of the extensions can be verified from the VSCode terminal logs . . We can now run our Python code from inside this container . . We can also easily debug our code directly from inside the container . . To close the remote connection, click on the Remote Window Icon at the bottom left corner. Use &quot;Reopen Folder Locally&quot; option to return back to local environment. Or &quot;Close Remote Connection&quot; to close the remote connection and also close the project. . . All the code used for this post can be obtained from the GitHub repository hassaanbinaslam/snapshots-docker-post-11032022 . Project code files | Project zip file | .",
            "url": "https://hassaanbinaslam.github.io/myblog/docker/python/2022/03/14/docker-app-debug.html",
            "relUrl": "/docker/python/2022/03/14/docker-app-debug.html",
            "date": " • Mar 14, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "Docker - Accessing Python Application Logs",
            "content": ". About . This post is about the challenges on accessing Python application logs running on a Docker container inside WSL2 linux environment. . Environment Details . Python = 3.8.x | WSL version = 2 | WSL Ubuntu version = 20.04 | Docker Engine version = 20.10.12 | Docker Desktop version = 4.4.4 | Host OS = Windows 10 | . Sample Application . Let us create a simple hello world application that will print &quot;hello world&quot; message to stdout, and also logs them in a logfile. After each message the application sleeps for 5 seconds, and keeps on doing this for 5 mins (300 sec). After this the program exists. . Project structure of this application is . app/ └── src/ ├── commons/ │ └── logger.py └── hello.py . Where . app/ is the project root folder | src/ folder contain the python application code | src/commons/logger.py is the logging module | src/hello.py is the main application | . Code files are provided below . # app/src/commons/logger.py import logging import os logformat = &quot;%(levelname)s %(asctime)s - %(message)s&quot; filename = &quot;logfile.log&quot; # Setting the config of the log object logging.basicConfig( format=logformat, filename=filename, level=logging.INFO, ) . # app/src/hello.py from datetime import datetime import time import commons.logger as logger def main(): # run for about 5 min: 300 sec for i in range(60): now = datetime.now() dt_string = now.strftime(&quot;%d/%m/%Y %H:%M:%S&quot;) # prepare message msg = f&quot;hello world at {dt_string}&quot; # put message to stdout and logs print(msg) logger.logging.info(msg) # sleep for some seconds time.sleep(5) if __name__ == &quot;__main__&quot;: main() . When I run the hello.py file I get the output on the termial with hello world messages like this. . . When we run the application a &quot;logfile.log&quot; will also appear in the project directory containing the logged messages. . . ├── app/ │ └── src/ │ ├── commons/ │ │ └── logger.py │ └── hello.py └── **logfile.log** . Contents of &quot;logfile.log&quot; file will look like this . INFO 2022-03-11 13:01:56,451 - hello world at 11/03/2022 13:01:56 INFO 2022-03-11 13:02:01,464 - hello world at 11/03/2022 13:02:01 INFO 2022-03-11 13:02:06,466 - hello world at 11/03/2022 13:02:06 INFO 2022-03-11 13:02:11,480 - hello world at 11/03/2022 13:02:11 INFO 2022-03-11 13:02:16,496 - hello world at 11/03/2022 13:02:16 . All the code till this point can be found at GitHub repository https://github.com/hassaanbinaslam/snapshots-docker-post-11032022 . Project code files | Project zip file | . Dockerize the application . Our hello-world application is ready now, and we can put it inside a docker container. For this let&#39;s create a Dockerfile and place it in app/ folder. . # app/Dockerfile FROM python:3.8-slim-buster # set the working directory in the container WORKDIR /app # copy the content of the local src directory to the working directory COPY src/ . # command to run on container start CMD [ &quot;python3&quot;, &quot;./hello.py&quot;] . We can build our docker image by running the command from terminal at folder app/ . docker build --tag python-docker . . Output of this command will look like this . We can check the created docker image using command from terminal . docker images . Output of this command will look like this . So our docker image is ready, we can now run it using command . docker run --name helloworld python-docker . After running this command you will observe that there is no output on the terminal. Even though we have run our image in an attached mode but still there is no output. We know that the container is running as control on terminal has not return back to us. We can also verify that the container is running by running command docker ps in a separate terminal. Output from this command will look like this . . We can also verify that the container is running from Docker Desktop container apps menu. Running container instance will appear like this . . The reason for logs not appearing on the terminal is because they are being buffered by docker internally. You will get all of them once docker container has finished execution and stopped. You can read more about docker buffereing the output from these StackOverflow posts . disable-output-buffering | python-app-does-not-print-anything-when-running-detached-in-docker | . To disable the output buffering we need to change the CMD in our docker file as . CMD [ &quot;python3&quot;, &quot;-u&quot;, &quot;./hello.py&quot;] . # app/Dockerfile FROM python:3.8-slim-buster # set the working directory in the container WORKDIR /app # copy the content of the local src directory to the working directory COPY src/ . # command to run on container start CMD [ &quot;python3&quot;, &quot;-u&quot;, &quot;./hello.py&quot;] . We need to rebuild our docker image and then run it. Let&#39;s do that will the following commands . docker build --tag python-docker . docker run --name helloworld python-docker . this time you can see the output directly on the terminal . We don&#39;t have to run the docker container in an attached mode, and can still get the logs from a running container using docker logs command. Let&#39;s do that then . first run the docker image in a detached mode . docker run -d --name helloworld python-docker . this command will give the running container ID which will look something like this 06918cf824210c015b08ef072feed01768be30ee569e5dbc0eef8d3cd855ab47. We can pass this ID to our next command to view the logs stream from a running container. Use this command . docker logs -f 06918cf824210c015b08ef072feed01768be30ee569e5dbc0eef8d3cd855ab47 . Also note that we don&#39;t need to provide the full container id, and can can also provide the first 2 or 3 digits of the ID that can uniquely identify the running container. So following command will also work . docker logs -f 069 . Output on the terminal will look like this . All the project files till this point can be found at . project code files | . Log Files Physical Location . Docker STDOUT / STDERR logs are also stored on host system as JSON files. . On linux You can find them at . /var/lib/docker/containers/&lt;container-id&gt;/&lt;container-id&gt;-json.log . On Windows You can find them at . wsl$ docker-desktop-data version-pack-data community docker containers &lt;container-id&gt;/&lt;container-id&gt;-json.log . In the above paths replace &lt;container-id&gt; with your full cotaniner ID. Also note that on Windows 10 you can open the folder location by directly pasting the path in folder explorer search bar. . You can read more about them in the following StackOverflow post . windows-10-and-docker-container-logs-docker-logging-driver | . Application Log Files . So far we have talked about the docker logs that were generated by application feed to STDOUT or STDERR. But now we are interested in app logs generated by logging module like the one we saw in our first example &quot;logfile.log&quot;. For this let&#39;s first connect to running docker container and see where is this file located inside the docker running instance. . Run a new docker container again using command . docker run -d --name helloworld python-docker . If container already exists you can just start it using command . docker start &lt;container-id&gt; . Or you can also use the docker desktop to start existing container by click the play button over it. . . Once the docker container is running, you can connect to it by either using the CLI from docker desktop . . or from the command below . docker exec -it &lt;cotainer-id&gt; /bin/bash . Remember that container-id of a running container can be obtained by command docker ps. Output from the above command will look like this . . Note that the location of &quot;logfile.log&quot; is under app/ folder . Docker Volume . We have seen the application logs by connecting to the docker container, but we would like to have these logs readily available outside the docker container so they can be consumed in real time for debugging. Fot this docker recommends using volume. Let&#39;s create a volume and then mount our application logs to it. . To create a volume use command . docker volume create applogs . To remove a volume use command . docker rm create applogs . To inspect a created volume use command . docker volume inspect applogs . Output of this command will be like this where Mountpoint is the location of logs on host system. . [ { &quot;CreatedAt&quot;: &quot;2022-03-11T13:12:57Z&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: {}, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/applogs/_data&quot;, &quot;Name&quot;: &quot;applogs&quot;, &quot;Options&quot;: {}, &quot;Scope&quot;: &quot;local&quot; } ] . On Windows 10 you can find the location of these volumes at . wsl$ docker-desktop-data version-pack-data community docker volumes applogs _data . Now let&#39;s run our docker container with this volume mounted to it. Command for this is . docker run -d --name helloworld -v applogs:/app python-docker . Docker container will run as before, but if we check the volume on our host we will find that this time all the files and folders available in app/ folder from inside the docker container are available. And they will persist on the host OS even if the container is stopped. . . Note that the log file &#39;logfile.log&#39; is also available outside the container, and we can poll it for debugging. But having all the contents of app/ folder exposed can be a security issue, as they can contain secrets and passwords. We should only mount the logfile.log file on the volume as a best practice. So let&#39;s do that next. . Application log files from the docker container . To do this we need to slightly update our application, and create the logfile.log file in a designated log/ folder inside the app/. This way we can only mount app/log/ folder on the volume. In our application we will update the logging module as . # app/src/commons/logger.py import logging import os if not os.path.exists(&quot;logs&quot;): os.makedirs(&quot;logs&quot;) logformat = &quot;%(levelname)s %(asctime)s - %(message)s&quot; filename = &quot;logs/logfile.log&quot; # Setting the config of the log object logging.basicConfig( format=logformat, filename=filename, level=logging.INFO, ) . This is all we need to change in our application. To cleanup the volume we can either remove the old one and recreate a new one. Or we can directly delete all the files &amp; folders from the host OS from the directory . wsl$ docker-desktop-data version-pack-data community docker volumes applogs _data . Since we have updated the application code we need to rebuild our docker image. Then create a new docker container, and this time only mount the logs/ folder on the volume. Command for this is . # delete the old volume docker volume rm applogs # create a new volume docker volume create applogs # build the docker image docker build --tag python-docker . # run the docker container and mount a specific folder on volume docker run -d --name helloworld -v applogs:/app/logs python-docker . If we check the mounted volume, this time only logfile.log is exposed. . . All the code for this post can be found at GitHub repository https://github.com/hassaanbinaslam/snapshots-docker-post-11032022 . Project code files | Project zip file | .",
            "url": "https://hassaanbinaslam.github.io/myblog/docker/python/2022/03/11/docker-app-logs.html",
            "relUrl": "/docker/python/2022/03/11/docker-app-logs.html",
            "date": " • Mar 11, 2022"
        }
        
    
  
    
        ,"post20": {
            "title": "Sklearn Pipeline and Transformers Deep Dive",
            "content": ". About . This notebook shows various ways to work with Skearn Pipelines. . We will start with some of the limitations of pipelines and how to overcome them | We will discuss getting a dataframe from a pipeline instead of a NumPy array, and the benefits of this approach | We will learn how to use CustomTransformer and a FunctionTransformer | We will also build a custom transformer to do some feature engineering | Along the way, we will also see how to avoid common mistakes while creating pipelines | . Setup . Environment Details . from platform import python_version import sklearn, numpy, matplotlib, pandas print(&quot;python==&quot; + python_version()) print(&quot;sklearn==&quot; + sklearn.__version__) print(&quot;numpy==&quot; + numpy.__version__) print(&quot;pandas==&quot; + pandas.__version__) print(&quot;matplotlib==&quot; + matplotlib.__version__) . . python==3.8.8 sklearn==1.0.2 numpy==1.20.1 pandas==1.2.3 matplotlib==3.5.1 . Loading Data . For this example we will use the original Titanic dataset, describing the survival status of individual passengers on the Titanic ship. . Some notes from original source: . The variables on our extracted dataset are &#39;pclass&#39;, &#39;name&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;ticket&#39;, &#39;fare&#39;,&#39;cabin&#39;, &#39;embarked&#39;, &#39;boat&#39;, &#39;body&#39;, and &#39;home.dest&#39;. | pclass refers to passenger class (1st, 2nd, 3rd), and is a proxy for socio-economic class. | Age is in years, and some infants had fractional values. | sibsp = Number of Siblings/Spouses aboard | parch = Number of Parents/Children aboard | The target is either a person survived or not (1 or 0) | . Important note: The purpose of this notebook is not to train a best model on titanic data, but to understand the working of Sklearn pipeline and transformers. So please be mindful of that. . from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split import numpy as np np.random.seed(42) # for consistency # Load data from https://www.openml.org/d/40945 X, y = fetch_openml(&quot;titanic&quot;, version=1, as_frame=True, return_X_y=True) X.head() . pclass name sex age sibsp parch ticket fare cabin embarked boat body home.dest . 0 1.0 | Allen, Miss. Elisabeth Walton | female | 29.0000 | 0.0 | 0.0 | 24160 | 211.3375 | B5 | S | 2 | NaN | St Louis, MO | . 1 1.0 | Allison, Master. Hudson Trevor | male | 0.9167 | 1.0 | 2.0 | 113781 | 151.5500 | C22 C26 | S | 11 | NaN | Montreal, PQ / Chesterville, ON | . 2 1.0 | Allison, Miss. Helen Loraine | female | 2.0000 | 1.0 | 2.0 | 113781 | 151.5500 | C22 C26 | S | None | NaN | Montreal, PQ / Chesterville, ON | . 3 1.0 | Allison, Mr. Hudson Joshua Creighton | male | 30.0000 | 1.0 | 2.0 | 113781 | 151.5500 | C22 C26 | S | None | 135.0 | Montreal, PQ / Chesterville, ON | . 4 1.0 | Allison, Mrs. Hudson J C (Bessie Waldo Daniels) | female | 25.0000 | 1.0 | 2.0 | 113781 | 151.5500 | C22 C26 | S | None | NaN | Montreal, PQ / Chesterville, ON | . # let&#39;s check the frequency of missing values in each feature X.isnull().sum().sort_values(ascending=False) . body 1188 cabin 1014 boat 823 home.dest 564 age 263 embarked 2 fare 1 pclass 0 name 0 sex 0 sibsp 0 parch 0 ticket 0 dtype: int64 . # let&#39;s drop top 4 features with highest percentage of missing data # This step is done to make our working with pipeline simpler and easier to understand X.drop([&#39;body&#39;, &#39;cabin&#39;, &#39;boat&#39;, &#39;home.dest&#39;], axis=1, inplace=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2) X_train.head() . pclass name sex age sibsp parch ticket fare embarked . 999 3.0 | McCarthy, Miss. Catherine &#39;Katie&#39; | female | NaN | 0.0 | 0.0 | 383123 | 7.7500 | Q | . 392 2.0 | del Carlo, Mrs. Sebastiano (Argenia Genovesi) | female | 24.0 | 1.0 | 0.0 | SC/PARIS 2167 | 27.7208 | C | . 628 3.0 | Andersson, Miss. Sigrid Elisabeth | female | 11.0 | 4.0 | 2.0 | 347082 | 31.2750 | S | . 1165 3.0 | Saad, Mr. Khalil | male | 25.0 | 0.0 | 0.0 | 2672 | 7.2250 | C | . 604 3.0 | Abelseth, Miss. Karen Marie | female | 16.0 | 0.0 | 0.0 | 348125 | 7.6500 | S | . Some Terminology First . Datasets . Scikit-learn deals with learning information from one or more datasets that are represented as 2D arrays. They can be understood as a list of multi-dimensional observations. We say that the first axis of these arrays is the samples axis, while the second is the features axis. . (n_samples, n_features) . # for our titanic dataset: # n_samples = 1309 # n_features = 9 X.shape . (1309, 9) . Estimator . An estimator is any object that learns from data; it may be a classification, regression or clustering algorithm or a transformer that extracts/filters useful features from raw data. . All estimator objects expose a fit method that takes a dataset (usually a 2-d array) . estimator.fit(data) . Transformer . An estimator supporting transform and/or fit_transform methods. . A transformer, transforms the input, usually only X, into some transformed space. Output is an array or sparse matrix of length n_samples and with the number of columns fixed after fitting. . Fit . The fit method is provided on every estimator. It usually takes some samples X, targets y if the model is supervised, and potentially other sample properties such as sample_weight. . It should: . clear any prior attributes stored on the estimator, unless warm_start is used | validate and interpret any parameters, ideally raising an error if invalid | validate the input data | estimate and store model attributes from the estimated parameters and provided data; and | return the now fitted estimator to facilitate method chaining | . Note: . Fitting = Calling fit (or fit_transform, fit_predict) method on an estimator. | Fitted = The state of an estimator after fitting. | . Sklearn Pipeline . class sklearn.pipeline.Pipeline(steps, *, memory=None, verbose=False) . It is a pipeline of transformers with a final estimator. . It sequentially applies a list of transforms and a final estimator. Intermediate steps of the pipeline must be transforms, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. . Lets create a simple pipeline to better understand its componets. Steps in our pipeline will be . replace missing values using the mean along each numerical feature column; and | then scale them | . from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler pipe = Pipeline(steps=[ (&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler()) ]) # our first pipeline has been initialized pipe . Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler())]) . We can also visualize the pipeline as a diagram. It has two steps: imputer and scaler in sequence. . from sklearn import set_config set_config(display=&quot;diagram&quot;) pipe . Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputer()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]) . Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputer()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]) . SimpleImputerSimpleImputer() . StandardScalerStandardScaler() . now lets call fit_transform method to run this pipeline, and preprocess our loaded data . pipe.fit_transform(X_train, y_train) . ValueError Traceback (most recent call last) Input In [8], in &lt;module&gt; 1 #collapse-output -&gt; 2 pipe.fit_transform(X_train, y_train) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn pipeline.py:426, in Pipeline.fit_transform(self, X, y, **fit_params) 399 &#34;&#34;&#34;Fit the model and transform with the final estimator. 400 401 Fits all the transformers one after the other and transform the (...) 423 Transformed samples. 424 &#34;&#34;&#34; 425 fit_params_steps = self._check_fit_params(**fit_params) --&gt; 426 Xt = self._fit(X, y, **fit_params_steps) 428 last_step = self._final_estimator 429 with _print_elapsed_time(&#34;Pipeline&#34;, self._log_message(len(self.steps) - 1)): File ~ anaconda3 envs sc_mlflow lib site-packages sklearn pipeline.py:348, in Pipeline._fit(self, X, y, **fit_params_steps) 346 cloned_transformer = clone(transformer) 347 # Fit or load from cache the current transformer --&gt; 348 X, fitted_transformer = fit_transform_one_cached( 349 cloned_transformer, 350 X, 351 y, 352 None, 353 message_clsname=&#34;Pipeline&#34;, 354 message=self._log_message(step_idx), 355 **fit_params_steps[name], 356 ) 357 # Replace the transformer of the step with the fitted 358 # transformer. This is necessary when loading the transformer 359 # from the cache. 360 self.steps[step_idx] = (name, fitted_transformer) File ~ anaconda3 envs sc_mlflow lib site-packages joblib memory.py:352, in NotMemorizedFunc.__call__(self, *args, **kwargs) 351 def __call__(self, *args, **kwargs): --&gt; 352 return self.func(*args, **kwargs) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn pipeline.py:893, in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params) 891 with _print_elapsed_time(message_clsname, message): 892 if hasattr(transformer, &#34;fit_transform&#34;): --&gt; 893 res = transformer.fit_transform(X, y, **fit_params) 894 else: 895 res = transformer.fit(X, y, **fit_params).transform(X) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn base.py:855, in TransformerMixin.fit_transform(self, X, y, **fit_params) 852 return self.fit(X, **fit_params).transform(X) 853 else: 854 # fit method of arity 2 (supervised transformation) --&gt; 855 return self.fit(X, y, **fit_params).transform(X) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn impute _base.py:319, in SimpleImputer.fit(self, X, y) 302 def fit(self, X, y=None): 303 &#34;&#34;&#34;Fit the imputer on `X`. 304 305 Parameters (...) 317 Fitted estimator. 318 &#34;&#34;&#34; --&gt; 319 X = self._validate_input(X, in_fit=True) 321 # default fill_value is 0 for numerical input and &#34;missing_value&#34; 322 # otherwise 323 if self.fill_value is None: File ~ anaconda3 envs sc_mlflow lib site-packages sklearn impute _base.py:285, in SimpleImputer._validate_input(self, X, in_fit) 279 if &#34;could not convert&#34; in str(ve): 280 new_ve = ValueError( 281 &#34;Cannot use {} strategy with non-numeric data: n{}&#34;.format( 282 self.strategy, ve 283 ) 284 ) --&gt; 285 raise new_ve from None 286 else: 287 raise ve ValueError: Cannot use mean strategy with non-numeric data: could not convert string to float: &#34;McCarthy, Miss. Catherine &#39;Katie&#39;&#34; . . Aaargh! this is not what we intended. Let us try to understand why our pipeline did not work and then fix it. The exception message says: . ValueError: Cannot use mean strategy with non-numeric data: could not convert string to float: &quot;McCarthy, Miss. Catherine &#39;Katie&#39; . From the error message we can deduce that Pipeline is trying to apply its transformers on all columns in the dataset. This was not our intention, as we wanted to apply the transformers to numeric data only. Let&#39;s limit our simple pipeline to numerical columns and run again. . num_cols = [&#39;age&#39;, &#39;fare&#39;] pipe.fit_transform(X_train[num_cols], y_train) . array([[ 0. , -0.49963779], [-0.43641134, -0.09097855], [-1.44872891, -0.01824953], ..., [-0.98150542, -0.49349894], [-0.82576425, -0.44336498], [-0.59215251, -0.49349894]]) . Alright, our pipeline has run now and we can also observe a few outcomes. . When we apply a pipeline to a dataset it will run transformers to all features in the dataset. | Output from one transformer will be passed on to the next one until we reach the end of the pipeline | If we want to apply different transformers for numerical and categorical features (heterogeneous data) then the pipeline will not work for us. We would have to create separate pipelines for the different feature sets and then join the output. | . To overcome the limitation of a pipeline for heterogeneous data, Sklearn recommends using ColumnTransformer. With ColumnTransformer we can provide column names against the transformers on which we want to apply them. . ColumnTransformer . Let&#39;s see our first ColumnTransformer in action. . from sklearn.compose import ColumnTransformer from sklearn.preprocessing import OneHotEncoder # Note the sequence when creating a ColumnTransformer # 1. a name for the transformer # 2. the transformer # 3. the column names pipe = ColumnTransformer([ (&#39;standardscaler&#39;, StandardScaler(), [&#39;age&#39;, &#39;fare&#39;] ), (&#39;onehotencoder&#39;, OneHotEncoder(), [&#39;sex&#39;]) ]) pipe . ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])]) . Please rerun this cell to show the HTML repr or trust the notebook.ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])]) . standardscaler[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder() . # Let&#39;s execute our pipeline to see the output. pipe.fit_transform(X_train, y_train) . array([[ nan, -0.49939913, 1. , 0. ], [-0.39043136, -0.09093509, 1. , 0. ], [-1.2960919 , -0.01824081, 1. , 0. ], ..., [-0.87809473, -0.49326321, 0. , 1. ], [-0.73876234, -0.4431532 , 0. , 1. ], [-0.52976375, -0.49326321, 0. , 1. ]]) . get_feature_names_out(input_features=None) . At this point I will also introduce a very useful function get_feature_names_out(input_features=None). Using this method we can get output feature names as well. . pipe.get_feature_names_out() . array([&#39;standardscaler__age&#39;, &#39;standardscaler__fare&#39;, &#39;onehotencoder__sex_female&#39;, &#39;onehotencoder__sex_male&#39;], dtype=object) . Notice the output . Output feature names appear as &lt;transformer_name&gt;__&lt;feature_name&gt; | For OneHotEncoded feature &quot;sex&quot;, output feature names have the label attached to them | . make_column_transformer . Sklean also provides a wrapper function for ColumnTransformer where we don&#39;t have to provide names for the transformers. . from sklearn.compose import make_column_transformer # Note the sequence when using make_column_transformer # 1. the transformer # 2. the column names pipe = make_column_transformer( (StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (OneHotEncoder(), [&#39;sex&#39;] ), verbose_feature_names_out=False # to keep output feature names simple ) pipe . ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False) . Please rerun this cell to show the HTML repr or trust the notebook.ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False) . standardscaler[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder() . # Let&#39;s execute our pipeline. pipe.fit_transform(X_train, y_train) . array([[ nan, -0.49939913, 1. , 0. ], [-0.39043136, -0.09093509, 1. , 0. ], [-1.2960919 , -0.01824081, 1. , 0. ], ..., [-0.87809473, -0.49326321, 0. , 1. ], [-0.73876234, -0.4431532 , 0. , 1. ], [-0.52976375, -0.49326321, 0. , 1. ]]) . # notice the feature names this time. they are shorter. # we have used attribute &quot;verbose_feature_names_out=False&quot; in our pipeline above. pipe.get_feature_names_out() . array([&#39;age&#39;, &#39;fare&#39;, &#39;sex_female&#39;, &#39;sex_male&#39;], dtype=object) . Important difference between Pipeline and ColumnTransformer . Pipeline applies transformer in sequence on all columns | ColumnTranformer applies transformers in parallel to specified columns and then concats the output | . Open questions? . So our ColumnTransformer is working. But we have a few more questions to address. . Why is the output from our pipeline or ColumnTransformer not shown as a dataframe with output features nicely separated in different columns? | Our input dataset had more features besides age, fare, and sex. Why are they not present in the output? | What happens if I change the sequence of transformers, and feature names in my ColumnTransformer? | . In the coming sections, we will try to address these questions. . Why is the output not a dataframe? . The output from a pipeline or a ColumnTransformer is an nd-array where the first index is the number of samples, and second index are the output features (n_samples, n_output_features). Since we are only getting numpy array as an output, we are losing information about the column names. . temp = pipe.fit_transform(X_train, y_train) print(type(temp)) print(temp.shape) . &lt;class &#39;numpy.ndarray&#39;&gt; (1047, 4) . Can we get the feature names back? . We have already seen that we can get the output feature names using method get_feature_names_out. But this time let&#39;s try to analyze our ColumnsTransformer more closely. The transformer attributes discussed here also applies to Pipeline object. . # print the internals of ColumnTransformer set_config(display=&#39;text&#39;) pipe . ColumnTransformer(transformers=[(&#39;standardscaler&#39;, StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (&#39;onehotencoder&#39;, OneHotEncoder(), [&#39;sex&#39;])], verbose_feature_names_out=False) . ColumnTransformer has an attribute &#39;transformers&#39; that is keeping a list of all the provided transformers. Let&#39;s print it. . pipe.transformers . [(&#39;standardscaler&#39;, StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (&#39;onehotencoder&#39;, OneHotEncoder(), [&#39;sex&#39;])] . These are the transformers list at the initialization time. If we want to check the transformers after fit function has been called, then we need to print a different attribute transformers_. . pipe.transformers_ . [(&#39;standardscaler&#39;, StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (&#39;onehotencoder&#39;, OneHotEncoder(), [&#39;sex&#39;]), (&#39;remainder&#39;, &#39;drop&#39;, [0, 1, 4, 5, 6, 8])] . You can see the difference. There is an extra transformer with the name remainder at the end. It was not present at the initialization time. What it does is that it drops all remaining columns from the dataset that have not been explicitly used in the ColumnTransformer. Since, at the initialization time, ColumnTransformer does not know about the other columns that it needs to drop this transformer is missing. During fit it sees the dataset and knows about the other columns, it then keeps a list of them to drop (0, 1, 4, 5, 6, 8). . We can also index through the transformers as well to fetch anyone from the list. . # second transformer from the list pipe.transformers_[1] . (&#39;onehotencoder&#39;, OneHotEncoder(), [&#39;sex&#39;]) . Notice the tuple sequence. . First is the name | Second is the transformer | Third are the column names | . We can also call get_feature_names_out method on a separate transformer from the list. . # output features from second tranformer pipe.transformers_[1][1].get_feature_names_out() . array([&#39;sex_female&#39;, &#39;sex_male&#39;], dtype=object) . # output features from last tranformer pipe.transformers_[-1][1].get_feature_names_out() # No. We cannot do this on last transformer (remainder). . AttributeError Traceback (most recent call last) Input In [22], in &lt;module&gt; 1 #collapse-output 2 # output features from last tranformer -&gt; 3 pipe.transformers_[-1][1].get_feature_names_out() AttributeError: &#39;str&#39; object has no attribute &#39;get_feature_names_out&#39; . We now have output feature names, and the output (nd-array). Can we convert them to a DataFrame? . import pandas as pd temp = pipe.fit_transform(X_train, y_train) col_names = pipe.get_feature_names_out() output = pd.DataFrame(temp.T, col_names).T output.head() . age fare sex_female sex_male . 0 NaN | -0.499399 | 1.0 | 0.0 | . 1 -0.390431 | -0.090935 | 1.0 | 0.0 | . 2 -1.296092 | -0.018241 | 1.0 | 0.0 | . 3 -0.320765 | -0.510137 | 0.0 | 1.0 | . 4 -0.947761 | -0.501444 | 1.0 | 0.0 | . FunctionTransformer . We know how to convert the transformer output to a DataFrame. It would be much simpler if we don&#39;t have to do an extra step, and can directly get a Dataframe from our fitted ColumnTransformer. . For this we can take the help of FunctionTransformer . A FunctionTransformer forwards its X (and optionally y) arguments to a user-defined function or function object and returns the result of this function. . Let&#39;s see a FunctionTransformer in action. . from sklearn.preprocessing import FunctionTransformer preprocessor = make_column_transformer( (StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (OneHotEncoder(), [&#39;sex&#39;] ), verbose_feature_names_out=False ) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;,FunctionTransformer(lambda x: pd.DataFrame(x, columns = preprocessor.get_feature_names_out()))) ]) set_config(display=&quot;diagram&quot;) pipe . Pipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;dataframer&amp;#x27;, FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x0000016196A1B040&gt;))]) . Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;dataframer&amp;#x27;, FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x0000016196A1B040&gt;))]) . preprocess: ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False) . standardscaler[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder() . FunctionTransformerFunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x0000016196A1B040&gt;) . Notice that we have applied FunctionTransformer after ColumnTransformer in a Pipeline. When we fit our pipeline on the dataset, ColumnTransformer will be fitted first and then the FunctionTransformer. Since the ColumnTransformer has been fitted first, we will be able to call get_feature_names_out on it while passing data to FunctionTransformer. . # let&#39;s run our pipeline again temp = pipe.fit_transform(X_train, y_train) temp.head() . age fare sex_female sex_male . 0 NaN | -0.499399 | 1.0 | 0.0 | . 1 -0.390431 | -0.090935 | 1.0 | 0.0 | . 2 -1.296092 | -0.018241 | 1.0 | 0.0 | . 3 -0.320765 | -0.510137 | 0.0 | 1.0 | . 4 -0.947761 | -0.501444 | 1.0 | 0.0 | . This is looking good. We are now getting back a dataframe directly from the pipeline. With a dataframe it is a lot easier to view and verify the output from the preprocessor. . But we have to be very careful with FunctionTransformer. In Sklearn docs, it says . Note: If a lambda is used as the function, then the resulting transformer will not be pickleable. . Huh! that is a very concerning point. We have also used a lambda function, and we will not be able to pickle it. Let&#39;s check it first. . import pickle # save our pipeline s1 = pickle.dumps(pipe) # reload it s2 = pickle.loads(s1) s2 . PicklingError Traceback (most recent call last) Input In [26], in &lt;module&gt; 1 import pickle 3 # save our pipeline -&gt; 4 s1 = pickle.dumps(pipe) 6 # reload it 7 s2 = pickle.loads(s1) PicklingError: Can&#39;t pickle &lt;function &lt;lambda&gt; at 0x0000016196A1B040&gt;: attribute lookup &lt;lambda&gt; on __main__ failed . The documentation was right about it. We have used a Lambda function in our FunctionTranformer and we got a pickle error. Since, the limitation is said for Lambda function, changing it with a normal function should work. Let&#39;s do that. . def get_dataframe(X, transformer): &quot;&quot;&quot; x: an nd-array transformer: fitted transformer &quot;&quot;&quot; col_names = transformer.get_feature_names_out() output = pd.DataFrame(X.T, col_names).T return output preprocessor = make_column_transformer( (StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (OneHotEncoder(), [&#39;sex&#39;] ), verbose_feature_names_out=False ) dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer) ]) set_config(display=&quot;diagram&quot;) pipe . Pipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;dataframer&amp;#x27;, FunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)}))]) . Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;dataframer&amp;#x27;, FunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)}))]) . preprocess: ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False) . standardscaler[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder() . FunctionTransformerFunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)}) . Notice the arguments for FunctionTransformer in the above code. . first argument is the function to be called | second argument are the parameters to be passed to our function | . The sequence of arguments for the callable function will be . first argument will be the output from any previous step in the pipeline (if there is any). In our case, it is nd-array coming from ColumnTransformer. It will be mapped to X. We don&#39;t have to do anything about it. | second argument (if any) we want to pass to function. In our case we need it to be the fitted transformer from the previous step so we have explicitly passed it using kw_args as key-value pair. Where key name is the same as callable method argument name (&#39;transformer&#39; in our case). | . Now let&#39;s do our pickle test one more time. . # save our pipeline s1 = pickle.dumps(pipe) # reload it s2 = pickle.loads(s1) s2 . Pipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;dataframer&amp;#x27;, FunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)}))]) . Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;dataframer&amp;#x27;, FunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)}))]) . preprocess: ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False) . standardscaler[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder() . FunctionTransformerFunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)}) . Alright, no more issues so let&#39;s proceed to our next question. . Where are the rest of the columns? . By default, only the specified columns in transformers are transformed and combined in the output, and the non-specified columns are dropped. (default of remainder=&#39;drop&#39;). By specifying remainder=&#39;passthrough&#39;, all remaining columns that were not specified in transformers will be automatically passed through. This subset of columns is concatenated with the output of the transformers. . Let&#39;s see it in action. . preprocessor = make_column_transformer( (StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (OneHotEncoder(), [&#39;sex&#39;] ), verbose_feature_names_out=False, remainder=&#39;passthrough&#39; ) # get_dataframe is already defined in last section. Intentionally omitted here. dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer) ]) temp = pipe.fit_transform(X_train, y_train) temp.head() . age fare sex_female sex_male pclass name sibsp parch ticket embarked . 0 NaN | -0.499399 | 1.0 | 0.0 | 3.0 | McCarthy, Miss. Catherine &#39;Katie&#39; | 0.0 | 0.0 | 383123 | Q | . 1 -0.390431 | -0.090935 | 1.0 | 0.0 | 2.0 | del Carlo, Mrs. Sebastiano (Argenia Genovesi) | 1.0 | 0.0 | SC/PARIS 2167 | C | . 2 -1.296092 | -0.018241 | 1.0 | 0.0 | 3.0 | Andersson, Miss. Sigrid Elisabeth | 4.0 | 2.0 | 347082 | S | . 3 -0.320765 | -0.510137 | 0.0 | 1.0 | 3.0 | Saad, Mr. Khalil | 0.0 | 0.0 | 2672 | C | . 4 -0.947761 | -0.501444 | 1.0 | 0.0 | 3.0 | Abelseth, Miss. Karen Marie | 0.0 | 0.0 | 348125 | S | . We have our remaining features back now, so let&#39;s proceed to our next question. . What happens if I change the sequence in ColumnTranformer? . It is better to make some changes and then see the results. I am making two changes in ColumnTransformer . Changed the order of transformers (OHE before scaling) | Changed the order of features inside the transformer (&#39;fare&#39; before &#39;age&#39;) | preprocessor = make_column_transformer( (OneHotEncoder(), [&#39;sex&#39;] ), (StandardScaler(), [&#39;fare&#39;, &#39;age&#39;]), verbose_feature_names_out=False, remainder=&#39;passthrough&#39; ) # get_dataframe is already defined in last section. Intentionally omitted here. dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer) ]) temp = pipe.fit_transform(X_train, y_train) temp.head() . sex_female sex_male fare age pclass name sibsp parch ticket embarked . 0 1.0 | 0.0 | -0.499399 | NaN | 3.0 | McCarthy, Miss. Catherine &#39;Katie&#39; | 0.0 | 0.0 | 383123 | Q | . 1 1.0 | 0.0 | -0.090935 | -0.390431 | 2.0 | del Carlo, Mrs. Sebastiano (Argenia Genovesi) | 1.0 | 0.0 | SC/PARIS 2167 | C | . 2 1.0 | 0.0 | -0.018241 | -1.296092 | 3.0 | Andersson, Miss. Sigrid Elisabeth | 4.0 | 2.0 | 347082 | S | . 3 0.0 | 1.0 | -0.510137 | -0.320765 | 3.0 | Saad, Mr. Khalil | 0.0 | 0.0 | 2672 | C | . 4 1.0 | 0.0 | -0.501444 | -0.947761 | 3.0 | Abelseth, Miss. Karen Marie | 0.0 | 0.0 | 348125 | S | . We can see that changing the sequence in ColumnTransformer does change the output. Also note . Specified columns in transformers are transformed and combined in the output | Transformers sequence in ColumnTransformer also represents the columns sequence in the output | When remainder=passthrough is used then remaining columns will be appended at the end. Remainder columns sequence will be same as in the input. | . Pipeline inside ColumnTransformer . Let&#39;s assume we have more requirements this time. I want . for numerical features (age, fare): impute the missing values first, and then scale them | for categorical features (sex): one hot encode them | . Our pipeline will look like this. . numeric_features = [&quot;age&quot;, &quot;fare&quot;] numeric_transformer = Pipeline( steps=[ (&quot;imputer&quot;, SimpleImputer(strategy=&quot;median&quot;)), (&quot;scaler&quot;, StandardScaler()) ]) categorical_features = [&quot;sex&quot;] categorical_transformer = OneHotEncoder(handle_unknown=&quot;ignore&quot;) preprocessor = make_column_transformer( (numeric_transformer, numeric_features), # note &quot;numeric_transformer&quot; is a pipeline this time (categorical_transformer, categorical_features), remainder=&#39;passthrough&#39;, verbose_feature_names_out=False ) dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer) ]) temp = pipe.fit_transform(X_train, y_train) temp.head() . AttributeError Traceback (most recent call last) Input In [31], in &lt;module&gt; 18 dataframer = FunctionTransformer(func=get_dataframe, kw_args={&#34;transformer&#34;: preprocessor}) 19 pipe = Pipeline([ 20 (&#34;preprocess&#34;, preprocessor), 21 (&#34;dataframer&#34;, dataframer) 22 ]) &gt; 24 temp = pipe.fit_transform(X_train, y_train) 25 temp.head() File ~ anaconda3 envs sc_mlflow lib site-packages sklearn pipeline.py:434, in Pipeline.fit_transform(self, X, y, **fit_params) 432 fit_params_last_step = fit_params_steps[self.steps[-1][0]] 433 if hasattr(last_step, &#34;fit_transform&#34;): --&gt; 434 return last_step.fit_transform(Xt, y, **fit_params_last_step) 435 else: 436 return last_step.fit(Xt, y, **fit_params_last_step).transform(Xt) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn base.py:855, in TransformerMixin.fit_transform(self, X, y, **fit_params) 852 return self.fit(X, **fit_params).transform(X) 853 else: 854 # fit method of arity 2 (supervised transformation) --&gt; 855 return self.fit(X, y, **fit_params).transform(X) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn preprocessing _function_transformer.py:182, in FunctionTransformer.transform(self, X) 169 &#34;&#34;&#34;Transform X using the forward function. 170 171 Parameters (...) 179 Transformed input. 180 &#34;&#34;&#34; 181 X = self._check_input(X, reset=False) --&gt; 182 return self._transform(X, func=self.func, kw_args=self.kw_args) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn preprocessing _function_transformer.py:205, in FunctionTransformer._transform(self, X, func, kw_args) 202 if func is None: 203 func = _identity --&gt; 205 return func(X, **(kw_args if kw_args else {})) Input In [27], in get_dataframe(X, transformer) 1 def get_dataframe(X, transformer): 2 &#34;&#34;&#34; 3 x: an nd-array 4 transformer: fitted transformer 5 &#34;&#34;&#34; -&gt; 6 col_names = transformer.get_feature_names_out() 7 output = pd.DataFrame(X.T, col_names).T 8 return output File ~ anaconda3 envs sc_mlflow lib site-packages sklearn compose _column_transformer.py:481, in ColumnTransformer.get_feature_names_out(self, input_features) 479 transformer_with_feature_names_out = [] 480 for name, trans, column, _ in self._iter(fitted=True): --&gt; 481 feature_names_out = self._get_feature_name_out_for_transformer( 482 name, trans, column, input_features 483 ) 484 if feature_names_out is None: 485 continue File ~ anaconda3 envs sc_mlflow lib site-packages sklearn compose _column_transformer.py:454, in ColumnTransformer._get_feature_name_out_for_transformer(self, name, trans, column, feature_names_in) 450 if isinstance(column, Iterable) and not all( 451 isinstance(col, str) for col in column 452 ): 453 column = _safe_indexing(feature_names_in, column) --&gt; 454 return trans.get_feature_names_out(column) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn pipeline.py:751, in Pipeline.get_feature_names_out(self, input_features) 749 for _, name, transform in self._iter(): 750 if not hasattr(transform, &#34;get_feature_names_out&#34;): --&gt; 751 raise AttributeError( 752 &#34;Estimator {} does not provide get_feature_names_out. &#34; 753 &#34;Did you mean to call pipeline[:-1].get_feature_names_out&#34; 754 &#34;()?&#34;.format(name) 755 ) 756 feature_names_out = transform.get_feature_names_out(feature_names_out) 757 return feature_names_out AttributeError: Estimator imputer does not provide get_feature_names_out. Did you mean to call pipeline[:-1].get_feature_names_out()? . . Oh geez! What went wrong this time. The error message says . AttributeError: Estimator imputer does not provide get_feature_names_out. Did you mean to call pipeline[:-1].get_feature_names_out()? . From the error message I am getting that . Estimator imputer does not provide get_feature_names_out . Hmmm, this is strange. Why is this estimator missing a very useful function? Let&#39;s check the docs first on SimpleImputer. For the docs I indeed could not find this method get_feature_names_out() for this transformer. A little googling lead me to this Sklearn Github issue page Implement get_feature_names_out for all estimators. Developers are actively adding get_feature_names_out() to all estimators and transformers, and it looks like this feature has not been implemented for SimpleImputer till Sklearn version==1.0.2. But no worries we can overcome this limitation, and implement this feature ourselves through a custom transformer. . Custom Transformer . We can create a custom transformer or an estimator simply by inheriting a class from BaseEstimator and optionally the mixin classes in sklearn.base. Sklean provides a template that we can use to create our custom transformer. Template link is here: https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/_template.py#L146 . Let us use the same pipeline as in last cell but replace SimpleImputer with a custom one. . from sklearn.base import BaseEstimator, TransformerMixin class SimpleImputerCustom(BaseEstimator, TransformerMixin): def __init__(self, strategy=&#39;mean&#39;): self.strategy = strategy self.imputer = SimpleImputer(strategy=self.strategy) def fit(self, X, y): self.imputer.fit(X, y) return self def transform(self, X): return self.imputer.transform(X) def get_feature_names_out(self, input_features=None): # we have returned the input features name as out features will have the same name return input_features numeric_features = [&quot;age&quot;, &quot;fare&quot;] numeric_transformer = Pipeline( steps=[ (&quot;imputer&quot;, SimpleImputerCustom(strategy=&#39;mean&#39;)), (&quot;scaler&quot;, StandardScaler()) ] ) categorical_features = [&quot;sex&quot;] categorical_transformer = OneHotEncoder(handle_unknown=&quot;ignore&quot;) preprocessor = make_column_transformer( (numeric_transformer, numeric_features), # note &quot;numeric_transformer&quot; is a pipeline (categorical_transformer, categorical_features), remainder=&#39;passthrough&#39;, verbose_feature_names_out=False ) dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer) ]) temp = pipe.fit_transform(X_train, y_train) temp.head() . age fare sex_female sex_male pclass name sibsp parch ticket embarked . 0 0.0 | -0.499638 | 1.0 | 0.0 | 3.0 | McCarthy, Miss. Catherine &#39;Katie&#39; | 0.0 | 0.0 | 383123 | Q | . 1 -0.436411 | -0.090979 | 1.0 | 0.0 | 2.0 | del Carlo, Mrs. Sebastiano (Argenia Genovesi) | 1.0 | 0.0 | SC/PARIS 2167 | C | . 2 -1.448729 | -0.01825 | 1.0 | 0.0 | 3.0 | Andersson, Miss. Sigrid Elisabeth | 4.0 | 2.0 | 347082 | S | . 3 -0.358541 | -0.510381 | 0.0 | 1.0 | 3.0 | Saad, Mr. Khalil | 0.0 | 0.0 | 2672 | C | . 4 -1.059376 | -0.501684 | 1.0 | 0.0 | 3.0 | Abelseth, Miss. Karen Marie | 0.0 | 0.0 | 348125 | S | . Feature Engineering with Custom Transformer . So far, so good! Let&#39;s assume that we have another requirement and it is about feature engineering. We have to combine &#39;sibsp&#39; and &#39;parch&#39; into two new features: family_size and is_alone. . Let&#39;s implement this now. . class FamilyFeatureTransformer(BaseEstimator, TransformerMixin): def __init__(self): pass def fit(self, X, y): return self def transform(self, X): X[&#39;family_size&#39;] = X[&#39;parch&#39;] + X[&#39;sibsp&#39;] X.drop([&#39;parch&#39;, &#39;sibsp&#39;], axis=1, inplace=True) # we can drop this feature now X[&#39;is_alone&#39;] = 1 X.loc[X[&#39;family_size&#39;] &gt; 1, &#39;is_alone&#39;] = 0 return X def get_feature_names_out(self, input_features=None): # this time we have created new features. Their names are different from input features. # so we have explicitly mentioned them here. return [&#39;family_size&#39;, &#39;is_alone&#39;] numeric_features = [&quot;age&quot;, &quot;fare&quot;] numeric_transformer = Pipeline( steps=[ (&quot;imputer&quot;, SimpleImputerCustom(strategy=&#39;mean&#39;)), (&quot;scaler&quot;, StandardScaler()) ] ) categorical_features = [&quot;sex&quot;] categorical_transformer = OneHotEncoder(handle_unknown=&quot;ignore&quot;) family_features = [&quot;parch&quot;, &quot;sibsp&quot;] family_transformer = FamilyFeatureTransformer() preprocessor = make_column_transformer( (numeric_transformer, numeric_features), (categorical_transformer, categorical_features), (family_transformer, family_features), remainder=&#39;drop&#39;, # let&#39;s drop extra features this time verbose_feature_names_out=False ) dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer) ]) temp = pipe.fit_transform(X_train, y_train) temp.head() . age fare sex_female sex_male family_size is_alone . 0 0.000000 | -0.499638 | 1.0 | 0.0 | 0.0 | 1.0 | . 1 -0.436411 | -0.090979 | 1.0 | 0.0 | 1.0 | 1.0 | . 2 -1.448729 | -0.018250 | 1.0 | 0.0 | 6.0 | 0.0 | . 3 -0.358541 | -0.510381 | 0.0 | 1.0 | 0.0 | 1.0 | . 4 -1.059376 | -0.501684 | 1.0 | 0.0 | 0.0 | 1.0 | . Sklean Pipeline with Feature Importance . Alright, we have our required features ready and we can now pass them to a classifier. Let&#39;s use RandomForrest as our classifier and run our pipeline with it. . from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score # &#39;preprocessor&#39; and &#39;dataframer&#39; are already declared in last section, and intentionally omitted here. pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer), (&#39;rf_estimator&#39;, RandomForestClassifier()) ]) temp = pipe.fit_transform(X_train, y_train) temp.head() . AttributeError Traceback (most recent call last) Input In [34], in &lt;module&gt; 4 # &#39;preprocessor&#39; and &#39;dataframer&#39; are already declared in last section, and intentionally omitted here. 5 pipe = Pipeline([ 6 (&#34;preprocess&#34;, preprocessor), 7 (&#34;dataframer&#34;, dataframer), 8 (&#39;rf_estimator&#39;, RandomForestClassifier()) 9 10 ]) &gt; 12 temp = pipe.fit_transform(X_train, y_train) 13 temp.head() File ~ anaconda3 envs sc_mlflow lib site-packages sklearn pipeline.py:436, in Pipeline.fit_transform(self, X, y, **fit_params) 434 return last_step.fit_transform(Xt, y, **fit_params_last_step) 435 else: --&gt; 436 return last_step.fit(Xt, y, **fit_params_last_step).transform(Xt) AttributeError: &#39;RandomForestClassifier&#39; object has no attribute &#39;transform&#39; . Okay, looks like we have made a mistake here. Error message is saying . AttributeError: &#39;RandomForestClassifier&#39; object has no attribute &#39;transform&#39; . I get that. In our pipeline we have an estimator that does not have a transform method defined for it. We should use predict method instead. . Note: . Estimators implement predict method (Template reference Estimator, Template reference Classifier) | Transformers implement transform method (Template reference Transformer) | fit_transform is same calling fit and then transform | . Let us fix the error and run our pipeline again. . # pipeline created in last section and intentionally omitted here. pipe.fit(X_train, y_train) y_pred = pipe.predict(X_test) accuracy_score(y_test, y_pred) . 0.7862595419847328 . Let&#39;s see how our final pipeline looks visually. . # set_config(display=&#39;text&#39;) pipe . Pipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;data... kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)})), (&amp;#x27;rf_estimator&amp;#x27;, RandomForestClassifier())]) . Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;data... kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)})), (&amp;#x27;rf_estimator&amp;#x27;, RandomForestClassifier())]) . preprocess: ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False) . pipeline[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . SimpleImputerCustomSimpleImputerCustom() . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;) . familyfeaturetransformer[&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;] . FamilyFeatureTransformerFamilyFeatureTransformer() . FunctionTransformerFunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)}) . RandomForestClassifierRandomForestClassifier() . We can also get the importance of features in our dataset from RandomForrest classifier. . import matplotlib.pyplot as plt clf = pipe[-1] # last estimator is the RF classifier importances = clf.feature_importances_ features = clf.feature_names_in_ indices = np.argsort(importances) plt.title(&#39;Feature Importances&#39;) plt.barh(range(len(indices)), importances[indices], color=&#39;b&#39;, align=&#39;center&#39;) plt.yticks(range(len(indices)), [features[i] for i in indices]) plt.xlabel(&#39;Relative Importance&#39;) plt.show() . Note that all the feature names were passed to the RF Classifier and that is why we were able to get them back using its attribute feature_names_in_. This can be super useful when you have many model deployed in the environment, and you can just use the model object to get information about the features it was trained on. . For a moment let&#39;s also remove the feature names from our pipeline and see how it will effect our feature importance plot. . pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), # (&quot;dataframer&quot;, dataframer), (&#39;rf_estimator&#39;, RandomForestClassifier()) ]) # fit the pipeline pipe.fit(X_train, y_train) # get the feature importance plot clf = pipe[-1] importances = clf.feature_importances_ features = clf.feature_names_in_ indices = np.argsort(importances) plt.title(&#39;Feature Importances&#39;) plt.barh(range(len(indices)), importances[indices], color=&#39;b&#39;, align=&#39;center&#39;) plt.yticks(range(len(indices)), [features[i] for i in indices]) plt.xlabel(&#39;Relative Importance&#39;) plt.show() . AttributeError Traceback (most recent call last) Input In [38], in &lt;module&gt; 12 clf = pipe[-1] 13 importances = clf.feature_importances_ &gt; 14 features = clf.feature_names_in_ 16 indices = np.argsort(importances) 18 plt.title(&#39;Feature Importances&#39;) AttributeError: &#39;RandomForestClassifier&#39; object has no attribute &#39;feature_names_in_&#39; . No feature names were passed to our classifier this time and it is missing feature_names_in_ attribute. We can circumvent this and still get feature importance plot. . pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), # (&quot;dataframer&quot;, dataframer), (&#39;rf_estimator&#39;, RandomForestClassifier()) ]) # fit the pipeline pipe.fit(X_train, y_train) # get the feature importance plot clf = pipe[-1] importances = clf.feature_importances_ # features = clf.feature_names_in_ indices = np.argsort(importances) plt.title(&#39;Feature Importances&#39;) plt.barh(range(len(indices)), importances[indices], color=&#39;b&#39;, align=&#39;center&#39;) plt.yticks(range(len(indices)), [i for i in indices]) plt.xlabel(&#39;Relative Importance&#39;) plt.show() . This time we get the same plot but not withOUT feature names, and it is not useful anymore. So definitely we need to keep the feature names with the final estimator. Feature names can help us a lot in interpreting the model. . The complete Pipeline . For an easy reference, let&#39;s put the whole pipeline in one place. . Load Data . from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split import pandas as pd import numpy as np np.random.seed(42) # for consistency X, y = fetch_openml(&quot;titanic&quot;, version=1, as_frame=True, return_X_y=True) X.drop([&#39;body&#39;, &#39;cabin&#39;, &#39;boat&#39;, &#39;home.dest&#39;], axis=1, inplace=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2) X_train.head() . pclass name sex age sibsp parch ticket fare embarked . 999 3.0 | McCarthy, Miss. Catherine &#39;Katie&#39; | female | NaN | 0.0 | 0.0 | 383123 | 7.7500 | Q | . 392 2.0 | del Carlo, Mrs. Sebastiano (Argenia Genovesi) | female | 24.0 | 1.0 | 0.0 | SC/PARIS 2167 | 27.7208 | C | . 628 3.0 | Andersson, Miss. Sigrid Elisabeth | female | 11.0 | 4.0 | 2.0 | 347082 | 31.2750 | S | . 1165 3.0 | Saad, Mr. Khalil | male | 25.0 | 0.0 | 0.0 | 2672 | 7.2250 | C | . 604 3.0 | Abelseth, Miss. Karen Marie | female | 16.0 | 0.0 | 0.0 | 348125 | 7.6500 | S | . Train Model . from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler from sklearn.base import BaseEstimator, TransformerMixin from sklearn.compose import make_column_transformer from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score from sklearn.impute import SimpleImputer class FamilyFeatureTransformer(BaseEstimator, TransformerMixin): def __init__(self): pass def fit(self, X, y): return self def transform(self, X): X[&#39;family_size&#39;] = X[&#39;parch&#39;] + X[&#39;sibsp&#39;] X.drop([&#39;parch&#39;, &#39;sibsp&#39;], axis=1, inplace=True) # we can drop this feature now X[&#39;is_alone&#39;] = 1 X.loc[X[&#39;family_size&#39;] &gt; 1, &#39;is_alone&#39;] = 0 return X def get_feature_names_out(self, input_features=None): # this time we have created new features. Their names are different from input features. # so we have explicitly mentioned them here. return [&#39;family_size&#39;, &#39;is_alone&#39;] class SimpleImputerCustom(BaseEstimator, TransformerMixin): def __init__(self, strategy=&#39;mean&#39;): self.strategy = strategy self.imputer = SimpleImputer(strategy=self.strategy) def fit(self, X, y): self.imputer.fit(X, y) return self def transform(self, X): return self.imputer.transform(X) def get_feature_names_out(self, input_features=None): # we have returned the input features name as out features will have the same name return input_features def get_dataframe(X, transformer): &quot;&quot;&quot; x: an nd-array transformer: fitted transformer &quot;&quot;&quot; col_names = transformer.get_feature_names_out() output = pd.DataFrame(X.T, col_names).T return output numeric_features = [&quot;age&quot;, &quot;fare&quot;] numeric_transformer = Pipeline( steps=[ (&quot;imputer&quot;, SimpleImputerCustom(strategy=&#39;mean&#39;)), (&quot;scaler&quot;, StandardScaler()) ] ) categorical_features = [&quot;sex&quot;] categorical_transformer = OneHotEncoder(handle_unknown=&quot;ignore&quot;) family_features = [&quot;parch&quot;, &quot;sibsp&quot;] family_transformer = FamilyFeatureTransformer() preprocessor = make_column_transformer( (numeric_transformer, numeric_features), (categorical_transformer, categorical_features), (family_transformer, family_features), remainder=&#39;drop&#39;, # let&#39;s drop extra features this time verbose_feature_names_out=False ) dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer), (&#39;rf_estimator&#39;, RandomForestClassifier()) ]) pipe.fit(X_train, y_train) y_pred = pipe.predict(X_test) accuracy_score(y_test, y_pred) . 0.8015267175572519 . Plot Feature Importance . import matplotlib.pyplot as plt clf = pipe[-1] # last estimator is the RF classifier importances = clf.feature_importances_ features = clf.feature_names_in_ indices = np.argsort(importances) plt.title(&#39;Feature Importances&#39;) plt.barh(range(len(indices)), importances[indices], color=&#39;b&#39;, align=&#39;center&#39;) plt.yticks(range(len(indices)), [features[i] for i in indices]) plt.xlabel(&#39;Relative Importance&#39;) plt.show() . Pickle Test . from sklearn import set_config set_config(display=&quot;diagram&quot;) import pickle # save our pipeline s1 = pickle.dumps(pipe) # reload it s2 = pickle.loads(s1) s2 . Pipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;data... kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)})), (&amp;#x27;rf_estimator&amp;#x27;, RandomForestClassifier())]) . Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;data... kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)})), (&amp;#x27;rf_estimator&amp;#x27;, RandomForestClassifier())]) . preprocess: ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False) . pipeline[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . SimpleImputerCustomSimpleImputerCustom() . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;) . familyfeaturetransformer[&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;] . FamilyFeatureTransformerFamilyFeatureTransformer() . FunctionTransformerFunctionTransformer(func=&lt;function get_dataframe at 0x00000161997DF3A0&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)}) . RandomForestClassifierRandomForestClassifier() .",
            "url": "https://hassaanbinaslam.github.io/myblog/ml/sklearn/2022/03/04/sklearn-pipeline-deep-dive.html",
            "relUrl": "/ml/sklearn/2022/03/04/sklearn-pipeline-deep-dive.html",
            "date": " • Mar 4, 2022"
        }
        
    
  
    
        ,"post21": {
            "title": "AWS CloudFormation Template, Functions, and Commands",
            "content": ". About . This post is a collection of useful notes on various sections of AWS CloudFormation template, and intrinsic functions. Knowledge about them is often tested in AWS certifications. For more details on this subject refer to its user guide (https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html) . Template Anatomy . The following is an example of AWS CloudFormation template and its sections in YAML format. There is no sequence to writing these sections besides that if there is a Description section then it must be put after AWSTemplateFormatVersion. . AWSTemplateFormatVersion: &quot;version date&quot; Description: String Metadata: template metadata Parameters: set of parameters Rules: set of rules Mappings: set of mappings Conditions: set of conditions Transform: set of transforms Resources: set of resources Outputs: set of outputs . Template Sections . AWSTemplateFormatVersion (optional) . The AWS CloudFormation template version that the template conforms to. . Syntax . AWSTemplateFormatVersion: &quot;2010-09-09&quot; . Description (optional) . A text string that describes the template. This section must always follow the template format version section. . Syntax . Description: &gt; Here are some details about the template. . Metadata (optional) . Objects that provide additional information about the template. . Difference between Metadata and Description is that some cloudformation features can refer to the objects that are defined in Metadata section. For example, you can use a metadata key AWS::CloudFormation::Interface to define how parameters are grouped and sorted on AWS cloudformation console. By default, cloudformation console alphbetically sorts the parameters by their logical ID. | AWS strongly recommends not to use this section for storing sensitive information such as passwords or secrets. | . Syntax . Metadata: Instances: Description: &quot;Information about the instances&quot; Databases: Description: &quot;Information about the databases&quot; . Parameters (optional) . Parameters enable you to input custom values to your template each time you create or update a stack. You can refer to parameters from the Resources and Outputs sections of the template using Ref intrinsic function. . CloudFormation currently supports the following parameter types . String – A literal string | Number – An integer or float | List&lt;Number&gt; – An array of integers or floats | CommaDelimitedList – An array of literal strings that are separated by commas | AWS::EC2::KeyPair::KeyName – An Amazon EC2 key pair name | AWS::EC2::SecurityGroup::Id – A security group ID | AWS::EC2::Subnet::Id – A subnet ID | AWS::EC2::VPC::Id – A VPC ID | List&lt;AWS::EC2::VPC::Id&gt; – An array of VPC IDs | List&lt;AWS::EC2::SecurityGroup::Id&gt; – An array of security group IDs | List&lt;AWS::EC2::Subnet::Id&gt; – An array of subnet IDs | . Syntax . The following example declares a parameter named InstanceTypeParameter. This parameter lets you specify the Amazon EC2 instance type for the stack to use when you create or update the stack. . Note that InstanceTypeParameter has a default value of t2.micro. This is the value that AWS CloudFormation will use to provision the stack unless another value is provided. . Parameters: InstanceTypeParameter: Type: String Default: t2.micro AllowedValues: - t2.micro - m1.small - m1.large Description: Enter t2.micro, m1.small, or m1.large. Default is t2.micro. . Referencing a parameter in template (Ref function) . In the following example, the InstanceType property of the EC2 instance resource references the InstanceTypeParameter parameter value. . Ec2Instance: Type: AWS::EC2::Instance Properties: InstanceType: Ref: InstanceTypeParameter ImageId: ami-0ff8a91507f77f867 . Rules (optional) . Validates a parameter or a combination of parameters that are passed to a template during a stack creation or stack update. . You can use the following rule-specific intrinsic functions to define rule conditions and assertions: . Fn::And | Fn::Contains | Fn::EachMemberEquals | Fn::EachMemberIn | Fn::Equals | Fn::If | Fn::Not | Fn::Or | Fn::RefAll | Fn::ValueOf | Fn::ValueOfAll | . Syntax . In the following example, the rule checks the value of the InstanceType parameter. The user must specify a1.medium, if the value of the environment parameter is test. . Rules: testInstanceType: RuleCondition: !Equals - !Ref Environment - test Assertions: - Assert: &#39;Fn::Contains&#39;: - - a1.medium - !Ref InstanceType AssertDescription: &#39;For a test environment, the instance type must be a1.medium&#39; . Mappings (optional) . The optional Mappings section matches a key to a corresponding set of named values similar to a lookup table. For example, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region. You use the Fn::FindInMap intrinsic function in the Resources and Outputs to retrieve values in a map. Note that you can&#39;t include parameters, pseudo parameters, or intrinsic functions in the Mappings section. . Fn::FindInMap . The intrinsic function Fn::FindInMap returns the value corresponding to keys in a two-level map that&#39;s declared in the Mappings section. . Syntax for the short form: . !FindInMap [ MapName, TopLevelKey, SecondLevelKey ] . Parameters . MapName The logical name of a mapping declared in the Mappings section that contains the keys and values. | . | TopLevelKey The top-level key name. Its value is a list of key-value pairs. | . | SecondLevelKey The second-level key name, which is set to one of the keys from the list assigned to TopLevelKey. | . | . A more concrete example . Mappings: RegionMap: us-east-1: HVM64: &quot;ami-0ff8a91507f77f867&quot; HVMG2: &quot;ami-0a584ac55a7631c0c&quot; Resources: myEC2Instance: Type: &quot;AWS::EC2::Instance&quot; Properties: ImageId: !FindInMap - RegionMap - !Ref &#39;AWS::Region&#39; # us-east-1 - HVM64 InstanceType: m1.small . Conditions (optional) . Conditions that control whether certain resources are created or whether certain resource properties are assigned a value during stack creation or update. For example, you could conditionally create a resource that depends on whether the stack is for a production or test environment. . Conditions are defined in Conditions section, and are then applied in following sections. . Parameters | Resources | Outputs | . You can use following intrinsic functions to define your conditions . Fn::And | Fn::Equals | Fn::If | Fn::Not | Fn::Or | . Syntax . Conditions: Logical ID: Intrinsic function . A more concrete example . AWSTemplateFormatVersion: 2010-09-09 Parameters: EnvType: Description: Environment type. Default: test Type: String AllowedValues: - prod - test ConstraintDescription: must specify prod or test. Conditions: CreateProdResources: !Equals - !Ref EnvType - prod Resources: EC2Instance: Type: &#39;AWS::EC2::Instance&#39; Properties: ImageId: ami-0ff8a91507f77f867 MountPoint: Type: &#39;AWS::EC2::VolumeAttachment&#39; Condition: CreateProdResources Properties: InstanceId: !Ref EC2Instance VolumeId: !Ref NewVolume Device: /dev/sdh NewVolume: Type: &#39;AWS::EC2::Volume&#39; Condition: CreateProdResources Properties: Size: 100 AvailabilityZone: !GetAtt - EC2Instance - AvailabilityZone . Difference between Rules and Conditions usage? . Rules are used to evaluate the input given by the user in Parameters | Conditions turn come after all rules have been evaluated | Conditions are not limited to Parameters and can also work with Resources and Outputs | . Transform (optional) . For serverless applications (also referred to as Lambda-based applications), specifies the version of the AWS Serverless Application Model (AWS SAM) to use. When you specify a transform, you can use AWS SAM syntax to declare resources in your template. The model defines the syntax that you can use and how it&#39;s processed. . You can also use AWS::Include transforms to work with template snippets that are stored separately from the main AWS CloudFormation template. You can store your snippet files in an Amazon S3 bucket and then reuse the functions across multiple templates. . Syntax . Transform: - MyMacro - &#39;AWS::Serverless&#39; . AWS::Include transform . Use the AWS::Include transform, which is a macro hosted by AWS CloudFormation, to insert boilerplate content into your templates. The AWS::Include transform lets you create a reference to a template snippet in an Amazon S3 bucket. The AWS::Include function behaves similarly to an include, copy, or import directive in programming languages. . Example . Transform: Name: &#39;AWS::Include&#39; Parameters: Location: &#39;s3://MyAmazonS3BucketName/MyFileName.yaml&#39; . Resources (required) . Specifies the stack resources and their properties, such as an Amazon Elastic Compute Cloud instance or an Amazon Simple Storage Service bucket. You can refer to resources in the Resources and Outputs sections of the template. . Syntax . Resources: Logical ID: Type: Resource type Properties: Set of properties . A more concrete example . Resources: MyEC2Instance: Type: &quot;AWS::EC2::Instance&quot; Properties: ImageId: &quot;ami-0ff8a91507f77f867&quot; . Outputs (optional) . The optional Outputs section declares output values that you can import into other stacks (to create cross-stack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name from a stack to make the bucket easier to find. . Notes . You can declare a maximum of 200 outputs in a template. | AWS strongly recommend you don&#39;t use this section to output sensitive information, such as passwords or secrets | Output values are available after the stack operation is complete. Stack output values aren&#39;t available when a stack status is in any of the IN_PROGRESS status. | AWS also does not recommend establishing dependencies between a service runtime and the stack output value because output values might not be available at all times. | . Syntax . Outputs: Logical ID: Description: Information about the value Value: Value to return Export: Name: Name of resource to export . A more concrete example where certain values are shown as output at the end of stack creation. . Outputs: BackupLoadBalancerDNSName: Description: The DNSName of the backup load balancer Value: !GetAtt BackupLoadBalancer.DNSName Condition: CreateProdResources InstanceID: Description: The Instance ID Value: !Ref EC2Instance . For Cross-Stack output use Export tag. Values outputed with &quot;Export&quot; tag can be imported in other stacks &quot;in the same region&quot;. Then, use the Fn::ImportValue intrinsic function to import the value in another stack &quot;in the same region&quot;. . Outputs: StackVPC: Description: The ID of the VPC Value: !Ref MyVPC Export: Name: !Sub &quot;${AWS::StackName}-VPCID&quot; . Some other important Intrinsic Functions . Fn::GetAtt . The Fn::GetAtt intrinsic function returns the value of an attribute from a resource in the template. . Syntax . !GetAtt logicalNameOfResource.attributeName . logicalNameOfResource The logical name (also called logical ID) of the resource that contains the attribute that you want. | . | attributeName The name of the resource-specific attribute whose value you want. See the resource&#39;s reference page for details about the attributes available for that resource type. | . | Return value The attribute value. | . | . A more concrete example . !GetAtt myELB.DNSName . Notes: . For the Fn::GetAtt logical resource name, you can&#39;t use functions. You must specify a string that&#39;s a resource&#39;s logical ID. | For the Fn::GetAtt attribute name, you can use the Ref function. | . Fn::ImportValue . The intrinsic function Fn::ImportValue returns the value of an output exported by another stack. You typically use this function to create cross-stack references. . Notes: . For each AWS account, Export names must be unique within a region. | You can&#39;t create cross-stack references across regions. You can use the intrinsic function Fn::ImportValue to import only values that have been exported within the same region. | You can&#39;t delete a stack if another stack references one of its outputs. | You can&#39;t modify or remove an output value that is referenced by another stack. | . Syntax . !ImportValue sharedValueToImport . A more concrete example. . Fn::ImportValue: !Sub &quot;${NetworkStackName}-SecurityGroupID&quot; . Fn::Sub . The intrinsic function Fn::Sub substitutes variables in an input string with values that you specify. In your templates, you can use this function to construct commands or outputs that include values that aren&#39;t available until you create or update a stack. . Syntax . !Sub - String - VarName: VarValue . Parameters . String A string with variables that AWS CloudFormation substitutes with their associated values at runtime. Write variables as ${MyVarName}. Variables can be template parameter names, resource logical IDs, resource attributes, or a variable in a key-value map. | . | VarName The name of a variable that you included in the String parameter. | . | VarValue The value that CloudFormation substitutes for the associated variable name at runtime. | . | . A more concrete example. The following example uses a mapping to substitute the ${Domain} variable with the resulting value from the Ref function. . Name: !Sub - &#39;www.${Domain}&#39; - Domain: !Ref RootDomainName . Important CloudFormation CLI Commands . Package a template using aws cloudformation package command | Validate a CloudFormation template using aws cloudformation validate-template command | Deploy a template using the aws cloudformation deploy command | .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/2022/02/28/aws-cloudformation-template.html",
            "relUrl": "/aws/2022/02/28/aws-cloudformation-template.html",
            "date": " • Feb 28, 2022"
        }
        
    
  
    
        ,"post22": {
            "title": "AWS IAM Policy Types",
            "content": ". About . Access is managed in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. . A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. . AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, Organizations SCPs, ACLs, and session policies. This post is summary of AWS IAM policy and permission types. . AWS Policy Types . Identity-based policies . Identity-based policies – Attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity. . Resource-based policies . Resource-based policies – Attach inline policies to resources. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies. Resource-based policies grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts. . Identity vs Resource based policy . . Identity-based policies are applied to IAM identities, and grant them access to AWS resources. | Resource-based policies are applied to AWS resources, and they grant access to Principals (IAM identities, and applications) | . Permissions boundaries . Permissions boundaries – Use a customer or AWS managed policy as the permissions boundary for an IAM entity (user or role). That policy defines the maximum permissions that the identity-based policies can grant to an entity, but does not grant permissions. Permissions boundaries do not define the maximum permissions that a resource-based policy can grant to an entity. . Organizations SCPs . Organizations SCPs – Use an AWS Organizations service control policy (SCP) to define the maximum permissions for account members of an organization or organizational unit (OU). SCPs limit permissions that identity-based policies or resource-based policies grant to entities (users or roles) within the account, but do not grant permissions. . Permission boundries vs Organization SCP . Both permission boundries and SCP only limit permissions. They don&#39;t give any permissions. | Permission boundries limits permissions of identity-based policies only. | SCP limits permissions on both identity and resource based policies. | . Access control lists (ACLs) . Access control lists (ACLs) – Use ACLs to control which principals in other accounts can access the resource to which the ACL is attached. ACLs are similar to resource-based policies, although they are the only policy type that does not use the JSON policy document structure. ACLs are cross-account permissions policies that grant permissions to the specified principal. ACLs cannot grant permissions to entities within the same account. . ACL vs Resource based policy . Resource based policies can grant permission to entities in same or different account | ACL can only grant permissions to entities in different account | Only a few resources support ACL including AWS Amazon S3, AWS WAF, and Amazon VPC. It is a legacy IAM policy type and AWS recommends not to use it. | . Session policies . Session policies – Pass advanced session policies when you use the AWS CLI or AWS API to assume a role or a federated user. Session policies limit the permissions that the role or user&#39;s identity-based policies grant to the session. Session policies limit permissions for a created session, but do not grant permissions. For more information, see Session Policies. .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/2022/02/23/aws-policy-types.html",
            "relUrl": "/aws/2022/02/23/aws-policy-types.html",
            "date": " • Feb 23, 2022"
        }
        
    
  
    
        ,"post23": {
            "title": "AWS DNS Records - A, CNAME, ALIAS, & MX",
            "content": ". About . This post summarizes the differences between AWS Route53 DNS records namely A record, CNAME, ALIAS, and MX. Knowledge about these differences is commonly checked in AWS certifications. . Credits . This post takes help from a few other really good articles. Please refer to them if you need more details on this subject . “Demystifying DNS Records – A, CNAME, ALIAS, MX &amp; AAAA” from Whizlabs (https://www.whizlabs.com/blog/dns-records/) . | “Why a domain’s root can’t be a CNAME — and other tidbits about the DNS” from freeCodeCamp (https://www.freecodecamp.org/news/why-cant-a-domain-s-root-be-a-cname-8cbab38e5f5c/) . | . First, some definitions . Domain Name . Domain + TLD = Domain Name | When you buy a ‘domain’ from a a registrar or reseller, you buy the rights to a specific domain name (example.com), and any subdomains you want to create (my-site.example.com, mail.example.com, etc). | The domain name (example.com) is also called the apex, root or naked domain name. | Examples of protocol are http, ftp, TCP, UDP, FTP, SMTP etc. | Examples of top level domains are .org, .net, .com, .ai etc. | . A Record . A record (or an address record) always points to an IP address. This IP address should be static like AWS Elastic IP Addresses (EIP) . Example use cases . You can point your root domain name example.com to an Elastic IP Address 192.0.2.23 . | We can also map EC2 instances IPv4 Public IP Address to an A record. But this is not recommended as EC2 instances public IP addresses change when you stop/start your server. We should always use Elastic IP addresses instead. . | . AAAA Record . AAAA record is similar to A record but for IPv6 addresses. . It always points to an IPv6 address . | Note that AWS currently does not support EIP for IPv6 (https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html) . | . MX Record . MX records (Mail Exchange records) are used for setting up email servers. . CNAME Record . CNAME records must always point to another domain name, never directly to an IP address. Since it does not point to an IP address, it is commonly used along with an A record. . One can, for example, point ftp.example.com and/or www.example.com to the DNS entry example.com, which in turn has an A record that points to the IP address. Then, if the IP address ever changes, one only has to record the change in one place within the network: in the DNS A record for example.com. . Example use cases . NAME TYPE VALUE . www.example.com | CNAME | example.com | . example.com | A | 192.0.2.23 | . An A record for example.com (root domain) points to server IP address . | A CNAME record points www.example.com to example.com . | . Now if the IP address of your server has changed you will have to update it only at one place A record. www.example.com and example.com will automatically inherit the changes. . IMPORTANT . CNAME entry for the root domain is not allowed. . | . NAME TYPE VALUE . example.com | CNAME | app.example.com | . app.example.com | A | 192.0.2.23 | . Alias Record . It is AWS Route 53 specific and only works with it. Alias works similar to CNAME but they are created by AWS to solve their specific problems discussed next. . AWS S3 buckets, Elastic Load Balancers, Elastic Beanstalk, and CloudFront offer you DNS names only and no IP addresses. e.g. when you create an S3 bucket you will get its DNS name bucket_name.s3.amazonaws.com. Now if you want to map your root domain example.com to S3 bucket DNS then we don’t have any options left as . A record points to IP addresses only . | CNAME cannot be used for root domain name . | . For this AWS came up with an Alias record in Route 53. With Alias record, you can point your domain root to another DNS name entry. . NAME TYPE VALUE . www.example.com | CNAME | example.com | . example.com | Alias | bucket_name.s3.amazonaws.com | .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/2022/02/22/aws-dns-records.html",
            "relUrl": "/aws/2022/02/22/aws-dns-records.html",
            "date": " • Feb 22, 2022"
        }
        
    
  
    
        ,"post24": {
            "title": "Python - A collection of output formatting tips",
            "content": ". About . This notebook is a collection of useful tips to format Python string literals and output. . Environment . from platform import python_version print(&quot;python==&quot; + python_version()) . . python==3.8.5 . f-string: Expressions inside a string . r = &#39;red&#39; g = &#39;green&#39; b = 1001 # f-string has a simple syntax. Put &#39;f&#39; at the start of string, and put expressions in {} f&quot;Stop = {r}, Go = {g}&quot; . &#39;Stop = red, Go = green&#39; . # &#39;F&#39; can also be used to start an f-string F&quot;binary = {b}. If you need value in brackets {{{b}}}&quot; . &#39;binary = 1001. If you need value in brackets {1001}&#39; . # f-string can also be started with &quot;&quot;&quot; quotes f&quot;&quot;&quot;{r} or {g}&quot;&quot;&quot; . &#39;red or green&#39; . # f-string on multiple lines. # 1. Use &quot;&quot;&quot; with backslash f&quot;&quot;&quot;{r} or {g}&quot;&quot;&quot; . &#39;red or green&#39; . # f-string on multiple lines. # 2. Use only backslash f&quot;{r}&quot; &quot; or &quot; f&quot;{g}&quot; . &#39;red or green&#39; . # f-string on multiple lines. # 3. Use brackets () (f&quot;{r}&quot; &quot; or &quot; f&quot;{g}&quot;) . &#39;red or green&#39; . # you can also compute an expression in an f-string f&quot;{ 40 + 2}&quot; . &#39;42&#39; . # functions can also be called from inside an f-string f&quot;This is in CAPS: { str.upper(r) }&quot; # same as above f&quot;This is in CAPS: { r.upper() }&quot; . &#39;This is in CAPS: RED&#39; . f-string: Padding the output . # Inside f-string, passing an integer after &#39;:&#39; will cause that field to be a minimum number of characters wide. # This is useful for making columns line up. groups = { &#39;small&#39;: 100, &#39;medium&#39;: 100100, &#39;large&#39;: 100100100 } for group, value in groups.items(): print(f&quot;{value:10} ==&gt; {group:20}&quot;) print(f&quot;{&#39;****&#39;*10}&quot;) # another nice trick for group, value in groups.items(): print(f&quot;{group:10} ==&gt; {value:20}&quot;) . 100 ==&gt; small 100100 ==&gt; medium 100100100 ==&gt; large **************************************** small ==&gt; 100 medium ==&gt; 100100 large ==&gt; 100100100 . f-string: Binary and hexadecimal format . # you can convert integers to binary and hexadecimal format print( f&quot;5 in binary {5:b}&quot; ) print( f&quot;5 in hexadecimal {5:#b}&quot; ) . 5 in binary 101 5 in hexadecimal 0b101 . f-string: Controlling the decimal places . import math print(f&#39;The value of pi is approximately (no formatting) {math.pi}&#39;) print(f&#39;The value of pi is approximately {math.pi :.3f}&#39;) . The value of pi is approximately (no formatting) 3.141592653589793 The value of pi is approximately 3.142 . f-string: Putting commas in numerical output . num = 3214298342.234 f&quot;{num:,}&quot; . &#39;3,214,298,342.234&#39; .",
            "url": "https://hassaanbinaslam.github.io/myblog/python/2022/02/18/python-tips-output-formatting.html",
            "relUrl": "/python/2022/02/18/python-tips-output-formatting.html",
            "date": " • Feb 18, 2022"
        }
        
    
  
    
        ,"post25": {
            "title": "Python - Getting more information from Tracebacks",
            "content": ". About . This notebook demonstrates what the Python Traceback object is, and how can we get more information out of it to better diagnose exception messages. . Credit . This blog post is based on an article originally written in Python Cookbook published by O&#39;Reilly Media, Inc. and released July 2002. In book&#39;s chapter 15, there is a section with the title Getting More Information from Tracebacks written by Bryn Keller. An online version of this article is available at https://www.oreilly.com/library/view/python-cookbook/0596001673/ch14s05.html. . The original article uses Python 2.2, but I have adapted it for Python 3.8. Also, I have added some commentary to give more insights on Python Traceback object. . Environment Details . from platform import python_version print(&quot;python==&quot; + python_version()) . . python==3.8.5 . Discussion . Consider the following toy example where we are getting some data from an external source (an API call, a DB call, etc.), and we need to find the length of individual items provided in the list. We know that items in the list will be of type str so we have used a len() function on it. . We got an exception when we ran our function on received data, and now we are trying to investigate what caused the error. . # this is intentionally hidden as we don&#39;t know about the data received from an external source. data = [&quot;1&quot;, &quot;22&quot;, 333, &quot;4444&quot;] . . # our toy example function. import sys, traceback def get_items_len(items: list) -&gt; list: &quot;&quot;&quot; this function returns the length of items received in a list. &quot;&quot;&quot; items_len = [] for i in items: items_len.append(len(i)) return items_len . # let&#39;s run our function on &quot;data&quot; received from an external source try: get_items_len(data) except Exception as e: print(traceback.print_exc()) . None . Traceback (most recent call last): File &#34;&lt;ipython-input-4-42cd486e1858&gt;&#34;, line 4, in &lt;module&gt; get_items_len(data) File &#34;&lt;ipython-input-3-8421f841ba77&gt;&#34;, line 11, in get_items_len items_len.append(len(i)) TypeError: object of type &#39;int&#39; has no len() . We got an exception while data processing and the Traceback message gives us some details. It tells us that we have received some data of type integer instead of string, and we are trying to call len() function on it. But we don&#39;t know the actual data value that caused the exception, and we don&#39;t know the index of the item in the list that caused this error. Depending on the use case, information about the local variables, or input data that caused the error can be crucial in diagnosing the root cause of an error. . Fortunately, all this information is already available to us in the Traceback object, but there are no built-in methods that give this information directly. Let us try some of the built-in methods on the Traceback object to see the kind of information we could get from them. . # calling traceback module built-in methods try: get_items_len(data) except Exception as e: print(&quot;***** Exception *****&quot;) print(e) exc_type, exc_value, exc_traceback = sys.exc_info() print(&quot; n***** print_tb *****&quot;) traceback.print_tb(exc_traceback, limit=1, file=sys.stdout) print(&quot; n***** print_exception *****&quot;) # exc_type below is ignored on 3.5 and later traceback.print_exception(exc_type, exc_value, exc_traceback, limit=2, file=sys.stdout) print(&quot; n***** print_exc *****&quot;) traceback.print_exc(limit=2, file=sys.stdout) print(&quot; n***** format_exc, first and last line *****&quot;) formatted_lines = traceback.format_exc().splitlines() print(formatted_lines[0]) print(formatted_lines[-1]) print(&quot; n***** format_exception *****&quot;) # exc_type below is ignored on 3.5 and later print(repr(traceback.format_exception(exc_type, exc_value, exc_traceback))) print(&quot; n***** extract_tb *****&quot;) print(repr(traceback.extract_tb(exc_traceback))) print(&quot; n***** format_tb *****&quot;) print(repr(traceback.format_tb(exc_traceback))) print(&quot; n***** tb_lineno *****&quot;, exc_traceback.tb_lineno) . ***** Exception ***** object of type &#39;int&#39; has no len() ***** print_tb ***** File &#34;&lt;ipython-input-5-73d5b316a567&gt;&#34;, line 4, in &lt;module&gt; get_items_len(data) ***** print_exception ***** Traceback (most recent call last): File &#34;&lt;ipython-input-5-73d5b316a567&gt;&#34;, line 4, in &lt;module&gt; get_items_len(data) File &#34;&lt;ipython-input-3-8421f841ba77&gt;&#34;, line 11, in get_items_len items_len.append(len(i)) TypeError: object of type &#39;int&#39; has no len() ***** print_exc ***** Traceback (most recent call last): File &#34;&lt;ipython-input-5-73d5b316a567&gt;&#34;, line 4, in &lt;module&gt; get_items_len(data) File &#34;&lt;ipython-input-3-8421f841ba77&gt;&#34;, line 11, in get_items_len items_len.append(len(i)) TypeError: object of type &#39;int&#39; has no len() ***** format_exc, first and last line ***** Traceback (most recent call last): TypeError: object of type &#39;int&#39; has no len() ***** format_exception ***** [&#39;Traceback (most recent call last): n&#39;, &#39; File &#34;&lt;ipython-input-5-73d5b316a567&gt;&#34;, line 4, in &lt;module&gt; n get_items_len(data) n&#39;, &#39; File &#34;&lt;ipython-input-3-8421f841ba77&gt;&#34;, line 11, in get_items_len n items_len.append(len(i)) n&#39;, &#34;TypeError: object of type &#39;int&#39; has no len() n&#34;] ***** extract_tb ***** [&lt;FrameSummary file &lt;ipython-input-5-73d5b316a567&gt;, line 4 in &lt;module&gt;&gt;, &lt;FrameSummary file &lt;ipython-input-3-8421f841ba77&gt;, line 11 in get_items_len&gt;] ***** format_tb ***** [&#39; File &#34;&lt;ipython-input-5-73d5b316a567&gt;&#34;, line 4, in &lt;module&gt; n get_items_len(data) n&#39;, &#39; File &#34;&lt;ipython-input-3-8421f841ba77&gt;&#34;, line 11, in get_items_len n items_len.append(len(i)) n&#39;] ***** tb_lineno ***** 4 . . All these methods are useful but we are still short on information about the state of local variables when the system crashed. Before writing our custom function to get the variables state at the time of exception, let us spend some time to understand the working of Traceback object. . Traceback Module . https://docs.python.org/3/library/traceback.html This module provides an easy-to-use interface to work with traceback objects. It provides multiple functions that we can use to extract the required information from traceback. So far, we have used methods from this module in the above examples. . Traceback Objects . https://docs.python.org/3/reference/datamodel.html &gt; On this page search for term &quot;Traceback objects&quot; . Traceback objects represent a stack trace of an exception. A traceback object is implicitly created when an exception occurs and may also be explicitly created by initializing an instance of class types.TracebackType. traceback object is also an instance of types.TracebackType class. When an exception occurs, a traceback object is initialized for us, and we can obtain it from any of the following two methods. . It is available as a third item of the tuple returned by sys.exc_info() &quot;(type, value, traceback)&quot; | It is available as the __traceback__ object of the caught exception. &quot;Exception.__traceback__&quot; | A traceback object is a linked list of nodes, where each node is a Frame object. Frame objects form their own linked list but in the opposite direction of traceback objects. Together they work like a doubly-linked list, and we can use them to move back and forth in the stack trace history. It is the frame objects that hold all the stack&#39;s important information. traceback object has some special attributes . tb_next point to the next level in the stack trace (towards the frame where the exception occurred), or None if there is no next level | tb_frame points to the execution frame of the current level | tb_lineno gives the line number where the exception occurred | . # method 1: get traceback object using sys.exc_info() try: get_items_len(data) except Exception as e: print(sys.exc_info()[2]) . &lt;traceback object at 0x7f5c6c60e9c0&gt; . # method 2: get traceback object using Exception.__traceback__ try: get_items_len(data) except Exception as e: print(e.__traceback__ ) . &lt;traceback object at 0x7f5c6c5c0180&gt; . If there is no exception in the system, then calling sys.exc_info() will only return None values. . # no exception is generated so sys.exc_info() will return None values. try: get_items_len([&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;]) except Exception as e: print(sys.exc_info()[2]) . Frame Objects . https://docs.python.org/3/reference/datamodel.html &gt; On this page search for term &quot;Frame objects&quot; . Frame objects represent execution frames. It has some special attributes . f_back is a reference to the previous stack frame (towards the caller), or None if this is the bottom stack frame | f_code is the code object being executed in this frame. We will discuss Code Objects in next the section | f_lineno is the current line number of the frame — writing to this from within a trace function jumps to the given line (only for the bottom-most frame). A debugger can implement a Jump command (aka Set Next Statement) by writing to f_lineno. This attribute will give you the line number in the code on which exception occurred | f_locals is a dictionary used to lookup local variables. From this dictionary we can get all the local variables and their state at the time of exception | f_globals is a dictionary for global varaibles | . Code Objects . https://docs.python.org/3/reference/datamodel.html &gt; On this page search for term &quot;Code Objects&quot; . Code objects represent byte-compiled executable Python code or bytecode. Some of its attributes include . co_name gives the function name being executed | co_filename gives the filename from which the code was compiled | . There are many other helpful attributes in this object, and you may read about them from the docs. . Visual representation of Traceback, Frame and Code Objects . figure 1: Visual representation of Traceback, Frame and Code Objects . Custom fuction for additional exception info . Now with this additional information on stack trace objects, let us create a function to get variables state at the time of exception. . def exc_info_plus(): &quot;&quot;&quot; Provides the usual traceback information, followed by a listing of all the local variables in each frame. &quot;&quot;&quot; tb = sys.exc_info()[2] # iterate forward to the last (most recent) traceback object. while 1: if not tb.tb_next: break tb = tb.tb_next stack = [] # get the most recent traceback frame f = tb.tb_frame # iterate backwards from recent to oldest traceback frame while f: stack.append(f) f = f.f_back # stack.reverse() # uncomment to get innermost (most recent) frame at the last # get exception information and stack trace entries from most recent traceback object exc_msg = traceback.format_exc() exc_msg += &quot; n*** Locals by frame, innermost first ***&quot; for frame in stack: exc_msg += f&quot; nFrame {frame.f_code.co_name} in {frame.f_code.co_filename} at line {frame.f_lineno}&quot; for key, value in frame.f_locals.items(): exc_msg += f&quot; n t {key:20} = &quot; try: data = str(value) # limit variable&#39;s output to a certain number. You can adjust it as per your requirement. # But not to remove it as output from large objects (e.g. Pandas DataFrame) can be troublesome. output_limit = 50 exc_msg += (data[:output_limit] + &quot;...&quot;) if len(data) &gt; output_limit else data except: exc_msg += &quot;&lt;ERROR WHILE PRINTING VALUE&gt;&quot; return exc_msg . . #now let us try our custom exception function and see the ouput try: get_items_len(data) except Exception as e: print(exc_info_plus()) . Traceback (most recent call last): File &#34;&lt;ipython-input-10-01264d9e470a&gt;&#34;, line 4, in &lt;module&gt; get_items_len(data) File &#34;&lt;ipython-input-3-8421f841ba77&gt;&#34;, line 11, in get_items_len items_len.append(len(i)) TypeError: object of type &#39;int&#39; has no len() *** Locals by frame, innermost first *** Frame get_items_len in &lt;ipython-input-3-8421f841ba77&gt; at line 11 items = [&#39;1&#39;, &#39;22&#39;, 333, &#39;4444&#39;] items_len = [1, 2] i = 333 Frame &lt;module&gt; in &lt;ipython-input-10-01264d9e470a&gt; at line 6 __name__ = __main__ __doc__ = Automatically created module for IPython interacti... __package__ = None __loader__ = None __spec__ = None __builtin__ = &lt;module &#39;builtins&#39; (built-in)&gt; __builtins__ = &lt;module &#39;builtins&#39; (built-in)&gt; _ih = [&#39;&#39;, &#39;#collapse-hide nfrom platform import python_... _oh = {} _dh = [&#39;/data/_notebooks&#39;] In = [&#39;&#39;, &#39;#collapse-hide nfrom platform import python_... Out = {} get_ipython = &lt;bound method InteractiveShell.get_ipython of &lt;ipy... exit = &lt;IPython.core.autocall.ZMQExitAutocall object at 0... quit = &lt;IPython.core.autocall.ZMQExitAutocall object at 0... _ = __ = ___ = _i = #collapse-show def exc_info_plus(): &#34;&#34;&#34; Pr... _ii = ## # no exception is generated so sys.exc_info() w... _iii = ## # method 2: get traceback object using Exceptio... _i1 = #collapse-hide from platform import python_version... python_version = &lt;function python_version at 0x7f5c72dbc430&gt; _i2 = #collapse-hide # this is intentionally hidden as w... data = [&#39;1&#39;, &#39;22&#39;, 333, &#39;4444&#39;] _i3 = ## # our toy example function. import sys, traceba... sys = &lt;module &#39;sys&#39; (built-in)&gt; traceback = &lt;module &#39;traceback&#39; from &#39;/usr/lib/python3.8/trace... get_items_len = &lt;function get_items_len at 0x7f5c6c62c790&gt; _i4 = ## # let&#39;s run our function on &#34;data&#34; received fro... _i5 = #collapse-output # calling traceback module built-... exc_type = &lt;class &#39;TypeError&#39;&gt; exc_value = object of type &#39;int&#39; has no len() exc_traceback = &lt;traceback object at 0x7f5c6c5cf700&gt; formatted_lines = [&#39;Traceback (most recent call last):&#39;, &#39; File &#34;&lt;i... _i6 = ## # method 1: get traceback object using sys.exc_... _i7 = ## # method 2: get traceback object using Exceptio... _i8 = ## # no exception is generated so sys.exc_info() w... _i9 = #collapse-show def exc_info_plus(): &#34;&#34;&#34; Pr... exc_info_plus = &lt;function exc_info_plus at 0x7f5c6c62cc10&gt; _i10 = #collapse-output #now let us try our custom except... e = object of type &#39;int&#39; has no len() Frame run_code in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 3418 self = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... code_obj = &lt;code object &lt;module&gt; at 0x7f5c6c62eea0, file &#34;&lt;ip... result = &lt;ExecutionResult object at 7f5c6c5c88e0, execution... async_ = False __tracebackhide__ = __ipython_bottom__ old_excepthook = &lt;bound method IPKernelApp.excepthook of &lt;ipykernel... outflag = True Frame run_ast_nodes in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 3338 self = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... nodelist = [&lt;_ast.Try object at 0x7f5c6c5c8850&gt;] cell_name = &lt;ipython-input-10-01264d9e470a&gt; interactivity = none compiler = &lt;IPython.core.compilerop.CachingCompiler object at... result = &lt;ExecutionResult object at 7f5c6c5c88e0, execution... to_run_exec = [&lt;_ast.Try object at 0x7f5c6c5c8850&gt;] to_run_interactive = [] mod = &lt;_ast.Module object at 0x7f5c6c5c8430&gt; compare = &lt;function InteractiveShell.run_ast_nodes.&lt;locals&gt;.... to_run = [(&lt;_ast.Try object at 0x7f5c6c5c8850&gt;, &#39;exec&#39;)] node = &lt;_ast.Try object at 0x7f5c6c5c8850&gt; mode = exec code = &lt;code object &lt;module&gt; at 0x7f5c6c62eea0, file &#34;&lt;ip... asy = False _async = False Frame run_cell_async in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 3146 raw_cell = #collapse-output #now let us try our custom except... silent = False shell_futures = True transformed_cell = #collapse-output #now let us try our custom except... preprocessing_exc_tuple = None info = &lt;ExecutionInfo object at 7f5c6c5c8be0, raw_cell=&#34;#... error_before_exec = &lt;function InteractiveShell.run_cell_async.&lt;locals&gt;... cell = #collapse-output #now let us try our custom except... compiler = &lt;IPython.core.compilerop.CachingCompiler object at... _run_async = False cell_name = &lt;ipython-input-10-01264d9e470a&gt; code_ast = &lt;_ast.Module object at 0x7f5c6c5c85e0&gt; interactivity = last_expr result = &lt;ExecutionResult object at 7f5c6c5c88e0, execution... self = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... store_history = True Frame _pseudo_sync_runner in /usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py at line 68 coro = &lt;coroutine object InteractiveShell.run_cell_async ... Frame _run_cell in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 2923 self = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... raw_cell = #collapse-output #now let us try our custom except... store_history = True silent = False shell_futures = True preprocessing_exc_tuple = None transformed_cell = #collapse-output #now let us try our custom except... coro = &lt;coroutine object InteractiveShell.run_cell_async ... runner = &lt;function _pseudo_sync_runner at 0x7f5c724ba040&gt; Frame run_cell in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 2877 self = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... raw_cell = #collapse-output #now let us try our custom except... store_history = True silent = False shell_futures = True result = None Frame run_cell in /usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py at line 539 self = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... args = (&#39;#collapse-output n#now let us try our custom exc... kwargs = {&#39;store_history&#39;: True, &#39;silent&#39;: False} __class__ = &lt;class &#39;ipykernel.zmqshell.ZMQInteractiveShell&#39;&gt; Frame do_execute in /usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py at line 302 self = &lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5c... code = #collapse-output #now let us try our custom except... silent = False store_history = True user_expressions = {} allow_stdin = True reply_content = {} run_cell = &lt;bound method InteractiveShell.run_cell_async of &lt;... should_run_async = &lt;bound method InteractiveShell.should_run_async of... shell = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... Frame wrapper in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 234 args = (&lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5... kwargs = {} future = &lt;Future pending&gt; ctx_run = &lt;built-in method run of Context object at 0x7f5c6c... result = &lt;generator object IPythonKernel.do_execute at 0x7f... func = &lt;function IPythonKernel.do_execute at 0x7f5c6f6978... Frame execute_request in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py at line 540 self = &lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5c... stream = &lt;zmq.eventloop.zmqstream.ZMQStream object at 0x7f5... ident = [b&#39;e2e3826d25fb4c63876268cdc5a787ad&#39;] parent = {&#39;header&#39;: {&#39;msg_id&#39;: &#39;218114cb9837444cbd29466d87b... content = {&#39;code&#39;: &#39;#collapse-output n#now let us try our cu... code = #collapse-output #now let us try our custom except... silent = False store_history = True user_expressions = {} allow_stdin = True stop_on_error = True metadata = {&#39;started&#39;: datetime.datetime(2022, 2, 14, 9, 30, ... Frame wrapper in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 234 args = (&lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5... kwargs = {} future = &lt;Future pending&gt; ctx_run = &lt;built-in method run of Context object at 0x7f5c6c... result = &lt;generator object Kernel.execute_request at 0x7f5c... func = &lt;function Kernel.execute_request at 0x7f5c6f747f70... Frame dispatch_shell in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py at line 265 self = &lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5c... stream = &lt;zmq.eventloop.zmqstream.ZMQStream object at 0x7f5... msg = {&#39;header&#39;: {&#39;msg_id&#39;: &#39;218114cb9837444cbd29466d87b... idents = [b&#39;e2e3826d25fb4c63876268cdc5a787ad&#39;] msg_type = execute_request handler = &lt;bound method Kernel.execute_request of &lt;ipykernel... Frame wrapper in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 234 args = (&lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5... kwargs = {} future = &lt;Future pending&gt; ctx_run = &lt;built-in method run of Context object at 0x7f5c6f... result = &lt;generator object Kernel.dispatch_shell at 0x7f5c6... func = &lt;function Kernel.dispatch_shell at 0x7f5c6f7473a0&gt; Frame process_one in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py at line 362 self = &lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5c... wait = True priority = 10 t = 13 dispatch = &lt;bound method Kernel.dispatch_shell of &lt;ipykernel.... args = (&lt;zmq.eventloop.zmqstream.ZMQStream object at 0x7f... Frame run in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 775 self = &lt;tornado.gen.Runner object at 0x7f5c6c60f8e0&gt; future = None exc_info = None value = (10, 13, &lt;bound method Kernel.dispatch_shell of &lt;i... Frame inner in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 814 f = None self = &lt;tornado.gen.Runner object at 0x7f5c6c60f8e0&gt; Frame _run_callback in /usr/local/lib/python3.8/dist-packages/tornado/ioloop.py at line 741 self = &lt;tornado.platform.asyncio.AsyncIOMainLoop object a... callback = functools.partial(&lt;function Runner.handle_yield.&lt;l... Frame &lt;lambda&gt; in /usr/local/lib/python3.8/dist-packages/tornado/ioloop.py at line 688 f = &lt;Future finished result=(10, 13, &lt;bound method...7... callback = &lt;function Runner.handle_yield.&lt;locals&gt;.inner at 0x... future = &lt;Future finished result=(10, 13, &lt;bound method...7... self = &lt;tornado.platform.asyncio.AsyncIOMainLoop object a... Frame _run in /usr/lib/python3.8/asyncio/events.py at line 81 self = &lt;Handle IOLoop.add_future.&lt;locals&gt;.&lt;lambda&gt;(&lt;Futur... Frame _run_once in /usr/lib/python3.8/asyncio/base_events.py at line 1859 self = &lt;_UnixSelectorEventLoop running=True closed=False ... sched_count = 0 handle = &lt;Handle IOLoop.add_future.&lt;locals&gt;.&lt;lambda&gt;(&lt;Futur... timeout = 0 event_list = [] end_time = 113697.83311910101 ntodo = 2 i = 0 Frame run_forever in /usr/lib/python3.8/asyncio/base_events.py at line 570 self = &lt;_UnixSelectorEventLoop running=True closed=False ... old_agen_hooks = asyncgen_hooks(firstiter=None, finalizer=None) Frame start in /usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py at line 199 self = &lt;tornado.platform.asyncio.AsyncIOMainLoop object a... old_loop = &lt;_UnixSelectorEventLoop running=True closed=False ... Frame start in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py at line 612 self = &lt;ipykernel.kernelapp.IPKernelApp object at 0x7f5c7... Frame launch_instance in /usr/local/lib/python3.8/dist-packages/traitlets/config/application.py at line 845 cls = &lt;class &#39;ipykernel.kernelapp.IPKernelApp&#39;&gt; argv = None kwargs = {} app = &lt;ipykernel.kernelapp.IPKernelApp object at 0x7f5c7... Frame &lt;module&gt; in /usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py at line 16 __name__ = __main__ __doc__ = Entry point for launching an IPython kernel. This... __package__ = __loader__ = &lt;_frozen_importlib_external.SourceFileLoader objec... __spec__ = ModuleSpec(name=&#39;ipykernel_launcher&#39;, loader=&lt;_fro... __annotations__ = {} __builtins__ = &lt;module &#39;builtins&#39; (built-in)&gt; __file__ = /usr/local/lib/python3.8/dist-packages/ipykernel_l... __cached__ = /usr/local/lib/python3.8/dist-packages/__pycache__... sys = &lt;module &#39;sys&#39; (built-in)&gt; app = &lt;module &#39;ipykernel.kernelapp&#39; from &#39;/usr/local/lib... Frame _run_code in /usr/lib/python3.8/runpy.py at line 87 code = &lt;code object &lt;module&gt; at 0x7f5c7317e030, file &#34;/us... run_globals = {&#39;__name__&#39;: &#39;__main__&#39;, &#39;__doc__&#39;: &#39;Entry point f... init_globals = None mod_name = __main__ mod_spec = ModuleSpec(name=&#39;ipykernel_launcher&#39;, loader=&lt;_fro... pkg_name = script_name = None loader = &lt;_frozen_importlib_external.SourceFileLoader objec... fname = /usr/local/lib/python3.8/dist-packages/ipykernel_l... cached = /usr/local/lib/python3.8/dist-packages/__pycache__... Frame _run_module_as_main in /usr/lib/python3.8/runpy.py at line 194 mod_name = ipykernel_launcher alter_argv = 1 mod_spec = ModuleSpec(name=&#39;ipykernel_launcher&#39;, loader=&lt;_fro... code = &lt;code object &lt;module&gt; at 0x7f5c7317e030, file &#34;/us... main_globals = {&#39;__name__&#39;: &#39;__main__&#39;, &#39;__doc__&#39;: &#39;Entry point f... . . Note the output from the first stack frame in the above stack trace. It is easy now to see (items) that we received in our function. The item at index i is also available (333) on which our function crashed. Using our custom function unexpected errors are logged in a format that makes it a lot easier to find and fix the errors. Let&#39;s fix our function to handle unexpected integer values. . # let&#39;s fix our function to handle unexpected &#39;int&#39; items by converting them to &#39;str&#39; def get_items_len(items: list) -&gt; list: &quot;&quot;&quot; this function returns the length of items received in a list. &quot;&quot;&quot; items_len = [] for i in map(str, items): items_len.append(len(i)) return items_len # test it again get_items_len(data) . [1, 2, 3, 4] .",
            "url": "https://hassaanbinaslam.github.io/myblog/python/2022/02/11/python-stack-traceback-more-info.html",
            "relUrl": "/python/2022/02/11/python-stack-traceback-more-info.html",
            "date": " • Feb 11, 2022"
        }
        
    
  
    
        ,"post26": {
            "title": "Python Dictionary - Multiple ways to get items",
            "content": ". About . This notebook demonstrates multiple ways to get items from a Python dictionary. . Environment Details . from platform import python_version print(&quot;python==&quot; + python_version()) . . python==3.8.5 . Example Dictionaries . # simple dictionary car = { &quot;brand&quot;: &quot;ford&quot;, &quot;model&quot;: &quot;mustang&quot; } car . {&#39;brand&#39;: &#39;ford&#39;, &#39;model&#39;: &#39;mustang&#39;} . # nested dictionary family = { &#39;gfather&#39; : { &#39;father&#39;: { &#39;son&#39;: {&#39;love&#39;:&#39;python&#39;} } } } family . {&#39;gfather&#39;: {&#39;father&#39;: {&#39;son&#39;: {&#39;love&#39;: &#39;python&#39;}}}} . Method 1: Square brackets . A square bracket is the simplest approach to getting any item from a dictionary. You can get a value from a dictionary by providing it a key in [] brackets. For example, to get a value of model from a car . car[&#39;model&#39;] . &#39;mustang&#39; . Problem with this approach is that if the provided key is not available in the dictionary then it will throw a KeyError exception. . car[&#39;year&#39;] . KeyErrorTraceback (most recent call last) &lt;ipython-input-5-ca220af55913&gt; in &lt;module&gt; -&gt; 1 car[&#39;year&#39;] KeyError: &#39;year&#39; . To avoid KeyError, you can first check if the key is available in dictionary. . if &#39;year&#39; in car: # check if given key is available in dictionary year = car[&#39;year&#39;] # now get the value else: year = &#39;1964&#39; # (Optional) otherwise give this car a default value year . &#39;1964&#39; . An alternate approach could be to use a Try-Except block to handle the KeyError exception. . try: year = car[&#39;year&#39;] except KeyError: year = &#39;1964&#39; # give this car a default value year . &#39;1964&#39; . For nested dictionaries, you can use chained [] brackets. But beware that if any of the Keys is missing in the chain, you will get a KeyError exception. . # this will work. All keys are present. family[&#39;gfather&#39;][&#39;father&#39;][&#39;son&#39;] . {&#39;love&#39;: &#39;python&#39;} . # this will not work. &#39;mother&#39; key is not in dictionary family[&#39;gfather&#39;][&#39;mother&#39;][&#39;son&#39;] . KeyErrorTraceback (most recent call last) &lt;ipython-input-9-3d615db6bfdf&gt; in &lt;module&gt; 1 # this will not work. &#39;mother&#39; key is not in dictionary -&gt; 2 family[&#39;gfather&#39;][&#39;mother&#39;][&#39;son&#39;] KeyError: &#39;mother&#39; . Method 2: Get function . https://docs.python.org/3/library/stdtypes.html#dict.get &gt; get(key[, default]) . Get function will return the value for key if key is in the dictionary. Otherwise, it will return a default value which is None. You can provide your default value as well. . year = car.get(&#39;year&#39;, &#39;1964&#39;) year # year key is not present so get function will return a default value &#39;1964&#39; . &#39;1964&#39; . Depending on your use case there can be confusion with this approach when your item can also have None value. In that case, you will not know whether the None value was returned from the dictionary or it was the Get function. . owner = car.get(&#39;owner&#39;) owner # owner has a None value. But is this value coming from dic or from Get function? # This can be confusing for large nested dictionaries. . For nested dictionaries you can use chained Get functions. But beware that missing Key items needs to be properly handled otherwise you will still get an exception. . # this will work. All keys are present. family.get(&#39;gfather&#39;).get(&#39;father&#39;).get(&#39;son&#39;) . {&#39;love&#39;: &#39;python&#39;} . # this will still work. &#39;daughter&#39; key is missing # but since it is at the end of chain it will return a default None value family.get(&#39;gfather&#39;).get(&#39;father&#39;).get(&#39;daughter&#39;) . # this will NOT work. &#39;mother&#39; key is missing and it returned a default None value. # but since it is not at the end, and we called Get function on returned value &#39;None&#39; family.get(&#39;gfather&#39;).get(&#39;mother&#39;).get(&#39;son&#39;) . AttributeErrorTraceback (most recent call last) &lt;ipython-input-14-a35a8f091991&gt; in &lt;module&gt; 1 # this will NOT work. &#39;mother&#39; key is missing and it returned a default None value. 2 # but since it is not at the end, and we called Get function on returned value &#39;None&#39; -&gt; 3 family.get(&#39;gfather&#39;).get(&#39;mother&#39;).get(&#39;son&#39;) AttributeError: &#39;NoneType&#39; object has no attribute &#39;get&#39; . # this will work. &#39;mother&#39; key is missing and it returned a default value. # but we have properly handled all the default values with empty dictionaries. family.get(&#39;gfather&#39;, {}).get(&#39;mother&#39;, {}).get(&#39;son&#39;, {}) . {} .",
            "url": "https://hassaanbinaslam.github.io/myblog/python/2022/02/10/python-dictionary.html",
            "relUrl": "/python/2022/02/10/python-dictionary.html",
            "date": " • Feb 10, 2022"
        }
        
    
  
    
        ,"post27": {
            "title": "My First Blog Post from Jupyter Notebook",
            "content": ". Well, this is my first post using Jupyter notebook as a publishing medium. Besides this notebook, I am also using &#39;nbdev&#39; library from FastAI as tooling to convert notebooks into static HTML pages. Once pushed to GitHub they will become new posts on my blog. I need to learn more about this setup, but it is looking very interesting. . # I can also include some code directly into the blog post. No need for GitHub snippets. print(&quot;nbdev and fastpages from Fast.AI are so cool! &quot;) . . nbdev and fastpages from Fast.AI are so cool! .",
            "url": "https://hassaanbinaslam.github.io/myblog/jupyter/fastpages/2022/02/09/hello-world.html",
            "relUrl": "/jupyter/fastpages/2022/02/09/hello-world.html",
            "date": " • Feb 9, 2022"
        }
        
    
  
    
        ,"post28": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://hassaanbinaslam.github.io/myblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://hassaanbinaslam.github.io/myblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Hi there! . I am Hassaan Bin Aslam and welcome to my blog. . I started this blog to document and share my learning. I make living by working as a Machine Learning Solutions Architect. AI/ML, Cloud Architecture, DevOps are very exciting topics and also close to my heart. Every day I face interesting problems and I like to share my understanding on them here. I am passionate about AWS as a strategic cloud platform and using AI/ML to solve business problems. I’m currently focusing a lot of my time on applying DevOps practices to Machine Learning workloads (MLOps) to enable customers to adopt Machine Learning at scale. . You can find and connect with me on . LinkedIn: linkedin.com/in/hassaanbinaslam | Twitter: twitter.com/hassaanbinaslam | GitHub: github.com/hassaanbinaslam | .",
          "url": "https://hassaanbinaslam.github.io/myblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hassaanbinaslam.github.io/myblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}