{
  
    
        "post0": {
            "title": "Data Preparation with SageMaker Data Wrangler (Part 5)",
            "content": ". Enviornment . This notebook is prepared with Amazon SageMaker Studio using Python 3 (Data Science) Kernel and ml.t3.medium instance. . About . This is a detailed guide on using AWS SageMaker Data Wrangler service to prepare data for machine learning models. SageMaker Data Wrangler is a multipurpose tool with which you can . import data from multiple sources | explore data with visualizations | apply transformations | export data for ml training | . This guide is divided into five parts . Part 1: Prepare synthetic data and place it on multiple sources | Part 2: Import data from multiple sources using Data Wrangler | Part 3: Explore data with Data Wrangler visualizations | Part 4: Preprocess data using Data Wrangler | Part 5: Export data for ML training (You are here) | . Part 5: Export data for ML training . It is important to note that the transformations we have used are not applied to the data yet. These transformations need to be executed to get the final transformed data. When we export Data Wrangler flow it generates the code that when executed will perform the data transformations. Data Wrangler supports four export methods: Save to S3, Pipeline, Python Code, and Feature Store. In this post, we will see how to export data to S3 as this is the most common use case. . Open the customer-churn-p4.flow file from part-4. On the last step click the plus sign and select Export to &gt; Amazon S3 (via Jupyter Notebook) . . Sagemaker Data Wrangler will auto-generate a Jupyter notebook that will contain all the required code to transform and put data on the S3 bucket. . . You may review the code and make any changes otherwise run it as it is till point (Optional)Next Steps. This is the first time SageMaker will process the data and place the output on S3 bucket. SageMaker may take a couple of minutes to execute all the cells. It is important to note that this notebook will initiate a container running on a separate machine to do all the processing. The machine specs are defined in the notebook as . # Processing Job Instance count and instance type. instance_count = 2 instance_type = &quot;ml.m5.4xlarge&quot; . Once execution is complete you see the output message containing the S3 bucket location where the final output is stored. . . The optional part of this notebook also contains code to generate xgboost model on the transformed data. To execute these steps make the following changes in the notebook. . Change the flag to run the optional steps. . run_optional_steps = True . Next, update the xgboost hyperparameters to train a binary classification model (customer churn or not?). . hyperparameters = { &quot;max_depth&quot;:&quot;5&quot;, &quot;objective&quot;: &quot;binary:logistic&quot;, &quot;num_round&quot;: &quot;10&quot;, } . Execute the optional steps. Again note that these steps will initiate a container running on a separate machine (&quot;ml.m5.2xlarge&quot;) to do the training work. The training job will take a few minutes to complete and once it is done trained model will be available on the S3 bucket for inference use. This autogenerated notebook customer-churn-p4.ipynb is available on GitHub here. . Summary . In this last post of the series, we used SageMaker Data Wrangler to auto-generate code to preprocess the data and store the final output on S3 bucket. We also used the same notebook to train an xgboost model on the processed data. .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/26/aws-sagemaker-wrangler-p5.html",
            "relUrl": "/aws/ml/sagemaker/2022/05/26/aws-sagemaker-wrangler-p5.html",
            "date": " • May 26, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Data Preparation with SageMaker Data Wrangler (Part 4)",
            "content": ". Enviornment . This notebook is prepared with Amazon SageMaker Studio using Python 3 (Data Science) Kernel and ml.t3.medium instance. . About . This is a detailed guide on using AWS SageMaker Data Wrangler service to prepare data for machine learning models. SageMaker Data Wrangler is a multipurpose tool with which you can . import data from multiple sources | explore data with visualizations | apply transformations | export data for ml training | . This guide is divided into five parts . Part 1: Prepare synthetic data and place it on multiple sources | Part 2: Import data from multiple sources using Data Wrangler | Part 3: Explore data with Data Wrangler visualizations | Part 4: Preprocess data using Data Wrangler (You are here) | Part 5: Export data for ML training | . Part 4: Preprocess data using Data Wrangler . We will continue from where we left in part-3. Open customer-churn.flow file in AWS SageMaker Data Wrangler console. Once opened our flow will look like this . . We will add the following transformations to our code. . Remove redundant columns | Remove features with low predictive power | Transform feature values to correct format | Encode categorical features | Move the target label to the start | . Remove redundant columns . When we made joins between tables (see part-2) it resulted in some redundant columns CustomerID_* . We will remove them first. For this click on plus sign beside 2nd Join, and select Add Transform. From the next transform UI clink Add Step and then search for transformer Manage Column. Inside Manage Columns transformer select . Transform = Drop Column | Columns to drop = CustomerID_0, CustomerID_1 | . Click preview and Add. . Remove features with low predictive power . In part-3 we used Quick Model to get the predictive power of features. When we analyze features with low importance we find that Phone is one such feature that does not hold much information for the model. For a model, a phone number is just some random collection of numbers and does not hold any meaning. There are other features with low importance too but they still hold some information for the model. So let&#39;s drop Phone. The steps will be same as in the last part. . Transform feature values to correct format . Churn? is our target label but its value has an extra &#39;.&#39; at the end. If we remove that symbol then it can easily be converted to a Boolean type. So let&#39;s do that. From the transformers list this time choose Format String and select . Transform = Remove Symbols | Input Columns = Churn? | Symbols = . | . Click Preview and Add. . . Now that the data is in the correct format (True/False) we can apply another transformer on it to convert it to Boolean feature. So select PARSE COLUMN AS TYPE transformer and configure . Column = Churn? | From = String | To = Boolean | . Click Preview and then Add. . Encode categorical features . At this point we have only two columns with String datatype: State and Area Code. If we look at the Area Code it has high variance and little feature importance. It is better to drop this feature. So Add another transformer and drop Area Code. For State we will apply one-hot encoding. So for this select transformer Encode Categorical and configure . Transform = One-hot encode | Input Columns = State | Output style = Columns | . Leave the rest of the options as default. Click Preview and Add. . . Move the target label to the start . SageMaker requires that the target label should be the first column in the dataset. So add another transformer Manage columns and configure . Transform = Move column | Move Type = Move to start | Column to move = Churn? | . . Evaluate model performance . We have done some key transformations. We can use Quick Model again to analyze the model performance at this point. We have done a similar analysis in part-3 so let&#39;s do it again and compare the results. From the last transformation step, click plus sign and choose Add Analysis . . We can see from the results that these transformations have a positive impact on the model performance and the F1 score has moved up from 0.841 to 0.861. . Summary . In this post we have seen how we can apply a transformation to our data and can use Quick Model to quickly analyze the model performance. customer-churn-p4.flow file used in this post can be found on the GitHub here. In the next post, we will discuss how to export data from Data Wrangler to different destinations. .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/25/aws-sagemaker-wrangler-p4.html",
            "relUrl": "/aws/ml/sagemaker/2022/05/25/aws-sagemaker-wrangler-p4.html",
            "date": " • May 25, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Data Preparation with SageMaker Data Wrangler (Part 3)",
            "content": ". Enviornment . This notebook is prepared with Amazon SageMaker Studio using Python 3 (Data Science) Kernel and ml.t3.medium instance. . About . This is a detailed guide on using AWS SageMaker Data Wrangler service to prepare data for machine learning models. SageMaker Data Wrangler is a multipurpose tool with which you can . import data from multiple sources | explore data with visualizations | apply transformations | export data for ml training | . This guide is divided into five parts . Part 1: Prepare synthetic data and place it on multiple sources | Part 2: Import data from multiple sources using Data Wrangler | Part 3: Explore data with Data Wrangler visualizations (You are here) | Part 4: Preprocess data using Data Wrangler | Part 5: Export data for ML training | . Part 3: Explore data with Data Wrangler visualizations . In this post, we will use SageMaker Data Wrangler to create some visualizations for exploratory data analysis (EDA). Open the customer-churn.flow from part-2. It is also available on GitHub here. We will create a histogram to explore the frequency distribution of daily calls. Once the flow process is open on the Data Flow UI it will look like this . . Click on the 2nd join plus sign and select &#39;Add Analysis&#39;. From the next analysis UI select . Analysis Type = Histogram | Analysis Name = call_minutes_churn | X_axis = day_min | Facet by = Churn? | . Click Preview and Data Wrangler will create the following histogram . . From this histogram you can see that customers whose calls duration are 4 minutes or less are more likely to stay, and customer having call duration longer than 4 minutes are more likely to churn. Save the flow to return back to main Data Flow UI. . Preview ML model performance using Quick Model . Quick Model is another great feature of SageMaker wrangler with which we can quickly train a Random Forrest Classification model and analyze the importance of features. For this again click on the plus sign against the 2nd Join, and select Add Analysis. Then from the Analysis UI select . Analysis Type = Quick Model | Analysis Name = Quick model | Label = Churn? | . Label is our target identifier. Click preview. Data Wrangler will take around a minute to train the model, and will provide a chart with feature importances. . . From this feature importance chart, we can see that the day_mins and night_charge features have the highest importance. It also shows that the model has achieved F1 score of 0.841 on the test data. We can take this model as a baseline and work on the important features and model tuning to improve its performance. Click Save to return to the main Data Flow UI. . Summary . In this post, we saw that we can quickly create visualizations from Data Wrangler to do our EDA work. There are many other built-in analysis reports available (check Data Leakage and Data Quality reports) that can quickly provide a very detailed analysis of the data. The customer-churn.flow file is available on GitHub here. In the next post, we will perform some preprocessing and transformations to make our data ready for ML training. .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/24/aws-sagemaker-wrangler-p3.html",
            "relUrl": "/aws/ml/sagemaker/2022/05/24/aws-sagemaker-wrangler-p3.html",
            "date": " • May 24, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Data Preparation with SageMaker Data Wrangler (Part 2)",
            "content": ". Enviornment . This notebook is prepared with Amazon SageMaker Studio using Python 3 (Data Science) Kernel and ml.t3.medium instance. . About . This is a detailed guide on using AWS SageMaker Data Wrangler service to prepare data for machine learning models. SageMaker Data Wrangler is a multipurpose tool with which you can . import data from multiple sources | explore data with visualizations | apply transformations | export data for ml training | . This guide is divided into five parts . Part 1: Prepare synthetic data and place it on multiple sources | Part 2: Import data from multiple sources using Data Wrangler (You are here) | Part 3: Explore data with Data Wrangler visualizations | Part 4: Preprocess data using Data Wrangler | Part 5: Export data for ML training | . Part 2: Import data from multiple sources using Data Wrangler . In this post, we will create SageMaker Data Wrangler Flow pipeline to import data from multiple sources. Once data is imported, we will then add a step to join the data into a single dataset that can be used for training ML models. . Launch SageMaker Data Wrangler Flow . Create a new Data Wrangler flow by clicking on the main menu tabs File &gt; New &gt; Data Wrangler Flow. . . Once launched SageMaker may take a minute to initialize a new flow. The reason for this is SageMaker will launch a separate machine in the background ml.m5.4xlarge with 16vCPU and 64 GiB memory for processing flow files. A flow file is a JSON file that just captures all the steps performed from the Flow UI console. When you execute the flow, the Flow engine parses this file and performs all the steps. Once a new flow file is available, rename it to customer-churn.flow. . . Import data from sources . First, we will create a flow to import data (created in the part-1 post) from S3 bucket. For this from the flow UI click on Amazon S3 bucket. From the next window select the bucket name S3://sagemaker-us-east-1-801598032724. In your case, it could be different where you have stored the data. From the UI select the filename &quot;telco_churn_customer_info.csv&quot; and click Import . . Once the data is imported repeat the steps for the filename &quot;telco_churn_account_info.csv&quot;. If you are not seeing the &quot;import from S3 bucket&quot; option on the UI then check the flow UI and click on the &#39;Import&#39; tab option. Once both files are imported, your Data Flow tab will look similar to this . . Now that we have imported data from S3, we can now work on importing data from the Athena database. For this from the Flow UI Import tab click on Amazon Athena option. From the next UI select AwsDataCatalog Data catalog option. For Databases drop down select telco_db and in the query pane write the below query. . select * from telco_churn_utility . You can also preview the data by clicking on the table preview option. Once satisfied with the results click &#39;Import&#39;. When asked about the database name write telco_churn_utility . . At this point, you will find all three tables imported in Data Flow UI. Against each table, a plus sign (+) will appear that you can use to add any transformations you want to apply on each table. . . for telco_churn_customer_info click on the plus sign and then select &#39;Edit&#39; to change data types. . . We will add the following transformations . Change Area Code from Long to String | Click Preview | Then click Apply | . . Similarly for telco_churn_account_info.csv edit data types as . Change Account Length to Long | Change Int&#39;l Plan and VMail Plan to Bool | Click Preview and then click Apply | . For telco_churn_utility.csv edit data types as . Change custserv_calls to Long | Click Preview and then click Apply | . At this point, we have imported the data from all three sources and have also properly transformed their column types. . Joining Tables . Now we will join all three tables to get a full dataset. For this from the Flow UI Data flow click on the plus sign next to customer_info data type and this time select &#39;Join&#39;. From the new window select account_info as the right dataset and click Configure . . From the next screen select . Join Type = Full Outer | Columns Left = CustomerID | Columns Right = CustomerID | Click Preview and then Add | . . A new join step will appear on the Data Flow UI. Click on the plus sign next to it and repeat the steps for utility table . . Join Type = Full Outer | Columns Left = CustomerID_0 | Columns Right = CustomerID | Click Preview and then Add | . . Summary . At this point, we have all the tables joined together. The customer-churn.flow created is available on the GitHub here. In the next post, we will clean duplicate columns and create some visualizations to analyze the data. .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/23/aws-sagemaker-wrangler-p2.html",
            "relUrl": "/aws/ml/sagemaker/2022/05/23/aws-sagemaker-wrangler-p2.html",
            "date": " • May 23, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Collecting metrics and logs from on-premises servers with the CloudWatch agent",
            "content": ". About . This post is a detailed guide on using AWS CloudWatch Agent to collect logs and metrics from on-premises Ubuntu server. . The CloudWatch agent is open-source tool under the MIT license, and is hosted on GitHub amazon-cloudwatch-agent | With this agent you can collect more system-level metrics from Amazon EC2 instances or onprem servers across operating systems. You can retrieve custom metrics from your applications or services using the StatsD and collectd protocols. StatsD is supported on both Linux servers and servers running Windows Server. collectd is supported only on Linux servers | For the list of metrics that can be collected by CloudWatch agent follow this link metrics-collected-by-CloudWatch-agent | . Environment Details . For on-premises Ubuntu server, we will use an EC2 machine with Ubuntu OS. Enable Auto-assign public IP and keep all the default settings. Once the instance is in a running state use SSH Key to connect to it. . If you are using Windows OS and while connecting to Ubuntu machine you are getting &quot;Permissions for &#39;ssh-key.pem&#39; are too open.&quot; then take help from this post to resolve it windows-ssh-permissions-for-private-key-are-too-open . . Once you are successfully connected to EC2 Ubuntu machine you will get the following message on the terminal. . CloudWatch Agent Installation and Configuration Steps . Create IAM roles and users for use with CloudWatch agent . Access to AWS resources requires permissions. You create an IAM role, an IAM user, or both to grant permissions that the CloudWatch agent needs to write metrics to CloudWatch. . If you&#39;re going to use the agent on Amazon EC2 instances, you should create an IAM role. | f you&#39;re going to use the agent on on-premises servers, you should create an IAM user. | . Since we want to use EC2 machine as an on-premises machine so we will create an IAM user. . To create the IAM user necessary for the CloudWatch agent to run on on-premises servers follow these steps . Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. | In the navigation pane on the left, choose Users and then Add users. | Enter the user name for the new user. | Select Access key - Programmatic access and choose Next: Permissions. | Choose Attach existing policies directly. | In the list of policies, select the check box next to CloudWatchAgentServerPolicy. If necessary, use the search box to find the policy. | Choose Next: Tags. | Optionally create tags for the new IAM user, and then choose Next:Review. | Confirm that the correct policy is listed, and choose Create user. | Next to the name of the new user, choose Show. Copy the access key and secret key to a file so that you can use them when installing the agent. Choose Close. | Install and configure AWS CLI on Ubuntu server . Connect to the Ubuntu server using any SSH client. We need to first download and install AWS CLI. Follow the below commands to download and install it. For installing AWS CLI on macOS and Windows take help from this post awscli-getting-started-install . 1. Download AWS CLI package . curl &quot;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip&quot; -o &quot;awscliv2.zip&quot; . 2. Install UNZIP package . sudo apt install unzip . 3. Unzip AWSCLI Package . unzip awscliv2.zip . 4. Install AWS CLI . sudo ./aws/install . 5. Verify AWS CLI Installation . aws --version . . 6. Configure AWS CLI . Make sure that you use AmazonCloudWatchAgent profile name as this is used by the OnPremise case by default. For more details, you may take help from this post install-CloudWatch-Agent-commandline-fleet . aws configure --profile AmazonCloudWatchAgent . . 7. Verify credentials in User home directory . cat /home/ubuntu/.aws/credentials . Install and run the CloudWatch agent on Ubuntu server . 1. Download the agent . The following download link is for Ubuntu. For any other OS you can take help from this post for downloaded agent download-cloudwatch-agent-commandline . wget https://s3.amazonaws.com/amazoncloudwatch-agent/ubuntu/amd64/latest/amazon-cloudwatch-agent.deb . 2. Install the agent . sudo dpkg -i -E ./amazon-cloudwatch-agent.deb . 3. Prepare agent configuration file . Prepare agent configuration file. This config file will be provided to the agent in the run command. One such sample is provided below. For more details on this config file you may take help from this link create-cloudwatch-agent-configuration-file. Note the path of this config file (agent config) as we will need it in later commands. . // config-cloudwatchagent.json { &quot;agent&quot;: { &quot;metrics_collection_interval&quot;: 10, &quot;logfile&quot;: &quot;/opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log&quot;, &quot;run_as_user&quot;: &quot;ubuntu&quot;, &quot;debug&quot;: false }, &quot;metrics&quot;: { &quot;namespace&quot;: &quot;myblog/cloudwatchagent/demo&quot;, &quot;metrics_collected&quot;: { &quot;cpu&quot;: { &quot;resources&quot;: [ &quot;*&quot; ], &quot;measurement&quot;: [ {&quot;name&quot;: &quot;cpu_usage_idle&quot;, &quot;rename&quot;: &quot;CPU_USAGE_IDLE&quot;, &quot;unit&quot;: &quot;Percent&quot;}, {&quot;name&quot;: &quot;cpu_usage_nice&quot;, &quot;unit&quot;: &quot;Percent&quot;}, &quot;cpu_usage_guest&quot;, &quot;cpu_usage_active&quot; ], &quot;totalcpu&quot;: true, &quot;metrics_collection_interval&quot;: 10 }, &quot;disk&quot;: { &quot;resources&quot;: [ &quot;/&quot;, &quot;/tmp&quot; ], &quot;measurement&quot;: [ {&quot;name&quot;: &quot;free&quot;, &quot;rename&quot;: &quot;DISK_FREE&quot;, &quot;unit&quot;: &quot;Gigabytes&quot;}, &quot;total&quot;, &quot;used&quot; ], &quot;ignore_file_system_types&quot;: [ &quot;sysfs&quot;, &quot;devtmpfs&quot; ], &quot;metrics_collection_interval&quot;: 60 }, &quot;diskio&quot;: { &quot;resources&quot;: [ &quot;*&quot; ], &quot;measurement&quot;: [ &quot;reads&quot;, &quot;writes&quot;, &quot;read_time&quot;, &quot;write_time&quot;, &quot;io_time&quot; ], &quot;metrics_collection_interval&quot;: 60 }, &quot;swap&quot;: { &quot;measurement&quot;: [ &quot;swap_used&quot;, &quot;swap_free&quot;, &quot;swap_used_percent&quot; ] }, &quot;mem&quot;: { &quot;measurement&quot;: [ &quot;mem_used&quot;, &quot;mem_cached&quot;, &quot;mem_total&quot; ], &quot;metrics_collection_interval&quot;: 1 }, &quot;net&quot;: { &quot;resources&quot;: [ &quot;eth0&quot; ], &quot;measurement&quot;: [ &quot;bytes_sent&quot;, &quot;bytes_recv&quot;, &quot;drop_in&quot;, &quot;drop_out&quot; ] }, &quot;netstat&quot;: { &quot;measurement&quot;: [ &quot;tcp_established&quot;, &quot;tcp_syn_sent&quot;, &quot;tcp_close&quot; ], &quot;metrics_collection_interval&quot;: 60 }, &quot;processes&quot;: { &quot;measurement&quot;: [ &quot;running&quot;, &quot;sleeping&quot;, &quot;dead&quot; ] } }, &quot;force_flush_interval&quot; : 30 }, &quot;logs&quot;: { &quot;logs_collected&quot;: { &quot;files&quot;: { &quot;collect_list&quot;: [ { &quot;file_path&quot;: &quot;/opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log&quot;, &quot;log_group_name&quot;: &quot;myblog/onprem/ubuntu/amazon-cloudwatch-agent&quot;, &quot;log_stream_name&quot;: &quot;myblog-cloudwatchagent-demo.log&quot;, &quot;timezone&quot;: &quot;UTC&quot; } ] } }, &quot;log_stream_name&quot;: &quot;my_log_stream_name&quot;, &quot;force_flush_interval&quot; : 15 } } . Some important parts of this config file . logfile . &quot;logfile&quot;: &quot;/opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log&quot; . CloudWatch agent log file location on on-premise server is specified by this tag. After running the agent you can check this log file for any exception messages. . log_group_name . &quot;log_group_name&quot;: &quot;myblog/onprem/ubuntu/amazon-cloudwatch-agent&quot; . An on-premise logfile is also uploaded to CloudWatch under log-group-name specified by this tag. . log_stream_name . &quot;log_stream_name&quot;: &quot;myblog-cloudwatchagent-demo.log&quot; . Log stream name of the CloudWatch where logfile log steam will be uploaded. . namespace . &quot;namespace&quot;: &quot;myblog/cloudwatchagent/demo&quot; . On CloudWatch console you find the uploaded metrics under the custom namespace specified by this tag. In our case, it is &quot;myblog/cloudwatchagent/demo&quot; . 4. Update shared configuration file . From the config file . Uncomment the [credentails] tag | Update shared_credentails_profile name. This is the profile name with which we have configured our AWS CLI &#39;AmazonCloudWatchAgent&#39;. If you have used any other name then use that name here. | Update shared_credentials_file path. This is the path for AWS user credentails file created by AWS CLI. &#39;/home/username/.aws/credentials&#39; and in our case it is /home/ubuntu/.aws/credentials | Configuration file is located at /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml. For more details on this shared configuration file follow this link CloudWatch-Agent-profile-instance-first . sudo vim /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml . . 5. Start the agent . sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m onPremise -s -c file:/home/ubuntu/config-cloudwatchagent.json . Make sure that you provide the correct path to the JSON config file. In our case, it is file:/home/ubuntu/config-cloudwatchagent.json. For more details check this link start-CloudWatch-Agent-on-premise-SSM-onprem . . 6. Check agent status . sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a status . If the agent is running you will get status : running otherwise you will get status : stopped . . 7. Check agent logs . The agent generates a log while it runs. This log includes troubleshooting information. This log is the amazon-cloudwatch-agent.log file. This file is located in /opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log on Linux servers. This is the same logfile path we also defined in the JSON config file. If you are using multiple agents on the machine then you can give them separate log file paths using their JSON configurations. . sudo tail -f /var/log/amazon/amazon-cloudwatch-agent/amazon-cloudwatch-agent.log . Check the logs if there is an exception message or not. . . Please note that both the log files are the same. It could be that agent is keeping multiple copies for internal processing. . /opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log . or . /var/log/amazon/amazon-cloudwatch-agent/amazon-cloudwatch-agent.log . Check the agent logs on AWS CloudWatch console . Agent logs are also uploaded to CloudWatch console under log group and stream that we mentioned in JSON config file. In our case it is . &quot;log_group_name&quot;: &quot;myblog/onprem/ubuntu/amazon-cloudwatch-agent&quot; &quot;log_stream_name&quot;: &quot;myblog-cloudwatchagent-demo.log&quot; . . Check the machine metrics on CloudWatch console . Now finally we can check the metrics uploaded by the agent on CloudWatch console under CloudWatch &gt; Metrics &gt; ALL metrics &gt; Custom namespaces . The name of the metrics namespace is the same as what we defined in our JSON config file . &quot;metrics&quot;: { &quot;namespace&quot;: &quot;myblog/cloudwatchagent/demo&quot; . . Common scenarios with the CloudWatch agent . For more trouble shooting scenerios follow these link . troubleshooting-CloudWatch-Agent | CloudWatch-Agent-common-scenarios | . To stop the CloudWatch agent locally using the command line . On a Linux server, enter the following . sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a stop . I updated my agent configuration but don&#8217;t see the new metrics or logs in the CloudWatch console . If you update your CloudWatch agent configuration file, the next time that you start the agent, you need to use the fetch-config option. For example, if you stored the updated file on the local computer, enter the following command. Replace &lt;configuration-file-path&gt; with the actual config file path. . sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -s -m ec2 -c file:&lt;configuration-file-path&gt; .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/cloudwatch/2022/05/21/cloudwatch-agent-onprem.html",
            "relUrl": "/aws/cloudwatch/2022/05/21/cloudwatch-agent-onprem.html",
            "date": " • May 21, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Data Preparation with SageMaker Data Wrangler (Part 1)",
            "content": ". Enviornment . This notebook is prepared with Amazon SageMaker Studio using Python 3 (Data Science) Kernel and ml.t3.medium instance. . About . This is a detailed guide on using AWS SageMaker Data Wrangler service to prepare data for machine learning models. SageMaker Data Wrangler is a multipurpose tool with which you can . import data from multiple sources | explore data with visualizations | apply transformations | export data for ml training | . This guide is divided into five parts . Part 1: Prepare synthetic data and place it on multiple sources (You are here) | Part 2: Import data from multiple sources using Data Wrangler | Part 3: Explore data with Data Wrangler visualizations | Part 4: Preprocess data using Data Wrangler | Part 5: Export data for ML training | . Credits . Getting Started with Amazon SageMaker Studio book by Michael Hsieh. Michael Hsieh is a senior AI/machine learning (ML) solutions architect at Amazon Web Services. He creates and evangelizes for ML solutions centered around Amazon SageMaker. He also works with enterprise customers to advance their ML journeys. . Part 1: Prepare synthetic data and place it on multiple sources . Let&#39;s prepare some dataset and place it on the S3 bucket and AWS Glue tables. Then we will use Data Wrangler to pull and join data from these two sources. The idea is to simulate some real project challenges where data is not coming from a single source but is distributed in multiple stores, and is in different formats. It is usually the preprocessing pipeline job to get data from these sources and join and preprocess it. . Data . Mobile operators have historical records on which customers ultimately ended up churning and which continued using the service. We can use this historical information to construct an ML model of one mobile operator’s churn using a process called training. After training the model, we can pass the profile information of an arbitrary customer (the same profile information that we used to train the model) to the model, and have the model predict whether this customer is going to churn. Of course, we expect the model to make mistakes. After all, predicting the future is a tricky business! But we’ll learn how to deal with prediction errors. . The dataset we use is publicly available and was mentioned in the book Discovering Knowledge in Data by Daniel T. Larose. It is attributed by the author to the University of California Irvine Repository of Machine Learning Datasets (Jafari-Marandi, R., Denton, J., Idris, A., Smith, B. K., &amp; Keramati, A. (2020). . Preparation . # install aws data wrangler package # restart kernel after installation # more on this package later in the notebook. !pip install -q awswrangler . WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv WARNING: You are using pip version 22.0.4; however, version 22.1 is available. You should consider upgrading via the &#39;/opt/conda/bin/python -m pip install --upgrade pip&#39; command. . import pandas as pd import sagemaker sess = sagemaker.Session() prefix = &#39;myblog/demo-customer-churn&#39; . !aws s3 cp s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt ./ . download: s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt to ./churn.txt . df = pd.read_csv(&quot;./churn.txt&quot;) # make &#39;CustomerID&#39; column from the index df[&#39;CustomerID&#39;]=df.index pd.set_option(&quot;display.max_columns&quot;, 500) df.head(10) . State Account Length Area Code Phone Int&#39;l Plan VMail Plan VMail Message Day Mins Day Calls Day Charge Eve Mins Eve Calls Eve Charge Night Mins Night Calls Night Charge Intl Mins Intl Calls Intl Charge CustServ Calls Churn? CustomerID . 0 PA | 163 | 806 | 403-2562 | no | yes | 300 | 8.162204 | 3 | 7.579174 | 3.933035 | 4 | 6.508639 | 4.065759 | 100 | 5.111624 | 4.928160 | 6 | 5.673203 | 3 | True. | 0 | . 1 SC | 15 | 836 | 158-8416 | yes | no | 0 | 10.018993 | 4 | 4.226289 | 2.325005 | 0 | 9.972592 | 7.141040 | 200 | 6.436188 | 3.221748 | 6 | 2.559749 | 8 | False. | 1 | . 2 MO | 131 | 777 | 896-6253 | no | yes | 300 | 4.708490 | 3 | 4.768160 | 4.537466 | 3 | 4.566715 | 5.363235 | 100 | 5.142451 | 7.139023 | 2 | 6.254157 | 4 | False. | 2 | . 3 WY | 75 | 878 | 817-5729 | yes | yes | 700 | 1.268734 | 3 | 2.567642 | 2.528748 | 5 | 2.333624 | 3.773586 | 450 | 3.814413 | 2.245779 | 6 | 1.080692 | 6 | False. | 3 | . 4 WY | 146 | 878 | 450-4942 | yes | no | 0 | 2.696177 | 3 | 5.908916 | 6.015337 | 3 | 3.670408 | 3.751673 | 250 | 2.796812 | 6.905545 | 4 | 7.134343 | 6 | True. | 4 | . 5 VA | 83 | 866 | 454-9110 | no | no | 0 | 3.634776 | 7 | 4.804892 | 6.051944 | 5 | 5.278437 | 2.937880 | 300 | 4.817958 | 4.948816 | 4 | 5.135323 | 5 | False. | 5 | . 6 IN | 140 | 737 | 331-5751 | yes | no | 0 | 3.229420 | 4 | 3.165082 | 2.440153 | 8 | 0.264543 | 2.352274 | 300 | 3.869176 | 5.393439 | 4 | 1.784765 | 4 | False. | 6 | . 7 LA | 54 | 766 | 871-3612 | no | no | 0 | 0.567920 | 6 | 1.950098 | 4.507027 | 0 | 4.473086 | 0.688785 | 400 | 6.132137 | 5.012747 | 5 | 0.417421 | 8 | False. | 7 | . 8 MO | 195 | 777 | 249-5723 | yes | no | 0 | 5.811116 | 6 | 4.331065 | 8.104126 | 2 | 4.475034 | 4.208352 | 250 | 5.974575 | 4.750153 | 7 | 3.320311 | 7 | True. | 8 | . 9 AL | 104 | 657 | 767-7682 | yes | no | 0 | 2.714430 | 7 | 5.138669 | 8.529944 | 6 | 3.321121 | 2.342177 | 300 | 4.328966 | 3.433554 | 5 | 5.677058 | 4 | False. | 9 | . df.shape . (5000, 22) . By modern standards, it’s a relatively small dataset, with only 5,000 records, where each record uses 21 attributes to describe the profile of a customer of an unknown US mobile operator. The attributes are: . State: the US state in which the customer resides, indicated by a two-letter abbreviation; for example, OH or NJ | Account Length: the number of days that this account has been active | Area Code: the three-digit area code of the corresponding customer’s phone number | Phone: the remaining seven-digit phone number | Int’l Plan: whether the customer has an international calling plan: yes/no | VMail Plan: whether the customer has a voice mail feature: yes/no | VMail Message: the average number of voice mail messages per month | Day Mins: the total number of calling minutes used during the day | Day Calls: the total number of calls placed during the day | Day Charge: the billed cost of daytime calls | Eve Mins, Eve Calls, Eve Charge: the billed cost for calls placed during the evening | Night Mins, Night Calls, Night Charge: the billed cost for calls placed during nighttime | Intl Mins, Intl Calls, Intl Charge: the billed cost for international calls | CustServ Calls: the number of calls placed to Customer Service | Churn?: whether the customer left the service: true/false | . The last attribute, Churn?, is known as the target attribute: the attribute that we want the ML model to predict. Because the target attribute is binary, our model will be performing binary prediction, also known as binary classification. . We have our dataset. Now we will split this dataset into three subsets . customer: customer data, and place it as a CSV file on the S3 bucket | account: accounts data, and place it as CSV on the same S3 bucket | utility: utility data, and place it as Glue tables | . customer_columns = [&#39;CustomerID&#39;, &#39;State&#39;, &#39;Area Code&#39;, &#39;Phone&#39;] account_columns = [&#39;CustomerID&#39;, &#39;Account Length&#39;, &quot;Int&#39;l Plan&quot;, &#39;VMail Plan&#39;, &#39;Churn?&#39;] utility_columns = [&#39;CustomerID&#39;, &#39;VMail Message&#39;, &#39;Day Mins&#39;, &#39;Day Calls&#39;, &#39;Day Charge&#39;, &#39;Eve Mins&#39;, &#39;Eve Calls&#39;, &#39;Eve Charge&#39;, &#39;Night Mins&#39;, &#39;Night Calls&#39;, &#39;Night Charge&#39;, &#39;Intl Mins&#39;, &#39;Intl Calls&#39;, &#39;Intl Charge&#39;, &#39;CustServ Calls&#39;] . We will use the default bucket associated with our SageMaker session. You may use any other bucket with proper access permissions. . bucket = sess.default_bucket() bucket . &#39;sagemaker-us-east-1-801598032724&#39; . Next, we will use AWS Data Wrangler Python package (awswrangler) to create an AWS Glue database. . awswrangler is an open source Python library maintained by AWS team, as is defined as . An AWS Professional Service open source python initiative that extends the power of Pandas library to AWS connecting DataFrames and AWS data related services. Easy integration with Athena, Glue, Redshift, Timestream, OpenSearch, Neptune, QuickSight, Chime, CloudWatchLogs, DynamoDB, EMR, SecretManager, PostgreSQL, MySQL, SQLServer and S3 (Parquet, CSV, JSON and EXCEL). . You may read more about this library here . Documentation:https://aws-data-wrangler.readthedocs.io/en/stable/what.html* Github repo: https://github.com/awslabs/aws-data-wrangler | . Please note that AWS SageMaker session needs some additional AWS Glue permissions to create a database. If you get an error while creating a Glue database in following steps then add those permissions. . Error: AccessDeniedException: An error occurred (AccessDeniedException) when calling the GetDatabase operation: User: arn:aws:sts::801598032724:assumed-role/AmazonSageMaker-ExecutionRole-20220516T161743/SageMaker is not authorized to perform: glue:GetDatabase on resource: arn:aws:glue:us-east-1:801598032724:database/telco_db because no identity-based policy allows the glue:GetDatabase action . Fix: Go to your SageMaker Execution Role and add permission AWSGlueConsoleFullAccess . # define the Glue DB name db_name = &#39;telco_db&#39; . import awswrangler as wr # get all the existing Glue db list databases = wr.catalog.databases() # print existing db names print(&quot;*** existing databases *** n&quot;) print(databases) # if our db does not exist then create it if db_name not in databases.values: wr.catalog.create_database(db_name, description = &#39;Demo DB for telco churn dataset&#39;) print(&quot; n*** existing + new databases *** n&quot;) print(wr.catalog.databases()) else: print(f&quot;Database {db_name} already exists&quot;) . *** existing databases *** Database Description 0 sagemaker_data_wrangler 1 sagemaker_processing *** existing + new databases *** Database Description 0 sagemaker_data_wrangler 1 sagemaker_processing 2 telco_db Demo DB for telco churn dataset . # in case you want to delete a database using this notebook # wr.catalog.delete_database(db_name) . Similarly you can go to AWS Glue console to see that the new database has been created. . . Now we will place the three data subsets into their respective locations. . suffix = [&#39;customer_info&#39;, &#39;account_info&#39;, &#39;utility&#39;] for i, columns in enumerate([customer_columns, account_columns, utility_columns]): # get the data subset df_tmp = df[columns] # prepare filename and output path fname = &#39;telco_churn_%s&#39; % suffix[i] outputpath = f&#39;s3://{bucket}/{prefix}/data/{fname}&#39; print(f&quot; n*** working on {suffix[i]}***&quot;) print(f&quot;filename: {fname}&quot;) print(f&quot;output path: {outputpath}&quot;) if i &gt; 1: # for utility wr.s3.to_csv( df=df_tmp, path=outputpath, dataset=True, database=db_name, # Athena/Glue database table=fname, # Athena/Glue table index=False, mode=&#39;overwrite&#39;) else: # for customer and account wr.s3.to_csv( df=df_tmp, path=f&#39;{outputpath}.csv&#39;, index=False) . *** working on customer_info*** filename: telco_churn_customer_info output path: s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_customer_info *** working on account_info*** filename: telco_churn_account_info output path: s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_account_info *** working on utility*** filename: telco_churn_utility output path: s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_utility . We can verify the uploaded data from the S3 bucket. . . Similarly, from Glue console we can verify that the utility table has been created. . . If you want to remain within the notebook and do the verification then that can also be done. . # list s3 objects wr.s3.list_objects(&#39;s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/&#39;) . [&#39;s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_account_info.csv&#39;, &#39;s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_customer_info.csv&#39;, &#39;s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_utility/b4003acdf33e48ce989401e92146923c.csv&#39;] . # list glue catalog tables wr.catalog.tables() . Database Table Description TableType Columns Partitions . 0 telco_db | telco_churn_utility | | EXTERNAL_TABLE | customerid, vmail_message, day_mins, day_calls... | | . Summary . At this point we have our dataset ready in AWS S3 and Glue, and in the next part we will use AWS SageMaker Data Wrangler to import and join this data. .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/17/aws-sagemaker-wrangler-p1.html",
            "relUrl": "/aws/ml/sagemaker/2022/05/17/aws-sagemaker-wrangler-p1.html",
            "date": " • May 17, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "AWS Machine Learning Certification Notes (MLS-C01)",
            "content": ". About . This post is a compilation of important notes and references for AWS Machine Learning Certification MLSC01. . Notes . Amazon Comprehend . Amazon Comprehend is a natural-language processing (NLP) service that uses machine learning to uncover valuable insights and connections in text. . References . https://aws.amazon.com/comprehend/ . Amazon Rekognition . Amazon Rekognition offers pre-trained and customizable computer vision (CV) capabilities to extract information and insights from your images and videos. . References . https://aws.amazon.com/rekognition/ . Amazon Polly . Amazon Polly is a service that turns text into lifelike speech, allowing you to create applications that talk, and build entirely new categories of speech-enabled products. . References . https://aws.amazon.com/polly/ . Amazon Lex . Amazon Lex is a fully managed artificial intelligence (AI) service with advanced natural language models to design, build, test, and deploy conversational interfaces in applications (chat bots). . References . https://aws.amazon.com/lex/ . Amazon Transcribe . Amazon Transcribe is an automatic speech recognition service that makes it easy to add speech to text capabilities to any application. Transcribe’s features enable you to ingest audio input, produce easy to read and review transcripts, improve accuracy with customization, and filter content to ensure customer privacy. . References . https://aws.amazon.com/transcribe/ . Latent Dirichlet Allocation (LDA) Algorithm . It is a topic modeling technique to generate abstract topics based on word frequency from a set of documents | It is similar to unsupervised classification of documents | It is useful for automatically organizing, summerizing, understanding and searching large electronic archives. It can help in discovering hidden themes in the collection | classifying document into dicoverable themes | organize/summerize/search the documents | . | . References . https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2 . Multinomial Logistic Regression Algorithm . Multinomial Logistic Regression is an extension of logistic regression (supervised) that allows more than two discrete outcomes (multiclass). . References . https://en.wikipedia.org/wiki/Multinomial_logistic_regression . Factorization Machines Algorithm . The Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation. . References . https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html . Sequence to Sequence (seq2seq) Algorithm . Amazon SageMaker Sequence to Sequence is a supervised learning algorithm where the input is a sequence of tokens (for example, text, audio) and the output generated is another sequence of tokens. Example applications include . machine translation (input a sentence from one language and predict what that sentence would be in another language) | text summarization (input a longer string of words and predict a shorter string of words that is a summary) | speech-to-text (audio clips converted into output sentences in tokens) | . Problems in this domain have been successfully modeled with deep neural networks that show a significant performance boost over previous methodologies. Amazon SageMaker seq2seq uses Recurrent Neural Networks (RNNs) and Convolutional Neural Network (CNN) models with attention as encoder-decoder architectures. . References . https://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq.html . Term frequency-inverse document frequency Algorithm . TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. . This is done by multiplying two metrics: how many times a word appears in a document (frequency), and the inverse document frequency of the word across a set of documents. . The term frequency of a word in a document. There are several ways of calculating this frequency, with the simplest being a raw count of instances a word appears in a document. | The inverse document frequency of the word across a set of documents. This means, how common or rare a word is in the entire document set. The closer it is to 0, the more common a word is. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm. So, if the word is very common and appears in many documents, this number will approach 0. Otherwise, it will approach 1 | . It has many uses, most importantly in automated text analysis, and is very useful for scoring words in machine learning algorithms for Natural Language Processing (NLP). . References . https://monkeylearn.com/blog/what-is-tf-idf . BlazingText Algorithm . The Amazon SageMaker BlazingText algorithm provides highly optimized implementations of the Word2vec and text classification algorithms. The Word2vec algorithm is useful for many downstream natural language processing (NLP) tasks, such as sentiment analysis, named entity recognition, machine translation, etc. Text classification is an important task for applications that perform web searches, information retrieval, ranking, and document classification. . The Word2vec algorithm maps words to high-quality distributed vectors. The resulting vector representation of a word is called a word embedding. Words that are semantically similar correspond to vectors that are close together. That way, word embeddings capture the semantic relationships between words. . References . https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html . Amazon SageMaker Batch Transform . Use batch transform when you need to do the following: . Preprocess datasets to remove noise or bias that interferes with training or inference from your dataset. | Get inferences from large datasets. | Run inference when you don&#39;t need a persistent endpoint. | Associate input records with inferences to assist the interpretation of results. | . References . https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html . Amazon SageMaker Real-time inference / Hosting Services . Real-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. You can deploy your model to SageMaker hosting services and get an endpoint that can be used for inference. These endpoints are fully managed and support autoscaling. . References . https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html . Amazon SageMaker Inference Pipeline . An inference pipeline is a Amazon SageMaker model that is composed of a linear sequence of two to fifteen containers that process requests for inferences on data. You use an inference pipeline to define and deploy any combination of pretrained SageMaker built-in algorithms and your own custom algorithms packaged in Docker containers. You can use an inference pipeline to combine preprocessing, predictions, and post-processing data science tasks. Inference pipelines are fully managed. . Within an inference pipeline model, SageMaker handles invocations as a sequence of HTTP requests. The first container in the pipeline handles the initial request, then the intermediate response is sent as a request to the second container, and so on, for each container in the pipeline. SageMaker returns the final response to the client. . When you deploy the pipeline model, SageMaker installs and runs all of the containers on each Amazon Elastic Compute Cloud (Amazon EC2) instance in the endpoint or transform job. Feature processing and inferences run with low latency because the containers are co-located on the same EC2 instances. . References . https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html . Amazon SageMaker Neo . Amazon SageMaker Neo automatically optimizes machine learning models for inference on cloud instances and edge devices to run faster with no loss in accuracy. You start with a machine learning model already built with DarkNet, Keras, MXNet, PyTorch, TensorFlow, TensorFlow-Lite, ONNX, or XGBoost and trained in Amazon SageMaker or anywhere else. Then you choose your target hardware platform, which can be a SageMaker hosting instance or an edge device based on processors from Ambarella, Apple, ARM, Intel, MediaTek, Nvidia, NXP, Qualcomm, RockChip, Texas Instruments, or Xilinx. With a single click, SageMaker Neo optimizes the trained model and compiles it into an executable. The compiler uses a machine learning model to apply the performance optimizations that extract the best available performance for your model on the cloud instance or edge device. You then deploy the model as a SageMaker endpoint or on supported edge devices and start making predictions. . References . https://aws.amazon.com/sagemaker/neo/ . LSTM / Long Short-Term Memory . LSTM is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can process not only single data points (such as images), but also entire sequences of data (such as speech or video). . LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTM is applicable to tasks such as anomaly detection in network traffic or IDSs (intrusion detection systems) . References . https://en.wikipedia.org/wiki/Long_short-term_memory . Semantic Segmentation Algorithm . The SageMaker semantic segmentation algorithm provides a fine-grained, pixel-level approach to developing computer vision applications. It tags every pixel in an image with a class label from a predefined set of classes. Tagging is fundamental for understanding scenes, which is critical to an increasing number of computer vision applications, such as self-driving vehicles, medical imaging diagnostics, and robot sensing. . For comparison, the SageMaker Image Classification Algorithm is a supervised learning algorithm that analyzes only whole images, classifying them into one of multiple output categories. The Object Detection Algorithm is a supervised learning algorithm that detects and classifies all instances of an object in an image. It indicates the location and scale of each object in the image with a rectangular bounding box. . Because the semantic segmentation algorithm classifies every pixel in an image, it also provides information about the shapes of the objects contained in the image. The segmentation output is represented as a grayscale image, called a segmentation mask. A segmentation mask is a grayscale image with the same shape as the input image. . The SageMaker semantic segmentation algorithm is built using the MXNet Gluon framework and the Gluon CV toolkit. . References . https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html | . Accuracy . Accuracy measures the fraction of correct predictions. The range is 0 to 1. . Accuracy = (TP + TN) / (TP + FP + TN + FN) . Precision . Precision measures the fraction of actual positives among those examples that are predicted as positive. The range is 0 to 1. This formula tells us that the larger value of FP (False Positives), the lower the Precision. . Precision = TP / (TP + FP) . For maximun precision there should be no FP. FP are also called Type 1 error. . Recall . The Recall measures the fraction of actual positives that are predicted as positive. The range is 0 to 1. This formula tells us that the larger value of FN (False Negatives), the lower the Recall. . Recall = TP / (TP + FN) . For maximun recall there should be no FN. FN are also called Type 2 error. . Note: Precision and Recall are inversely proportional to eachother. . References . https://towardsdatascience.com/model-evaluation-i-precision-and-recall-166ddb257c7b . L1 regularization . L1 regularization, also known as L1 norm or Lasso (in regression problems), combats overfitting by shrinking the parameters towards 0. This makes some features obsolete. . It’s a form of feature selection, because when we assign a feature with a 0 weight, we’re multiplying the feature values by 0 which returns 0, eradicating the significance of that feature. If the input features of our model have weights closer to 0, our L1 norm would be sparse. A selection of the input features would have weights equal to zero, and the rest would be non-zero. . L2 regularization . L2 regularization, or the L2 norm, or Ridge (in regression problems), combats overfitting by forcing weights to be small, but not making them exactly 0. This regularization returns a non-sparse solution since the weights will be non-zero (although some may be close to 0). A major snag to consider when using L2 regularization is that it’s not robust to outliers. . References . https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization . K-means . K-means is a clustering algorithm that tries to partition a set of points into K sets (clusters) such that the points in each cluster tend to be near each other. It is unsupervised because the points have no external classification. . K-nearest neighbors . K-nearest neighbors is a classification (or regression) algorithm that in order to determine the classification of a point, combines the classification of the K nearest points. It is supervised because you are trying to classify a point based on the known classification of other points. . Courses . Exam Readiness: AWS Certified Machine Learning - Specialty . https://explore.skillbuilder.aws/learn/course/27/play/54/exam-readiness-aws-certified-machine-learning-specialty . This is overall a very good short course that can help you identify your strengths and weaknesses in each exam domain so you know where to focus when studying for the exam. . ACloudGuru AWS Certified Machine Learning - Specialty 2020 . https://acloudguru.com/course/aws-certified-machine-learning-specialty . This is a detailed course on the topics covered in the exam. But this course lacks on &quot;Modeling&quot; domain and hands-on labs. Besides taking this course you should have a good knowledge and working experience in data science and machine learning domain. I already have AI/ML background so it was not an issue for me. Some people have recommended taking Machine Learning, Data Science and Deep Learning with Python from Frank Kane on Udemy if you don&#39;t have an ML background but I am not sure about it&#39;s worth. . Practice Projects . Besides preparing for the exam you should do some projects to build good hands-on knowledge. For this you can use How-To Guides from AWS Getting Started Resource Center (Link Here). Some of my favorite projects are . Build, train, deploy, and monitor a machine learning model with Amazon SageMaker Studio | Optimizing and Scaling Machine Learning Training with Managed Spot Training for Amazon SageMaker | . Practice Dumps . For exam practice tests I have used Jon Bonso Udemy course AWS Certified Machine Learning Specialty Practice Exams . Other Tips . About a week before your exam date start checking Reddit Communities. From time to time people post about their achievements and experiences on taking the exam. People also mention the services or topics that they were asked about during the exam. Keep a close eye on such posts and try to find any topic that you have not covered before. .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/ml/2022/05/14/aws-ml-cert-notes.html",
            "relUrl": "/aws/ml/2022/05/14/aws-ml-cert-notes.html",
            "date": " • May 14, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Kaggle - Blue Book for Bulldozers",
            "content": ". About . This notebook explores and builds a model for a bulldozer auction prices dataset from the Kaggle competition. . Environment Details . from platform import python_version import sklearn, numpy, matplotlib, pandas print(&quot;python==&quot; + python_version()) print(&quot;sklearn==&quot; + sklearn.__version__) print(&quot;numpy==&quot; + numpy.__version__) print(&quot;pandas==&quot; + pandas.__version__) print(&quot;matplotlib==&quot; + matplotlib.__version__) . . python==3.8.8 sklearn==1.0.2 numpy==1.20.1 pandas==1.2.3 matplotlib==3.5.1 . # Notebook settings import pandas as pd # display all dataframe columns pd.set_option(&#39;display.max_columns&#39;, None) . Prepare the dataset . Download the dataset files . Train.zip and extract Train.csv. This is our training dataset. | Valid.csv. This is our validation dataset. | Test.csv. This is our test dataset. | . This dataset can be downloaded from the Kaggle competition site, and extracted files should be placed under folder ./datasets/2022-04-35-bluebook-for-bulldozers/. These files are made available with this notebook in the GitHub repository and can be downloaded from there too. If you are using Git then it will not download them from the remote server as they exceed 50MB limit (read more here). For working with large files Git needs an extra extension to work with them called git-lfs. . Follow the steps from Git-LFS site to install it on the system. To install it directly from the notebook (running on Linux) use these commands . # download and install git-lfs. Uncomment them as execute. # !curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash # !sudo yum install git-lfs -y # !git lfs install . Once git-lfs is installed use the below command to download large files from the remote server (GitHub). . # uncomment them and execute # git lfs fetch --all # git lfs checkout . Take an initial look at the training data . Load the training data and look for the following information . column names | column data types | how much data is missing? | sample data elements | . # load the training dataset dataset_path = &#39;datasets/2022-04-35-bluebook-for-bulldozers/&#39; df_raw = pd.read_csv(dataset_path+&#39;Train.csv&#39;, low_memory=False) df = df_raw.copy() . # print training dataset summary information df_info = pd.DataFrame() df_info[&#39;sample&#39;] = df.iloc[0] df_info[&#39;data_type&#39;] = df.dtypes df_info[&#39;percent_missing&#39;] = 100*df.isnull().sum() / len(df) print(f&quot;Total features: {len(df.columns)}&quot;) df_info.sort_values(&#39;percent_missing&#39;) . sample data_type percent_missing . SalesID 1139246 | int64 | 0.000000 | . state Alabama | object | 0.000000 | . fiProductClassDesc Wheel Loader - 110.0 to 120.0 Horsepower | object | 0.000000 | . fiBaseModel 521 | object | 0.000000 | . fiModelDesc 521D | object | 0.000000 | . ProductGroup WL | object | 0.000000 | . saledate 11/16/2006 0:00 | object | 0.000000 | . datasource 121 | int64 | 0.000000 | . ModelID 3157 | int64 | 0.000000 | . MachineID 999089 | int64 | 0.000000 | . SalePrice 66000 | int64 | 0.000000 | . YearMade 2004 | int64 | 0.000000 | . ProductGroupDesc Wheel Loader | object | 0.000000 | . Enclosure EROPS w AC | object | 0.081022 | . auctioneerID 3.0 | float64 | 5.019882 | . Hydraulics 2 Valve | object | 20.082269 | . fiSecondaryDesc D | object | 34.201558 | . Coupler None or Unspecified | object | 46.662013 | . Forks None or Unspecified | object | 52.115425 | . ProductSize NaN | object | 52.545964 | . Transmission NaN | object | 54.320972 | . Ride_Control None or Unspecified | object | 62.952696 | . MachineHoursCurrentMeter 68.0 | float64 | 64.408850 | . Drive_System NaN | object | 73.982923 | . Ripper NaN | object | 74.038766 | . Undercarriage_Pad_Width NaN | object | 75.102026 | . Thumb NaN | object | 75.247616 | . Stick_Length NaN | object | 75.265067 | . Pattern_Changer NaN | object | 75.265067 | . Grouser_Type NaN | object | 75.281271 | . Track_Type NaN | object | 75.281271 | . Tire_Size None or Unspecified | object | 76.386912 | . Travel_Controls NaN | object | 80.097476 | . Blade_Type NaN | object | 80.097725 | . Turbocharged NaN | object | 80.271985 | . Stick NaN | object | 80.271985 | . Pad_Type NaN | object | 80.271985 | . Backhoe_Mounting NaN | object | 80.387161 | . fiModelDescriptor NaN | object | 82.070676 | . UsageBand Low | object | 82.639078 | . Differential_Type Standard | object | 82.695918 | . Steering_Controls Conventional | object | 82.706388 | . fiModelSeries NaN | object | 85.812901 | . Coupler_System NaN | object | 89.165971 | . Grouser_Tracks NaN | object | 89.189903 | . Hydraulics_Flow NaN | object | 89.189903 | . Scarifier NaN | object | 93.710190 | . Pushblock NaN | object | 93.712932 | . Engine_Horsepower NaN | object | 93.712932 | . Enclosure_Type NaN | object | 93.712932 | . Blade_Width NaN | object | 93.712932 | . Blade_Extension NaN | object | 93.712932 | . Tip_Control NaN | object | 93.712932 | . # print some unique values against each feature def sniff(df, rows=7): &quot;&quot;&quot; For each column return a set of unique values &quot;&quot;&quot; data = {} for col in df.columns: data[col] = df[col].unique()[:rows] return pd.DataFrame.from_dict(data, orient=&#39;index&#39;).T . sniff(df) . SalesID SalePrice MachineID ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter UsageBand saledate fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Enclosure Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Hydraulics_Flow Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls . 0 1139246 | 66000 | 999089 | 3157 | 121 | 3.0 | 2004 | 68.0 | Low | 11/16/2006 0:00 | 521D | 521 | D | NaN | NaN | NaN | Wheel Loader - 110.0 to 120.0 Horsepower | Alabama | WL | Wheel Loader | NaN | EROPS w AC | None or Unspecified | NaN | None or Unspecified | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 2 Valve | NaN | NaN | NaN | NaN | None or Unspecified | None or Unspecified | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Standard | Conventional | . 1 1139248 | 57000 | 117657 | 77 | 132 | 1.0 | 1996 | 4640.0 | High | 3/26/2004 0:00 | 950FII | 950 | F | II | LC | Medium | Wheel Loader - 150.0 to 175.0 Horsepower | North Carolina | SSL | Skid Steer Loaders | Four Wheel Drive | OROPS | NaN | None or Unspecified | NaN | Extended | Powershuttle | None or Unspecified | Yes | None or Unspecified | None or Unspecified | No | Auxiliary | None or Unspecified | None or Unspecified | Yes | Sideshift &amp; Tip | 23.5 | NaN | None or Unspecified | None or Unspecified | Standard | Steel | None or Unspecified | None or Unspecified | None or Unspecified | None or Unspecified | Double | None or Unspecified | PAT | None or Unspecified | NaN | NaN | . 2 1139249 | 10000 | 434808 | 7009 | 136 | 2.0 | 2001 | 2838.0 | Medium | 2/26/2004 0:00 | 226 | 226 | NaN | -6E | 6 | Small | Skid Steer Loader - 1351.0 to 1601.0 Lb Operat... | New York | TEX | Track Excavators | Two Wheel Drive | EROPS | Yes | Reversible | No | Standard | Standard | Yes | None or Unspecified | 12&#39; | Low Profile | Variable | NaN | Yes | Yes | None or Unspecified | None or Unspecified | NaN | Manual | Yes | Yes | High Flow | Rubber | 16 inch | 11&#39; 0&quot; | Hydraulic | Yes | Triple | Yes | None or Unspecified | Differential Steer | Limited Slip | Command Control | . 3 1139251 | 38500 | 1026470 | 332 | 149 | 11.0 | 2007 | 3486.0 | NaN | 5/19/2011 0:00 | PC120-6E | PC120 | G | LC | L | Large / Medium | Hydraulic Excavator, Track - 12.0 to 14.0 Metr... | Texas | BL | Backhoe Loaders | No | NaN | None | Street | Yes | None | Powershift | None | None | 14&#39; | High Profile | None | Standard | None | Single Shank | None | Tip | 13&quot; | Hydraulic | None | None | None or Unspecified | None | 32 inch | 15&#39; 9&quot; | Manual | No | Single | None | Semi U | Lever | No Spin | Four Wheel Standard | . 4 1139253 | 11000 | 1057373 | 17311 | 172 | 4.0 | 1993 | 722.0 | None | 7/23/2009 0:00 | S175 | S175 | E | -5 | LT | Mini | Skid Steer Loader - 1601.0 to 1751.0 Lb Operat... | Arizona | TTT | Track Type Tractors | All Wheel Drive | EROPS AC | None | Grouser | None | None | None or Unspecified | None | None | 13&#39; | None | None | Base + 1 Function | None | Multi Shank | None | None | 26.5 | None | None | None | None | None | 28 inch | 10&#39; 2&quot; | None | None | None | None | VPAT | Finger Tip | Locking | Wheel | . 5 1139255 | 26500 | 1001274 | 4605 | None | 7.0 | 2008 | 508.0 | None | 12/18/2008 0:00 | 310G | 310 | HAG | III | CR | Large | Backhoe Loader - 14.0 to 15.0 Ft Standard Digg... | Florida | MG | Motor Graders | None | NO ROPS | None | None | None | None | Hydrostatic | None | None | 16&#39; | None | None | Base + 3 Function | None | None | None | None | 29.5 | None | None | None | None | None | 30 inch | 10&#39; 6&quot; | None | None | None | None | Straight | 2 Pedal | None | No | . 6 1139256 | 21000 | 772701 | 1937 | None | 99.0 | 1000 | 11540.0 | None | 8/26/2004 0:00 | 790ELC | 790 | B | -1 | SB | Compact | Hydraulic Excavator, Track - 21.0 to 24.0 Metr... | Illinois | None | None | None | None or Unspecified | None | None | None | None | Autoshift | None | None | &lt;12&#39; | None | None | 4 Valve | None | None | None | None | 14&quot; | None | None | None | None | None | 22 inch | 9&#39; 10&quot; | None | None | None | None | Angle | Pedal | None | None | . From this first look at the data, we can see that . data is of three types numeric | string | datetime | . | some columns have missing data up to 94% e.g. Tip_Control | missing data is represented as NaN | None or unspecified | . | some columns&#39; data types need to be corrected for example SaleID, MachineID are represented as integers but they are categorical nominal features meaning each value is discrete and has no relation among them | UsageBand is of type string but is a categorical ordinal feature meaning their values cannot be measured but have some order between them | Tire_size, Stick_length are actual measurements and need to be converted to appropriate units | . | . Baseline Model . It is a good idea to create a baseline model early in the data science project as it can help to establish a baseline for . time it takes to train a model if the baseline model is taking too much time then we may use a smaller set of the training data for further steps | . | feature importances it can help us establish a relationship between features and the target | help us remove features that have no relationship with the target sooner | . | model performance we can take this model performance as a baseline, and compare it to see how much cleanup and feature engineering steps improve the model performance | . | . For the baseline model, we would have to rely on numerical features as they don&#39;t require any preprocessing and can be readily used. Some numerical features have too much missing data so we have to be selective here. . # filter columns that are not string along with their percentage of missing data numerical_features = df_info.loc[df_info.data_type != &#39;object&#39;].sort_values(&#39;percent_missing&#39;) numerical_features . sample data_type percent_missing . SalesID 1139246 | int64 | 0.000000 | . SalePrice 66000 | int64 | 0.000000 | . MachineID 999089 | int64 | 0.000000 | . ModelID 3157 | int64 | 0.000000 | . datasource 121 | int64 | 0.000000 | . YearMade 2004 | int64 | 0.000000 | . auctioneerID 3.0 | float64 | 5.019882 | . MachineHoursCurrentMeter 68.0 | float64 | 64.408850 | . From these numerical features MachineHoursCurrentMeter has around 64% missing data. Let&#39;s keep this feature as well for our baseline model. . # establish target and baseline features target = &#39;SalePrice&#39; # this is the feature we are trying to predict baseline_features = list(numerical_features.index) baseline_features.remove(target) # remove target feature form input variables baseline_features . [&#39;SalesID&#39;, &#39;MachineID&#39;, &#39;ModelID&#39;, &#39;datasource&#39;, &#39;YearMade&#39;, &#39;auctioneerID&#39;, &#39;MachineHoursCurrentMeter&#39;] . We have established our target and features, and can now train our baseline model. We will use only RandomForrest for this dataset. We have 7 features to learn from so let&#39;s start with n_estimators=70 . from sklearn.ensemble import RandomForestRegressor X, y = df[baseline_features], df[target] X = X.fillna(0) # replace missing numerical values with 0 rf = RandomForestRegressor(n_estimators=70, oob_score=True, n_jobs=-1, verbose=1) rf.fit(X, y) oob_score = rf.oob_score_ oob_score . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 1.2min [Parallel(n_jobs=-1)]: Done 70 out of 70 | elapsed: 1.9min finished . 0.7901663917842495 . Besides the OOB score, we can also interpret our model by looking into the trained model trees&#39; depth and leaves. If OOB score is same but our trees are smaller with fewer nodes then that model is better and more generalized. Larger trees make the model more complex and less generalized. For this let&#39;s create two more functions. . import numpy as np def rf_n_leaves(rf): &quot;&quot;&quot; return the total number of nodes in all the trees of the forrest. &quot;&quot;&quot; return sum(est.tree_.n_leaves for est in rf.estimators_) def rf_m_depth(rf): &quot;&quot;&quot; return a median height of all the trees of the forrest. &quot;&quot;&quot; return np.median([est.tree_.max_depth for est in rf.estimators_]) . # print model oob_score, number of forrest leaves and median height n_leaves = rf_n_leaves(rf) m_depth = rf_m_depth(rf) print(f&quot;OOB scrore = {oob_score: .3f} nTree leaves = {n_leaves: ,d} nMedian depth = {m_depth}&quot;) . OOB scrore = 0.790 Tree leaves = 16,209,726 Median depth = 53.5 . Our baseline model has an OOB score of just around 79% which is not bad as a starter. Now let&#39;s also plot the feature importance for this model. . def plot_feature_importance(feature_importance, feature_names, figsize=(7,7)): &quot;&quot;&quot; plot the feature importances in a bar graph along with feature names. &quot;&quot;&quot; fimp = pd.Series(feature_importance, feature_names) fimp.nlargest(len(fimp)).plot(figsize=figsize, kind=&#39;barh&#39;).invert_yaxis() . feature_importance = rf.feature_importances_ feature_names = X.columns plot_feature_importance(feature_importance, feature_names) . From this feature importance plot, we can see that . ModelID is the highest predictor of SalePrice. This could be because vehicles belonging to a certain ModelID category could have their SalePrice in the same range. | SalesID and MachineID are coming up next as important features. This is not a good signal as both these features are unique for sale transactions and machine identification. MachineID also has inconsistencies as noted in this kaggle discussion. A model using these features will not be generalized. It would be better if we remove these features altogether otherwise they can affect the model&#39;s performance. | YearMade comes next which also makes sense as older vehicles will have less price compared to the new ones. | . Cleaning up . In this section we will remove unimportanct features and fix the data types of remaining features. . Remove ID columns . As noted in last section we noted that following ID features can be removed from the dataset. . SalesID | MachineID | . del df[&#39;SalesID&#39;] del df[&#39;MachineID&#39;] . Fix data types and data issues . Let&#39;s visit each feature from our dataset and check whether we need to fix the data type. Use df_info created in the last section to verify the data types of each feature. . Numerical features . Let&#39;s first visit the numerical feature. . auctioneerID . It has the datatype as float64 but this feature is actually categorical nominal as each ID is discrete and has no relation between them. It should be of type str. So let&#39;s fix that. . df[&#39;auctioneerID&#39;] = df[&#39;auctioneerID&#39;].astype(str) . Datetime feature . Let&#39;s visit DateTime features and correct their data type . saledate . &#39;saledate&#39; is a DateTime feature. So let&#39;s correct its data type. . df[&#39;saledate&#39;] = pd.to_datetime(df[&#39;saledate&#39;]) . Categorical features . Let&#39;s now visit the categorical features. . For categorical features, there is no better way than printing the unique values for each column and spend some time analyzing the values. Analyze . if the feature has some missing values | if there are any missing values but are represented by some other value like &#39;Unspecified&#39;, &#39;None or Unspecified&#39; | keep a separate sheet with all the features and make notes for each feature like there are no further actions required. The feature is good for use | need to replace missing values | any other observations | etc. | . | . Transform missing values . After visiting all the features we have found that missing values are represented in multiple ways like . Unspecified | None or Unspecified | None | #NAME? | &quot;&quot; | . So we would transform and replace all these values with np.nan so they all represent the same thing. . # before transformation. # let&#39;s use this feature to verify results. df[&#39;Hydraulics&#39;].unique() . array([&#39;2 Valve&#39;, &#39;Auxiliary&#39;, nan, &#39;Standard&#39;, &#39;Base + 1 Function&#39;, &#39;Base + 3 Function&#39;, &#39;4 Valve&#39;, &#39;3 Valve&#39;, &#39;Base + 2 Function&#39;, &#39;Base + 4 Function&#39;, &#39;None or Unspecified&#39;, &#39;Base + 5 Function&#39;, &#39;Base + 6 Function&#39;], dtype=object) . def normalize_str_values(df): &quot;&quot;&quot; normalize dataframe str values * transform case to lowercase * replace missing values with np.nan &quot;&quot;&quot; for col in df.columns: if df[col].dtype == object: print(f&quot;normalize column: {col}&quot;) df[col] = df[col].str.lower() df[col] = df[col].fillna(np.nan) df[col] = df[col].replace(&#39;unspecified&#39;, np.nan) df[col] = df[col].replace(&#39;none or unspecified&#39;, np.nan) df[col] = df[col].replace(&#39;none&#39;, np.nan) df[col] = df[col].replace(&#39;#name?&#39;, np.nan) df[col] = df[col].replace(&#39;&#39;, np.nan) normalize_str_values(df) . normalize column: auctioneerID normalize column: UsageBand normalize column: fiModelDesc normalize column: fiBaseModel normalize column: fiSecondaryDesc normalize column: fiModelSeries normalize column: fiModelDescriptor normalize column: ProductSize normalize column: fiProductClassDesc normalize column: state normalize column: ProductGroup normalize column: ProductGroupDesc normalize column: Drive_System normalize column: Enclosure normalize column: Forks normalize column: Pad_Type normalize column: Ride_Control normalize column: Stick normalize column: Transmission normalize column: Turbocharged normalize column: Blade_Extension normalize column: Blade_Width normalize column: Enclosure_Type normalize column: Engine_Horsepower normalize column: Hydraulics normalize column: Pushblock normalize column: Ripper normalize column: Scarifier normalize column: Tip_Control normalize column: Tire_Size normalize column: Coupler normalize column: Coupler_System normalize column: Grouser_Tracks normalize column: Hydraulics_Flow normalize column: Track_Type normalize column: Undercarriage_Pad_Width normalize column: Stick_Length normalize column: Thumb normalize column: Pattern_Changer normalize column: Grouser_Type normalize column: Backhoe_Mounting normalize column: Blade_Type normalize column: Travel_Controls normalize column: Differential_Type normalize column: Steering_Controls . . # after transformation. # remember that transformation is applied to all string type columns. We are using just one column to verify the results. df[&#39;Hydraulics&#39;].unique() . array([&#39;2 valve&#39;, &#39;auxiliary&#39;, nan, &#39;standard&#39;, &#39;base + 1 function&#39;, &#39;base + 3 function&#39;, &#39;4 valve&#39;, &#39;3 valve&#39;, &#39;base + 2 function&#39;, &#39;base + 4 function&#39;, &#39;base + 5 function&#39;, &#39;base + 6 function&#39;], dtype=object) . Transform measurements . Some features are represented as a string but actually they are numerical measurement values. For example . Tire_Size has the size in inches with a symbol attached &quot; | Undercarriage_Pad_Width has the size in inches with the unit attached inch | Blade_Width has the size in cm with a symbol attached &#39;. It also has values less the 12cm represented as &lt;12&#39; | Stick_Length has values in both feet and inches. We can simply convert them from 19 &#39;8&quot; to 19.8 | After the above transformations, their data types should be converted to numeric | . let&#39;s apply these changes to our dataset. . # before transformation for col in [&#39;Tire_Size&#39;, &#39;Undercarriage_Pad_Width&#39;, &#39;Blade_Width&#39;, &#39;Stick_Length&#39;]: print(f&quot;**{col}**: &quot;, df[col].unique()) . **Tire_Size**: [nan &#39;23.5&#39; &#39;13&#34;&#39; &#39;26.5&#39; &#39;29.5&#39; &#39;14&#34;&#39; &#39;20.5&#39; &#39;17.5&#34;&#39; &#39;15.5&#34;&#39; &#39;20.5&#34;&#39; &#39;17.5&#39; &#39;7.0&#34;&#39; &#39;15.5&#39; &#39;23.5&#34;&#39; &#39;10&#34;&#39; &#39;23.1&#34;&#39; &#39;10 inch&#39;] **Undercarriage_Pad_Width**: [nan &#39;16 inch&#39; &#39;32 inch&#39; &#39;28 inch&#39; &#39;30 inch&#39; &#39;22 inch&#39; &#39;24 inch&#39; &#39;18 inch&#39; &#39;36 inch&#39; &#39;20 inch&#39; &#39;27 inch&#39; &#39;15 inch&#39; &#39;26 inch&#39; &#39;34 inch&#39; &#39;33 inch&#39; &#39;14 inch&#39; &#39;31 inch&#39; &#39;25 inch&#39; &#39;31.5 inch&#39;] **Blade_Width**: [nan &#34;12&#39;&#34; &#34;14&#39;&#34; &#34;13&#39;&#34; &#34;16&#39;&#34; &#34;&lt;12&#39;&#34;] **Stick_Length**: [nan &#39;11 &#39; 0&#34;&#39; &#39;15 &#39; 9&#34;&#39; &#39;10 &#39; 2&#34;&#39; &#39;10 &#39; 6&#34;&#39; &#39;9 &#39; 10&#34;&#39; &#39;10 &#39; 10&#34;&#39; &#39;9 &#39; 6&#34;&#39; &#39;9 &#39; 7&#34;&#39; &#39;12 &#39; 8&#34;&#39; &#39;8 &#39; 2&#34;&#39; &#39;8 &#39; 6&#34;&#39; &#39;9 &#39; 8&#34;&#39; &#39;12 &#39; 10&#34;&#39; &#39;11 &#39; 10&#34;&#39; &#39;8 &#39; 10&#34;&#39; &#39;8 &#39; 4&#34;&#39; &#39;12 &#39; 4&#34;&#39; &#39;9 &#39; 5&#34;&#39; &#39;6 &#39; 3&#34;&#39; &#39;14 &#39; 1&#34;&#39; &#39;13 &#39; 7&#34;&#39; &#39;13 &#39; 10&#34;&#39; &#39;13 &#39; 9&#34;&#39; &#39;7 &#39; 10&#34;&#39; &#39;15 &#39; 4&#34;&#39; &#39;9 &#39; 2&#34;&#39; &#39;24 &#39; 3&#34;&#39; &#39;19 &#39; 8&#34;&#39;] . df[&#39;Stick_Length&#39;] = df[&#39;Stick_Length&#39;].replace(r&quot;&#39; &quot;, &quot;.&quot;, regex=True) for col in [&#39;Tire_Size&#39;, &#39;Undercarriage_Pad_Width&#39;, &#39;Blade_Width&#39;, &#39;Stick_Length&#39;]: df[col] = df[col].str.extract(r&#39;([0-9.]*)&#39;, expand=True) df[col] = df[col].replace(&#39;&#39;, np.nan) df[col] = pd.to_numeric(df[col]) . # after transformation for col in [&#39;Tire_Size&#39;, &#39;Undercarriage_Pad_Width&#39;, &#39;Blade_Width&#39;, &#39;Stick_Length&#39;]: print(f&quot;**{col}**: &quot;, df[col].unique()) . **Tire_Size**: [ nan 23.5 13. 26.5 29.5 14. 20.5 17.5 15.5 7. 10. 23.1] **Undercarriage_Pad_Width**: [ nan 16. 32. 28. 30. 22. 24. 18. 36. 20. 27. 15. 26. 34. 33. 14. 31. 25. 31.5] **Blade_Width**: [nan 12. 14. 13. 16.] **Stick_Length**: [ nan 11. 15.9 10.2 10.6 9.1 10.1 9.6 9.7 12.8 8.2 8.6 9.8 12.1 11.1 8.1 8.4 12.4 9.5 6.3 14.1 13.7 13.1 13.9 7.1 15.4 9.2 24.3 19.8] . Dealing with missing data . Replace missing numeric values . For numerical features, we will follow the following approach to replace missing values . For a column x create a new column x_na where x_na[i] is marked as True if x[i] is missing | Replace the missing values in the x column with a median value | . def fix_missing_num(df, colname): &quot;&quot;&quot; replace missing values with * median value * flag the missing value in a separate *_na column &quot;&quot;&quot; df[colname+&#39;_na&#39;] = pd.isnull(df[colname]) df[colname].fillna(df[colname].median(), inplace=True) . YearMade . &quot;YearMade&quot; doesn&#39;t show any missing values but if we look closely at the data we will find that some instances have the value &quot;1000&quot;. The year 1000 is very unlikely for any vehicle to be made in and we can consider these instances as missing values. Let&#39;s do that . # befor transformation df.plot.scatter(&#39;YearMade&#39;, &#39;SalePrice&#39;) . &lt;AxesSubplot:xlabel=&#39;YearMade&#39;, ylabel=&#39;SalePrice&#39;&gt; . df.loc[df.YearMade==1000, &#39;YearMade&#39;] = np.nan . # after transformation df.plot.scatter(&#39;YearMade&#39;, &#39;SalePrice&#39;) . &lt;AxesSubplot:xlabel=&#39;YearMade&#39;, ylabel=&#39;SalePrice&#39;&gt; . The plot now shows a more clear relationship between &#39;YearMade&#39; and &#39;SalePrice&#39;. But the spike in the year 1920 is still concerning. Most probably it is also a recording error when the manufacturing year was not known then it was assigned some lowest available value in the system (similar to the year 1000). Let&#39;s take this assumption that manufacturing years before 1950 are unknown and should be assigned np.nan . df.loc[df.YearMade&lt;1950, &#39;YearMade&#39;] = np.nan . # after transformation df.plot.scatter(&#39;YearMade&#39;, &#39;SalePrice&#39;) . &lt;AxesSubplot:xlabel=&#39;YearMade&#39;, ylabel=&#39;SalePrice&#39;&gt; . Let&#39;s also replace the missing values with the function created above. . fix_missing_num(df, &#39;YearMade&#39;) . MachineHoursCurrentMeter . The next numerical feature that comes is MachineHoursCurrentMeter. This feature tells us the number of hours a machine has been in use when it was brought to the auction. So older machines are much more likely to have more hours on them as compared to newer machines. There should be a correlation between machine hours and the vehicle in use period (a period between manufacturing and auction). To verify this relationship we first need to find the period in years between manufacturing and auction. We have the &#39;YearMade&#39; that tells us when the vehicle was made. We have the &#39;saledate&#39; which is a DateTime string object but we can use it to find the &#39;YearSold&#39;. . df[&#39;YearSold&#39;] = df[&#39;saledate&#39;].dt.year . # verify that we have correct data df[[&#39;saledate&#39;, &#39;YearSold&#39;]].head() . saledate YearSold . 0 2006-11-16 | 2006 | . 1 2004-03-26 | 2004 | . 2 2004-02-26 | 2004 | . 3 2011-05-19 | 2011 | . 4 2009-07-23 | 2009 | . Now we can use &#39;YearMade&#39; and &#39;YearSold&#39; to find the number of years the vehicle remained in use. Let&#39;s call this new column &#39;YearsInUse&#39; . df[&#39;YearsInUse&#39;] = df[&#39;YearSold&#39;] - df[&#39;YearMade&#39;] . # verify the results df[[&#39;YearsInUse&#39;, &#39;YearSold&#39;, &#39;YearMade&#39;]].head() . YearsInUse YearSold YearMade . 0 2.0 | 2006 | 2004.0 | . 1 8.0 | 2004 | 1996.0 | . 2 3.0 | 2004 | 2001.0 | . 3 10.0 | 2011 | 2001.0 | . 4 2.0 | 2009 | 2007.0 | . A sold year cannot be less than a manufacturing year. So let&#39;s verify data integrity as well. . df.loc[df.YearsInUse&lt;0, [&#39;YearsInUse&#39;, &#39;saledate&#39;, &#39;YearSold&#39;, &#39;YearMade&#39;]].head() . YearsInUse saledate YearSold YearMade . 24007 -2.0 | 1994-02-11 | 1994 | 1996.0 | . 24009 -1.0 | 1995-04-18 | 1995 | 1996.0 | . 24015 -2.0 | 1994-09-20 | 1994 | 1996.0 | . 24029 -1.0 | 1995-04-28 | 1995 | 1996.0 | . 24064 -1.0 | 1995-04-28 | 1995 | 1996.0 | . YearInUse cannot have a negative value and this shows that either &#39;YearMade&#39; or &#39;saledate&#39; is incorrect. We can assume that error can be with &#39;YearMade&#39; as this is an auction dataset and &#39;saledate&#39; will be more reliable. For entries where &#39;YearMade&#39; is greater than &#39;YearSold&#39; we can replace &#39;YearMade&#39; with &#39;YearSold&#39; (better to have &#39;YearsInUse&#39; equal to zero than negative). . df.loc[df.YearMade&gt;df.YearSold, &#39;YearMade&#39;] = df.YearSold . Let&#39;s recalculate the &#39;YearsInUse&#39; with corrected data. . df[&#39;YearsInUse&#39;] = df[&#39;YearSold&#39;] - df[&#39;YearMade&#39;] . Let&#39;s verify that the data is consistent and all vehicles have &#39;YearMade&#39; less than their &#39;YearSold&#39; . df.loc[df.YearsInUse&lt;0, [&#39;YearsInUse&#39;, &#39;saledate&#39;, &#39;YearSold&#39;, &#39;YearMade&#39;]].head() . YearsInUse saledate YearSold YearMade . We can now plot the relationship between &#39;YearsInUse&#39; and &#39;MachineHoursCurrentMeter&#39; . df.plot.scatter(&#39;YearsInUse&#39;, &#39;MachineHoursCurrentMeter&#39;) . &lt;AxesSubplot:xlabel=&#39;YearsInUse&#39;, ylabel=&#39;MachineHoursCurrentMeter&#39;&gt; . This plot shows that there is some relation between a vehicle being in use and its meter hours. As the &#39;YearsInUse&#39; value increases we also see an increase in meter hours, but after around 15 &#39;YearsInUse&#39; the relationship does not hold on and meter hours start dropping to zero. It means that MachineHoursCurrentMeter data has inconsistencies as many vehicles remained in use for multiple years but they also have zero meter readings. This is very unrealistic and vehicles will not be sitting idle for many years till their auction. It could be that the meter reading for them was not known and 0 could have been used for the &#39;Unspecified or Unknown&#39; value. . Let&#39;s take this assumption and transform &#39;MachineHoursCurrentMeter&#39; to correctly represent that . df.loc[df.MachineHoursCurrentMeter==0, &#39;MachineHoursCurrentMeter&#39;] = np.nan . Also apply our missing values fix on this feature . fix_missing_num(df, &#39;MachineHoursCurrentMeter&#39;) . Tire_Size . The next numerical feature is &#39;Tire_Size&#39;. We can plot the distribution of tire sizes to find any outliers. . df[&#39;Tire_Size&#39;].hist() . &lt;AxesSubplot:&gt; . # print tire sizes np.sort(df[&#39;Tire_Size&#39;].unique()) . array([ 7. , 10. , 13. , 14. , 15.5, 17.5, 20.5, 23.1, 23.5, 26.5, 29.5, nan]) . The plot does not show any outliers and data seems consistant, so we can apply our missing values fix on this feature. . fix_missing_num(df, &#39;Tire_Size&#39;) . Stick_Length . The Next numerical feature is &#39;Stick_Lenght&#39;. Let&#39;s plot the distribution to check for any outliers. . np.sort(df[&#39;Stick_Length&#39;].unique()) . array([ 6.3, 7.1, 8.1, 8.2, 8.4, 8.6, 9.1, 9.2, 9.5, 9.6, 9.7, 9.8, 10.1, 10.2, 10.6, 11. , 11.1, 12.1, 12.4, 12.8, 13.1, 13.7, 13.9, 14.1, 15.4, 15.9, 19.8, 24.3, nan]) . df[&#39;Stick_Length&#39;].plot.hist() . &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . The above plot shows a normal distribution and no outliers. So we can apply our missing values fix on this feature. . fix_missing_num(df, &#39;Stick_Length&#39;) . Undercarriage_Pad_Width . Next numerical feature is &#39;Undercarriage_Pad_Width&#39;. Let&#39;s follow the same steps for this feature. . np.sort(df[&#39;Undercarriage_Pad_Width&#39;].unique()) . array([14. , 15. , 16. , 18. , 20. , 22. , 24. , 25. , 26. , 27. , 28. , 30. , 31. , 31.5, 32. , 33. , 34. , 36. , nan]) . df[&#39;Undercarriage_Pad_Width&#39;].plot.hist() . &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . The distribution for this feature looks fine, and we can apply missing values fix on it. . fix_missing_num(df, &#39;Undercarriage_Pad_Width&#39;) . Blade_Width . Next numerical feature in &#39;Blade_Width&#39;. Following the same steps as before. . np.sort(df[&#39;Blade_Width&#39;].unique()) . array([12., 13., 14., 16., nan]) . df[&#39;Blade_Width&#39;].plot.hist() . &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . Apply the fix on this feature. . fix_missing_num(df, &#39;Blade_Width&#39;) . Replace missing categorical values . encoding and checking the importance We will now replace missing values for categorical features in the following way. . We will label encode them. We will treat them as ordinal features and assign them a numeric value | Missing values will automatically be assigned a value, and that will be 0 | . Some important discussion points on treating nominal categorical features as ordinal and then encoding them. A more prevalent approach is to one hot encode (OHE) them. The drawback of OHE approach is that it makes the decision trees very unbalanced if the dataset has multiple categorical features with high variance. So instead of applying OHE to all features, we will do it in a two-step approach. First, we will label encode them and train a model on them. After that, we will check their feature importance, and if a feature comes up as an important with a low variance then we will use OHE for it. Otherwise we will leave them with label encoding. . More can be read about categorical features encoding from these references . The Mechanics of Machine Learning by Terence Parr and Jeremy Howard section 6.2 | Getting Deeper into Categorical Encodings for Machine Learning | One-Hot Encoding is making your Tree-Based Ensembles worse, here’s why? | . Let&#39;s create some functions to encode our categorical features. . from pandas.api.types import is_categorical_dtype, is_string_dtype def df_string_to_cat(df): for col in df.columns: if is_string_dtype(df[col]): print(f&quot;label encoding applied on {col}&quot;) df[col] = df[col].astype(&#39;category&#39;).cat.as_ordered() def df_cat_to_catcode(df): for col in df.columns: if is_categorical_dtype(df[col]): df[col] = df[col].cat.codes + 1 . Please note that Pandas represents np.nan with category code &quot;-1&quot;, and so adding &quot;1&quot; in function df_cat_to_catcode shifts np.nan to 0 and all category codes to be 1 and above. . # before transformation df.head(5).T.head(10) . 0 1 2 3 4 . SalePrice 66000 | 57000 | 10000 | 38500 | 11000 | . ModelID 3157 | 77 | 7009 | 332 | 17311 | . datasource 121 | 121 | 121 | 121 | 121 | . auctioneerID 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | . YearMade 2004.0 | 1996.0 | 2001.0 | 2001.0 | 2007.0 | . MachineHoursCurrentMeter 68.0 | 4640.0 | 2838.0 | 3486.0 | 722.0 | . UsageBand low | low | high | high | medium | . saledate 2006-11-16 00:00:00 | 2004-03-26 00:00:00 | 2004-02-26 00:00:00 | 2011-05-19 00:00:00 | 2009-07-23 00:00:00 | . fiModelDesc 521d | 950fii | 226 | pc120-6e | s175 | . fiBaseModel 521 | 950 | 226 | pc120 | s175 | . # apply the cat transformation df_string_to_cat(df) df_cat_to_catcode(df) . label encoding applied on auctioneerID label encoding applied on UsageBand label encoding applied on fiModelDesc label encoding applied on fiBaseModel label encoding applied on fiSecondaryDesc label encoding applied on fiModelSeries label encoding applied on fiModelDescriptor label encoding applied on ProductSize label encoding applied on fiProductClassDesc label encoding applied on state label encoding applied on ProductGroup label encoding applied on ProductGroupDesc label encoding applied on Drive_System label encoding applied on Enclosure label encoding applied on Forks label encoding applied on Pad_Type label encoding applied on Ride_Control label encoding applied on Stick label encoding applied on Transmission label encoding applied on Turbocharged label encoding applied on Blade_Extension label encoding applied on Enclosure_Type label encoding applied on Engine_Horsepower label encoding applied on Hydraulics label encoding applied on Pushblock label encoding applied on Ripper label encoding applied on Scarifier label encoding applied on Tip_Control label encoding applied on Coupler label encoding applied on Coupler_System label encoding applied on Grouser_Tracks label encoding applied on Hydraulics_Flow label encoding applied on Track_Type label encoding applied on Thumb label encoding applied on Pattern_Changer label encoding applied on Grouser_Type label encoding applied on Backhoe_Mounting label encoding applied on Blade_Type label encoding applied on Travel_Controls label encoding applied on Differential_Type label encoding applied on Steering_Controls . . # after transformation df.head(5).T.head(10) . 0 1 2 3 4 . SalePrice 66000 | 57000 | 10000 | 38500 | 11000 | . ModelID 3157 | 77 | 7009 | 332 | 17311 | . datasource 121 | 121 | 121 | 121 | 121 | . auctioneerID 23 | 23 | 23 | 23 | 23 | . YearMade 2004.0 | 1996.0 | 2001.0 | 2001.0 | 2007.0 | . MachineHoursCurrentMeter 68.0 | 4640.0 | 2838.0 | 3486.0 | 722.0 | . UsageBand 2 | 2 | 1 | 1 | 3 | . saledate 2006-11-16 00:00:00 | 2004-03-26 00:00:00 | 2004-02-26 00:00:00 | 2011-05-19 00:00:00 | 2009-07-23 00:00:00 | . fiModelDesc 950 | 1725 | 331 | 3674 | 4208 | . fiBaseModel 296 | 527 | 110 | 1375 | 1529 | . Preprocessed dataset . At this point, all our numerical and categorical features have been preprocessed. There should be no missing values, and all categorical features should have been encoded. Only DateTime columns are remaining to be processed and we will do that in the next section. . Let&#39;s verify the data using summary information. . df_info = pd.DataFrame() df_info[&#39;sample&#39;] = df.iloc[0] df_info[&#39;data_type&#39;] = df.dtypes df_info[&#39;percent_missing&#39;] = 100*df.isnull().sum() / len(df) print(f&quot;Total features: {len(df.columns)}&quot;) df_info.sort_values(&#39;percent_missing&#39;) . Total features: 59 . sample data_type percent_missing . SalePrice 66000 | int64 | 0.0 | . Pushblock 0 | int8 | 0.0 | . Ripper 0 | int8 | 0.0 | . Scarifier 0 | int8 | 0.0 | . Tip_Control 0 | int8 | 0.0 | . Tire_Size 20.5 | float64 | 0.0 | . Coupler 0 | int8 | 0.0 | . Coupler_System 0 | int8 | 0.0 | . Grouser_Tracks 0 | int8 | 0.0 | . Hydraulics_Flow 0 | int8 | 0.0 | . Track_Type 0 | int8 | 0.0 | . Undercarriage_Pad_Width 28.0 | float64 | 0.0 | . Stick_Length 9.7 | float64 | 0.0 | . Hydraulics 1 | int8 | 0.0 | . Thumb 0 | int8 | 0.0 | . Grouser_Type 0 | int8 | 0.0 | . Backhoe_Mounting 0 | int8 | 0.0 | . Blade_Type 0 | int8 | 0.0 | . Travel_Controls 0 | int8 | 0.0 | . Differential_Type 4 | int8 | 0.0 | . Steering_Controls 2 | int8 | 0.0 | . YearMade_na False | bool | 0.0 | . YearSold 2006 | int64 | 0.0 | . YearsInUse 2.0 | float64 | 0.0 | . MachineHoursCurrentMeter_na False | bool | 0.0 | . Tire_Size_na True | bool | 0.0 | . Stick_Length_na True | bool | 0.0 | . Pattern_Changer 0 | int8 | 0.0 | . Undercarriage_Pad_Width_na True | bool | 0.0 | . Engine_Horsepower 0 | int8 | 0.0 | . Blade_Width 14.0 | float64 | 0.0 | . ModelID 3157 | int64 | 0.0 | . datasource 121 | int64 | 0.0 | . auctioneerID 23 | int8 | 0.0 | . YearMade 2004.0 | float64 | 0.0 | . MachineHoursCurrentMeter 68.0 | float64 | 0.0 | . UsageBand 2 | int8 | 0.0 | . saledate 2006-11-16 00:00:00 | datetime64[ns] | 0.0 | . fiModelDesc 950 | int16 | 0.0 | . fiBaseModel 296 | int16 | 0.0 | . fiSecondaryDesc 40 | int16 | 0.0 | . fiModelSeries 0 | int8 | 0.0 | . fiModelDescriptor 0 | int16 | 0.0 | . Enclosure_Type 0 | int8 | 0.0 | . ProductSize 0 | int8 | 0.0 | . state 1 | int8 | 0.0 | . ProductGroup 6 | int8 | 0.0 | . ProductGroupDesc 6 | int8 | 0.0 | . Drive_System 0 | int8 | 0.0 | . Enclosure 3 | int8 | 0.0 | . Forks 0 | int8 | 0.0 | . Pad_Type 0 | int8 | 0.0 | . Ride_Control 0 | int8 | 0.0 | . Stick 0 | int8 | 0.0 | . Transmission 0 | int8 | 0.0 | . Turbocharged 0 | int8 | 0.0 | . Blade_Extension 0 | int8 | 0.0 | . fiProductClassDesc 59 | int8 | 0.0 | . Blade_Width_na True | bool | 0.0 | . Let&#39;s retrain our base model one more time but this time with all the features except datetime columns to see where we stand in our OOB score. Below is a utility function created to quickly iterate over model training. . def train_and_plot_model(df, target=&#39;SalePrice&#39;, drop_features=[], n_estimators=70, plot=True, verbose=1): &quot;&quot;&quot; A utility function to train a RandomForrest model on the provided data, and plot the feature importances. Parameters - df: pandas.DataFrame input dataset to be used for training target: str target feature. this is the feature we are trying to predict drop_features: list any features to be dropped before training. Default is empty list. n_estimators: int number of estimators to be used for model training. Default is 50. &quot;&quot;&quot; # target = &#39;SalePrice&#39; # this is the feature we are trying to predict features = list(df.columns) # remove target feature and other specified features form the input variables features.remove(target) for f in drop_features: features.remove(f) X, y = df[features], df[target] rf = RandomForestRegressor(n_estimators, oob_score=True, n_jobs=-1, verbose=verbose) rf.fit(X, y) oob_score = rf.oob_score_ # get trained model leaves and depth n_leaves = rf_n_leaves(rf) m_depth = rf_m_depth(rf) # print trained model info print(f&quot;OOB scrore = {oob_score: .3f} nTree leaves = {n_leaves: ,d} nMedian depth = {m_depth}&quot;) # plot trained model feature importance feature_importance = rf.feature_importances_ if plot: plot_feature_importance(feature_importance, features, (10,15)) # return trained model, feature names, and their importances return (rf, features, feature_importance, oob_score) . # keeping n_estimators same as previous base model i.e. 70 (rf, feature_names, feature_importance, oob_pre) = train_and_plot_model(df, drop_features=[&#39;saledate&#39;], n_estimators=70) . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 2.7min [Parallel(n_jobs=-1)]: Done 70 out of 70 | elapsed: 4.2min finished . OOB scrore = 0.904 Tree leaves = 14,660,873 Median depth = 45.0 . This is a big improvement in our model performance. Our base model had 0.790 OOB score and now we are at 0.904. Our features count has also increased from 7 to 59, so we can take one more shot at it by increasing the estomators count (n_estimators). Let&#39;s use 150 trees this time (double that last time) to see how much effect it can have on model performance. . (rf, feature_names, feature_importance, oob_pre) = train_and_plot_model(df, drop_features=[&#39;saledate&#39;], n_estimators=150, plot=False) . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 2.8min [Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 9.2min finished . OOB scrore = 0.906 Tree leaves = 31,408,663 Median depth = 46.0 . Though there is only a slight increase in model performance but it took us significantly more time to train the model. So we will keep our estimators low and revisit them during the tuning phase. With &quot;70&quot; estimators our model performance is . OOB scrore = 0.904 Tree leaves = 14,660,873 Median depth = 45.0 . At this point, our features have correct data types and their missing values are properly adjusted. We can now focus on some feature engineering aspects. Before moving further let&#39;s also save our dataset till this point so if we make an error we can restart from this checkpoint. . # store preprocessed data as a check point for this state df.to_pickle(dataset_path+&#39;preprocessed.pkl&#39;) . We have used pickle format to preserve data types for saved data. . # load preprocessed data (optional step) # df = pd.read_pickle(dataset_path+&#39;preprocessed.pkl&#39;) # (rf, feature_names, feature_importance, oob_pre) = train_and_plot_model(df, drop_features=[&#39;saledate&#39;], n_estimators=70) . Feature Engineering . For feature engineering, we will give priority to important features. For this let us again analyze the preprocessed dataset starting from important features to see what can be done against each feature. . # sort the dataframe with important features at the start temp = pd.Series(feature_importance, feature_names) cols = temp.nlargest(len(temp)).index sniff(df[cols], 10) . ProductSize YearsInUse fiBaseModel fiSecondaryDesc YearMade fiProductClassDesc ModelID YearMade_na fiModelDesc Hydraulics_Flow YearSold state Enclosure Hydraulics auctioneerID fiModelSeries fiModelDescriptor MachineHoursCurrentMeter Transmission Pushblock Engine_Horsepower Ripper Drive_System datasource Blade_Type Stick_Length UsageBand Coupler Tire_Size Tire_Size_na Undercarriage_Pad_Width Thumb Grouser_Type Travel_Controls Track_Type Stick_Length_na Forks MachineHoursCurrentMeter_na Ride_Control Undercarriage_Pad_Width_na Tip_Control Differential_Type Pattern_Changer ProductGroup ProductGroupDesc Scarifier Enclosure_Type Stick Steering_Controls Blade_Width Blade_Width_na Pad_Type Blade_Extension Turbocharged Grouser_Tracks Coupler_System Backhoe_Mounting . 0 0 | 2.0 | 296 | 40 | 2004.0 | 59 | 3157 | False | 950 | 0 | 2006 | 1 | 3 | 1 | 23 | 0 | 0 | 68.0 | 0 | 0 | 0 | 0 | 0 | 121 | 0 | 9.7 | 2 | 0 | 20.5 | True | 28.0 | 0 | 0 | 0 | 0 | True | 0 | False | 0 | True | 0 | 4 | 0 | 6 | 6 | 0 | 0 | 0 | 2 | 14.0 | True | 0 | 0 | 0 | 0 | 0 | 0 | . 1 4 | 8.0 | 527 | 54 | 1996.0 | 62 | 77 | True | 1725 | 2 | 2004 | 33 | 5 | 4 | 2 | 97 | 65 | 4640.0 | 5 | 1 | 1 | 3 | 2 | 132 | 5 | 11.0 | 1 | 2 | 23.5 | False | 16.0 | 1 | 1 | 3 | 2 | False | 1 | True | 1 | False | 1 | 0 | 2 | 3 | 3 | 1 | 2 | 1 | 0 | 12.0 | False | 2 | 1 | 1 | 1 | 1 | 1 | . 2 6.0 | 3.0 | 110.0 | 0.0 | 2001.0 | 39.0 | 7009.0 | NaN | 331.0 | 1.0 | 2011.0 | 32.0 | 1.0 | 0.0 | 13.0 | 44.0 | 20.0 | 2838.0 | 6.0 | NaN | 2.0 | 2.0 | 4.0 | 136.0 | 6.0 | 15.9 | 3.0 | 1.0 | 13.0 | NaN | 32.0 | 2.0 | 3.0 | 5.0 | 1.0 | NaN | NaN | NaN | 2.0 | NaN | 2.0 | 1.0 | 1.0 | 4.0 | 4.0 | NaN | 1.0 | 2.0 | 1.0 | 13.0 | NaN | 3.0 | NaN | NaN | NaN | NaN | NaN | . 3 3.0 | 10.0 | 1375.0 | 56.0 | 2007.0 | 8.0 | 332.0 | NaN | 3674.0 | NaN | 2009.0 | 44.0 | 0.0 | 11.0 | 4.0 | 102.0 | 64.0 | 3486.0 | 4.0 | NaN | NaN | 1.0 | 3.0 | 149.0 | 9.0 | 10.2 | 0.0 | NaN | 26.5 | NaN | 30.0 | NaN | 2.0 | 4.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 3.0 | NaN | 1.0 | 1.0 | NaN | NaN | NaN | 3.0 | 16.0 | NaN | 1.0 | NaN | NaN | NaN | NaN | NaN | . 4 5.0 | 4.0 | 1529.0 | 47.0 | 1993.0 | 40.0 | 17311.0 | NaN | 4208.0 | NaN | 2008.0 | 3.0 | 2.0 | 5.0 | 24.0 | 33.0 | 83.0 | 722.0 | 3.0 | NaN | NaN | NaN | 1.0 | 172.0 | 7.0 | 10.6 | NaN | NaN | 29.5 | NaN | 22.0 | NaN | NaN | 2.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 2.0 | NaN | 5.0 | 5.0 | NaN | NaN | NaN | 5.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 5 2.0 | 11.0 | 175.0 | 61.0 | 2008.0 | 2.0 | 4605.0 | NaN | 493.0 | NaN | 2005.0 | 9.0 | 4.0 | 7.0 | 27.0 | 98.0 | 33.0 | 508.0 | 1.0 | NaN | NaN | NaN | NaN | NaN | 1.0 | 9.1 | NaN | NaN | 14.0 | NaN | 24.0 | NaN | NaN | 6.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 2.0 | 2.0 | NaN | NaN | NaN | 4.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 6 1.0 | 1.0 | 419.0 | 20.0 | 1998.0 | 14.0 | 1937.0 | NaN | 1453.0 | NaN | 2007.0 | 13.0 | NaN | 3.0 | 30.0 | 2.0 | 100.0 | 11540.0 | 2.0 | NaN | NaN | NaN | NaN | NaN | 4.0 | 10.1 | NaN | NaN | 17.5 | NaN | 18.0 | NaN | NaN | 1.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 7 NaN | 7.0 | 243.0 | 105.0 | 1999.0 | 17.0 | 3539.0 | NaN | 740.0 | NaN | 2010.0 | 37.0 | NaN | 2.0 | 26.0 | 73.0 | 128.0 | 4883.0 | NaN | NaN | NaN | NaN | NaN | NaN | 8.0 | 9.6 | NaN | NaN | 15.5 | NaN | 36.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 8 NaN | 5.0 | 250.0 | 133.0 | 2003.0 | 68.0 | 36003.0 | NaN | 779.0 | NaN | 2000.0 | 35.0 | NaN | 6.0 | 25.0 | 13.0 | 71.0 | 302.0 | NaN | NaN | NaN | NaN | NaN | NaN | 3.0 | 12.8 | NaN | NaN | 7.0 | NaN | 20.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 9 NaN | 14.0 | 540.0 | 129.0 | 1991.0 | 51.0 | 3883.0 | NaN | 1771.0 | NaN | 2002.0 | 4.0 | NaN | 8.0 | 11.0 | 54.0 | 122.0 | 20700.0 | NaN | NaN | NaN | NaN | NaN | NaN | 2.0 | 8.2 | NaN | NaN | 10.0 | NaN | 27.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . The above table is sorted based on the importance of each feature. Features at the start have more importance. So let&#39;s visit each feature to see if any feature engineering (FE) can be used to gain more insights from the data. . ProductSize, YearsInUse: These features have numbers. Not a candidate for FE. | fiBaseModel: It is label encoded. Let&#39;s visit this column&#39;s original values to see if any more features can be generated from it. | . df_raw[&#39;fiBaseModel&#39;].unique()[:50] . array([&#39;521D&#39;, &#39;950FII&#39;, &#39;226&#39;, &#39;PC120-6E&#39;, &#39;S175&#39;, &#39;310G&#39;, &#39;790ELC&#39;, &#39;416D&#39;, &#39;430HAG&#39;, &#39;988B&#39;, &#39;D31E&#39;, &#39;PC200LC6&#39;, &#39;420D&#39;, &#39;214E&#39;, &#39;310E&#39;, &#39;334&#39;, &#39;45NX&#39;, &#39;302.5&#39;, &#39;580SUPER K&#39;, &#39;JS260&#39;, &#39;120G&#39;, &#39;966FII&#39;, &#39;EX550STD&#39;, &#39;685B&#39;, &#39;345BL&#39;, &#39;330BL&#39;, &#39;873&#39;, &#39;WA250&#39;, &#39;750BLT&#39;, &#39;303CR&#39;, &#39;95ZII&#39;, &#39;416&#39;, &#39;303.5&#39;, &#39;CTL60&#39;, &#39;140G&#39;, &#39;307CSB&#39;, &#39;EC210LC&#39;, &#39;MF650&#39;, &#39;RC30&#39;, &#39;EX120-5&#39;, &#39;70XT&#39;, &#39;772A&#39;, &#39;160HNA&#39;, &#39;216&#39;, &#39;304CR&#39;, &#39;D3CIIIXL&#39;, &#39;236&#39;, &#39;120C&#39;, &#39;PC228&#39;, &#39;SK160LC&#39;], dtype=object) . fiBaseModel: original values look very random and do not give much information. There are two other columns in the importance list &#39;fiModelDesc&#39;, and &#39;fiSecondaryDesc&#39; and from their name they look related to &#39;fiBaseModel&#39;. So let&#39;s analyze them together. | . df_raw[[&#39;fiBaseModel&#39;, &#39;fiModelDesc&#39;, &#39;fiSecondaryDesc&#39;]].head(10) . fiBaseModel fiModelDesc fiSecondaryDesc . 0 521 | 521D | D | . 1 950 | 950FII | F | . 2 226 | 226 | NaN | . 3 PC120 | PC120-6E | NaN | . 4 S175 | S175 | NaN | . 5 310 | 310G | G | . 6 790 | 790ELC | E | . 7 416 | 416D | D | . 8 430 | 430HAG | HAG | . 9 988 | 988B | B | . fiBaseModel, fiModelDesc, fiSecondaryDesc: From the above table all these three features are very much related but their values are very random and do not give us much information. So let&#39;s leave them as it is. | YearMade: It has numbers. Not a candidate for FE. | fiProductClassDesc. This feature is also encode so let&#39;s visit it&#39;s original values. | . df_raw[&#39;fiProductClassDesc&#39;].unique()[:15] . array([&#39;Wheel Loader - 110.0 to 120.0 Horsepower&#39;, &#39;Wheel Loader - 150.0 to 175.0 Horsepower&#39;, &#39;Skid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity&#39;, &#39;Hydraulic Excavator, Track - 12.0 to 14.0 Metric Tons&#39;, &#39;Skid Steer Loader - 1601.0 to 1751.0 Lb Operating Capacity&#39;, &#39;Backhoe Loader - 14.0 to 15.0 Ft Standard Digging Depth&#39;, &#39;Hydraulic Excavator, Track - 21.0 to 24.0 Metric Tons&#39;, &#39;Hydraulic Excavator, Track - 3.0 to 4.0 Metric Tons&#39;, &#39;Wheel Loader - 350.0 to 500.0 Horsepower&#39;, &#39;Track Type Tractor, Dozer - 20.0 to 75.0 Horsepower&#39;, &#39;Hydraulic Excavator, Track - 19.0 to 21.0 Metric Tons&#39;, &#39;Hydraulic Excavator, Track - 4.0 to 5.0 Metric Tons&#39;, &#39;Hydraulic Excavator, Track - 2.0 to 3.0 Metric Tons&#39;, &#39;Hydraulic Excavator, Track - 24.0 to 28.0 Metric Tons&#39;, &#39;Motorgrader - 45.0 to 130.0 Horsepower&#39;], dtype=object) . fiProductClassDesc: This feature has text strings and it is also showing that they are not randow but has some pattern in then. They seems to be a good candidate for FE. We will do that in next section. | ModelID, YearSold: These features have numbers. Not a candidate for FE. | Hydraulics_Flow: It is encoded so let&#39;s visit original values first. | . df_raw[&#39;Hydraulics_Flow&#39;].unique() . array([nan, &#39;Standard&#39;, &#39;High Flow&#39;, &#39;None or Unspecified&#39;], dtype=object) . Hydraulics_Flow: We label encoded it and it came up as an important feature. Its values are showing low variance so it is a better candidate for one-hot encoding. We will do that in the next section. | state: It is encoded so let&#39;s visit its original values. | . print(f&quot;total unique values: {len(df_raw[&#39;state&#39;].unique())}&quot;) df_raw[&#39;state&#39;].unique()[:15] . total unique values: 53 . array([&#39;Alabama&#39;, &#39;North Carolina&#39;, &#39;New York&#39;, &#39;Texas&#39;, &#39;Arizona&#39;, &#39;Florida&#39;, &#39;Illinois&#39;, &#39;Oregon&#39;, &#39;Ohio&#39;, &#39;Arkansas&#39;, &#39;Wisconsin&#39;, &#39;Kansas&#39;, &#39;Nevada&#39;, &#39;Iowa&#39;, &#39;Maine&#39;], dtype=object) . state: By looking at original values we can see that it is a categorical nominal feature. It can be one hot encoded but since it has high variance (53 unique values) it is better to keep it as label encoded. So leave this feature as it is. | Enclosure: It is encoded. So let&#39;s check original values | . print(f&quot;total unique values: {len(df_raw[&#39;Enclosure&#39;].unique())}&quot;) df_raw[&#39;Enclosure&#39;].unique() . total unique values: 7 . array([&#39;EROPS w AC&#39;, &#39;OROPS&#39;, &#39;EROPS&#39;, nan, &#39;EROPS AC&#39;, &#39;NO ROPS&#39;, &#39;None or Unspecified&#39;], dtype=object) . Enclosure: Original values show that it is a categorical feature with good importance and low variance, so it is also suitable for OHE. | Hydraulics: It is also encoded. So let&#39;s check original values | . print(f&quot;total unique values: {len(df_raw[&#39;Hydraulics&#39;].unique())}&quot;) df_raw[&#39;Hydraulics&#39;].unique() . total unique values: 13 . array([&#39;2 Valve&#39;, &#39;Auxiliary&#39;, nan, &#39;Standard&#39;, &#39;Base + 1 Function&#39;, &#39;Base + 3 Function&#39;, &#39;4 Valve&#39;, &#39;3 Valve&#39;, &#39;Base + 2 Function&#39;, &#39;Base + 4 Function&#39;, &#39;None or Unspecified&#39;, &#39;Base + 5 Function&#39;, &#39;Base + 6 Function&#39;], dtype=object) . Hydraulics: Now this feature is again categorical, does not have high variance but also has low importance. We can consider it for OHE but since it is coming at the lower end feature importance, it will not have much impact on model performace. So we can skip it for OHE. | For the remaining features, importance is not significant enough to be considered for any FE. We can keep them as it is. | saledate: We did not use this feature in our last model training. But from the feature importance we can see that features that contain any date information are showing significant importance. So we should also include this feature in our next model. | . To summarize this section, the features that are suitable for any FE are . fiProductClassDesc | Hydraulics_Flow | Enclosure | saledate | . fiProductClassDesc . Let&#39;s check the original values for this feature one more time. . df_raw[&#39;fiProductClassDesc&#39;].head() . 0 Wheel Loader - 110.0 to 120.0 Horsepower 1 Wheel Loader - 150.0 to 175.0 Horsepower 2 Skid Steer Loader - 1351.0 to 1601.0 Lb Operat... 3 Hydraulic Excavator, Track - 12.0 to 14.0 Metr... 4 Skid Steer Loader - 1601.0 to 1751.0 Lb Operat... Name: fiProductClassDesc, dtype: object . Though this feature is named &#39;ProductClassDesc&#39; but by looking at its value we can see that besides class description there is also information on class specification. If we take the first value then . &#39;Wheel Loader&#39; -&gt; this is the class description | &#39;110.0 to 120.0 Horsepower&#39; -&gt; this is class specification | . and even in the class specification we have . 110 -&gt; spec lower limit | 120 -&gt; spec upper limit | &#39;Horsepower&#39; -&gt; spec unit | . Use this information to create new columns . # split the class description df_split = df_raw.fiProductClassDesc.str.split(&#39; - &#39;,expand=True).values . # on 0 index we have class description df_split[:,0] . array([&#39;Wheel Loader&#39;, &#39;Wheel Loader&#39;, &#39;Skid Steer Loader&#39;, ..., &#39;Hydraulic Excavator, Track&#39;, &#39;Hydraulic Excavator, Track&#39;, &#39;Hydraulic Excavator, Track&#39;], dtype=object) . # on 1 index we have class specification df_split[:,1] . array([&#39;110.0 to 120.0 Horsepower&#39;, &#39;150.0 to 175.0 Horsepower&#39;, &#39;1351.0 to 1601.0 Lb Operating Capacity&#39;, ..., &#39;3.0 to 4.0 Metric Tons&#39;, &#39;2.0 to 3.0 Metric Tons&#39;, &#39;2.0 to 3.0 Metric Tons&#39;], dtype=object) . # let&#39;s create two new columns for this df[&#39;fiProductClassDesc&#39;] = df_split[:,0] df[&#39;fiProductClassSpec&#39;] = df_split[:,1] . # split class spec further to get limits and units pattern = r&#39;([0-9. +]*)(?: to ([0-9. +]*)| +) ([a-zA-Z ]*)&#39; df_split = df[&#39;fiProductClassSpec&#39;].str.extract(pattern, expand=True).values df_split = pd.DataFrame(df_split, columns=[&#39;fiProductClassSpec_lower&#39;, &#39;fiProductClassSpec_upper&#39;, &#39;fiProductClassSpec_units&#39;]) df_split.head() . fiProductClassSpec_lower fiProductClassSpec_upper fiProductClassSpec_units . 0 110.0 | 120.0 | Horsepower | . 1 150.0 | 175.0 | Horsepower | . 2 1351.0 | 1601.0 | Lb Operating Capacity | . 3 12.0 | 14.0 | Metric Tons | . 4 1601.0 | 1751.0 | Lb Operating Capacity | . # merge new columns to our dataset df = pd.concat([df, df_split], axis=1) del df[&#39;fiProductClassSpec&#39;] # class spec is no more required. we have it&#39;s sub-features df.head() . SalePrice ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter UsageBand saledate fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Enclosure Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Hydraulics_Flow Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls YearMade_na YearSold YearsInUse MachineHoursCurrentMeter_na Tire_Size_na Stick_Length_na Undercarriage_Pad_Width_na Blade_Width_na fiProductClassSpec_lower fiProductClassSpec_upper fiProductClassSpec_units . 0 66000 | 3157 | 121 | 23 | 2004.0 | 68.0 | 2 | 2006-11-16 | 950 | 296 | 40 | 0 | 0 | 0 | Wheel Loader | 1 | 6 | 6 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2006 | 2.0 | False | True | True | True | True | 110.0 | 120.0 | Horsepower | . 1 57000 | 77 | 121 | 23 | 1996.0 | 4640.0 | 2 | 2004-03-26 | 1725 | 527 | 54 | 97 | 0 | 4 | Wheel Loader | 33 | 6 | 6 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 23.5 | 0 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2004 | 8.0 | False | False | True | True | True | 150.0 | 175.0 | Horsepower | . 2 10000 | 7009 | 121 | 23 | 2001.0 | 2838.0 | 1 | 2004-02-26 | 331 | 110 | 0 | 0 | 0 | 0 | Skid Steer Loader | 32 | 3 | 3 | 0 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 2 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2004 | 3.0 | False | True | True | True | True | 1351.0 | 1601.0 | Lb Operating Capacity | . 3 38500 | 332 | 121 | 23 | 2001.0 | 3486.0 | 1 | 2011-05-19 | 3674 | 1375 | 0 | 44 | 0 | 6 | Hydraulic Excavator, Track | 44 | 4 | 4 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2011 | 10.0 | False | True | True | True | True | 12.0 | 14.0 | Metric Tons | . 4 11000 | 17311 | 121 | 23 | 2007.0 | 722.0 | 3 | 2009-07-23 | 4208 | 1529 | 0 | 0 | 0 | 0 | Skid Steer Loader | 32 | 3 | 3 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 2 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2009 | 2.0 | False | True | True | True | True | 1601.0 | 1751.0 | Lb Operating Capacity | . # convert to numerical features df[&#39;fiProductClassSpec_lower&#39;] = pd.to_numeric(df[&#39;fiProductClassSpec_lower&#39;]) df[&#39;fiProductClassSpec_upper&#39;] = pd.to_numeric(df[&#39;fiProductClassSpec_upper&#39;]) # apply fix for numerical features fix_missing_num(df, &#39;fiProductClassSpec_lower&#39;) fix_missing_num(df, &#39;fiProductClassSpec_upper&#39;) # apply fix for categorical features df_string_to_cat(df) df_cat_to_catcode(df) . label encoding applied on fiProductClassDesc label encoding applied on fiProductClassSpec_units . (rf, feature_names, feature_importance, oob_hydralics) = train_and_plot_model(df, drop_features=[&#39;saledate&#39;]) . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 1.6min [Parallel(n_jobs=-1)]: Done 70 out of 70 | elapsed: 2.6min finished . OOB scrore = 0.905 Tree leaves = 14,650,747 Median depth = 47.0 . There is only a slight increase in OOB score but if we check the feature importance plot both fiProductClassSpec_upper and fiProductClassSpec_lower are showing high importance. We can take this as a positive signal for good features. . Hydralics_Flow . We need to apply one hot encoding (OHE) to this feature. Let&#39;s start by checking unique values for Hydraulics_Flow. . df[&#39;Hydraulics_Flow&#39;].value_counts() . 0 357788 2 42784 1 553 Name: Hydraulics_Flow, dtype: int64 . We have encoded this feature in the preprocessing section. Although we can use this encoded feature for one-hot encoding but we don&#39;t have original labels at this point. It would be better if we use original labels for OHE so that the dummy columns created as a result of that also have proper names with labels. Using encoded dummy column names makes them difficult to understand and follow. let&#39;s use the original dataframe to check the unique values. . df_raw[&#39;Hydraulics_Flow&#39;].value_counts(dropna=False) . NaN 357763 Standard 42784 High Flow 553 None or Unspecified 25 Name: Hydraulics_Flow, dtype: int64 . Before applying OHE we need to preprocess &#39;None or Unspecified&#39; as they repsent the same as np.nan. So let&#39;s do that. . # get the original values df[&#39;Hydraulics_Flow&#39;] = df_raw[&#39;Hydraulics_Flow&#39;] df[&#39;Hydraulics_Flow&#39;] = df[&#39;Hydraulics_Flow&#39;].replace(&#39;None or Unspecified&#39;, np.nan) df[&#39;Hydraulics_Flow&#39;].value_counts(dropna=False) . NaN 357788 Standard 42784 High Flow 553 Name: Hydraulics_Flow, dtype: int64 . Let&#39;s check the first few rows of this column. We will use them to verify our final result. . df[&#39;Hydraulics_Flow&#39;].head() . 0 NaN 1 NaN 2 Standard 3 NaN 4 Standard Name: Hydraulics_Flow, dtype: object . Notice that in the first five rows there are &#39;Standard&#39; values at row index 2 and 4, and the remaining are &#39;NaN&#39; values. We will OHE them in the next step and compare the results to ensure encoding is properly working. . from sklearn.preprocessing import OneHotEncoder onehot_encoder = OneHotEncoder() onehot_output = onehot_encoder.fit_transform(df[[&#39;Hydraulics_Flow&#39;]]) # check the output print(onehot_output[:5].toarray()) . [[0. 0. 1.] [0. 0. 1.] [0. 1. 0.] [0. 0. 1.] [0. 1. 0.]] . These are same five rows but this time encoded with one-hot values. From the position of &#39;1&#39; appearing in different columns we can deduce that first column is for label &#39;High Flow&#39; and second is for &#39;Standard&#39; and third is for &#39;NaN&#39;. It would be easier for us to track these dummy columns if we have proper names on them. So let&#39;s do that. . We can get the dummy column names by calling get_feature_names_out() on our encoder. . onehot_encoder.get_feature_names_out() . array([&#39;Hydraulics_Flow_High Flow&#39;, &#39;Hydraulics_Flow_Standard&#39;, &#39;Hydraulics_Flow_nan&#39;], dtype=object) . To create a dataframe of these dummy variables. . df_onehot = pd.DataFrame(onehot_output.toarray(), columns=onehot_encoder.get_feature_names_out()) df_onehot.head() . Hydraulics_Flow_High Flow Hydraulics_Flow_Standard Hydraulics_Flow_nan . 0 0.0 | 0.0 | 1.0 | . 1 0.0 | 0.0 | 1.0 | . 2 0.0 | 1.0 | 0.0 | . 3 0.0 | 0.0 | 1.0 | . 4 0.0 | 1.0 | 0.0 | . At this point Hydraulics_Flow is OHE so we can drop the original column from the dataset and add these encoded columns. . del df[&#39;Hydraulics_Flow&#39;] df = pd.concat([df, df_onehot], axis=1) # concat dataframes column wise df.head() . SalePrice ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter UsageBand saledate fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Enclosure Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls YearMade_na YearSold YearsInUse MachineHoursCurrentMeter_na Tire_Size_na Stick_Length_na Undercarriage_Pad_Width_na Blade_Width_na fiProductClassSpec_lower fiProductClassSpec_upper fiProductClassSpec_units fiProductClassSpec_lower_na fiProductClassSpec_upper_na Hydraulics_Flow_High Flow Hydraulics_Flow_Standard Hydraulics_Flow_nan . 0 66000 | 3157 | 121 | 23 | 2004.0 | 68.0 | 2 | 2006-11-16 | 950 | 296 | 40 | 0 | 0 | 0 | 6 | 1 | 6 | 6 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2006 | 2.0 | False | True | True | True | True | 110.0 | 120.0 | 2 | False | False | 0.0 | 0.0 | 1.0 | . 1 57000 | 77 | 121 | 23 | 1996.0 | 4640.0 | 2 | 2004-03-26 | 1725 | 527 | 54 | 97 | 0 | 4 | 6 | 33 | 6 | 6 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 23.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2004 | 8.0 | False | False | True | True | True | 150.0 | 175.0 | 2 | False | False | 0.0 | 0.0 | 1.0 | . 2 10000 | 7009 | 121 | 23 | 2001.0 | 2838.0 | 1 | 2004-02-26 | 331 | 110 | 0 | 0 | 0 | 0 | 4 | 32 | 3 | 3 | 0 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2004 | 3.0 | False | True | True | True | True | 1351.0 | 1601.0 | 3 | False | False | 0.0 | 1.0 | 0.0 | . 3 38500 | 332 | 121 | 23 | 2001.0 | 3486.0 | 1 | 2011-05-19 | 3674 | 1375 | 0 | 44 | 0 | 6 | 2 | 44 | 4 | 4 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2011 | 10.0 | False | True | True | True | True | 12.0 | 14.0 | 4 | False | False | 0.0 | 0.0 | 1.0 | . 4 11000 | 17311 | 121 | 23 | 2007.0 | 722.0 | 3 | 2009-07-23 | 4208 | 1529 | 0 | 0 | 0 | 0 | 4 | 32 | 3 | 3 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2009 | 2.0 | False | True | True | True | True | 1601.0 | 1751.0 | 3 | False | False | 0.0 | 1.0 | 0.0 | . Let&#39;s retain our model to check if there is any affect on model performance. . (rf, feature_names, feature_importance, oob_hydralics) = train_and_plot_model(df, drop_features=[&#39;saledate&#39;]) . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 1.6min [Parallel(n_jobs=-1)]: Done 70 out of 70 | elapsed: 2.6min finished . OOB scrore = 0.905 Tree leaves = 14,650,376 Median depth = 48.0 . There is no effect on the model performance but one feature &#39;Hydraulics_Flow_nan&#39; is showing some importance on the plot. The remaining features (&#39;Hydraulics_Flow_High Flow&#39; and &#39;Hydraulics_Flow_Standard&#39;) do not affect the model&#39;s performance. If it was not of &#39;Hydraulics_Flow_nan&#39; importance we could have skipped OHE for &#39;Hydraulics_Flow&#39;. . Enclosure . Next feature is Enclosure, and we will follow the same steps as for last feature to one-hot encode it. . # check value counts df_raw[&#39;Enclosure&#39;].value_counts(dropna=False) . OROPS 173932 EROPS 139026 EROPS w AC 87820 NaN 325 EROPS AC 17 NO ROPS 3 None or Unspecified 2 Name: Enclosure, dtype: int64 . Here ROPS is an abbreviation for Roll Over Protection System and there are multiple variants of this standard . OROPS = Open ROPS | EROPS = Enclosed ROPS | EROPS AC = Enclosed ROPS with Air Conditioning | EROPS w AC = Enclosed ROPS with Air Conditioning. Same as &#39;EROPS AC&#39; | NO ROPS = No ROPS. Same as &#39;NaN&#39; or &#39;None or Unspecified&#39; | . You can read more about ROPS standards here . http://www.miningrops.com.au/ropsintro.html | https://www.youtube.com/watch?v=LZ40O1My8E4&amp;ab_channel=MissouriEarthMovers | . Using this information we can also preprocess this feature to make its values more consistent. . # get the original values df[&#39;Enclosure&#39;] = df_raw[&#39;Enclosure&#39;] # change &#39;None or Unspecified&#39; and &#39;NO ROPS&#39; to np.nan df[&#39;Enclosure&#39;] = df[&#39;Enclosure&#39;].replace(&#39;None or Unspecified&#39;, np.nan) df[&#39;Enclosure&#39;] = df[&#39;Enclosure&#39;].replace(&#39;NO ROPS&#39;, np.nan) # change &#39;EROPS w AC&#39; to &#39;EROPS AC&#39; df[&#39;Enclosure&#39;] = df[&#39;Enclosure&#39;].replace(&#39;EROPS w AC&#39;, &#39;EROPS AC&#39;) df[&#39;Enclosure&#39;].value_counts(dropna=False) . OROPS 173932 EROPS 139026 EROPS AC 87837 NaN 330 Name: Enclosure, dtype: int64 . # before OHE df.head() . SalePrice ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter UsageBand saledate fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Enclosure Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls YearMade_na YearSold YearsInUse MachineHoursCurrentMeter_na Tire_Size_na Stick_Length_na Undercarriage_Pad_Width_na Blade_Width_na fiProductClassSpec_lower fiProductClassSpec_upper fiProductClassSpec_units fiProductClassSpec_lower_na fiProductClassSpec_upper_na Hydraulics_Flow_High Flow Hydraulics_Flow_Standard Hydraulics_Flow_nan . 0 66000 | 3157 | 121 | 23 | 2004.0 | 68.0 | 2 | 2006-11-16 | 950 | 296 | 40 | 0 | 0 | 0 | 6 | 1 | 6 | 6 | 0 | EROPS AC | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2006 | 2.0 | False | True | True | True | True | 110.0 | 120.0 | 2 | False | False | 0.0 | 0.0 | 1.0 | . 1 57000 | 77 | 121 | 23 | 1996.0 | 4640.0 | 2 | 2004-03-26 | 1725 | 527 | 54 | 97 | 0 | 4 | 6 | 33 | 6 | 6 | 0 | EROPS AC | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 23.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2004 | 8.0 | False | False | True | True | True | 150.0 | 175.0 | 2 | False | False | 0.0 | 0.0 | 1.0 | . 2 10000 | 7009 | 121 | 23 | 2001.0 | 2838.0 | 1 | 2004-02-26 | 331 | 110 | 0 | 0 | 0 | 0 | 4 | 32 | 3 | 3 | 0 | OROPS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2004 | 3.0 | False | True | True | True | True | 1351.0 | 1601.0 | 3 | False | False | 0.0 | 1.0 | 0.0 | . 3 38500 | 332 | 121 | 23 | 2001.0 | 3486.0 | 1 | 2011-05-19 | 3674 | 1375 | 0 | 44 | 0 | 6 | 2 | 44 | 4 | 4 | 0 | EROPS AC | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2011 | 10.0 | False | True | True | True | True | 12.0 | 14.0 | 4 | False | False | 0.0 | 0.0 | 1.0 | . 4 11000 | 17311 | 121 | 23 | 2007.0 | 722.0 | 3 | 2009-07-23 | 4208 | 1529 | 0 | 0 | 0 | 0 | 4 | 32 | 3 | 3 | 0 | EROPS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2009 | 2.0 | False | True | True | True | True | 1601.0 | 1751.0 | 3 | False | False | 0.0 | 1.0 | 0.0 | . # one hot encode &#39;Enclosure&#39; onehot_encoder = OneHotEncoder() onehot_output = onehot_encoder.fit_transform(df[[&#39;Enclosure&#39;]]) df_onehot = pd.DataFrame(onehot_output.toarray(), columns=onehot_encoder.get_feature_names_out()) df_onehot.head() . Enclosure_EROPS Enclosure_EROPS AC Enclosure_OROPS Enclosure_nan . 0 0.0 | 1.0 | 0.0 | 0.0 | . 1 0.0 | 1.0 | 0.0 | 0.0 | . 2 0.0 | 0.0 | 1.0 | 0.0 | . 3 0.0 | 1.0 | 0.0 | 0.0 | . 4 1.0 | 0.0 | 0.0 | 0.0 | . # drop original column del df[&#39;Enclosure&#39;] # add dummy columns to the dataframe df = pd.concat([df, df_onehot], axis=1) # concat dataframes column wise # after OHE df.head() . SalePrice ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter UsageBand saledate fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls YearMade_na YearSold YearsInUse MachineHoursCurrentMeter_na Tire_Size_na Stick_Length_na Undercarriage_Pad_Width_na Blade_Width_na fiProductClassSpec_lower fiProductClassSpec_upper fiProductClassSpec_units fiProductClassSpec_lower_na fiProductClassSpec_upper_na Hydraulics_Flow_High Flow Hydraulics_Flow_Standard Hydraulics_Flow_nan Enclosure_EROPS Enclosure_EROPS AC Enclosure_OROPS Enclosure_nan . 0 66000 | 3157 | 121 | 23 | 2004.0 | 68.0 | 2 | 2006-11-16 | 950 | 296 | 40 | 0 | 0 | 0 | 6 | 1 | 6 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2006 | 2.0 | False | True | True | True | True | 110.0 | 120.0 | 2 | False | False | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 1 57000 | 77 | 121 | 23 | 1996.0 | 4640.0 | 2 | 2004-03-26 | 1725 | 527 | 54 | 97 | 0 | 4 | 6 | 33 | 6 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 23.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2004 | 8.0 | False | False | True | True | True | 150.0 | 175.0 | 2 | False | False | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 2 10000 | 7009 | 121 | 23 | 2001.0 | 2838.0 | 1 | 2004-02-26 | 331 | 110 | 0 | 0 | 0 | 0 | 4 | 32 | 3 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2004 | 3.0 | False | True | True | True | True | 1351.0 | 1601.0 | 3 | False | False | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | . 3 38500 | 332 | 121 | 23 | 2001.0 | 3486.0 | 1 | 2011-05-19 | 3674 | 1375 | 0 | 44 | 0 | 6 | 2 | 44 | 4 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2011 | 10.0 | False | True | True | True | True | 12.0 | 14.0 | 4 | False | False | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 4 11000 | 17311 | 121 | 23 | 2007.0 | 722.0 | 3 | 2009-07-23 | 4208 | 1529 | 0 | 0 | 0 | 0 | 4 | 32 | 3 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2009 | 2.0 | False | True | True | True | True | 1601.0 | 1751.0 | 3 | False | False | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | . Let&#39;s retain our model to check if there is any affect on model performance. . (rf, feature_names, feature_importance, oob_enclosure) = train_and_plot_model(df, drop_features=[&#39;saledate&#39;]) . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 2.0min [Parallel(n_jobs=-1)]: Done 70 out of 70 | elapsed: 3.6min finished . OOB scrore = 0.902 Tree leaves = 14,668,686 Median depth = 45.0 . There is a slight decrease in model performance but one new feature &#39;Enclosure_EROPS AC&#39; is showing very high on the importance plot. . saledate . We have already created &#39;yearsold&#39; feature. We can more consequent features from &#39;saledate&#39;. . df[&quot;salemonth&quot;] = df[&#39;saledate&#39;].dt.month df[&quot;saleday&quot;] = df[&#39;saledate&#39;].dt.day df[&quot;saledayofweek&quot;] = df[&#39;saledate&#39;].dt.dayofweek df[&quot;saledayofyear&quot;] = df[&#39;saledate&#39;].dt.dayofyear # we can drop the orignal del df[&#39;saledate&#39;] . df.head() . SalePrice ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter UsageBand fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls YearMade_na YearSold YearsInUse MachineHoursCurrentMeter_na Tire_Size_na Stick_Length_na Undercarriage_Pad_Width_na Blade_Width_na fiProductClassSpec_lower fiProductClassSpec_upper fiProductClassSpec_units fiProductClassSpec_lower_na fiProductClassSpec_upper_na Hydraulics_Flow_High Flow Hydraulics_Flow_Standard Hydraulics_Flow_nan Enclosure_EROPS Enclosure_EROPS AC Enclosure_OROPS Enclosure_nan salemonth saleday saledayofweek saledayofyear . 0 66000 | 3157 | 121 | 23 | 2004.0 | 68.0 | 2 | 950 | 296 | 40 | 0 | 0 | 0 | 6 | 1 | 6 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2006 | 2.0 | False | True | True | True | True | 110.0 | 120.0 | 2 | False | False | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 11 | 16 | 3 | 320 | . 1 57000 | 77 | 121 | 23 | 1996.0 | 4640.0 | 2 | 1725 | 527 | 54 | 97 | 0 | 4 | 6 | 33 | 6 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 23.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | False | 2004 | 8.0 | False | False | True | True | True | 150.0 | 175.0 | 2 | False | False | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 3 | 26 | 4 | 86 | . 2 10000 | 7009 | 121 | 23 | 2001.0 | 2838.0 | 1 | 331 | 110 | 0 | 0 | 0 | 0 | 4 | 32 | 3 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2004 | 3.0 | False | True | True | True | True | 1351.0 | 1601.0 | 3 | False | False | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 2 | 26 | 3 | 57 | . 3 38500 | 332 | 121 | 23 | 2001.0 | 3486.0 | 1 | 3674 | 1375 | 0 | 44 | 0 | 6 | 2 | 44 | 4 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2011 | 10.0 | False | True | True | True | True | 12.0 | 14.0 | 4 | False | False | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 5 | 19 | 3 | 139 | . 4 11000 | 17311 | 121 | 23 | 2007.0 | 722.0 | 3 | 4208 | 1529 | 0 | 0 | 0 | 0 | 4 | 32 | 3 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14.0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 20.5 | 0 | 0 | 0 | 0 | 28.0 | 9.7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | False | 2009 | 2.0 | False | True | True | True | True | 1601.0 | 1751.0 | 3 | False | False | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 7 | 23 | 3 | 204 | . (rf, feature_names, feature_importance, oob_date) = train_and_plot_model(df) . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 2.4min [Parallel(n_jobs=-1)]: Done 70 out of 70 | elapsed: 3.9min finished . OOB scrore = 0.907 Tree leaves = 14,493,456 Median depth = 45.0 . There is an increase in model performance and multiple newly created date features are showing good importance on the plot. .",
            "url": "https://hassaanbinaslam.github.io/myblog/ml/2022/04/25/bluebook-for-bulldozers.html",
            "relUrl": "/ml/2022/04/25/bluebook-for-bulldozers.html",
            "date": " • Apr 25, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Docker - Send Container Logs to AWS CloudWatch",
            "content": ". About . This post is about configuring docker container to send application logs to Amazon CloudWatch. Logs entries can be retrieved from AWS Management Console. . Environment Details . Python = 3.8.x | Docker version = 20.10.7 | OS = Amazon Linux 2 | . iamadmin:~/environment $ docker version Client: Version: 20.10.7 API version: 1.41 Go version: go1.15.14 Git commit: f0df350 Built: Wed Nov 17 03:05:36 2021 OS/Arch: linux/amd64 Context: default Experimental: true Server: Engine: Version: 20.10.7 API version: 1.41 (minimum version 1.12) Go version: go1.15.14 Git commit: b0f5bc3 Built: Wed Nov 17 03:06:14 2021 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.4.6 GitCommit: d71fcd7d8303cbf684402823e425e9dd2e99285d runc: Version: 1.0.0 GitCommit: 84113eef6fc27af1b01b3181f31bbaf708715301 docker-init: Version: 0.19.0 GitCommit: de40ad0 iamadmin:~/environment $ cat /etc/os-release NAME=&quot;Amazon Linux&quot; VERSION=&quot;2&quot; ID=&quot;amzn&quot; ID_LIKE=&quot;centos rhel fedora&quot; VERSION_ID=&quot;2&quot; PRETTY_NAME=&quot;Amazon Linux 2&quot; ANSI_COLOR=&quot;0;33&quot; CPE_NAME=&quot;cpe:2.3:o:amazon:amazon_linux:2&quot; HOME_URL=&quot;https://amazonlinux.com/&quot; . Sample Application . Let us create a simple hello world application that will print &quot;hello world&quot; message to stdout. After each message the application sleeps for 5 seconds, and keeps on doing this for 5 mins (300 sec). After this the program exists. . Project structure of this application is . app/ └── src/ └── hello.py . Where . app/ is the project root folder | src/ folder contain the python application code | src/hello.py is the main application | . Code files are provided below . # app/src/hello.py from datetime import datetime import time def main(): # run for about 5 min: 300 sec for i in range(60): now = datetime.now() dt_string = now.strftime(&quot;%d/%m/%Y %H:%M:%S&quot;) # prepare message msg = f&quot;hello world at {dt_string}&quot; # put message to stdout and logs print(msg) # sleep for some seconds time.sleep(5) if __name__ == &quot;__main__&quot;: main() . When I run the hello.py file I get the output on the termial with hello world messages like this. . . Dockerize the application . Let&#39;s put it inside a docker container. For this let&#39;s create a Dockerfile and place it in app/ folder. . # app/Dockerfile FROM python:3.8-slim-buster # set the working directory in the container WORKDIR /app # copy the content of the local src directory to the working directory COPY src/ . # command to run on container start CMD [ &quot;python3&quot;, &quot;-u&quot;, &quot;./hello.py&quot;] . We can build our docker image by running the command from terminal at folder app/ . docker build --tag python-docker . . Output of this command will look like this . We can check the created docker image using command from terminal . docker images . Output of this command will look like this . So our docker image is ready, we can now run it using command . docker run python-docker . After running this command you will see the application logs on the terminal. . Get AWS Credentials . Now that we have our sample application and it&#39;s docker container ready, we can work on pushing the docker logs to AWS CloudWatch. For this we need access credentials to AWS account where we want our logs to be available. We will create a separate account in AWS with CloudWatch access and use it&#39;s credentials with docker daemon. Our steps will be . Create IAM policy with CloudWatch access | Create IAM group with that policy | Create IAM user and add that to this group | . Create IAM Policy . From AWS Console go to IAM Console | Select Policies, and click &#39;Create Policy&#39; | From Create Policy window, select Service = CloudWatch Logs | Actions = CreateLogStream, GetLogRecord, DescribeLogGroups, DescribeLogStreams, GetLogEvents, CreateLogGroup, PutLogEvents | Resources = All | . | . After giving required permissions, policy summary will be like . { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Sid&quot;: &quot;DockerContainerLogs&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;logs:CreateLogStream&quot;, &quot;logs:GetLogRecord&quot;, &quot;logs:DescribeLogGroups&quot;, &quot;logs:DescribeLogStreams&quot;, &quot;logs:GetLogEvents&quot;, &quot;logs:CreateLogGroup&quot;, &quot;logs:PutLogEvents&quot; ], &quot;Resource&quot;: &quot;*&quot; } ] } . Create IAM Group and User . From IAM console create a new IAM group and give it some appropriate name &#39;docker-logs-group&#39; | Attach the above created policy to that group | From the console create a new IAM user with &quot;Access key - Programmatic access&quot;. Give it some appropriate name &#39;docker-logs-user&#39; | Store access key ID and secret access key | Add the user to the group created in last step | . Configure AWS credentials for docker daemon . To configure docker daemon to use AWS access credentials, execute command from the terminal sudo systemctl edit docker. A new window will open for text to edit, and add the following lines to it. Replace my-aws-access-key and my-secret-access-key with your access keys. . [Service] Environment=&quot;AWS_ACCESS_KEY_ID=my-aws-access-key&quot; Environment=&quot;AWS_SECRET_ACCESS_KEY=my-secret-access-key&quot; . This command will update the credentials in file /etc/systemd/system/docker.service.d/override.conf. Verify it using command . $ cat /etc/systemd/system/docker.service.d/override.conf [Service] Environment=&quot;AWS_ACCESS_KEY_ID=AKIA3VIXXJNKPUSIOR3Y&quot; Environment=&quot;AWS_SECRET_ACCESS_KEY=XhjlKVkZm1XdXedjgBcfLVM3FBU6zkGU&quot; . After making changes to Docker daemon we need to restart it. For this . Flush the change with command sudo systemctl daemon-reload | Restart the docker daemon with command sudo systemctl restart docker | . Run docker container with awslogs driver . We can now run the docker image with awslogs driver using command . docker run --log-driver=awslogs --log-opt awslogs-region=us-east-1 --log-opt awslogs-group=myLogGroup --log-opt awslogs-create-group=true python-docker . log-driver configures the driver to be used for logs. Default driver is &#39;json-file&#39; and awslogs is for CloudWatch | awslogs-region specifies the region for AWS CloudWatch logs | awslogs-group specifies the log group for CloudWatch | awslogs-create-group specifes that if provided log group does not exists on CloudWatch then create one | . . Verify Logs from CloudWatch . Go to CloudWatch console and select Log Groups and then myLogGroup. You will find the logs generated by docker container. . . All the code used for this post can be obtained from the GitHub repository hassaanbinaslam/2022-04-11-docker-logs-cloudwatch . Project code files | Project zip file | . Error Messages . If docker daemon is not able to find AWS credentails then it will generate an error message similar to pasted below . docker: Error response from daemon: failed to initialize logging driver: failed to create Cloudwatch log stream: NoCredentialProviders: no valid providers in chain. Deprecated. For verbose messaging see aws.Config.CredentialsChainVerboseErrors. . If you get this message then you need to recheck the credentails passed to docker daemon. . One thing I noticed is that on Windows there is no way to pass AWS credentials to docker daemon. People have reported similar issues with docker running on MAC OS. Refer to below link for this discussion . https://github.com/docker/for-win/issues/9684 | . Other method to provide AWS credentials to docker daemon . Docker documentation mentions that AWS credentails can also be set . By configuring the environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. I have tried this approach but docker daemon is not able to pick AWS credentials from environment variables | By using AWS credentials file ~/.aws/credentials. I have also tried this approach and it does not work either | . Important References . Docker configuring logging drivers | Amazon CloudWatch Logs logging driver | https://transang.me/configure-docker-to-send-log-to-aws/ | .",
            "url": "https://hassaanbinaslam.github.io/myblog/docker/python/aws/cloudwatch/2022/04/11/docker-logs-cloudwatch.html",
            "relUrl": "/docker/python/aws/cloudwatch/2022/04/11/docker-logs-cloudwatch.html",
            "date": " • Apr 11, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Machine Learning Nomenclature",
            "content": ". About . This post is a collection of commonly used machine learning (ML) terminologies. . Dataset . The data we use in ML is usually defined as dataset, and datasets are a collection of data. The dataset contains the features and target to predict. . It has other names . data | input data | train and test data | . Instance . An instance is a row in the dataset. . Is has other names . row | observation | sample | (data) point | . Feature . Feature is a column in the dataset. It is used as an input used for prediction or classification. Features are commonly represented by x variable. . It has other names . column | attribute | (input) variable | . Features are of two types . Categorical or qualitative | Numerical or quantitative | . Target . It is the information a machine learning algorithm learns to predict. Target is commonly represented by y variable. . It has other names . label | output | . Labeled Data . A data that has both the feature and target attributes defined . Unlabeled Data . A data that has the features defined but has no target attribute. . Categorical Feature . A feature that is not measureable and has discrete set of values like gender, family retionships, movie categories etc. We commonly use bar charts and pie graphs for categorical features. . It has other names . qualitative feature | . Categorical features are of two types . Nominal | Ordinal | . Nominal feature . Nominal (categorical) feature is one that can not be measured and has no order assgined to it e.g. eye colors, gender etc. . Ordinal feature . Ordinal (categorical) feature is one that can not be measured but has some order assgined to it like movie ratings, military ranks etc. . Numerical feature . Numerical features are those that can be measured or counted and have some ascending or descending order assigned to them. . It has other names . Continious feature | Quantitative feature | . Numerical features can be of two types . Discrete | Continous | . Discrete feature . Discrete (numerical) feature is one that has specified values and are usually counted e.g. number of facebook like, number of tickets sold etc. . Continous feature . Continous (numerical) feature is one that can have any value assigned to it, and is usually measured e.g. temperature, wind speed etc. . Data ├── Categorical / Qualitative │ ├── Nominal │ └── Ordinal └── Numerical / Quantitative ├── Discrete └── Continous . Classification . If the target feature is categorical then the ML task is called classification. . Regression . If the target feature is numerical then the ML task is called regression. . Positive class . In binary classification the output class is usually labelled as positive or negative. The positive class is the thing we are testing for. For example, positive class for an email classifier is &#39;spam&#39;, and positive class for a medical test can be &#39;tumor&#39;. . Negative class . Negative class is the opposite to positive class. For example, negative class for an email classifier is &#39;not spam&#39;, and a negative class for a medical test can be &#39;not tumor&#39;. . True positive (TP) . Model correctly predicted the positve class . True negative (TN) . Model correctly predicted the negative class . False positive (FP) . Model incorrectly predicted the positive class. Actual class is negative. It has other names . Type I error | . False negative (FN) . Model incorrectly predicted the negative class. Actual class is positive. It has other names . Type II error | . Accuracy . Accuracy = (TP + TN) / (TP + TN + FP + FN) . Precision . It tells how accurate the positive predictions are. . Precision = TP / (TP + FP) . It is a good metric when cost of false positives is high. . True Positive Rate (TPR) . TPR = TP / (TP + FN) . It is the probability that an actual positive class will test positive. . It has other names . Recall | Sensitivity | . True positive is the y-axis in an ROC curve. It is a good metric when cost of false negatives is high. . False Positive Rate (FPR) . FPR = FP / (FP + TN) . It has other names . 1 - specificity | . It is x-axis on ROC curve. . ROC Curve . Receiver Operating Characteristic (ROC) is a curve of TPR vs FPR at different classification thresholds. . True Negative Rate (TNR) . TNR = TN / (TN + FP) . It is the probability of a negative class to test negative. . It has other names . Specificity | .",
            "url": "https://hassaanbinaslam.github.io/myblog/ml/2022/03/31/ml-nomenclature.html",
            "relUrl": "/ml/2022/03/31/ml-nomenclature.html",
            "date": " • Mar 31, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "AWS EFS Sync to S3 Using DataSync",
            "content": ". About . This post is to document all the steps required to synchronize AWS EFS with an S3 bucket using DataSync service. The flow of information is from S3 to EFS and not vice versa. . We will discuss two approaches to trigger the datasync service . Trigger-based. Whenever there is a new file uploaded in S3 bucket | Schedule-based. A trigger will run datasync at scheduled intervals | . Once a datasync service is invoked it will take care of syncing files from S3 bucket to EFS. . Environment Details . Python = 3.9.x | . Steps for trigger based approach . . Create an S3 bucket . Let&#39;s first create an S3 bucket that will contain our data, and this is the bucket we would like to be in sync with EFS. I am naming the bucket as mydata-202203. You may name it as you please. Choose a region of your choice and leave the rest of the settings as defaults. . Create an EFS . From EFS console give name as mydata-efs. I am using default VPC for this post. Use Regional availability settings. Click Create. Once file system is created, click on Access points and create an access point for this efs to be mounted in other service. For access point use following settings . name = mydata-ap | root dir path = / | POSIX User POSIX UID = 1000 | Group ID = 1000 | . | Root directory creation permissions Owner user id = 1000 | Owner group id = 1000 | POSIX permissions = 777 | . | . Click Create. . Note on EFS security group settings . In the last section, I have used a default VPC security group (sg) while creating EFS. Default sg allows traffic for all protocols and all ports, both inbound and outbound. But if you are using a custom security group then make sure that you have an inbound rule for . Type = NFS | Protocol = TCP | Port range = 2049 | . Otherwise, you will not be able to access EFS using NFS clients, and if you find an error similar to the below then it means you need to check the security group settings. . . Create DataSync service task . configure source location = create a new location location type = Amazon S3 | region = us-east-1 | s3 bucket = mydata-202203 | s3 storage class = standard | folder = [leave empty] | IAM role = click on auto generate | . | configure destination location = create a new location location type = EFS | region = us-east-1 | efs file system = mydata-efs | mount path = /efs | subnet = us-east-1a | security group = default | . | configure settings task name = mydata-datasync | task execution configuration verify data = verify only the data transferred | set bandwidth limit = use available | . | data transfer configuration data to scan = entire source location | transfer mode = transfer only the data that has changed | uncheck &quot;keep deleted files&quot; | check &quot;overwrite files&quot; | . | schedule frequency = not scheduled | . | task logging cloudwatch log group = autogenerate | . | . | . Click &quot;next&quot;. Review and Launch. . Test DataSync Service . Let&#39;s test datasync service by manually starting it. If S3 bucket is empty then datasync will throw an exception as below . . This is not an issue. Just place some files (test1.txt in my case) in the bucket and start the datasync service again. If it executes successfully then you will get a message as Execution Status = Success . DataSync can work without Internet Gateway or VPC Endpoint . One thing I noticed is that DataSync service can work even without the presence of an internet gateway or S3 service endpoint. EFS is VPC bound and S3 is global but DataSync can still communicate with both of them. This was different for Lambda. Once Lambda is configured for a VPC then it is not able to access S3 without an internet gateway or VPC endpoint. . Verify EFS by mounting it to the EC2 machine . In the last section, we ran DataSync and it successfully copied files from S3 to EFS. So let&#39;s verify our files from EFS by mounting it to an EC2 instance. . Create an EC2 machine . AMI = Amazon Linux 2 AMI (HVM) - Kernel 5.10, SSD Volume Type | Intance type = t2.micro (free tier) | Instance details Network = default VPC | Auto-assign Public IP = Enable | . | Review and Lanunch &gt; Launch &gt; Proceed without key pair. | . Once the instance is up and running, click on it and connect using EC2 instance connect option. Create a dir &#39;efs&#39; using the command . mkdir efs . In a separate tab open EFS, and click on the file system we have created. Click Attach. From &quot;Mount via DNS&quot; copy command for NFS client. paste that in EC2 bash terminal . sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-01acd308743098251.efs.us-east-1.amazonaws.com:/ efs . Once successfully mounted, verify that the file &#39;test1.txt&#39; exists in EFS. . . Create Lambda function to trigger DataSync task . Now let&#39;s create a lambda function that will trigger the datasync task. This function will itself be triggered by an S3 event notification whenever a file is uploaded or deleted. . Create a lambda function as | name = datasync-trigger-s3 | runtime = Python 3.9 | . Leave the rest of the settings as default, update the code as below, and deploy. . In the code, we are first filtering the object key for which the event is generated. Then we trigger the datasync task and pass the object key as a filter string. With the filter key provided datasync job will only sync provided object from S3 to EFS. . import json import boto3 import os DataSync_task_arn = &#39;arn:aws:datasync:us-east-1:801598032724:task/task-0c04a4a15668b6b8a&#39; DataSync = boto3.client(&#39;datasync&#39;) def lambda_handler(event, context): objectKey = &#39;&#39; try: objectKey = event[&quot;Records&quot;][0][&quot;s3&quot;][&quot;object&quot;][&quot;key&quot;] except KeyError: raise KeyError(&quot;Received invalid event - unable to locate Object key to upload.&quot;, event) response = DataSync.start_task_execution( TaskArn=DataSync_task_arn, OverrideOptions={ &#39;OverwriteMode&#39; : &#39;ALWAYS&#39;, &#39;PreserveDeletedFiles&#39; : &#39;REMOVE&#39;, }, Includes=[ { &#39;FilterType&#39;: &#39;SIMPLE_PATTERN&#39;, &#39;Value&#39;: &#39;/&#39; + os.path.basename(objectKey) } ] ) print(f&quot;response= {response}&quot;) return { &#39;response&#39; : response } . Add policy AWSDataSyncFullAccess to this lambda function role otherwise it will not be able to trigger datasync task. . Configure S3 bucket event notifications . Our lambda function is ready. Now we can enable S3 bucket event notifications as put the lambda function as a target. For this from S3 bucket Properties &gt; Event notifications &gt; Create event notifications . event name = object-put-delete | event type = s3:ObjectCreated:Put, and s3:ObjectRemoved:Delete | destination = lambda function (datasync-trigger-s3) | . Click Save changes . Test DataSync task through S3 events trigger . Now let&#39;s test our trigger by placing a new file in S3 bucket. In my case it is &#39;test2.txt&#39;. Once file is successfully uploaded we can check the EC2 instance to verify the file presence. . . We can also verify that the datasync job was triggered from lambda CloudWatch logs. . response= {&#39;TaskExecutionArn&#39;: &#39;arn:aws:datasync:us-east-1:801598032724:task/task-0c04a4a15668b6b8a/execution/exec-020e456f670ca2419&#39;, &#39;ResponseMetadata&#39;: {&#39;RequestId&#39;: &#39;c8166ce4-ef14-415c-beff-09cc7720f4a3&#39;, &#39;HTTPStatusCode&#39;: 200, &#39;HTTPHeaders&#39;: {&#39;date&#39;: &#39;Wed, 30 Mar 2022 13:27:45 GMT&#39;, &#39;content-type&#39;: &#39;application/x-amz-json-1.1&#39;, &#39;content-length&#39;: &#39;123&#39;, &#39;connection&#39;: &#39;keep-alive&#39;, &#39;x-amzn-requestid&#39;: &#39;c8166ce4-ef14-415c-beff-09cc7720f4a3&#39;}, &#39;RetryAttempts&#39;: 0}} . In the logs we have task execution id exec-020e456f670ca2419 , and we can use that to verify task&#39;s status from datasync console. . . Steps for scheduled based approach . We have seen in the last section that we can use S3 event notifications to trigger datasync tasks. Now we will discuss a schedule-based trigger for datasync task. This can be done in two ways . While creating a datasync task we can define a frequency for it to follow. But the limitation on this is that it can not be lower than a 1 hour window. | If we want to schedule a datasync task on a smaller than 1 hour window then we can use AWS EventBridge (previously CloudWatch Events) to trigger a lambda function that can inturn invoke a datasync task. In the coming section, we will follow this approach. | . . Create a lambda function . Let&#39;s create a new lambda function with the following code. This lambda will invoke the datasync task. Add permissions to this lambda AWSDataSyncFullAccess . function name = datasync-trigger-scheduled | runtime = Python 3.9 | . import json import boto3 import os DataSync_task_arn = &#39;arn:aws:datasync:us-east-1:801598032724:task/task-0c04a4a15668b6b8a&#39; DataSync = boto3.client(&#39;datasync&#39;) def lambda_handler(event, context): response = DataSync.start_task_execution( TaskArn=DataSync_task_arn, OverrideOptions={ &#39;OverwriteMode&#39; : &#39;ALWAYS&#39;, &#39;PreserveDeletedFiles&#39; : &#39;REMOVE&#39;, } ) print(f&quot;response= {response}&quot;) return { &#39;response&#39; : response } . Create EventBridge event . Go to EventBridge Events &gt; Rules &gt; select Create Rule . Define rule details name = datasync-trigger | event bus = default | rule type = scheduled | . | Define schedule Sample event = {} | Schedule Pattern Rate expression = 5 min | . | . | Select Targets target = Lambda function | function = datasync-trigger-scheduled | . | . Click Next and Create Rule . EventBridge will automatically add a policy statement to lambda function (datasync-trigger-scheduled) allowing it to trigger lambda. You can verify the policy from lambda Configurations &gt; Permissions &gt; Resource based policy. If no resource policy exists then you need to manually add a policy to allow EventBridge to invoke it. For this click on Resource based policy &gt; Policy statements &gt; Add permissions. . policy statement = AWS service | service = EventBridge | statement id = eventbridge-1000 (or any unique id) | principal = events.amazonaws.com | source ARN = arn:aws:events:us-east-1:801598032724:rule/datasync-trigger (arn for eventbridge event) | . Verify event and datasync task execution . We have configured eventbridge to fire an event after every 5 min we can verify it from eventbrige monitoring tab and its cloudwatch logs. | Lambda function invocations can be verified from its cloudwatch logs | Datasync task execution status can be verified from its history tab and cloudwatch logs. | .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/lambda/efs/s3/synchonization/datasync/2022/03/29/efs-s3-datasync.html",
            "relUrl": "/aws/lambda/efs/s3/synchonization/datasync/2022/03/29/efs-s3-datasync.html",
            "date": " • Mar 29, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "AWS EFS Sync to S3 Using Lambda",
            "content": ". About . This post is to document all the steps required to synchronize AWS EFS with an S3 bucket using a lambda function. The flow of information is from S3 to EFS and not vice versa. . The approach is whenever a new file is uploaded or deleted from the S3 bucket, it will create an event notification. This event will trigger a lambda function. This lambda function will have the efs file system mounted to it. Lambda function synchronizes the files from S3 to EFS. . . Environment Details . Python = 3.9.x | . Steps . Create an S3 bucket . Let&#39;s first create an S3 bucket that will contain our data, and this is the bucket we would like to be in sync with EFS. I am naming the bucket mydata-202203. You may name it as you please. Choose a region of your choice and leave the rest of the settings as defaults. . Create a Lambda function . Now create a lambda function that will receive event notifications from the S3 bucket, and sync files on efs. I am naming it mydata-sync and our runtime will be Python 3.9. Keep the rest of the settings as default, and create the function. . Create S3 event notifications . From the bucket, mydata-sync go to Properties. Scroll down to Event notifications and click create. Give any name to the event. I am calling it object-sync. From the provided event types select . s3:ObjectCreated:Put | s3:ObjectRemoved:Delete | . From the section Destination select Lambda Function, and from the list choose the lambda function name we created in the last section mydata-sync . Click Save Changes . Test S3 notifications . Let&#39;s now test if S3 event notifications are being received by our lambda function. For this update lambda function code and simply print the event received. After updating the lambda function, make sure to deploy it. . import json def lambda_handler(event, context): print(event) return { &#39;statusCode&#39;: 200, &#39;body&#39;: json.dumps(&#39;Hello from Lambda!&#39;) } . Now upload some files in our S3 bucket, and it should trigger our lambda function. For testing, I have uploaded an empty test1.txt file in our bucket. Once successfully uploaded I check the Lambda function logs to see if any event is received. For this go to lambda function mydata-sync &gt; Monitor &gt; Logs &gt; View logs in CloudWatch. For the CloudWatch console view the latest log stream. Below is the event I have received in the logs . {&#39;Records&#39;: [{&#39;eventVersion&#39;: &#39;2.1&#39;, &#39;eventSource&#39;: &#39;aws:s3&#39;, &#39;awsRegion&#39;: &#39;us-east-1&#39;, &#39;eventTime&#39;: &#39;2022-03-28T16:08:00.896Z&#39;, &#39;eventName&#39;: &#39;ObjectCreated:Put&#39;, &#39;userIdentity&#39;: {&#39;principalId&#39;: &#39;AWS:AIDA3VIXXJNKIVU6P5NY3&#39;}, &#39;requestParameters&#39;: {&#39;sourceIPAddress&#39;: &#39;202.163.113.76&#39;}, &#39;responseElements&#39;: {&#39;x-amz-request-id&#39;: &#39;39MD61ZS00SNK2RT&#39;, &#39;x-amz-id-2&#39;: &#39;U+zPUWOrfzTuVi7kbaBONLHoJXKqUICsVqyKBg4yPKYbUV7pQLGc4Z5A2fSIVvDFtSJHC6v29EDJoXhypWsj2wXanUu8YrLocr3z+yK1qoo=&#39;}, &#39;s3&#39;: {&#39;s3SchemaVersion&#39;: &#39;1.0&#39;, &#39;configurationId&#39;: &#39;object-sync&#39;, &#39;bucket&#39;: {&#39;name&#39;: &#39;mydata-202203&#39;, &#39;ownerIdentity&#39;: {&#39;principalId&#39;: &#39;AYAQOSFZ1VPK&#39;}, &#39;arn&#39;: &#39;arn:aws:s3:::mydata-202203&#39;}, &#39;object&#39;: {&#39;key&#39;: &#39;test1.txt&#39;, &#39;size&#39;: 0, &#39;eTag&#39;: &#39;d41d8cd98f00b204e9800998ecf8427e&#39;, &#39;sequencer&#39;: &#39;006241DD60D67A4556&#39;}}}]} . let&#39;s load this event in a dictionary and find some important parameters . event = {&#39;Records&#39;: [{&#39;eventVersion&#39;: &#39;2.1&#39;, &#39;eventSource&#39;: &#39;aws:s3&#39;, &#39;awsRegion&#39;: &#39;us-east-1&#39;, &#39;eventTime&#39;: &#39;2022-03-28T16:08:00.896Z&#39;, &#39;eventName&#39;: &#39;ObjectCreated:Put&#39;, &#39;userIdentity&#39;: {&#39;principalId&#39;: &#39;AWS:AIDA3VIXXJNKIVU6P5NY3&#39;}, &#39;requestParameters&#39;: {&#39;sourceIPAddress&#39;: &#39;202.163.113.76&#39;}, &#39;responseElements&#39;: {&#39;x-amz-request-id&#39;: &#39;39MD61ZS00SNK2RT&#39;, &#39;x-amz-id-2&#39;: &#39;U+zPUWOrfzTuVi7kbaBONLHoJXKqUICsVqyKBg4yPKYbUV7pQLGc4Z5A2fSIVvDFtSJHC6v29EDJoXhypWsj2wXanUu8YrLocr3z+yK1qoo=&#39;}, &#39;s3&#39;: {&#39;s3SchemaVersion&#39;: &#39;1.0&#39;, &#39;configurationId&#39;: &#39;object-sync&#39;, &#39;bucket&#39;: {&#39;name&#39;: &#39;mydata-202203&#39;, &#39;ownerIdentity&#39;: {&#39;principalId&#39;: &#39;AYAQOSFZ1VPK&#39;}, &#39;arn&#39;: &#39;arn:aws:s3:::mydata-202203&#39;}, &#39;object&#39;: {&#39;key&#39;: &#39;test1.txt&#39;, &#39;size&#39;: 0, &#39;eTag&#39;: &#39;d41d8cd98f00b204e9800998ecf8427e&#39;, &#39;sequencer&#39;: &#39;006241DD60D67A4556&#39;}}}]} event . {&#39;Records&#39;: [{&#39;eventVersion&#39;: &#39;2.1&#39;, &#39;eventSource&#39;: &#39;aws:s3&#39;, &#39;awsRegion&#39;: &#39;us-east-1&#39;, &#39;eventTime&#39;: &#39;2022-03-28T16:08:00.896Z&#39;, &#39;eventName&#39;: &#39;ObjectCreated:Put&#39;, &#39;userIdentity&#39;: {&#39;principalId&#39;: &#39;AWS:AIDA3VIXXJNKIVU6P5NY3&#39;}, &#39;requestParameters&#39;: {&#39;sourceIPAddress&#39;: &#39;202.163.113.76&#39;}, &#39;responseElements&#39;: {&#39;x-amz-request-id&#39;: &#39;39MD61ZS00SNK2RT&#39;, &#39;x-amz-id-2&#39;: &#39;U+zPUWOrfzTuVi7kbaBONLHoJXKqUICsVqyKBg4yPKYbUV7pQLGc4Z5A2fSIVvDFtSJHC6v29EDJoXhypWsj2wXanUu8YrLocr3z+yK1qoo=&#39;}, &#39;s3&#39;: {&#39;s3SchemaVersion&#39;: &#39;1.0&#39;, &#39;configurationId&#39;: &#39;object-sync&#39;, &#39;bucket&#39;: {&#39;name&#39;: &#39;mydata-202203&#39;, &#39;ownerIdentity&#39;: {&#39;principalId&#39;: &#39;AYAQOSFZ1VPK&#39;}, &#39;arn&#39;: &#39;arn:aws:s3:::mydata-202203&#39;}, &#39;object&#39;: {&#39;key&#39;: &#39;test1.txt&#39;, &#39;size&#39;: 0, &#39;eTag&#39;: &#39;d41d8cd98f00b204e9800998ecf8427e&#39;, &#39;sequencer&#39;: &#39;006241DD60D67A4556&#39;}}}]} . # event name event[&#39;Records&#39;][0][&#39;eventName&#39;] . &#39;ObjectCreated:Put&#39; . # bucket name event[&#39;Records&#39;][0][&#39;s3&#39;][&#39;bucket&#39;][&#39;name&#39;] . &#39;mydata-202203&#39; . # uploaded object key event[&#39;Records&#39;][0][&#39;s3&#39;][&#39;object&#39;][&#39;key&#39;] . &#39;test1.txt&#39; . Alright, we have seen that we are receiving notifications from S3 bucket so let&#39;s now move on to the next section. . Create an EFS . From EFS console give name as mydata-efs. I am using default VPC for this post. Use Regional availability settings. Click Create. Once file system is created, click on Access points and create an access point for this efs to be mounted in other service. For access point use following settings . name = mydata-ap | root dir path = /efs | POSIX User POSIX UID = 1000 | Group ID = 1000 | . | Root directory creation permissions Owner user id = 1000 | Owner group id = 1000 | POSIX permissions = 777 | . | . Click Create. . Here I have used the root dir path as /efs this means that from this access point my access will be limited to folder /efs. If you want to provide full access to all folders then set to root path to /. . Note on EFS security group settings . In the last section, I have used a default VPC security group (sg) while creating EFS. default sg allows traffic for all protocols and all ports, both for inbound and outbound traffic. But if you are using a custom security group then make sure that you have an inbound rule for . Type = NFS | Protocol = TCP | Port range = 2049 | . Otherwise, you will not be able to access EFS using NFS clients. . Mount EFS to Lambda Function . To mount an EFS to the Lambda function requires some additional steps. . First add permissions to Lambda function. From lambda function &gt; Configurations &gt; Permissions &gt; Execution role. Click on the execution role to open it in IAM concole. For the selected role attach an additional policy AmazonElasticFileSystemFullAccess. . Second, add the lambda to a VPC group in which efs was created. We have created efs in default VPC so let&#39;s add lambda to it. For this from lambda Configurations &gt; VPC click edit. For the next pane select default VPC, all subnets, default VPC security group, and click save. . Now we can add EFS to lambda. Go to lambda Configurations &gt; File Systems &gt; Add file system. Select the file system mydata-efs and associated access point mydata-ap and local mount point as /mnt/efs. The local mount point is the mounted directory from where we can access our EFS from inside the lambda environment. Click Save . Check EFS mount point from Lambda . Let&#39;s verify from lambda that EFS has been mounted and can we access it. So update the lambda code as below and deploy it. . import json import os def lambda_handler(event, context): mount_path = &#39;/mnt/efs&#39; if os.path.exists(mount_path): print(f&quot;{mount_path} exists&quot;) print(os.listdir(&#39;/mnt/efs&#39;)) print(event) return { &#39;statusCode&#39;: 200, &#39;body&#39;: json.dumps(&#39;Hello from Lambda!&#39;) } . Now test this code using a test event S3 Put. For this go to lambda Test &gt; Create new event &gt; Template (s3-put). &#39;S3 Put&#39; test event is similar to the one we saw in the last section. We can use this request template to simulate the event received from S3 bucket. Once the test is successfully executed, check the log output. . START RequestId: 2e307a14-f373-46d5-b763-594d5f406ae6 Version: $LATEST /mnt/efs exists [] {&#39;Records&#39;: [{&#39;eventVersion&#39;: &#39;2.0&#39;, &#39;eventSource&#39;: &#39;aws:s3&#39;, &#39;awsRegion&#39;: &#39;us-east-1&#39;, &#39;eventTime&#39;: &#39;1970-01-01T00:00:00.000Z&#39;, &#39;eventName&#39;: &#39;ObjectCreated:Put&#39;, &#39;userIdentity&#39;: {&#39;principalId&#39;: &#39;EXAMPLE&#39;}, &#39;requestParameters&#39;: {&#39;sourceIPAddress&#39;: &#39;127.0.0.1&#39;}, &#39;responseElements&#39;: {&#39;x-amz-request-id&#39;: &#39;EXAMPLE123456789&#39;, &#39;x-amz-id-2&#39;: &#39;EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH&#39;}, &#39;s3&#39;: {&#39;s3SchemaVersion&#39;: &#39;1.0&#39;, &#39;configurationId&#39;: &#39;testConfigRule&#39;, &#39;bucket&#39;: {&#39;name&#39;: &#39;example-bucket&#39;, &#39;ownerIdentity&#39;: {&#39;principalId&#39;: &#39;EXAMPLE&#39;}, &#39;arn&#39;: &#39;arn:aws:s3:::example-bucket&#39;}, &#39;object&#39;: {&#39;key&#39;: &#39;test%2Fkey&#39;, &#39;size&#39;: 1024, &#39;eTag&#39;: &#39;0123456789abcdef0123456789abcdef&#39;, &#39;sequencer&#39;: &#39;0A1B2C3D4E5F678901&#39;}}}]} END RequestId: 2e307a14-f373-46d5-b763-594d5f406ae6 REPORT RequestId: 2e307a14-f373-46d5-b763-594d5f406ae6 Duration: 7.02 ms Billed Duration: 8 ms Memory Size: 128 MB Max Memory Used: 37 MB Init Duration: 93.81 ms . From the logs we can see that the mounted EFS directory exists /mnt/efs but currently the folder is empty. . Configure VPC endpoint for S3 . Till now we have configured S3 notifications to trigger a lambda function and also mounted EFS to it. Our next step is to process the event received in lambda, and download the file from S3 to EFS. But since our lambda function is configured for a VPC we cannot connect to S3 from it. Even though we can still receive S3 event notification, when we try to connect to S3 to download any file we will get a timeout error. To fix this we will create a VPC endpoint for S3 bucket. . For this go to VPC console &gt; Endpoints &gt; Create endpoint, and set the following . name = mydata-ep | service category = aws services | services = com.amazonaws.us-east-1.s3 (Gateway) | vpc = default | route table = default (main route table) | policy = full access | . Click Create endpoint . Configure S3 permissions for Lambda . For lambda to be able to connect to S3 we also need to give it proper permissions. For this go to Lambda &gt; Configurations &gt; Permissions &gt; Execution Role &gt; click on role name. From the IAM Role console select add permissions, and then select AmazonS3FullAccess . Process S3 event notifications . Our lambda and EFS are ready and we can now process S3 events. Update the lambda code as below to process S3 events. It will download and delete from EFS to keep it in sync with S3 bucket. . import json import boto3 import os s3 = boto3.client(&quot;s3&quot;) def lambda_handler(event, context): event_name = event[&quot;Records&quot;][0][&quot;eventName&quot;] bucket_name = event[&quot;Records&quot;][0][&quot;s3&quot;][&quot;bucket&quot;][&quot;name&quot;] object_key = event[&quot;Records&quot;][0][&quot;s3&quot;][&quot;object&quot;][&quot;key&quot;] efs_file_name = &quot;/mnt/efs/&quot; + object_key # S3 put if event_name == &quot;ObjectCreated:Put&quot;: s3.download_file(bucket_name, object_key, efs_file_name) print(f&quot;file downloaded: {efs_file_name}&quot;) # S3 delete if event_name == &quot;ObjectRemoved:Delete&quot;: # check if file exists on efs if os.path.exists(efs_file_name): os.remove(efs_file_name) print(f&quot;file deleted: {efs_file_name}&quot;) return {&quot;statusCode&quot;: 200, &quot;body&quot;: json.dumps(event)} . We can test this code using the S3-put test event we used last time. Modify the event for bucket name and object key as below. . { &quot;Records&quot;: [ { &quot;eventVersion&quot;: &quot;2.0&quot;, &quot;eventSource&quot;: &quot;aws:s3&quot;, &quot;awsRegion&quot;: &quot;us-east-1&quot;, &quot;eventTime&quot;: &quot;1970-01-01T00:00:00.000Z&quot;, &quot;eventName&quot;: &quot;ObjectCreated:Put&quot;, &quot;userIdentity&quot;: { &quot;principalId&quot;: &quot;EXAMPLE&quot; }, &quot;requestParameters&quot;: { &quot;sourceIPAddress&quot;: &quot;127.0.0.1&quot; }, &quot;responseElements&quot;: { &quot;x-amz-request-id&quot;: &quot;EXAMPLE123456789&quot;, &quot;x-amz-id-2&quot;: &quot;EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH&quot; }, &quot;s3&quot;: { &quot;s3SchemaVersion&quot;: &quot;1.0&quot;, &quot;configurationId&quot;: &quot;testConfigRule&quot;, &quot;bucket&quot;: { &quot;name&quot;: &quot;mydata-202203&quot;, &quot;ownerIdentity&quot;: { &quot;principalId&quot;: &quot;EXAMPLE&quot; }, &quot;arn&quot;: &quot;arn:aws:s3:::example-bucket&quot; }, &quot;object&quot;: { &quot;key&quot;: &quot;test1.txt&quot;, &quot;size&quot;: 1024, &quot;eTag&quot;: &quot;0123456789abcdef0123456789abcdef&quot;, &quot;sequencer&quot;: &quot;0A1B2C3D4E5F678901&quot; } } } ] } . Click test. From the output logs, we can see that our code was able to download the file from S3 bucket and write it on EFS. . START RequestId: 7e9c0dc2-f970-426e-8372-e59b07f5536c Version: $LATEST file downloaded: /mnt/efs/test1.txt END RequestId: 7e9c0dc2-f970-426e-8372-e59b07f5536c REPORT RequestId: 7e9c0dc2-f970-426e-8372-e59b07f5536c Duration: 370.00 ms Billed Duration: 371 ms Memory Size: 128 MB Max Memory Used: 72 MB Init Duration: 367.68 ms . Note that if you get any permission errors then it could be due to the mounting path errors. Please do check the access point path and lambda mount path. . Verify file on EFS . We can verify files on EFS by directly mounting them to an EC2 machine and verifying from there. So let&#39;s do that. . Create an EC2 machine . AMI = Amazon Linux 2 AMI (HVM) - Kernel 5.10, SSD Volume Type | Intance type = t2.micro (free tier) | Instance details Network = default VPC | Auto-assign Public IP = Enable | . | Review and Lanunch &gt; Launch &gt; Proceed without key pair. | . Once the instance is up and running, click on it and connect using EC2 instance connect option. Create a dir &#39;efs&#39; using the command . mkdir efs . In a separate tab open EFS, and click on the file system we have created. Click Attach. From &quot;Mount via DNS&quot; copy command for NFS client. paste that in EC2 bash terminal . sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-0c9526e2f48ece247.efs.us-east-1.amazonaws.com:/ efs . Once successfully mounted verify that the file &#39;test1.txt&#39; exists in EFS. We can also delete the file from S3 and similarly verify from EFS that the file has been removed. . Summary . A summary of all the steps . Create an S3 bucket | Create a Lambda function | Create event notifications on the S3 bucket to trigger the lambda function | Create an EFS file system and its access point. Check the security group setting for inbound rules for NFS traffic | Add EFS and S3 permissions to lambda | Add lambda to VPC | Create VPC endpoint for S3 bucket | Update lambda code to process event notifications | Use EC2 to mount EFS and verify the files | .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/lambda/efs/s3/synchonization/2022/03/28/efs-s3-sync-lambda.html",
            "relUrl": "/aws/lambda/efs/s3/synchonization/2022/03/28/efs-s3-sync-lambda.html",
            "date": " • Mar 28, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "AWS Lambda - Test and Debug Locally in Visual Studio Code",
            "content": ". About . This post is about running, and debugging AWS Lambda function locally from Visual Studio Code environment and it extensions AWS Toolkit. . Environment Details . Python = 3.8.x | WSL version = 2 | WSL Ubuntu version = 20.04 | Docker Engine version = 20.10.12 | Docker Desktop version = 4.4.4 | Host OS = Windows 10 | VSCode version = 1.65 | SAM CLI version = 1.40.1 | . Sample Application . For this post we will use a simple hello world application as our focus is on debugging. We will use AWS SAM CLI to create our application. You can follow the steps provided in tutorial AWS SAM Developer Guide&gt;Getting started with AWS SAM to create this application. . From the provided link (SAM Developer Guide): This application implements a basic API backend. It consists of an Amazon API Gateway endpoint and an AWS Lambda function. When you send a GET request to the API Gateway endpoint, the Lambda function is invoked. This function returns a hello world message. . The following diagram shows the components of this application: . . To initialize a serverless app use command . sam init . Complete the SAM initialization setup steps . PS C: MyWorkspace gitrepos 2022-03-16-lambda-debug&gt; sam init You can preselect a particular runtime or package type when using the `sam init` experience. Call `sam init --help` to learn more. Which template source would you like to use? 1 - AWS Quick Start Templates 2 - Custom Template Location Choice: 1 Choose an AWS Quick Start application template 1 - Hello World Example 2 - Multi-step workflow 3 - Serverless API 4 - Scheduled task 5 - Standalone function 6 - Data processing 7 - Infrastructure event management 8 - Machine Learning Template: 1 Use the most popular runtime and package type? (Python and zip) [y/N]: y Project name [sam-app]: Cloning from https://github.com/aws/aws-sam-cli-app-templates (process may take a moment) -- Generating application: -- Name: sam-app Runtime: python3.9 Architectures: x86_64 Dependency Manager: pip Application Template: hello-world Output Directory: . Next steps can be found in the README file at ./sam-app/README.md Commands you can use next ========================= [*] Create pipeline: cd sam-app &amp;&amp; sam pipeline init --bootstrap [*] Test Function in the Cloud: sam sync --stack-name {stack-name} --watch . Once the application is initialized the project structure will look like this . sam-app/ ├── README.md ├── events/ │ └── event.json ├── hello_world/ │ ├── __init__.py │ ├── app.py #Contains your AWS Lambda handler logic. │ └── requirements.txt #Contains any Python dependencies the application requires, used for sam build ├── template.yaml #Contains the AWS SAM template defining your application&#39;s AWS resources. └── tests/ └── unit/ ├── __init__.py └── test_handler.py . There are three especially important files: . template.yaml: Contains the AWS SAM template that defines your application&#39;s AWS resources. | hello_world/app.py: Contains your actual Lambda handler logic. | hello_world/requirements.txt: Contains any Python dependencies that the application requires, and is used for sam build. | . Follow the instructions from the tutorial to build, test, and deploy the application. . All the code used for this post can be obtained from the GitHub repository hassaanbinaslam/2022-03-16-lambda-debug . Project code files | Project zip file | . SAM Project Directory . SAM CLI After project initialization from sam init make sure that you move to project root folder | Project root folder is the one that contain template.yaml defining application AWS resources. In this app case project root folder is sam-app/ | All the subsequest commands including project sam build, sam deploy, invoke and test lambda should be done from project root folder | . | VSCode When you open the project make sure that your project root directory is pointing to sam-app/ folder as shown in image below | . | . . Run Lambda Locally . To invoke lambda function locally use SAM CLI command . sam local invoke . PS C: MyWorkspace gitrepos 2022-03-16-lambda-debug sam-app&gt; sam local invoke Invoking app.lambda_handler (python3.9) Skip pulling image and use local one: public.ecr.aws/sam/emulation-python3.9:rapid-1.40.1-x86_64. Mounting C: MyWorkspace gitrepos 2022-03-16-lambda-debug sam-app hello_world as /var/task:ro,delegated inside runtime container START RequestId: 1d4e45de-38be-4b91-8e9f-4d3c7da8788a Version: $LATEST END RequestId: 1d4e45de-38be-4b91-8e9f-4d3c7da8788a REPORT RequestId: 1d4e45de-38be-4b91-8e9f-4d3c7da8788a Init Duration: 1.40 ms Duration: 990.84 ms Billed Duration: 991 ms Memory Size: 128 MB Max Memory Used: 128 MB {&quot;statusCode&quot;: 200, &quot;body&quot;: &quot;{ &quot;message &quot;: &quot;hello world &quot;}&quot;} . If you have multiple lambda functions in the app, you can invoke a specific lambda function by using it&#39;s name in invoke command as . sam local invoke &quot;HelloWorldFunction&quot; . Run API Gateway Locally . You can run API Gateway locally to test HTTP request response functionality using command . sam local start-api . This command will start a local instance of API Gateway and provide you with a URL that you can use to send a request using CURL commmand . PS C: MyWorkspace gitrepos 2022-03-16-lambda-debug sam-app&gt; sam local start-api Mounting HelloWorldFunction at http://127.0.0.1:3000/hello [GET] You can now browse to the above endpoints to invoke your functions. You do not need to restart/reload SAM CLI while working on your functions, changes will be reflected instantly/automatically. You only need to restart SAM CLI if you update your AWS SAM template 2022-03-17 11:41:55 * Running on http://127.0.0.1:3000/ (Press CTRL+C to quit) . From terminal output you can find tha HelloWorldFunction is mounted at http://127.0.0.1:3000/hello. From another terminal we can call this URL . PS C: MyWorkspace gitrepos 2022-03-16-lambda-debug sam-app&gt; curl http://127.0.0.1:3000/hello StatusCode : 200 StatusDescription : OK Content : {&quot;message&quot;: &quot;hello world&quot;} RawContent : HTTP/1.0 200 OK Content-Length: 26 Content-Type: application/json Date: Thu, 17 Mar 2022 06:43:08 GMT Server: Werkzeug/1.0.1 Python/3.8.8 {&quot;message&quot;: &quot;hello world&quot;} Forms : {} Headers : {[Content-Length, 26], [Content-Type, application/json], [Date, Thu, 17 Mar 2022 06:43:08 GMT], [Server, Werkzeug/1.0.1 Python/3.8.8]} Images : {} InputFields : {} Links : {} ParsedHtml : mshtml.HTMLDocumentClass RawContentLength : 26 . Debug Lambda Application Locally . To debug a lambda function we have following options . Option 1: Debug through SAM template . From VSCode open template.yaml, and go to the resources section of the template that defines serverless resources. Click on the lambda function resource, which in our case is HelloWorldFunction. A tooltip will appear over it saying AWS: Add Debug Configuration. Click it as shown below. . . This will create a new folder in the project with debug launch configuration launch.json. . . Let&#39;s add a breakpoint in our lambda handler code hello_world/app.py, and start debugging by clicking the green &quot;play&quot; button in the RUN view. When the debugging sessions starts, the DEBUG CONSOLE panel shows debugging output and displays any values returned by the Lambda function. . . Option 2: Debug Lambda Directly from Code . From VSCode open lambda handler code sam-app/hello_world/app.py. A tooltip will appear above the lambda_handler function with options . AWS: Add Debug Configuration | AWS: Edit Debug Configuration | . . Click on AWS: Add Debug Configuration and it will show two further options . template.yaml:HelloWorldFunction (to debug only the lambda function) | template.yaml:HelloWorldFunction (API Event: HelloWorld) (to debug lambda function along with API gateway) | . . Let&#39;s select API option this time. It will again create a launch configuration, and now we can debug our code. Click on the green &quot;play&quot; button again to start the debug session with request request coming from API Gateway to Lambda function. . You can also edit the debug config visually by selecting the AWS: Edit Debug Configuration, and a side pane will appear from where we can easily edit and update our debug configuration. . . All the code used for this post can be obtained from the GitHub repository hassaanbinaslam/2022-03-16-lambda-debug . Project code files | Project zip file | .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/lambda/docker/2022/03/16/lambda-debug.html",
            "relUrl": "/aws/lambda/docker/2022/03/16/lambda-debug.html",
            "date": " • Mar 16, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "Docker - Debugging Python Application",
            "content": ". About . This post is about debugging a Python application running on a Docker container inside WSL2 linux environment. Highlight of this post is Visual Studio Code environment and it extensions Remote Containers. . Environment Details . Python = 3.8.x | WSL version = 2 | WSL Ubuntu version = 20.04 | Docker Engine version = 20.10.12 | Docker Desktop version = 4.4.4 | Host OS = Windows 10 | VSCode version = 1.65 | . Sample Application . For this post I will use a a simple hello world application that will print &quot;hello world&quot; messages to stdout, and also logs them in a logfilelog. After each message the application sleeps for 5 seconds, and keeps on doing this for 5 mins (300 sec). After this the program exists. . This application was created as part of the previous blog post Docker - Accessing Python Application Logs. It is a very simple application, and you can find all the code in GitHub repository snapshots-docker-post-11032022 . Project code files | Project zip file | . Project structure of this application is . app/ ├── src/ │ ├── commons/ │ │ └── logger.py │ └── hello.py └── Dockerfile . Where . app/ is the project root folder | src/ folder contain the python application code | src/commons/logger.py is the logging module | src/hello.py is the main application | app/Dockerfile is the Docker image build file | . When I run the src/hello.py file from my local machine (Windows 10) I get the output on the termial with hello world messages like this. . . A &quot;logfile.log&quot; will also appear in the project directory containing the logged messages. . . ├── app/ │ ├── src/ │ │ ├── commons/ │ │ │ └── logger.py │ │ └── hello.py │ └── Dockerfile └── **logfile.log** . Contents of &quot;logfile.log&quot; file will look like this . INFO 2022-03-11 13:01:56,451 - hello world at 11/03/2022 13:01:56 INFO 2022-03-11 13:02:01,464 - hello world at 11/03/2022 13:02:01 INFO 2022-03-11 13:02:06,466 - hello world at 11/03/2022 13:02:06 INFO 2022-03-11 13:02:11,480 - hello world at 11/03/2022 13:02:11 INFO 2022-03-11 13:02:16,496 - hello world at 11/03/2022 13:02:16 . We can build our docker image and run it using commands . docker build --tag python-docker . docker run --name helloworld python-docker . Output on the terminal will be like this . Notice the difference in the print message when the application was is locally, and from the docker container. . Local (Win10) message = hello world at 14/03/2022 18:04:02 from Windows | Docker container message = hello world at 14/03/2022 13:12:14 from Linux | . Debug Docker Application . To debug the application from inside the docker container we will use VSCode extention Visual Studio Code Remote - Containers. From the extension docs . The Visual Studio Code Remote - Containers extension lets you use a Docker container as a full-featured development environment. It allows you to open any folder inside (or mounted into) a container and take advantage of Visual Studio Code&#39;s full feature set. . Once this extension is installed a new icon ( Remote Window ) will appear at the bottom left corner of the VSCode window. Once clicked on the icon, a dropped down will appear as shown below. From this drop down choose option Reopen in Container . . Now it is important to understand that Visual Studio Code Remote - Containers extension let&#39;s you use a Docker container as a full-featured development environment. It allows you to open any folder or repository inside a container and take advantage of Visual Studio Code&#39;s full feature set including code debugging, linting, formatting, intellisense, and other tooling. VSCode also provides its own prebuild docker images with all the necessary tools installed into them. Or we can we instruct the VSCode to create a new development container using our docker file. You can find a list of prebuild docker images here: microsoft-vscode-devcontainers . VSCode uses a configuration file called &quot;devcontainer.json&quot; to store instructions on how to create and attach to a development container. You can read more about this config file here: devcontainerjson-reference . Now let&#39;s create a new docker development environment using our Dockerfile. . Open VSCode Commands Palette (F1 or CTL+Shift+P on Win10) | Select &quot;Remote-Containers: Add Development Container Configuration Files&quot; option from dropdown | Then select &quot;from Docker&quot; since we want our development container environment same as defined in the Dockerfile | . If this option is not available, and the drop down is showing other options like in the image below, then VSCode is unable to find a Dockerfile associated with the project. . . Notice my project dir in the image below. The root folder of my project snapshots-docker-post-11032022 does not contain any Dockerfile. . . VSCode remote extension assumes that there is a Docker file at the root of the project directory. My project root contain app/ folder and inside this folder Dockerfile is located. When we select &quot;Remote-Containers: Add Development Container Configuration Files&quot; option docker extension checks the project root folder for a Dockerfile. It could not find one in my project and that is why it removed &#39;From Dockerfile&#39; option from the dropdown. Let&#39;s correctly open the project with app/ as the root folder (or place the Dockerfile at the project root folder). After correcting this mistake, my project in VSCode looks like this . . Now use the extension one more time to create a development container. This time you will find the option &quot;From Dockerfile&quot; in the dropdown menu . . Once this option is selected, VSCode will add a folder &quot;.devcontainer&quot; in the project root containing instructions on how to build and launch the development container. Then it will run those instruction to launch a container and connect to it. VSCode terminal will show the logs of all the commands used in launching that container, and at bottom left of VSCode window it will show the name of the container to which it is currently connected. . . Note that at this point we are actually working from inside a container. But to actually develop and debug the code from this container you will be required to install more extensions to it. If we had used a VSCode prebuild image then all the required extensions will be automatically available. To install required extension we can use VSCode extensions tab. . . Python extension will be required to work with Python code. So let&#39;s intall in our working container. You can also copy the names of installed extensions and paste them in the &quot;.devcontainer&quot; config file as shown below . // Add the IDs of extensions you want installed when the container is created. &quot;extensions&quot;: [ &quot;ms-python.python&quot;, &quot;ms-python.vscode-pylance&quot; ] . This way when next time we use this config file to launch a new dev container, all these extensions will be automatically installed for us. To customise the config file you can take help from this template provided by VSCode team python-3/.devcontainer . Installation of the extensions can be verified from the VSCode terminal logs . . We can now run our Python code from inside this container . . We can also easily debug our code directly from inside the container . . To close the remote connection, click on the Remote Window Icon at the bottom left corner. Use &quot;Reopen Folder Locally&quot; option to return back to local environment. Or &quot;Close Remote Connection&quot; to close the remote connection and also close the project. . . All the code used for this post can be obtained from the GitHub repository hassaanbinaslam/snapshots-docker-post-11032022 . Project code files | Project zip file | .",
            "url": "https://hassaanbinaslam.github.io/myblog/docker/python/2022/03/14/docker-app-debug.html",
            "relUrl": "/docker/python/2022/03/14/docker-app-debug.html",
            "date": " • Mar 14, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "Docker - Accessing Python Application Logs",
            "content": ". About . This post is about the challenges on accessing Python application logs running on a Docker container inside WSL2 linux environment. . Environment Details . Python = 3.8.x | WSL version = 2 | WSL Ubuntu version = 20.04 | Docker Engine version = 20.10.12 | Docker Desktop version = 4.4.4 | Host OS = Windows 10 | . Sample Application . Let us create a simple hello world application that will print &quot;hello world&quot; message to stdout, and also logs them in a logfile. After each message the application sleeps for 5 seconds, and keeps on doing this for 5 mins (300 sec). After this the program exists. . Project structure of this application is . app/ └── src/ ├── commons/ │ └── logger.py └── hello.py . Where . app/ is the project root folder | src/ folder contain the python application code | src/commons/logger.py is the logging module | src/hello.py is the main application | . Code files are provided below . # app/src/commons/logger.py import logging import os logformat = &quot;%(levelname)s %(asctime)s - %(message)s&quot; filename = &quot;logfile.log&quot; # Setting the config of the log object logging.basicConfig( format=logformat, filename=filename, level=logging.INFO, ) . # app/src/hello.py from datetime import datetime import time import commons.logger as logger def main(): # run for about 5 min: 300 sec for i in range(60): now = datetime.now() dt_string = now.strftime(&quot;%d/%m/%Y %H:%M:%S&quot;) # prepare message msg = f&quot;hello world at {dt_string}&quot; # put message to stdout and logs print(msg) logger.logging.info(msg) # sleep for some seconds time.sleep(5) if __name__ == &quot;__main__&quot;: main() . When I run the hello.py file I get the output on the termial with hello world messages like this. . . When we run the application a &quot;logfile.log&quot; will also appear in the project directory containing the logged messages. . . ├── app/ │ └── src/ │ ├── commons/ │ │ └── logger.py │ └── hello.py └── **logfile.log** . Contents of &quot;logfile.log&quot; file will look like this . INFO 2022-03-11 13:01:56,451 - hello world at 11/03/2022 13:01:56 INFO 2022-03-11 13:02:01,464 - hello world at 11/03/2022 13:02:01 INFO 2022-03-11 13:02:06,466 - hello world at 11/03/2022 13:02:06 INFO 2022-03-11 13:02:11,480 - hello world at 11/03/2022 13:02:11 INFO 2022-03-11 13:02:16,496 - hello world at 11/03/2022 13:02:16 . All the code till this point can be found at GitHub repository https://github.com/hassaanbinaslam/snapshots-docker-post-11032022 . Project code files | Project zip file | . Dockerize the application . Our hello-world application is ready now, and we can put it inside a docker container. For this let&#39;s create a Dockerfile and place it in app/ folder. . # app/Dockerfile FROM python:3.8-slim-buster # set the working directory in the container WORKDIR /app # copy the content of the local src directory to the working directory COPY src/ . # command to run on container start CMD [ &quot;python3&quot;, &quot;./hello.py&quot;] . We can build our docker image by running the command from terminal at folder app/ . docker build --tag python-docker . . Output of this command will look like this . We can check the created docker image using command from terminal . docker images . Output of this command will look like this . So our docker image is ready, we can now run it using command . docker run --name helloworld python-docker . After running this command you will observe that there is no output on the terminal. Even though we have run our image in an attached mode but still there is no output. We know that the container is running as control on terminal has not return back to us. We can also verify that the container is running by running command docker ps in a separate terminal. Output from this command will look like this . . We can also verify that the container is running from Docker Desktop container apps menu. Running container instance will appear like this . . The reason for logs not appearing on the terminal is because they are being buffered by docker internally. You will get all of them once docker container has finished execution and stopped. You can read more about docker buffereing the output from these StackOverflow posts . disable-output-buffering | python-app-does-not-print-anything-when-running-detached-in-docker | . To disable the output buffering we need to change the CMD in our docker file as . CMD [ &quot;python3&quot;, &quot;-u&quot;, &quot;./hello.py&quot;] . # app/Dockerfile FROM python:3.8-slim-buster # set the working directory in the container WORKDIR /app # copy the content of the local src directory to the working directory COPY src/ . # command to run on container start CMD [ &quot;python3&quot;, &quot;-u&quot;, &quot;./hello.py&quot;] . We need to rebuild our docker image and then run it. Let&#39;s do that will the following commands . docker build --tag python-docker . docker run --name helloworld python-docker . this time you can see the output directly on the terminal . We don&#39;t have to run the docker container in an attached mode, and can still get the logs from a running container using docker logs command. Let&#39;s do that then . first run the docker image in a detached mode . docker run -d --name helloworld python-docker . this command will give the running container ID which will look something like this 06918cf824210c015b08ef072feed01768be30ee569e5dbc0eef8d3cd855ab47. We can pass this ID to our next command to view the logs stream from a running container. Use this command . docker logs -f 06918cf824210c015b08ef072feed01768be30ee569e5dbc0eef8d3cd855ab47 . Also note that we don&#39;t need to provide the full container id, and can can also provide the first 2 or 3 digits of the ID that can uniquely identify the running container. So following command will also work . docker logs -f 069 . Output on the terminal will look like this . All the project files till this point can be found at . project code files | . Log Files Physical Location . Docker STDOUT / STDERR logs are also stored on host system as JSON files. . On linux You can find them at . /var/lib/docker/containers/&lt;container-id&gt;/&lt;container-id&gt;-json.log . On Windows You can find them at . wsl$ docker-desktop-data version-pack-data community docker containers &lt;container-id&gt;/&lt;container-id&gt;-json.log . In the above paths replace &lt;container-id&gt; with your full cotaniner ID. Also note that on Windows 10 you can open the folder location by directly pasting the path in folder explorer search bar. . You can read more about them in the following StackOverflow post . windows-10-and-docker-container-logs-docker-logging-driver | . Application Log Files . So far we have talked about the docker logs that were generated by application feed to STDOUT or STDERR. But now we are interested in app logs generated by logging module like the one we saw in our first example &quot;logfile.log&quot;. For this let&#39;s first connect to running docker container and see where is this file located inside the docker running instance. . Run a new docker container again using command . docker run -d --name helloworld python-docker . If container already exists you can just start it using command . docker start &lt;container-id&gt; . Or you can also use the docker desktop to start existing container by click the play button over it. . . Once the docker container is running, you can connect to it by either using the CLI from docker desktop . . or from the command below . docker exec -it &lt;cotainer-id&gt; /bin/bash . Remember that container-id of a running container can be obtained by command docker ps. Output from the above command will look like this . . Note that the location of &quot;logfile.log&quot; is under app/ folder . Docker Volume . We have seen the application logs by connecting to the docker container, but we would like to have these logs readily available outside the docker container so they can be consumed in real time for debugging. Fot this docker recommends using volume. Let&#39;s create a volume and then mount our application logs to it. . To create a volume use command . docker volume create applogs . To remove a volume use command . docker rm create applogs . To inspect a created volume use command . docker volume inspect applogs . Output of this command will be like this where Mountpoint is the location of logs on host system. . [ { &quot;CreatedAt&quot;: &quot;2022-03-11T13:12:57Z&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: {}, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/applogs/_data&quot;, &quot;Name&quot;: &quot;applogs&quot;, &quot;Options&quot;: {}, &quot;Scope&quot;: &quot;local&quot; } ] . On Windows 10 you can find the location of these volumes at . wsl$ docker-desktop-data version-pack-data community docker volumes applogs _data . Now let&#39;s run our docker container with this volume mounted to it. Command for this is . docker run -d --name helloworld -v applogs:/app python-docker . Docker container will run as before, but if we check the volume on our host we will find that this time all the files and folders available in app/ folder from inside the docker container are available. And they will persist on the host OS even if the container is stopped. . . Note that the log file &#39;logfile.log&#39; is also available outside the container, and we can poll it for debugging. But having all the contents of app/ folder exposed can be a security issue, as they can contain secrets and passwords. We should only mount the logfile.log file on the volume as a best practice. So let&#39;s do that next. . Application log files from the docker container . To do this we need to slightly update our application, and create the logfile.log file in a designated log/ folder inside the app/. This way we can only mount app/log/ folder on the volume. In our application we will update the logging module as . # app/src/commons/logger.py import logging import os if not os.path.exists(&quot;logs&quot;): os.makedirs(&quot;logs&quot;) logformat = &quot;%(levelname)s %(asctime)s - %(message)s&quot; filename = &quot;logs/logfile.log&quot; # Setting the config of the log object logging.basicConfig( format=logformat, filename=filename, level=logging.INFO, ) . This is all we need to change in our application. To cleanup the volume we can either remove the old one and recreate a new one. Or we can directly delete all the files &amp; folders from the host OS from the directory . wsl$ docker-desktop-data version-pack-data community docker volumes applogs _data . Since we have updated the application code we need to rebuild our docker image. Then create a new docker container, and this time only mount the logs/ folder on the volume. Command for this is . # delete the old volume docker volume rm applogs # create a new volume docker volume create applogs # build the docker image docker build --tag python-docker . # run the docker container and mount a specific folder on volume docker run -d --name helloworld -v applogs:/app/logs python-docker . If we check the mounted volume, this time only logfile.log is exposed. . . All the code for this post can be found at GitHub repository https://github.com/hassaanbinaslam/snapshots-docker-post-11032022 . Project code files | Project zip file | .",
            "url": "https://hassaanbinaslam.github.io/myblog/docker/python/2022/03/11/docker-app-logs.html",
            "relUrl": "/docker/python/2022/03/11/docker-app-logs.html",
            "date": " • Mar 11, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "Sklearn Pipeline and Transformers Deep Dive",
            "content": ". About . This notebook shows various ways to work with Skearn Pipelines. . We will start with some of the limitations of pipelines and how to overcome them | We will discuss getting a dataframe from a pipeline instead of a NumPy array, and the benefits of this approach | We will learn how to use CustomTransformer and a FunctionTrasformer | We will also build a custom transformer to do some feature engineering | Along the way, we will also see how to avoid common mistakes while creating pipelines | . Setup . Environment Details . from platform import python_version import sklearn, numpy, matplotlib, pandas print(&quot;python==&quot; + python_version()) print(&quot;sklearn==&quot; + sklearn.__version__) print(&quot;numpy==&quot; + numpy.__version__) print(&quot;pandas==&quot; + pandas.__version__) print(&quot;matplotlib==&quot; + matplotlib.__version__) . . python==3.8.8 sklearn==1.0.2 numpy==1.20.1 pandas==1.2.3 matplotlib==3.5.1 . Loading Data . For this example we will use the original Titanic dataset, describing the survival status of individual passengers on the Titanic ship. . Some notes from original source: . The variables on our extracted dataset are &#39;pclass&#39;, &#39;name&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;ticket&#39;, &#39;fare&#39;,&#39;cabin&#39;, &#39;embarked&#39;, &#39;boat&#39;, &#39;body&#39;, and &#39;home.dest&#39;. | pclass refers to passenger class (1st, 2nd, 3rd), and is a proxy for socio-economic class. | Age is in years, and some infants had fractional values. | sibsp = Number of Siblings/Spouses aboard | parch = Number of Parents/Children aboard | The target is either a person survived or not (1 or 0) | . Important note: The purpose of this notebook is not to train a best model on titanic data, but to understand the working of Sklearn pipeline and transformers. So please be mindful of that. . from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split import numpy as np np.random.seed(42) # for consistency # Load data from https://www.openml.org/d/40945 X, y = fetch_openml(&quot;titanic&quot;, version=1, as_frame=True, return_X_y=True) X.head() . pclass name sex age sibsp parch ticket fare cabin embarked boat body home.dest . 0 1.0 | Allen, Miss. Elisabeth Walton | female | 29.0000 | 0.0 | 0.0 | 24160 | 211.3375 | B5 | S | 2 | NaN | St Louis, MO | . 1 1.0 | Allison, Master. Hudson Trevor | male | 0.9167 | 1.0 | 2.0 | 113781 | 151.5500 | C22 C26 | S | 11 | NaN | Montreal, PQ / Chesterville, ON | . 2 1.0 | Allison, Miss. Helen Loraine | female | 2.0000 | 1.0 | 2.0 | 113781 | 151.5500 | C22 C26 | S | None | NaN | Montreal, PQ / Chesterville, ON | . 3 1.0 | Allison, Mr. Hudson Joshua Creighton | male | 30.0000 | 1.0 | 2.0 | 113781 | 151.5500 | C22 C26 | S | None | 135.0 | Montreal, PQ / Chesterville, ON | . 4 1.0 | Allison, Mrs. Hudson J C (Bessie Waldo Daniels) | female | 25.0000 | 1.0 | 2.0 | 113781 | 151.5500 | C22 C26 | S | None | NaN | Montreal, PQ / Chesterville, ON | . # let&#39;s check the frequency of missing values in each feature X.isnull().sum().sort_values(ascending=False) . body 1188 cabin 1014 boat 823 home.dest 564 age 263 embarked 2 fare 1 pclass 0 name 0 sex 0 sibsp 0 parch 0 ticket 0 dtype: int64 . # let&#39;s drop top 4 features with highest percentage of missing data # This step is done to make our working with pipeline simpler and easier to understand X.drop([&#39;body&#39;, &#39;cabin&#39;, &#39;boat&#39;, &#39;home.dest&#39;], axis=1, inplace=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2) X_train.head() . pclass name sex age sibsp parch ticket fare embarked . 999 3.0 | McCarthy, Miss. Catherine &#39;Katie&#39; | female | NaN | 0.0 | 0.0 | 383123 | 7.7500 | Q | . 392 2.0 | del Carlo, Mrs. Sebastiano (Argenia Genovesi) | female | 24.0 | 1.0 | 0.0 | SC/PARIS 2167 | 27.7208 | C | . 628 3.0 | Andersson, Miss. Sigrid Elisabeth | female | 11.0 | 4.0 | 2.0 | 347082 | 31.2750 | S | . 1165 3.0 | Saad, Mr. Khalil | male | 25.0 | 0.0 | 0.0 | 2672 | 7.2250 | C | . 604 3.0 | Abelseth, Miss. Karen Marie | female | 16.0 | 0.0 | 0.0 | 348125 | 7.6500 | S | . Some Terminology First . Datasets . Scikit-learn deals with learning information from one or more datasets that are represented as 2D arrays. They can be understood as a list of multi-dimensional observations. We say that the first axis of these arrays is the samples axis, while the second is the features axis. . (n_samples, n_features) . # for our titanic dataset: # n_samples = 1309 # n_features = 9 X.shape . (1309, 9) . Estimator . An estimator is any object that learns from data; it may be a classification, regression or clustering algorithm or a transformer that extracts/filters useful features from raw data. . All estimator objects expose a fit method that takes a dataset (usually a 2-d array) . estimator.fit(data) . Transformer . An estimator supporting transform and/or fit_transform methods. . A transformer, transforms the input, usually only X, into some transformed space. Output is an array or sparse matrix of length n_samples and with the number of columns fixed after fitting. . Fit . The fit method is provided on every estimator. It usually takes some samples X, targets y if the model is supervised, and potentially other sample properties such as sample_weight. . It should: . clear any prior attributes stored on the estimator, unless warm_start is used | validate and interpret any parameters, ideally raising an error if invalid | validate the input data | estimate and store model attributes from the estimated parameters and provided data; and | return the now fitted estimator to facilitate method chaining | . Note: . Fitting = Calling fit (or fit_transform, fit_predict) method on an estimator. | Fitted = The state of an estimator after fitting. | . Sklearn Pipeline . class sklearn.pipeline.Pipeline(steps, *, memory=None, verbose=False) . It is a pipeline of transformers with a final estimator. . It sequentially applies a list of transforms and a final estimator. Intermediate steps of the pipeline must be transforms, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. . Lets create a simple pipeline to better understand its componets. Steps in our pipeline will be . replace missing values using the mean along each numerical feature column; and | then scale them | . from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler pipe = Pipeline(steps=[ (&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler()) ]) # our first pipeline has been initialized pipe . Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler())]) . We can also visualize the pipeline as a diagram. It has two steps: imputer and scaler in sequence. . from sklearn import set_config set_config(display=&quot;diagram&quot;) pipe . Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputer()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]) . Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputer()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]) . SimpleImputerSimpleImputer() . StandardScalerStandardScaler() . now lets call fit_transform method to run this pipeline, and preprocess our loaded data . pipe.fit_transform(X_train, y_train) . ValueError Traceback (most recent call last) Input In [8], in &lt;module&gt; 1 #collapse-output -&gt; 2 pipe.fit_transform(X_train, y_train) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn pipeline.py:426, in Pipeline.fit_transform(self, X, y, **fit_params) 399 &#34;&#34;&#34;Fit the model and transform with the final estimator. 400 401 Fits all the transformers one after the other and transform the (...) 423 Transformed samples. 424 &#34;&#34;&#34; 425 fit_params_steps = self._check_fit_params(**fit_params) --&gt; 426 Xt = self._fit(X, y, **fit_params_steps) 428 last_step = self._final_estimator 429 with _print_elapsed_time(&#34;Pipeline&#34;, self._log_message(len(self.steps) - 1)): File ~ anaconda3 envs sc_mlflow lib site-packages sklearn pipeline.py:348, in Pipeline._fit(self, X, y, **fit_params_steps) 346 cloned_transformer = clone(transformer) 347 # Fit or load from cache the current transformer --&gt; 348 X, fitted_transformer = fit_transform_one_cached( 349 cloned_transformer, 350 X, 351 y, 352 None, 353 message_clsname=&#34;Pipeline&#34;, 354 message=self._log_message(step_idx), 355 **fit_params_steps[name], 356 ) 357 # Replace the transformer of the step with the fitted 358 # transformer. This is necessary when loading the transformer 359 # from the cache. 360 self.steps[step_idx] = (name, fitted_transformer) File ~ anaconda3 envs sc_mlflow lib site-packages joblib memory.py:352, in NotMemorizedFunc.__call__(self, *args, **kwargs) 351 def __call__(self, *args, **kwargs): --&gt; 352 return self.func(*args, **kwargs) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn pipeline.py:893, in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params) 891 with _print_elapsed_time(message_clsname, message): 892 if hasattr(transformer, &#34;fit_transform&#34;): --&gt; 893 res = transformer.fit_transform(X, y, **fit_params) 894 else: 895 res = transformer.fit(X, y, **fit_params).transform(X) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn base.py:855, in TransformerMixin.fit_transform(self, X, y, **fit_params) 852 return self.fit(X, **fit_params).transform(X) 853 else: 854 # fit method of arity 2 (supervised transformation) --&gt; 855 return self.fit(X, y, **fit_params).transform(X) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn impute _base.py:319, in SimpleImputer.fit(self, X, y) 302 def fit(self, X, y=None): 303 &#34;&#34;&#34;Fit the imputer on `X`. 304 305 Parameters (...) 317 Fitted estimator. 318 &#34;&#34;&#34; --&gt; 319 X = self._validate_input(X, in_fit=True) 321 # default fill_value is 0 for numerical input and &#34;missing_value&#34; 322 # otherwise 323 if self.fill_value is None: File ~ anaconda3 envs sc_mlflow lib site-packages sklearn impute _base.py:285, in SimpleImputer._validate_input(self, X, in_fit) 279 if &#34;could not convert&#34; in str(ve): 280 new_ve = ValueError( 281 &#34;Cannot use {} strategy with non-numeric data: n{}&#34;.format( 282 self.strategy, ve 283 ) 284 ) --&gt; 285 raise new_ve from None 286 else: 287 raise ve ValueError: Cannot use mean strategy with non-numeric data: could not convert string to float: &#34;McCarthy, Miss. Catherine &#39;Katie&#39;&#34; . . Aaargh! this is not what we intended. Let us try to understand why our pipeline did not work and then fix it. The exception message says: . ValueError: Cannot use mean strategy with non-numeric data: could not convert string to float: &quot;McCarthy, Miss. Catherine &#39;Katie&#39; . From the error message we can deduce that Pipeline is trying to apply its transformers on all columns in the dataset. This was not our intention, as we wanted to apply the transformers to numeric data only. Let&#39;s limit our simple pipeline to numerical columns and run again. . num_cols = [&#39;age&#39;, &#39;fare&#39;] pipe.fit_transform(X_train[num_cols], y_train) . array([[ 0. , -0.49963779], [-0.43641134, -0.09097855], [-1.44872891, -0.01824953], ..., [-0.98150542, -0.49349894], [-0.82576425, -0.44336498], [-0.59215251, -0.49349894]]) . Alright, our pipeline has run now and we can also observe a few outcomes. . When we apply a pipeline to a dataset it will run transformers to all features in the dataset. | Output from one transformer will be passed on to the next one until we reach the end of the pipeline | If we want to apply different transformers for numerical and categorical features (heterogeneous data) then the pipeline will not work for us. We would have to create separate pipelines for the different feature sets and then join the output. | . To overcome the limitation of a pipeline for heterogeneous data, Sklearn recommends using ColumnTransformer. With ColumnTransformer we can provide column names against the transformers on which we want to apply them. . ColumnTransformer . Let&#39;s see our first ColumnTransformer in action. . from sklearn.compose import ColumnTransformer from sklearn.preprocessing import OneHotEncoder # Note the sequence when creating a ColumnTransformer # 1. a name for the transformer # 2. the transformer # 3. the column names pipe = ColumnTransformer([ (&#39;standardscaler&#39;, StandardScaler(), [&#39;age&#39;, &#39;fare&#39;] ), (&#39;onehotencoder&#39;, OneHotEncoder(), [&#39;sex&#39;]) ]) pipe . ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])]) . Please rerun this cell to show the HTML repr or trust the notebook.ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])]) . standardscaler[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder() . # Let&#39;s execute our pipeline to see the output. pipe.fit_transform(X_train, y_train) . array([[ nan, -0.49939913, 1. , 0. ], [-0.39043136, -0.09093509, 1. , 0. ], [-1.2960919 , -0.01824081, 1. , 0. ], ..., [-0.87809473, -0.49326321, 0. , 1. ], [-0.73876234, -0.4431532 , 0. , 1. ], [-0.52976375, -0.49326321, 0. , 1. ]]) . get_feature_names_out(input_features=None) . At this point I will also introduce a very useful function get_feature_names_out(input_features=None). Using this method we can get output feature names as well. . pipe.get_feature_names_out() . array([&#39;standardscaler__age&#39;, &#39;standardscaler__fare&#39;, &#39;onehotencoder__sex_female&#39;, &#39;onehotencoder__sex_male&#39;], dtype=object) . Notice the output . Output feature names appear as &lt;transformer_name&gt;__&lt;feature_name&gt; | For OneHotEncoded feature &quot;sex&quot;, output feature names have the label attached to them | . make_column_transformer . Sklean also provides a wrapper function for ColumnTransformer where we don&#39;t have to provide names for the transformers. . from sklearn.compose import make_column_transformer # Note the sequence when using make_column_transformer # 1. the transformer # 2. the column names pipe = make_column_transformer( (StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (OneHotEncoder(), [&#39;sex&#39;] ), verbose_feature_names_out=False # to keep output feature names simple ) pipe . ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False) . Please rerun this cell to show the HTML repr or trust the notebook.ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False) . standardscaler[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder() . # Let&#39;s execute our pipeline. pipe.fit_transform(X_train, y_train) . array([[ nan, -0.49939913, 1. , 0. ], [-0.39043136, -0.09093509, 1. , 0. ], [-1.2960919 , -0.01824081, 1. , 0. ], ..., [-0.87809473, -0.49326321, 0. , 1. ], [-0.73876234, -0.4431532 , 0. , 1. ], [-0.52976375, -0.49326321, 0. , 1. ]]) . # notice the feature names this time. they are shorter. # we have used attribute &quot;verbose_feature_names_out=False&quot; in our pipeline above. pipe.get_feature_names_out() . array([&#39;age&#39;, &#39;fare&#39;, &#39;sex_female&#39;, &#39;sex_male&#39;], dtype=object) . Important difference between Pipeline and ColumnTransformer . Pipeline applies transformer in sequence on all columns | ColumnTranformer applies transformers in parallel to specified columns and then concats the output | . Open questions? . So our ColumnTransformer is working. But we have a few more questions to address. . Why is the output from our pipeline or ColumnTransformer not shown as a dataframe with output features nicely separated in different columns? | Our input dataset had more features besides age, fare, and sex. Why are they not present in the output? | What happens if I change the sequence of transformers, and feature names in my ColumnTransformer? | . In the coming sections, we will try to address these questions. . Why is the output not a dataframe? . The output from a pipeline or a ColumnTransformer is an nd-array where the first index is the number of samples, and second index are the output features (n_samples, n_output_features). Since we are only getting numpy array as an output, we are losing information about the column names. . temp = pipe.fit_transform(X_train, y_train) print(type(temp)) print(temp.shape) . &lt;class &#39;numpy.ndarray&#39;&gt; (1047, 4) . Can we get the feature names back? . We have already seen that we can get the output feature names using method get_feature_names_out. But this time let&#39;s try to analyze our ColumnsTransformer more closely. The transformer attributes discussed here also applies to Pipeline object. . # print the internals of ColumnTransformer set_config(display=&#39;text&#39;) pipe . ColumnTransformer(transformers=[(&#39;standardscaler&#39;, StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (&#39;onehotencoder&#39;, OneHotEncoder(), [&#39;sex&#39;])], verbose_feature_names_out=False) . ColumnTransformer has an attribute &#39;transformers&#39; that is keeping a list of all the provided transformers. Let&#39;s print it. . pipe.transformers . [(&#39;standardscaler&#39;, StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (&#39;onehotencoder&#39;, OneHotEncoder(), [&#39;sex&#39;])] . These are the transformers list at the initialization time. If we want to check the transformers after fit function has been called, then we need to print a different attribute transformers_. . pipe.transformers_ . [(&#39;standardscaler&#39;, StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (&#39;onehotencoder&#39;, OneHotEncoder(), [&#39;sex&#39;]), (&#39;remainder&#39;, &#39;drop&#39;, [0, 1, 4, 5, 6, 8])] . You can see the difference. There is an extra transformer with the name remainder at the end. It was not present at the initialization time. What it does is that it drops all remaining columns from the dataset that have not been explicitly used in the ColumnTransformer. Since, at the initialization time, ColumnTransformer does not know about the other columns that it needs to drop this transformer is missing. During fit it sees the dataset and knows about the other columns, it then keeps a list of them to drop (0, 1, 4, 5, 6, 8). . We can also index through the transformers as well to fetch anyone from the list. . # second transformer from the list pipe.transformers_[1] . (&#39;onehotencoder&#39;, OneHotEncoder(), [&#39;sex&#39;]) . Notice the tuple sequence. . First is the name | Second is the transformer | Third are the column names | . We can also call get_feature_names_out method on a separate transformer from the list. . # output features from second tranformer pipe.transformers_[1][1].get_feature_names_out() . array([&#39;sex_female&#39;, &#39;sex_male&#39;], dtype=object) . # output features from last tranformer pipe.transformers_[-1][1].get_feature_names_out() # No. We cannot do this on last transformer (remainder). . AttributeError Traceback (most recent call last) Input In [22], in &lt;module&gt; 1 #collapse-output 2 # output features from last tranformer -&gt; 3 pipe.transformers_[-1][1].get_feature_names_out() AttributeError: &#39;str&#39; object has no attribute &#39;get_feature_names_out&#39; . We now have output feature names, and the output (nd-array). Can we convert them to a DataFrame? . import pandas as pd temp = pipe.fit_transform(X_train, y_train) col_names = pipe.get_feature_names_out() output = pd.DataFrame(temp.T, col_names).T output.head() . age fare sex_female sex_male . 0 NaN | -0.499399 | 1.0 | 0.0 | . 1 -0.390431 | -0.090935 | 1.0 | 0.0 | . 2 -1.296092 | -0.018241 | 1.0 | 0.0 | . 3 -0.320765 | -0.510137 | 0.0 | 1.0 | . 4 -0.947761 | -0.501444 | 1.0 | 0.0 | . FunctionTransformer . We know how to convert the transformer output to a DataFrame. It would be much simpler if we don&#39;t have to do an extra step, and can directly get a Dataframe from our fitted ColumnTransformer. . For this we can take the help of FunctionTransformer . A FunctionTransformer forwards its X (and optionally y) arguments to a user-defined function or function object and returns the result of this function. . Let&#39;s see a FunctionTransformer in action. . from sklearn.preprocessing import FunctionTransformer preprocessor = make_column_transformer( (StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (OneHotEncoder(), [&#39;sex&#39;] ), verbose_feature_names_out=False ) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;,FunctionTransformer(lambda x: pd.DataFrame(x, columns = preprocessor.get_feature_names_out()))) ]) set_config(display=&quot;diagram&quot;) pipe . Pipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;dataframer&amp;#x27;, FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x0000016196A1B040&gt;))]) . Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;dataframer&amp;#x27;, FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x0000016196A1B040&gt;))]) . preprocess: ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False) . standardscaler[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder() . FunctionTransformerFunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x0000016196A1B040&gt;) . Notice that we have applied FunctionTransformer after ColumnTransformer in a Pipeline. When we fit our pipeline on the dataset, ColumnTransformer will be fitted first and then the FunctionTransformer. Since the ColumnTransformer has been fitted first, we will be able to call get_feature_names_out on it while passing data to FunctionTransformer. . # let&#39;s run our pipeline again temp = pipe.fit_transform(X_train, y_train) temp.head() . age fare sex_female sex_male . 0 NaN | -0.499399 | 1.0 | 0.0 | . 1 -0.390431 | -0.090935 | 1.0 | 0.0 | . 2 -1.296092 | -0.018241 | 1.0 | 0.0 | . 3 -0.320765 | -0.510137 | 0.0 | 1.0 | . 4 -0.947761 | -0.501444 | 1.0 | 0.0 | . This is looking good. We are now getting back a dataframe directly from the pipeline. With a dataframe it is a lot easier to view and verify the output from the preprocessor. . But we have to be very careful with FunctionTransformer. In Sklearn docs, it says . Note: If a lambda is used as the function, then the resulting transformer will not be pickleable. . Huh! that is a very concerning point. We have also used a lambda function, and we will not be able to pickle it. Let&#39;s check it first. . import pickle # save our pipeline s1 = pickle.dumps(pipe) # reload it s2 = pickle.loads(s1) s2 . PicklingError Traceback (most recent call last) Input In [26], in &lt;module&gt; 1 import pickle 3 # save our pipeline -&gt; 4 s1 = pickle.dumps(pipe) 6 # reload it 7 s2 = pickle.loads(s1) PicklingError: Can&#39;t pickle &lt;function &lt;lambda&gt; at 0x0000016196A1B040&gt;: attribute lookup &lt;lambda&gt; on __main__ failed . The documentation was right about it. We have used a Lambda function in our FunctionTranformer and we got a pickle error. Since, the limitation is said for Lambda function, changing it with a normal function should work. Let&#39;s do that. . def get_dataframe(X, transformer): &quot;&quot;&quot; x: an nd-array transformer: fitted transformer &quot;&quot;&quot; col_names = transformer.get_feature_names_out() output = pd.DataFrame(X.T, col_names).T return output preprocessor = make_column_transformer( (StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (OneHotEncoder(), [&#39;sex&#39;] ), verbose_feature_names_out=False ) dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer) ]) set_config(display=&quot;diagram&quot;) pipe . Pipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;dataframer&amp;#x27;, FunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)}))]) . Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;dataframer&amp;#x27;, FunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)}))]) . preprocess: ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False) . standardscaler[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder() . FunctionTransformerFunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)}) . Notice the arguments for FunctionTransformer in the above code. . first argument is the function to be called | second argument are the parameters to be passed to our function | . The sequence of arguments for the callable function will be . first argument will be the output from any previous step in the pipeline (if there is any). In our case, it is nd-array coming from ColumnTransformer. It will be mapped to X. We don&#39;t have to do anything about it. | second argument (if any) we want to pass to function. In our case we need it to be the fitted transformer from the previous step so we have explicitly passed it using kw_args as key-value pair. Where key name is the same as callable method argument name (&#39;transformer&#39; in our case). | . Now let&#39;s do our pickle test one more time. . # save our pipeline s1 = pickle.dumps(pipe) # reload it s2 = pickle.loads(s1) s2 . Pipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;dataframer&amp;#x27;, FunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)}))]) . Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;dataframer&amp;#x27;, FunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)}))]) . preprocess: ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False) . standardscaler[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder() . FunctionTransformerFunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;standardscaler&amp;#x27;, StandardScaler(), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(), [&amp;#x27;sex&amp;#x27;])], verbose_feature_names_out=False)}) . Alright, no more issues so let&#39;s proceed to our next question. . Where are the rest of the columns? . By default, only the specified columns in transformers are transformed and combined in the output, and the non-specified columns are dropped. (default of remainder=&#39;drop&#39;). By specifying remainder=&#39;passthrough&#39;, all remaining columns that were not specified in transformers will be automatically passed through. This subset of columns is concatenated with the output of the transformers. . Let&#39;s see it in action. . preprocessor = make_column_transformer( (StandardScaler(), [&#39;age&#39;, &#39;fare&#39;]), (OneHotEncoder(), [&#39;sex&#39;] ), verbose_feature_names_out=False, remainder=&#39;passthrough&#39; ) # get_dataframe is already defined in last section. Intentionally omitted here. dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer) ]) temp = pipe.fit_transform(X_train, y_train) temp.head() . age fare sex_female sex_male pclass name sibsp parch ticket embarked . 0 NaN | -0.499399 | 1.0 | 0.0 | 3.0 | McCarthy, Miss. Catherine &#39;Katie&#39; | 0.0 | 0.0 | 383123 | Q | . 1 -0.390431 | -0.090935 | 1.0 | 0.0 | 2.0 | del Carlo, Mrs. Sebastiano (Argenia Genovesi) | 1.0 | 0.0 | SC/PARIS 2167 | C | . 2 -1.296092 | -0.018241 | 1.0 | 0.0 | 3.0 | Andersson, Miss. Sigrid Elisabeth | 4.0 | 2.0 | 347082 | S | . 3 -0.320765 | -0.510137 | 0.0 | 1.0 | 3.0 | Saad, Mr. Khalil | 0.0 | 0.0 | 2672 | C | . 4 -0.947761 | -0.501444 | 1.0 | 0.0 | 3.0 | Abelseth, Miss. Karen Marie | 0.0 | 0.0 | 348125 | S | . We have our remaining features back now, so let&#39;s proceed to our next question. . What happens if I change the sequence in ColumnTranformer? . It is better to make some changes and then see the results. I am making two changes in ColumnTransformer . Changed the order of transformers (OHE before scaling) | Changed the order of features inside the transformer (&#39;fare&#39; before &#39;age&#39;) | preprocessor = make_column_transformer( (OneHotEncoder(), [&#39;sex&#39;] ), (StandardScaler(), [&#39;fare&#39;, &#39;age&#39;]), verbose_feature_names_out=False, remainder=&#39;passthrough&#39; ) # get_dataframe is already defined in last section. Intentionally omitted here. dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer) ]) temp = pipe.fit_transform(X_train, y_train) temp.head() . sex_female sex_male fare age pclass name sibsp parch ticket embarked . 0 1.0 | 0.0 | -0.499399 | NaN | 3.0 | McCarthy, Miss. Catherine &#39;Katie&#39; | 0.0 | 0.0 | 383123 | Q | . 1 1.0 | 0.0 | -0.090935 | -0.390431 | 2.0 | del Carlo, Mrs. Sebastiano (Argenia Genovesi) | 1.0 | 0.0 | SC/PARIS 2167 | C | . 2 1.0 | 0.0 | -0.018241 | -1.296092 | 3.0 | Andersson, Miss. Sigrid Elisabeth | 4.0 | 2.0 | 347082 | S | . 3 0.0 | 1.0 | -0.510137 | -0.320765 | 3.0 | Saad, Mr. Khalil | 0.0 | 0.0 | 2672 | C | . 4 1.0 | 0.0 | -0.501444 | -0.947761 | 3.0 | Abelseth, Miss. Karen Marie | 0.0 | 0.0 | 348125 | S | . We can see that changing the sequence in ColumnTransformer does change the output. Also note . Specified columns in transformers are transformed and combined in the output | Transformers sequence in ColumnTransformer also represents the columns sequence in the output | When remainder=passthrough is used then remaining columns will be appended at the end. Remainder columns sequence will be same as in the input. | . Pipeline inside ColumnTransformer . Let&#39;s assume we have more requirements this time. I want . for numerical features (age, fare): impute the missing values first, and then scale them | for categorical features (sex): one hot encode them | . Our pipeline will look like this. . numeric_features = [&quot;age&quot;, &quot;fare&quot;] numeric_transformer = Pipeline( steps=[ (&quot;imputer&quot;, SimpleImputer(strategy=&quot;median&quot;)), (&quot;scaler&quot;, StandardScaler()) ]) categorical_features = [&quot;sex&quot;] categorical_transformer = OneHotEncoder(handle_unknown=&quot;ignore&quot;) preprocessor = make_column_transformer( (numeric_transformer, numeric_features), # note &quot;numeric_transformer&quot; is a pipeline this time (categorical_transformer, categorical_features), remainder=&#39;passthrough&#39;, verbose_feature_names_out=False ) dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer) ]) temp = pipe.fit_transform(X_train, y_train) temp.head() . AttributeError Traceback (most recent call last) Input In [31], in &lt;module&gt; 18 dataframer = FunctionTransformer(func=get_dataframe, kw_args={&#34;transformer&#34;: preprocessor}) 19 pipe = Pipeline([ 20 (&#34;preprocess&#34;, preprocessor), 21 (&#34;dataframer&#34;, dataframer) 22 ]) &gt; 24 temp = pipe.fit_transform(X_train, y_train) 25 temp.head() File ~ anaconda3 envs sc_mlflow lib site-packages sklearn pipeline.py:434, in Pipeline.fit_transform(self, X, y, **fit_params) 432 fit_params_last_step = fit_params_steps[self.steps[-1][0]] 433 if hasattr(last_step, &#34;fit_transform&#34;): --&gt; 434 return last_step.fit_transform(Xt, y, **fit_params_last_step) 435 else: 436 return last_step.fit(Xt, y, **fit_params_last_step).transform(Xt) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn base.py:855, in TransformerMixin.fit_transform(self, X, y, **fit_params) 852 return self.fit(X, **fit_params).transform(X) 853 else: 854 # fit method of arity 2 (supervised transformation) --&gt; 855 return self.fit(X, y, **fit_params).transform(X) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn preprocessing _function_transformer.py:182, in FunctionTransformer.transform(self, X) 169 &#34;&#34;&#34;Transform X using the forward function. 170 171 Parameters (...) 179 Transformed input. 180 &#34;&#34;&#34; 181 X = self._check_input(X, reset=False) --&gt; 182 return self._transform(X, func=self.func, kw_args=self.kw_args) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn preprocessing _function_transformer.py:205, in FunctionTransformer._transform(self, X, func, kw_args) 202 if func is None: 203 func = _identity --&gt; 205 return func(X, **(kw_args if kw_args else {})) Input In [27], in get_dataframe(X, transformer) 1 def get_dataframe(X, transformer): 2 &#34;&#34;&#34; 3 x: an nd-array 4 transformer: fitted transformer 5 &#34;&#34;&#34; -&gt; 6 col_names = transformer.get_feature_names_out() 7 output = pd.DataFrame(X.T, col_names).T 8 return output File ~ anaconda3 envs sc_mlflow lib site-packages sklearn compose _column_transformer.py:481, in ColumnTransformer.get_feature_names_out(self, input_features) 479 transformer_with_feature_names_out = [] 480 for name, trans, column, _ in self._iter(fitted=True): --&gt; 481 feature_names_out = self._get_feature_name_out_for_transformer( 482 name, trans, column, input_features 483 ) 484 if feature_names_out is None: 485 continue File ~ anaconda3 envs sc_mlflow lib site-packages sklearn compose _column_transformer.py:454, in ColumnTransformer._get_feature_name_out_for_transformer(self, name, trans, column, feature_names_in) 450 if isinstance(column, Iterable) and not all( 451 isinstance(col, str) for col in column 452 ): 453 column = _safe_indexing(feature_names_in, column) --&gt; 454 return trans.get_feature_names_out(column) File ~ anaconda3 envs sc_mlflow lib site-packages sklearn pipeline.py:751, in Pipeline.get_feature_names_out(self, input_features) 749 for _, name, transform in self._iter(): 750 if not hasattr(transform, &#34;get_feature_names_out&#34;): --&gt; 751 raise AttributeError( 752 &#34;Estimator {} does not provide get_feature_names_out. &#34; 753 &#34;Did you mean to call pipeline[:-1].get_feature_names_out&#34; 754 &#34;()?&#34;.format(name) 755 ) 756 feature_names_out = transform.get_feature_names_out(feature_names_out) 757 return feature_names_out AttributeError: Estimator imputer does not provide get_feature_names_out. Did you mean to call pipeline[:-1].get_feature_names_out()? . . Oh geez! What went wrong this time. The error message says . AttributeError: Estimator imputer does not provide get_feature_names_out. Did you mean to call pipeline[:-1].get_feature_names_out()? . From the error message I am getting that . Estimator imputer does not provide get_feature_names_out . Hmmm, this is strange. Why is this estimator missing a very useful function? Let&#39;s check the docs first on SimpleImputer. For the docs I indeed could not find this method get_feature_names_out() for this transformer. A little googling lead me to this Sklearn Github issue page Implement get_feature_names_out for all estimators. Developers are actively adding get_feature_names_out() to all estimators and transformers, and it looks like this feature has not been implemented for SimpleImputer till Sklearn version==1.0.2. But no worries we can overcome this limitation, and implement this feature ourselves through a custom transformer. . Custom Transformer . We can create a custom transformer or an estimator simply by inheriting a class from BaseEstimator and optionally the mixin classes in sklearn.base. Sklean provides a template that we can use to create our custom transformer. Template link is here: https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/_template.py#L146 . Let us use the same pipeline as in last cell but replace SimpleImputer with a custom one. . from sklearn.base import BaseEstimator, TransformerMixin class SimpleImputerCustom(BaseEstimator, TransformerMixin): def __init__(self, strategy=&#39;mean&#39;): self.strategy = strategy self.imputer = SimpleImputer(strategy=self.strategy) def fit(self, X, y): self.imputer.fit(X, y) return self def transform(self, X): return self.imputer.transform(X) def get_feature_names_out(self, input_features=None): # we have returned the input features name as out features will have the same name return input_features numeric_features = [&quot;age&quot;, &quot;fare&quot;] numeric_transformer = Pipeline( steps=[ (&quot;imputer&quot;, SimpleImputerCustom(strategy=&#39;mean&#39;)), (&quot;scaler&quot;, StandardScaler()) ] ) categorical_features = [&quot;sex&quot;] categorical_transformer = OneHotEncoder(handle_unknown=&quot;ignore&quot;) preprocessor = make_column_transformer( (numeric_transformer, numeric_features), # note &quot;numeric_transformer&quot; is a pipeline (categorical_transformer, categorical_features), remainder=&#39;passthrough&#39;, verbose_feature_names_out=False ) dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer) ]) temp = pipe.fit_transform(X_train, y_train) temp.head() . age fare sex_female sex_male pclass name sibsp parch ticket embarked . 0 0.0 | -0.499638 | 1.0 | 0.0 | 3.0 | McCarthy, Miss. Catherine &#39;Katie&#39; | 0.0 | 0.0 | 383123 | Q | . 1 -0.436411 | -0.090979 | 1.0 | 0.0 | 2.0 | del Carlo, Mrs. Sebastiano (Argenia Genovesi) | 1.0 | 0.0 | SC/PARIS 2167 | C | . 2 -1.448729 | -0.01825 | 1.0 | 0.0 | 3.0 | Andersson, Miss. Sigrid Elisabeth | 4.0 | 2.0 | 347082 | S | . 3 -0.358541 | -0.510381 | 0.0 | 1.0 | 3.0 | Saad, Mr. Khalil | 0.0 | 0.0 | 2672 | C | . 4 -1.059376 | -0.501684 | 1.0 | 0.0 | 3.0 | Abelseth, Miss. Karen Marie | 0.0 | 0.0 | 348125 | S | . Feature Engineering with Custom Transformer . So far, so good! Let&#39;s assume that we have another requirement and it is about feature engineering. We have to combine &#39;sibsp&#39; and &#39;parch&#39; into two new features: family_size and is_alone. . Let&#39;s implement this now. . class FamilyFeatureTransformer(BaseEstimator, TransformerMixin): def __init__(self): pass def fit(self, X, y): return self def transform(self, X): X[&#39;family_size&#39;] = X[&#39;parch&#39;] + X[&#39;sibsp&#39;] X.drop([&#39;parch&#39;, &#39;sibsp&#39;], axis=1, inplace=True) # we can drop this feature now X[&#39;is_alone&#39;] = 1 X.loc[X[&#39;family_size&#39;] &gt; 1, &#39;is_alone&#39;] = 0 return X def get_feature_names_out(self, input_features=None): # this time we have created new features. Their names are different from input features. # so we have explicitly mentioned them here. return [&#39;family_size&#39;, &#39;is_alone&#39;] numeric_features = [&quot;age&quot;, &quot;fare&quot;] numeric_transformer = Pipeline( steps=[ (&quot;imputer&quot;, SimpleImputerCustom(strategy=&#39;mean&#39;)), (&quot;scaler&quot;, StandardScaler()) ] ) categorical_features = [&quot;sex&quot;] categorical_transformer = OneHotEncoder(handle_unknown=&quot;ignore&quot;) family_features = [&quot;parch&quot;, &quot;sibsp&quot;] family_transformer = FamilyFeatureTransformer() preprocessor = make_column_transformer( (numeric_transformer, numeric_features), (categorical_transformer, categorical_features), (family_transformer, family_features), remainder=&#39;drop&#39;, # let&#39;s drop extra features this time verbose_feature_names_out=False ) dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer) ]) temp = pipe.fit_transform(X_train, y_train) temp.head() . age fare sex_female sex_male family_size is_alone . 0 0.000000 | -0.499638 | 1.0 | 0.0 | 0.0 | 1.0 | . 1 -0.436411 | -0.090979 | 1.0 | 0.0 | 1.0 | 1.0 | . 2 -1.448729 | -0.018250 | 1.0 | 0.0 | 6.0 | 0.0 | . 3 -0.358541 | -0.510381 | 0.0 | 1.0 | 0.0 | 1.0 | . 4 -1.059376 | -0.501684 | 1.0 | 0.0 | 0.0 | 1.0 | . Sklean Pipeline with Feature Importance . Alright, we have our required features ready and we can now pass them to a classifier. Let&#39;s use RandomForrest as our classifier and run our pipeline with it. . from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score # &#39;preprocessor&#39; and &#39;dataframer&#39; are already declared in last section, and intentionally omitted here. pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer), (&#39;rf_estimator&#39;, RandomForestClassifier()) ]) temp = pipe.fit_transform(X_train, y_train) temp.head() . AttributeError Traceback (most recent call last) Input In [34], in &lt;module&gt; 4 # &#39;preprocessor&#39; and &#39;dataframer&#39; are already declared in last section, and intentionally omitted here. 5 pipe = Pipeline([ 6 (&#34;preprocess&#34;, preprocessor), 7 (&#34;dataframer&#34;, dataframer), 8 (&#39;rf_estimator&#39;, RandomForestClassifier()) 9 10 ]) &gt; 12 temp = pipe.fit_transform(X_train, y_train) 13 temp.head() File ~ anaconda3 envs sc_mlflow lib site-packages sklearn pipeline.py:436, in Pipeline.fit_transform(self, X, y, **fit_params) 434 return last_step.fit_transform(Xt, y, **fit_params_last_step) 435 else: --&gt; 436 return last_step.fit(Xt, y, **fit_params_last_step).transform(Xt) AttributeError: &#39;RandomForestClassifier&#39; object has no attribute &#39;transform&#39; . Okay, looks like we have made a mistake here. Error message is saying . AttributeError: &#39;RandomForestClassifier&#39; object has no attribute &#39;transform&#39; . I get that. In our pipeline we have an estimator that does not have a transform method defined for it. We should use predict method instead. . Note: . Estimators implement predict method (Template reference Estimator, Template reference Classifier) | Transformers implement transform method (Template reference Transformer) | fit_transform is same calling fit and then transform | . Let us fix the error and run our pipeline again. . # pipeline created in last section and intentionally omitted here. pipe.fit(X_train, y_train) y_pred = pipe.predict(X_test) accuracy_score(y_test, y_pred) . 0.7862595419847328 . Let&#39;s see how our final pipeline looks visually. . # set_config(display=&#39;text&#39;) pipe . Pipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;data... kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)})), (&amp;#x27;rf_estimator&amp;#x27;, RandomForestClassifier())]) . Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;data... kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)})), (&amp;#x27;rf_estimator&amp;#x27;, RandomForestClassifier())]) . preprocess: ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False) . pipeline[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . SimpleImputerCustomSimpleImputerCustom() . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;) . familyfeaturetransformer[&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;] . FamilyFeatureTransformerFamilyFeatureTransformer() . FunctionTransformerFunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)}) . RandomForestClassifierRandomForestClassifier() . We can also get the importance of features in our dataset from RandomForrest classifier. . import matplotlib.pyplot as plt clf = pipe[-1] # last estimator is the RF classifier importances = clf.feature_importances_ features = clf.feature_names_in_ indices = np.argsort(importances) plt.title(&#39;Feature Importances&#39;) plt.barh(range(len(indices)), importances[indices], color=&#39;b&#39;, align=&#39;center&#39;) plt.yticks(range(len(indices)), [features[i] for i in indices]) plt.xlabel(&#39;Relative Importance&#39;) plt.show() . Note that all the feature names were passed to the RF Classifier and that is why we were able to get them back using its attribute feature_names_in_. This can be super useful when you have many model deployed in the environment, and you can just use the model object to get information about the features it was trained on. . For a moment let&#39;s also remove the feature names from our pipeline and see how it will effect our feature importance plot. . pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), # (&quot;dataframer&quot;, dataframer), (&#39;rf_estimator&#39;, RandomForestClassifier()) ]) # fit the pipeline pipe.fit(X_train, y_train) # get the feature importance plot clf = pipe[-1] importances = clf.feature_importances_ features = clf.feature_names_in_ indices = np.argsort(importances) plt.title(&#39;Feature Importances&#39;) plt.barh(range(len(indices)), importances[indices], color=&#39;b&#39;, align=&#39;center&#39;) plt.yticks(range(len(indices)), [features[i] for i in indices]) plt.xlabel(&#39;Relative Importance&#39;) plt.show() . AttributeError Traceback (most recent call last) Input In [38], in &lt;module&gt; 12 clf = pipe[-1] 13 importances = clf.feature_importances_ &gt; 14 features = clf.feature_names_in_ 16 indices = np.argsort(importances) 18 plt.title(&#39;Feature Importances&#39;) AttributeError: &#39;RandomForestClassifier&#39; object has no attribute &#39;feature_names_in_&#39; . No feature names were passed to our classifier this time and it is missing feature_names_in_ attribute. We can circumvent this and still get feature importance plot. . pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), # (&quot;dataframer&quot;, dataframer), (&#39;rf_estimator&#39;, RandomForestClassifier()) ]) # fit the pipeline pipe.fit(X_train, y_train) # get the feature importance plot clf = pipe[-1] importances = clf.feature_importances_ # features = clf.feature_names_in_ indices = np.argsort(importances) plt.title(&#39;Feature Importances&#39;) plt.barh(range(len(indices)), importances[indices], color=&#39;b&#39;, align=&#39;center&#39;) plt.yticks(range(len(indices)), [i for i in indices]) plt.xlabel(&#39;Relative Importance&#39;) plt.show() . This time we get the same plot but not withOUT feature names, and it is not useful anymore. So definitely we need to keep the feature names with the final estimator. Feature names can help us a lot in interpreting the model. . The complete Pipeline . For an easy reference, let&#39;s put the whole pipeline in one place. . Load Data . from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split import pandas as pd import numpy as np np.random.seed(42) # for consistency X, y = fetch_openml(&quot;titanic&quot;, version=1, as_frame=True, return_X_y=True) X.drop([&#39;body&#39;, &#39;cabin&#39;, &#39;boat&#39;, &#39;home.dest&#39;], axis=1, inplace=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2) X_train.head() . pclass name sex age sibsp parch ticket fare embarked . 999 3.0 | McCarthy, Miss. Catherine &#39;Katie&#39; | female | NaN | 0.0 | 0.0 | 383123 | 7.7500 | Q | . 392 2.0 | del Carlo, Mrs. Sebastiano (Argenia Genovesi) | female | 24.0 | 1.0 | 0.0 | SC/PARIS 2167 | 27.7208 | C | . 628 3.0 | Andersson, Miss. Sigrid Elisabeth | female | 11.0 | 4.0 | 2.0 | 347082 | 31.2750 | S | . 1165 3.0 | Saad, Mr. Khalil | male | 25.0 | 0.0 | 0.0 | 2672 | 7.2250 | C | . 604 3.0 | Abelseth, Miss. Karen Marie | female | 16.0 | 0.0 | 0.0 | 348125 | 7.6500 | S | . Train Model . from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler from sklearn.base import BaseEstimator, TransformerMixin from sklearn.compose import make_column_transformer from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score from sklearn.impute import SimpleImputer class FamilyFeatureTransformer(BaseEstimator, TransformerMixin): def __init__(self): pass def fit(self, X, y): return self def transform(self, X): X[&#39;family_size&#39;] = X[&#39;parch&#39;] + X[&#39;sibsp&#39;] X.drop([&#39;parch&#39;, &#39;sibsp&#39;], axis=1, inplace=True) # we can drop this feature now X[&#39;is_alone&#39;] = 1 X.loc[X[&#39;family_size&#39;] &gt; 1, &#39;is_alone&#39;] = 0 return X def get_feature_names_out(self, input_features=None): # this time we have created new features. Their names are different from input features. # so we have explicitly mentioned them here. return [&#39;family_size&#39;, &#39;is_alone&#39;] class SimpleImputerCustom(BaseEstimator, TransformerMixin): def __init__(self, strategy=&#39;mean&#39;): self.strategy = strategy self.imputer = SimpleImputer(strategy=self.strategy) def fit(self, X, y): self.imputer.fit(X, y) return self def transform(self, X): return self.imputer.transform(X) def get_feature_names_out(self, input_features=None): # we have returned the input features name as out features will have the same name return input_features def get_dataframe(X, transformer): &quot;&quot;&quot; x: an nd-array transformer: fitted transformer &quot;&quot;&quot; col_names = transformer.get_feature_names_out() output = pd.DataFrame(X.T, col_names).T return output numeric_features = [&quot;age&quot;, &quot;fare&quot;] numeric_transformer = Pipeline( steps=[ (&quot;imputer&quot;, SimpleImputerCustom(strategy=&#39;mean&#39;)), (&quot;scaler&quot;, StandardScaler()) ] ) categorical_features = [&quot;sex&quot;] categorical_transformer = OneHotEncoder(handle_unknown=&quot;ignore&quot;) family_features = [&quot;parch&quot;, &quot;sibsp&quot;] family_transformer = FamilyFeatureTransformer() preprocessor = make_column_transformer( (numeric_transformer, numeric_features), (categorical_transformer, categorical_features), (family_transformer, family_features), remainder=&#39;drop&#39;, # let&#39;s drop extra features this time verbose_feature_names_out=False ) dataframer = FunctionTransformer(func=get_dataframe, kw_args={&quot;transformer&quot;: preprocessor}) pipe = Pipeline([ (&quot;preprocess&quot;, preprocessor), (&quot;dataframer&quot;, dataframer), (&#39;rf_estimator&#39;, RandomForestClassifier()) ]) pipe.fit(X_train, y_train) y_pred = pipe.predict(X_test) accuracy_score(y_test, y_pred) . 0.8015267175572519 . Plot Feature Importance . import matplotlib.pyplot as plt clf = pipe[-1] # last estimator is the RF classifier importances = clf.feature_importances_ features = clf.feature_names_in_ indices = np.argsort(importances) plt.title(&#39;Feature Importances&#39;) plt.barh(range(len(indices)), importances[indices], color=&#39;b&#39;, align=&#39;center&#39;) plt.yticks(range(len(indices)), [features[i] for i in indices]) plt.xlabel(&#39;Relative Importance&#39;) plt.show() . Pickle Test . from sklearn import set_config set_config(display=&quot;diagram&quot;) import pickle # save our pipeline s1 = pickle.dumps(pipe) # reload it s2 = pickle.loads(s1) s2 . Pipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;data... kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)})), (&amp;#x27;rf_estimator&amp;#x27;, RandomForestClassifier())]) . Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[(&amp;#x27;preprocess&amp;#x27;, ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)), (&amp;#x27;data... kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)})), (&amp;#x27;rf_estimator&amp;#x27;, RandomForestClassifier())]) . preprocess: ColumnTransformerColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False) . pipeline[&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;] . SimpleImputerCustomSimpleImputerCustom() . StandardScalerStandardScaler() . onehotencoder[&amp;#x27;sex&amp;#x27;] . OneHotEncoderOneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;) . familyfeaturetransformer[&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;] . FamilyFeatureTransformerFamilyFeatureTransformer() . FunctionTransformerFunctionTransformer(func=&lt;function get_dataframe at 0x00000161997DF3A0&gt;, kw_args={&amp;#x27;transformer&amp;#x27;: ColumnTransformer(transformers=[(&amp;#x27;pipeline&amp;#x27;, Pipeline(steps=[(&amp;#x27;imputer&amp;#x27;, SimpleImputerCustom()), (&amp;#x27;scaler&amp;#x27;, StandardScaler())]), [&amp;#x27;age&amp;#x27;, &amp;#x27;fare&amp;#x27;]), (&amp;#x27;onehotencoder&amp;#x27;, OneHotEncoder(handle_unknown=&amp;#x27;ignore&amp;#x27;), [&amp;#x27;sex&amp;#x27;]), (&amp;#x27;familyfeaturetransformer&amp;#x27;, FamilyFeatureTransformer(), [&amp;#x27;parch&amp;#x27;, &amp;#x27;sibsp&amp;#x27;])], verbose_feature_names_out=False)}) . RandomForestClassifierRandomForestClassifier() .",
            "url": "https://hassaanbinaslam.github.io/myblog/ml/sklearn/2022/03/04/sklearn-pipeline-deep-dive.html",
            "relUrl": "/ml/sklearn/2022/03/04/sklearn-pipeline-deep-dive.html",
            "date": " • Mar 4, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "AWS CloudFormation Template, Functions, and Commands",
            "content": ". About . This post is a collection of useful notes on various sections of AWS CloudFormation template, and intrinsic functions. Knowledge about them is often tested in AWS certifications. For more details on this subject refer to its user guide (https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html) . Template Anatomy . The following is an example of AWS CloudFormation template and its sections in YAML format. There is no sequence to writing these sections besides that if there is a Description section then it must be put after AWSTemplateFormatVersion. . AWSTemplateFormatVersion: &quot;version date&quot; Description: String Metadata: template metadata Parameters: set of parameters Rules: set of rules Mappings: set of mappings Conditions: set of conditions Transform: set of transforms Resources: set of resources Outputs: set of outputs . Template Sections . AWSTemplateFormatVersion (optional) . The AWS CloudFormation template version that the template conforms to. . Syntax . AWSTemplateFormatVersion: &quot;2010-09-09&quot; . Description (optional) . A text string that describes the template. This section must always follow the template format version section. . Syntax . Description: &gt; Here are some details about the template. . Metadata (optional) . Objects that provide additional information about the template. . Difference between Metadata and Description is that some cloudformation features can refer to the objects that are defined in Metadata section. For example, you can use a metadata key AWS::CloudFormation::Interface to define how parameters are grouped and sorted on AWS cloudformation console. By default, cloudformation console alphbetically sorts the parameters by their logical ID. | AWS strongly recommends not to use this section for storing sensitive information such as passwords or secrets. | . Syntax . Metadata: Instances: Description: &quot;Information about the instances&quot; Databases: Description: &quot;Information about the databases&quot; . Parameters (optional) . Parameters enable you to input custom values to your template each time you create or update a stack. You can refer to parameters from the Resources and Outputs sections of the template using Ref intrinsic function. . CloudFormation currently supports the following parameter types . String – A literal string | Number – An integer or float | List&lt;Number&gt; – An array of integers or floats | CommaDelimitedList – An array of literal strings that are separated by commas | AWS::EC2::KeyPair::KeyName – An Amazon EC2 key pair name | AWS::EC2::SecurityGroup::Id – A security group ID | AWS::EC2::Subnet::Id – A subnet ID | AWS::EC2::VPC::Id – A VPC ID | List&lt;AWS::EC2::VPC::Id&gt; – An array of VPC IDs | List&lt;AWS::EC2::SecurityGroup::Id&gt; – An array of security group IDs | List&lt;AWS::EC2::Subnet::Id&gt; – An array of subnet IDs | . Syntax . The following example declares a parameter named InstanceTypeParameter. This parameter lets you specify the Amazon EC2 instance type for the stack to use when you create or update the stack. . Note that InstanceTypeParameter has a default value of t2.micro. This is the value that AWS CloudFormation will use to provision the stack unless another value is provided. . Parameters: InstanceTypeParameter: Type: String Default: t2.micro AllowedValues: - t2.micro - m1.small - m1.large Description: Enter t2.micro, m1.small, or m1.large. Default is t2.micro. . Referencing a parameter in template (Ref function) . In the following example, the InstanceType property of the EC2 instance resource references the InstanceTypeParameter parameter value. . Ec2Instance: Type: AWS::EC2::Instance Properties: InstanceType: Ref: InstanceTypeParameter ImageId: ami-0ff8a91507f77f867 . Rules (optional) . Validates a parameter or a combination of parameters that are passed to a template during a stack creation or stack update. . You can use the following rule-specific intrinsic functions to define rule conditions and assertions: . Fn::And | Fn::Contains | Fn::EachMemberEquals | Fn::EachMemberIn | Fn::Equals | Fn::If | Fn::Not | Fn::Or | Fn::RefAll | Fn::ValueOf | Fn::ValueOfAll | . Syntax . In the following example, the rule checks the value of the InstanceType parameter. The user must specify a1.medium, if the value of the environment parameter is test. . Rules: testInstanceType: RuleCondition: !Equals - !Ref Environment - test Assertions: - Assert: &#39;Fn::Contains&#39;: - - a1.medium - !Ref InstanceType AssertDescription: &#39;For a test environment, the instance type must be a1.medium&#39; . Mappings (optional) . The optional Mappings section matches a key to a corresponding set of named values similar to a lookup table. For example, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region. You use the Fn::FindInMap intrinsic function in the Resources and Outputs to retrieve values in a map. Note that you can&#39;t include parameters, pseudo parameters, or intrinsic functions in the Mappings section. . Fn::FindInMap . The intrinsic function Fn::FindInMap returns the value corresponding to keys in a two-level map that&#39;s declared in the Mappings section. . Syntax for the short form: . !FindInMap [ MapName, TopLevelKey, SecondLevelKey ] . Parameters . MapName The logical name of a mapping declared in the Mappings section that contains the keys and values. | . | TopLevelKey The top-level key name. Its value is a list of key-value pairs. | . | SecondLevelKey The second-level key name, which is set to one of the keys from the list assigned to TopLevelKey. | . | . A more concrete example . Mappings: RegionMap: us-east-1: HVM64: &quot;ami-0ff8a91507f77f867&quot; HVMG2: &quot;ami-0a584ac55a7631c0c&quot; Resources: myEC2Instance: Type: &quot;AWS::EC2::Instance&quot; Properties: ImageId: !FindInMap - RegionMap - !Ref &#39;AWS::Region&#39; # us-east-1 - HVM64 InstanceType: m1.small . Conditions (optional) . Conditions that control whether certain resources are created or whether certain resource properties are assigned a value during stack creation or update. For example, you could conditionally create a resource that depends on whether the stack is for a production or test environment. . Conditions are defined in Conditions section, and are then applied in following sections. . Parameters | Resources | Outputs | . You can use following intrinsic functions to define your conditions . Fn::And | Fn::Equals | Fn::If | Fn::Not | Fn::Or | . Syntax . Conditions: Logical ID: Intrinsic function . A more concrete example . AWSTemplateFormatVersion: 2010-09-09 Parameters: EnvType: Description: Environment type. Default: test Type: String AllowedValues: - prod - test ConstraintDescription: must specify prod or test. Conditions: CreateProdResources: !Equals - !Ref EnvType - prod Resources: EC2Instance: Type: &#39;AWS::EC2::Instance&#39; Properties: ImageId: ami-0ff8a91507f77f867 MountPoint: Type: &#39;AWS::EC2::VolumeAttachment&#39; Condition: CreateProdResources Properties: InstanceId: !Ref EC2Instance VolumeId: !Ref NewVolume Device: /dev/sdh NewVolume: Type: &#39;AWS::EC2::Volume&#39; Condition: CreateProdResources Properties: Size: 100 AvailabilityZone: !GetAtt - EC2Instance - AvailabilityZone . Difference between Rules and Conditions usage? . Rules are used to evaluate the input given by the user in Parameters | Conditions turn come after all rules have been evaluated | Conditions are not limited to Parameters and can also work with Resources and Outputs | . Transform (optional) . For serverless applications (also referred to as Lambda-based applications), specifies the version of the AWS Serverless Application Model (AWS SAM) to use. When you specify a transform, you can use AWS SAM syntax to declare resources in your template. The model defines the syntax that you can use and how it&#39;s processed. . You can also use AWS::Include transforms to work with template snippets that are stored separately from the main AWS CloudFormation template. You can store your snippet files in an Amazon S3 bucket and then reuse the functions across multiple templates. . Syntax . Transform: - MyMacro - &#39;AWS::Serverless&#39; . AWS::Include transform . Use the AWS::Include transform, which is a macro hosted by AWS CloudFormation, to insert boilerplate content into your templates. The AWS::Include transform lets you create a reference to a template snippet in an Amazon S3 bucket. The AWS::Include function behaves similarly to an include, copy, or import directive in programming languages. . Example . Transform: Name: &#39;AWS::Include&#39; Parameters: Location: &#39;s3://MyAmazonS3BucketName/MyFileName.yaml&#39; . Resources (required) . Specifies the stack resources and their properties, such as an Amazon Elastic Compute Cloud instance or an Amazon Simple Storage Service bucket. You can refer to resources in the Resources and Outputs sections of the template. . Syntax . Resources: Logical ID: Type: Resource type Properties: Set of properties . A more concrete example . Resources: MyEC2Instance: Type: &quot;AWS::EC2::Instance&quot; Properties: ImageId: &quot;ami-0ff8a91507f77f867&quot; . Outputs (optional) . The optional Outputs section declares output values that you can import into other stacks (to create cross-stack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name from a stack to make the bucket easier to find. . Notes . You can declare a maximum of 200 outputs in a template. | AWS strongly recommend you don&#39;t use this section to output sensitive information, such as passwords or secrets | Output values are available after the stack operation is complete. Stack output values aren&#39;t available when a stack status is in any of the IN_PROGRESS status. | AWS also does not recommend establishing dependencies between a service runtime and the stack output value because output values might not be available at all times. | . Syntax . Outputs: Logical ID: Description: Information about the value Value: Value to return Export: Name: Name of resource to export . A more concrete example where certain values are shown as output at the end of stack creation. . Outputs: BackupLoadBalancerDNSName: Description: The DNSName of the backup load balancer Value: !GetAtt BackupLoadBalancer.DNSName Condition: CreateProdResources InstanceID: Description: The Instance ID Value: !Ref EC2Instance . For Cross-Stack output use Export tag. Values outputed with &quot;Export&quot; tag can be imported in other stacks &quot;in the same region&quot;. Then, use the Fn::ImportValue intrinsic function to import the value in another stack &quot;in the same region&quot;. . Outputs: StackVPC: Description: The ID of the VPC Value: !Ref MyVPC Export: Name: !Sub &quot;${AWS::StackName}-VPCID&quot; . Some other important Intrinsic Functions . Fn::GetAtt . The Fn::GetAtt intrinsic function returns the value of an attribute from a resource in the template. . Syntax . !GetAtt logicalNameOfResource.attributeName . logicalNameOfResource The logical name (also called logical ID) of the resource that contains the attribute that you want. | . | attributeName The name of the resource-specific attribute whose value you want. See the resource&#39;s reference page for details about the attributes available for that resource type. | . | Return value The attribute value. | . | . A more concrete example . !GetAtt myELB.DNSName . Notes: . For the Fn::GetAtt logical resource name, you can&#39;t use functions. You must specify a string that&#39;s a resource&#39;s logical ID. | For the Fn::GetAtt attribute name, you can use the Ref function. | . Fn::ImportValue . The intrinsic function Fn::ImportValue returns the value of an output exported by another stack. You typically use this function to create cross-stack references. . Notes: . For each AWS account, Export names must be unique within a region. | You can&#39;t create cross-stack references across regions. You can use the intrinsic function Fn::ImportValue to import only values that have been exported within the same region. | You can&#39;t delete a stack if another stack references one of its outputs. | You can&#39;t modify or remove an output value that is referenced by another stack. | . Syntax . !ImportValue sharedValueToImport . A more concrete example. . Fn::ImportValue: !Sub &quot;${NetworkStackName}-SecurityGroupID&quot; . Fn::Sub . The intrinsic function Fn::Sub substitutes variables in an input string with values that you specify. In your templates, you can use this function to construct commands or outputs that include values that aren&#39;t available until you create or update a stack. . Syntax . !Sub - String - VarName: VarValue . Parameters . String A string with variables that AWS CloudFormation substitutes with their associated values at runtime. Write variables as ${MyVarName}. Variables can be template parameter names, resource logical IDs, resource attributes, or a variable in a key-value map. | . | VarName The name of a variable that you included in the String parameter. | . | VarValue The value that CloudFormation substitutes for the associated variable name at runtime. | . | . A more concrete example. The following example uses a mapping to substitute the ${Domain} variable with the resulting value from the Ref function. . Name: !Sub - &#39;www.${Domain}&#39; - Domain: !Ref RootDomainName . Important CloudFormation CLI Commands . Package a template using aws cloudformation package command | Validate a CloudFormation template using aws cloudformation validate-template command | Deploy a template using the aws cloudformation deploy command | .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/2022/02/28/aws-cloudformation-template.html",
            "relUrl": "/aws/2022/02/28/aws-cloudformation-template.html",
            "date": " • Feb 28, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "AWS IAM Policy Types",
            "content": ". About . Access is managed in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. . A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. . AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, Organizations SCPs, ACLs, and session policies. This post is summary of AWS IAM policy and permission types. . AWS Policy Types . Identity-based policies . Identity-based policies – Attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity. . Resource-based policies . Resource-based policies – Attach inline policies to resources. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies. Resource-based policies grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts. . Identity vs Resource based policy . . Identity-based policies are applied to IAM identities, and grant them access to AWS resources. | Resource-based policies are applied to AWS resources, and they grant access to Principals (IAM identities, and applications) | . Permissions boundaries . Permissions boundaries – Use a customer or AWS managed policy as the permissions boundary for an IAM entity (user or role). That policy defines the maximum permissions that the identity-based policies can grant to an entity, but does not grant permissions. Permissions boundaries do not define the maximum permissions that a resource-based policy can grant to an entity. . Organizations SCPs . Organizations SCPs – Use an AWS Organizations service control policy (SCP) to define the maximum permissions for account members of an organization or organizational unit (OU). SCPs limit permissions that identity-based policies or resource-based policies grant to entities (users or roles) within the account, but do not grant permissions. . Permission boundries vs Organization SCP . Both permission boundries and SCP only limit permissions. They don&#39;t give any permissions. | Permission boundries limits permissions of identity-based policies only. | SCP limits permissions on both identity and resource based policies. | . Access control lists (ACLs) . Access control lists (ACLs) – Use ACLs to control which principals in other accounts can access the resource to which the ACL is attached. ACLs are similar to resource-based policies, although they are the only policy type that does not use the JSON policy document structure. ACLs are cross-account permissions policies that grant permissions to the specified principal. ACLs cannot grant permissions to entities within the same account. . ACL vs Resource based policy . Resource based policies can grant permission to entities in same or different account | ACL can only grant permissions to entities in different account | Only a few resources support ACL including AWS Amazon S3, AWS WAF, and Amazon VPC. It is a legacy IAM policy type and AWS recommends not to use it. | . Session policies . Session policies – Pass advanced session policies when you use the AWS CLI or AWS API to assume a role or a federated user. Session policies limit the permissions that the role or user&#39;s identity-based policies grant to the session. Session policies limit permissions for a created session, but do not grant permissions. For more information, see Session Policies. .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/2022/02/23/aws-policy-types.html",
            "relUrl": "/aws/2022/02/23/aws-policy-types.html",
            "date": " • Feb 23, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "AWS DNS Records - A, CNAME, ALIAS, & MX",
            "content": ". About . This post summarizes the differences between AWS Route53 DNS records namely A record, CNAME, ALIAS, and MX. Knowledge about these differences is commonly checked in AWS certifications. . Credits . This post takes help from a few other really good articles. Please refer to them if you need more details on this subject . “Demystifying DNS Records – A, CNAME, ALIAS, MX &amp; AAAA” from Whizlabs (https://www.whizlabs.com/blog/dns-records/) . | “Why a domain’s root can’t be a CNAME — and other tidbits about the DNS” from freeCodeCamp (https://www.freecodecamp.org/news/why-cant-a-domain-s-root-be-a-cname-8cbab38e5f5c/) . | . First, some definitions . Domain Name . Domain + TLD = Domain Name | When you buy a ‘domain’ from a a registrar or reseller, you buy the rights to a specific domain name (example.com), and any subdomains you want to create (my-site.example.com, mail.example.com, etc). | The domain name (example.com) is also called the apex, root or naked domain name. | Examples of protocol are http, ftp, TCP, UDP, FTP, SMTP etc. | Examples of top level domains are .org, .net, .com, .ai etc. | . A Record . A record (or an address record) always points to an IP address. This IP address should be static like AWS Elastic IP Addresses (EIP) . Example use cases . You can point your root domain name example.com to an Elastic IP Address 192.0.2.23 . | We can also map EC2 instances IPv4 Public IP Address to an A record. But this is not recommended as EC2 instances public IP addresses change when you stop/start your server. We should always use Elastic IP addresses instead. . | . AAAA Record . AAAA record is similar to A record but for IPv6 addresses. . It always points to an IPv6 address . | Note that AWS currently does not support EIP for IPv6 (https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html) . | . MX Record . MX records (Mail Exchange records) are used for setting up email servers. . CNAME Record . CNAME records must always point to another domain name, never directly to an IP address. Since it does not point to an IP address, it is commonly used along with an A record. . One can, for example, point ftp.example.com and/or www.example.com to the DNS entry example.com, which in turn has an A record that points to the IP address. Then, if the IP address ever changes, one only has to record the change in one place within the network: in the DNS A record for example.com. . Example use cases . NAME TYPE VALUE . www.example.com | CNAME | example.com | . example.com | A | 192.0.2.23 | . An A record for example.com (root domain) points to server IP address . | A CNAME record points www.example.com to example.com . | . Now if the IP address of your server has changed you will have to update it only at one place A record. www.example.com and example.com will automatically inherit the changes. . IMPORTANT . CNAME entry for the root domain is not allowed. . | . NAME TYPE VALUE . example.com | CNAME | app.example.com | . app.example.com | A | 192.0.2.23 | . Alias Record . It is AWS Route 53 specific and only works with it. Alias works similar to CNAME but they are created by AWS to solve their specific problems discussed next. . AWS S3 buckets, Elastic Load Balancers, Elastic Beanstalk, and CloudFront offer you DNS names only and no IP addresses. e.g. when you create an S3 bucket you will get its DNS name bucket_name.s3.amazonaws.com. Now if you want to map your root domain example.com to S3 bucket DNS then we don’t have any options left as . A record points to IP addresses only . | CNAME cannot be used for root domain name . | . For this AWS came up with an Alias record in Route 53. With Alias record, you can point your domain root to another DNS name entry. . NAME TYPE VALUE . www.example.com | CNAME | example.com | . example.com | Alias | bucket_name.s3.amazonaws.com | .",
            "url": "https://hassaanbinaslam.github.io/myblog/aws/2022/02/22/aws-dns-records.html",
            "relUrl": "/aws/2022/02/22/aws-dns-records.html",
            "date": " • Feb 22, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "Python - A collection of output formatting tips",
            "content": ". About . This notebook is a collection of useful tips to format Python string literals and output. . Environment . from platform import python_version print(&quot;python==&quot; + python_version()) . . python==3.8.5 . f-string: Expressions inside a string . r = &#39;red&#39; g = &#39;green&#39; b = 1001 # f-string has a simple syntax. Put &#39;f&#39; at the start of string, and put expressions in {} f&quot;Stop = {r}, Go = {g}&quot; . &#39;Stop = red, Go = green&#39; . # &#39;F&#39; can also be used to start an f-string F&quot;binary = {b}. If you need value in brackets {{{b}}}&quot; . &#39;binary = 1001. If you need value in brackets {1001}&#39; . # f-string can also be started with &quot;&quot;&quot; quotes f&quot;&quot;&quot;{r} or {g}&quot;&quot;&quot; . &#39;red or green&#39; . # f-string on multiple lines. # 1. Use &quot;&quot;&quot; with backslash f&quot;&quot;&quot;{r} or {g}&quot;&quot;&quot; . &#39;red or green&#39; . # f-string on multiple lines. # 2. Use only backslash f&quot;{r}&quot; &quot; or &quot; f&quot;{g}&quot; . &#39;red or green&#39; . # f-string on multiple lines. # 3. Use brackets () (f&quot;{r}&quot; &quot; or &quot; f&quot;{g}&quot;) . &#39;red or green&#39; . # you can also compute an expression in an f-string f&quot;{ 40 + 2}&quot; . &#39;42&#39; . # functions can also be called from inside an f-string f&quot;This is in CAPS: { str.upper(r) }&quot; # same as above f&quot;This is in CAPS: { r.upper() }&quot; . &#39;This is in CAPS: RED&#39; . f-string: Padding the output . # Inside f-string, passing an integer after &#39;:&#39; will cause that field to be a minimum number of characters wide. # This is useful for making columns line up. groups = { &#39;small&#39;: 100, &#39;medium&#39;: 100100, &#39;large&#39;: 100100100 } for group, value in groups.items(): print(f&quot;{value:10} ==&gt; {group:20}&quot;) print(f&quot;{&#39;****&#39;*10}&quot;) # another nice trick for group, value in groups.items(): print(f&quot;{group:10} ==&gt; {value:20}&quot;) . 100 ==&gt; small 100100 ==&gt; medium 100100100 ==&gt; large **************************************** small ==&gt; 100 medium ==&gt; 100100 large ==&gt; 100100100 . f-string: Binary and hexadecimal format . # you can convert integers to binary and hexadecimal format print( f&quot;5 in binary {5:b}&quot; ) print( f&quot;5 in hexadecimal {5:#b}&quot; ) . 5 in binary 101 5 in hexadecimal 0b101 . f-string: Controlling the decimal places . import math print(f&#39;The value of pi is approximately (no formatting) {math.pi}&#39;) print(f&#39;The value of pi is approximately {math.pi :.3f}&#39;) . The value of pi is approximately (no formatting) 3.141592653589793 The value of pi is approximately 3.142 . f-string: Putting commas in numerical output . num = 3214298342.234 f&quot;{num:,}&quot; . &#39;3,214,298,342.234&#39; .",
            "url": "https://hassaanbinaslam.github.io/myblog/python/2022/02/18/python-tips-output-formatting.html",
            "relUrl": "/python/2022/02/18/python-tips-output-formatting.html",
            "date": " • Feb 18, 2022"
        }
        
    
  
    
        ,"post20": {
            "title": "Python - Getting more information from Tracebacks",
            "content": ". About . This notebook demonstrates what the Python Traceback object is, and how can we get more information out of it to better diagnose exception messages. . Credit . This blog post is based on an article originally written in Python Cookbook published by O&#39;Reilly Media, Inc. and released July 2002. In book&#39;s chapter 15, there is a section with the title Getting More Information from Tracebacks written by Bryn Keller. An online version of this article is available at https://www.oreilly.com/library/view/python-cookbook/0596001673/ch14s05.html. . The original article uses Python 2.2, but I have adapted it for Python 3.8. Also, I have added some commentary to give more insights on Python Traceback object. . Environment Details . from platform import python_version print(&quot;python==&quot; + python_version()) . . python==3.8.5 . Discussion . Consider the following toy example where we are getting some data from an external source (an API call, a DB call, etc.), and we need to find the length of individual items provided in the list. We know that items in the list will be of type str so we have used a len() function on it. . We got an exception when we ran our function on received data, and now we are trying to investigate what caused the error. . # this is intentionally hidden as we don&#39;t know about the data received from an external source. data = [&quot;1&quot;, &quot;22&quot;, 333, &quot;4444&quot;] . . # our toy example function. import sys, traceback def get_items_len(items: list) -&gt; list: &quot;&quot;&quot; this function returns the length of items received in a list. &quot;&quot;&quot; items_len = [] for i in items: items_len.append(len(i)) return items_len . # let&#39;s run our function on &quot;data&quot; received from an external source try: get_items_len(data) except Exception as e: print(traceback.print_exc()) . None . Traceback (most recent call last): File &#34;&lt;ipython-input-4-42cd486e1858&gt;&#34;, line 4, in &lt;module&gt; get_items_len(data) File &#34;&lt;ipython-input-3-8421f841ba77&gt;&#34;, line 11, in get_items_len items_len.append(len(i)) TypeError: object of type &#39;int&#39; has no len() . We got an exception while data processing and the Traceback message gives us some details. It tells us that we have received some data of type integer instead of string, and we are trying to call len() function on it. But we don&#39;t know the actual data value that caused the exception, and we don&#39;t know the index of the item in the list that caused this error. Depending on the use case, information about the local variables, or input data that caused the error can be crucial in diagnosing the root cause of an error. . Fortunately, all this information is already available to us in the Traceback object, but there are no built-in methods that give this information directly. Let us try some of the built-in methods on the Traceback object to see the kind of information we could get from them. . # calling traceback module built-in methods try: get_items_len(data) except Exception as e: print(&quot;***** Exception *****&quot;) print(e) exc_type, exc_value, exc_traceback = sys.exc_info() print(&quot; n***** print_tb *****&quot;) traceback.print_tb(exc_traceback, limit=1, file=sys.stdout) print(&quot; n***** print_exception *****&quot;) # exc_type below is ignored on 3.5 and later traceback.print_exception(exc_type, exc_value, exc_traceback, limit=2, file=sys.stdout) print(&quot; n***** print_exc *****&quot;) traceback.print_exc(limit=2, file=sys.stdout) print(&quot; n***** format_exc, first and last line *****&quot;) formatted_lines = traceback.format_exc().splitlines() print(formatted_lines[0]) print(formatted_lines[-1]) print(&quot; n***** format_exception *****&quot;) # exc_type below is ignored on 3.5 and later print(repr(traceback.format_exception(exc_type, exc_value, exc_traceback))) print(&quot; n***** extract_tb *****&quot;) print(repr(traceback.extract_tb(exc_traceback))) print(&quot; n***** format_tb *****&quot;) print(repr(traceback.format_tb(exc_traceback))) print(&quot; n***** tb_lineno *****&quot;, exc_traceback.tb_lineno) . ***** Exception ***** object of type &#39;int&#39; has no len() ***** print_tb ***** File &#34;&lt;ipython-input-5-73d5b316a567&gt;&#34;, line 4, in &lt;module&gt; get_items_len(data) ***** print_exception ***** Traceback (most recent call last): File &#34;&lt;ipython-input-5-73d5b316a567&gt;&#34;, line 4, in &lt;module&gt; get_items_len(data) File &#34;&lt;ipython-input-3-8421f841ba77&gt;&#34;, line 11, in get_items_len items_len.append(len(i)) TypeError: object of type &#39;int&#39; has no len() ***** print_exc ***** Traceback (most recent call last): File &#34;&lt;ipython-input-5-73d5b316a567&gt;&#34;, line 4, in &lt;module&gt; get_items_len(data) File &#34;&lt;ipython-input-3-8421f841ba77&gt;&#34;, line 11, in get_items_len items_len.append(len(i)) TypeError: object of type &#39;int&#39; has no len() ***** format_exc, first and last line ***** Traceback (most recent call last): TypeError: object of type &#39;int&#39; has no len() ***** format_exception ***** [&#39;Traceback (most recent call last): n&#39;, &#39; File &#34;&lt;ipython-input-5-73d5b316a567&gt;&#34;, line 4, in &lt;module&gt; n get_items_len(data) n&#39;, &#39; File &#34;&lt;ipython-input-3-8421f841ba77&gt;&#34;, line 11, in get_items_len n items_len.append(len(i)) n&#39;, &#34;TypeError: object of type &#39;int&#39; has no len() n&#34;] ***** extract_tb ***** [&lt;FrameSummary file &lt;ipython-input-5-73d5b316a567&gt;, line 4 in &lt;module&gt;&gt;, &lt;FrameSummary file &lt;ipython-input-3-8421f841ba77&gt;, line 11 in get_items_len&gt;] ***** format_tb ***** [&#39; File &#34;&lt;ipython-input-5-73d5b316a567&gt;&#34;, line 4, in &lt;module&gt; n get_items_len(data) n&#39;, &#39; File &#34;&lt;ipython-input-3-8421f841ba77&gt;&#34;, line 11, in get_items_len n items_len.append(len(i)) n&#39;] ***** tb_lineno ***** 4 . . All these methods are useful but we are still short on information about the state of local variables when the system crashed. Before writing our custom function to get the variables state at the time of exception, let us spend some time to understand the working of Traceback object. . Traceback Module . https://docs.python.org/3/library/traceback.html This module provides an easy-to-use interface to work with traceback objects. It provides multiple functions that we can use to extract the required information from traceback. So far, we have used methods from this module in the above examples. . Traceback Objects . https://docs.python.org/3/reference/datamodel.html &gt; On this page search for term &quot;Traceback objects&quot; . Traceback objects represent a stack trace of an exception. A traceback object is implicitly created when an exception occurs and may also be explicitly created by initializing an instance of class types.TracebackType. traceback object is also an instance of types.TracebackType class. When an exception occurs, a traceback object is initialized for us, and we can obtain it from any of the following two methods. . It is available as a third item of the tuple returned by sys.exc_info() &quot;(type, value, traceback)&quot; | It is available as the __traceback__ object of the caught exception. &quot;Exception.__traceback__&quot; | A traceback object is a linked list of nodes, where each node is a Frame object. Frame objects form their own linked list but in the opposite direction of traceback objects. Together they work like a doubly-linked list, and we can use them to move back and forth in the stack trace history. It is the frame objects that hold all the stack&#39;s important information. traceback object has some special attributes . tb_next point to the next level in the stack trace (towards the frame where the exception occurred), or None if there is no next level | tb_frame points to the execution frame of the current level | tb_lineno gives the line number where the exception occurred | . # method 1: get traceback object using sys.exc_info() try: get_items_len(data) except Exception as e: print(sys.exc_info()[2]) . &lt;traceback object at 0x7f5c6c60e9c0&gt; . # method 2: get traceback object using Exception.__traceback__ try: get_items_len(data) except Exception as e: print(e.__traceback__ ) . &lt;traceback object at 0x7f5c6c5c0180&gt; . If there is no exception in the system, then calling sys.exc_info() will only return None values. . # no exception is generated so sys.exc_info() will return None values. try: get_items_len([&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;]) except Exception as e: print(sys.exc_info()[2]) . Frame Objects . https://docs.python.org/3/reference/datamodel.html &gt; On this page search for term &quot;Frame objects&quot; . Frame objects represent execution frames. It has some special attributes . f_back is a reference to the previous stack frame (towards the caller), or None if this is the bottom stack frame | f_code is the code object being executed in this frame. We will discuss Code Objects in next the section | f_lineno is the current line number of the frame — writing to this from within a trace function jumps to the given line (only for the bottom-most frame). A debugger can implement a Jump command (aka Set Next Statement) by writing to f_lineno. This attribute will give you the line number in the code on which exception occurred | f_locals is a dictionary used to lookup local variables. From this dictionary we can get all the local variables and their state at the time of exception | f_globals is a dictionary for global varaibles | . Code Objects . https://docs.python.org/3/reference/datamodel.html &gt; On this page search for term &quot;Code Objects&quot; . Code objects represent byte-compiled executable Python code or bytecode. Some of its attributes include . co_name gives the function name being executed | co_filename gives the filename from which the code was compiled | . There are many other helpful attributes in this object, and you may read about them from the docs. . Visual representation of Traceback, Frame and Code Objects . figure 1: Visual representation of Traceback, Frame and Code Objects . Custom fuction for additional exception info . Now with this additional information on stack trace objects, let us create a function to get variables state at the time of exception. . def exc_info_plus(): &quot;&quot;&quot; Provides the usual traceback information, followed by a listing of all the local variables in each frame. &quot;&quot;&quot; tb = sys.exc_info()[2] # iterate forward to the last (most recent) traceback object. while 1: if not tb.tb_next: break tb = tb.tb_next stack = [] # get the most recent traceback frame f = tb.tb_frame # iterate backwards from recent to oldest traceback frame while f: stack.append(f) f = f.f_back # stack.reverse() # uncomment to get innermost (most recent) frame at the last # get exception information and stack trace entries from most recent traceback object exc_msg = traceback.format_exc() exc_msg += &quot; n*** Locals by frame, innermost first ***&quot; for frame in stack: exc_msg += f&quot; nFrame {frame.f_code.co_name} in {frame.f_code.co_filename} at line {frame.f_lineno}&quot; for key, value in frame.f_locals.items(): exc_msg += f&quot; n t {key:20} = &quot; try: data = str(value) # limit variable&#39;s output to a certain number. You can adjust it as per your requirement. # But not to remove it as output from large objects (e.g. Pandas DataFrame) can be troublesome. output_limit = 50 exc_msg += (data[:output_limit] + &quot;...&quot;) if len(data) &gt; output_limit else data except: exc_msg += &quot;&lt;ERROR WHILE PRINTING VALUE&gt;&quot; return exc_msg . . #now let us try our custom exception function and see the ouput try: get_items_len(data) except Exception as e: print(exc_info_plus()) . Traceback (most recent call last): File &#34;&lt;ipython-input-10-01264d9e470a&gt;&#34;, line 4, in &lt;module&gt; get_items_len(data) File &#34;&lt;ipython-input-3-8421f841ba77&gt;&#34;, line 11, in get_items_len items_len.append(len(i)) TypeError: object of type &#39;int&#39; has no len() *** Locals by frame, innermost first *** Frame get_items_len in &lt;ipython-input-3-8421f841ba77&gt; at line 11 items = [&#39;1&#39;, &#39;22&#39;, 333, &#39;4444&#39;] items_len = [1, 2] i = 333 Frame &lt;module&gt; in &lt;ipython-input-10-01264d9e470a&gt; at line 6 __name__ = __main__ __doc__ = Automatically created module for IPython interacti... __package__ = None __loader__ = None __spec__ = None __builtin__ = &lt;module &#39;builtins&#39; (built-in)&gt; __builtins__ = &lt;module &#39;builtins&#39; (built-in)&gt; _ih = [&#39;&#39;, &#39;#collapse-hide nfrom platform import python_... _oh = {} _dh = [&#39;/data/_notebooks&#39;] In = [&#39;&#39;, &#39;#collapse-hide nfrom platform import python_... Out = {} get_ipython = &lt;bound method InteractiveShell.get_ipython of &lt;ipy... exit = &lt;IPython.core.autocall.ZMQExitAutocall object at 0... quit = &lt;IPython.core.autocall.ZMQExitAutocall object at 0... _ = __ = ___ = _i = #collapse-show def exc_info_plus(): &#34;&#34;&#34; Pr... _ii = ## # no exception is generated so sys.exc_info() w... _iii = ## # method 2: get traceback object using Exceptio... _i1 = #collapse-hide from platform import python_version... python_version = &lt;function python_version at 0x7f5c72dbc430&gt; _i2 = #collapse-hide # this is intentionally hidden as w... data = [&#39;1&#39;, &#39;22&#39;, 333, &#39;4444&#39;] _i3 = ## # our toy example function. import sys, traceba... sys = &lt;module &#39;sys&#39; (built-in)&gt; traceback = &lt;module &#39;traceback&#39; from &#39;/usr/lib/python3.8/trace... get_items_len = &lt;function get_items_len at 0x7f5c6c62c790&gt; _i4 = ## # let&#39;s run our function on &#34;data&#34; received fro... _i5 = #collapse-output # calling traceback module built-... exc_type = &lt;class &#39;TypeError&#39;&gt; exc_value = object of type &#39;int&#39; has no len() exc_traceback = &lt;traceback object at 0x7f5c6c5cf700&gt; formatted_lines = [&#39;Traceback (most recent call last):&#39;, &#39; File &#34;&lt;i... _i6 = ## # method 1: get traceback object using sys.exc_... _i7 = ## # method 2: get traceback object using Exceptio... _i8 = ## # no exception is generated so sys.exc_info() w... _i9 = #collapse-show def exc_info_plus(): &#34;&#34;&#34; Pr... exc_info_plus = &lt;function exc_info_plus at 0x7f5c6c62cc10&gt; _i10 = #collapse-output #now let us try our custom except... e = object of type &#39;int&#39; has no len() Frame run_code in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 3418 self = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... code_obj = &lt;code object &lt;module&gt; at 0x7f5c6c62eea0, file &#34;&lt;ip... result = &lt;ExecutionResult object at 7f5c6c5c88e0, execution... async_ = False __tracebackhide__ = __ipython_bottom__ old_excepthook = &lt;bound method IPKernelApp.excepthook of &lt;ipykernel... outflag = True Frame run_ast_nodes in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 3338 self = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... nodelist = [&lt;_ast.Try object at 0x7f5c6c5c8850&gt;] cell_name = &lt;ipython-input-10-01264d9e470a&gt; interactivity = none compiler = &lt;IPython.core.compilerop.CachingCompiler object at... result = &lt;ExecutionResult object at 7f5c6c5c88e0, execution... to_run_exec = [&lt;_ast.Try object at 0x7f5c6c5c8850&gt;] to_run_interactive = [] mod = &lt;_ast.Module object at 0x7f5c6c5c8430&gt; compare = &lt;function InteractiveShell.run_ast_nodes.&lt;locals&gt;.... to_run = [(&lt;_ast.Try object at 0x7f5c6c5c8850&gt;, &#39;exec&#39;)] node = &lt;_ast.Try object at 0x7f5c6c5c8850&gt; mode = exec code = &lt;code object &lt;module&gt; at 0x7f5c6c62eea0, file &#34;&lt;ip... asy = False _async = False Frame run_cell_async in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 3146 raw_cell = #collapse-output #now let us try our custom except... silent = False shell_futures = True transformed_cell = #collapse-output #now let us try our custom except... preprocessing_exc_tuple = None info = &lt;ExecutionInfo object at 7f5c6c5c8be0, raw_cell=&#34;#... error_before_exec = &lt;function InteractiveShell.run_cell_async.&lt;locals&gt;... cell = #collapse-output #now let us try our custom except... compiler = &lt;IPython.core.compilerop.CachingCompiler object at... _run_async = False cell_name = &lt;ipython-input-10-01264d9e470a&gt; code_ast = &lt;_ast.Module object at 0x7f5c6c5c85e0&gt; interactivity = last_expr result = &lt;ExecutionResult object at 7f5c6c5c88e0, execution... self = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... store_history = True Frame _pseudo_sync_runner in /usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py at line 68 coro = &lt;coroutine object InteractiveShell.run_cell_async ... Frame _run_cell in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 2923 self = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... raw_cell = #collapse-output #now let us try our custom except... store_history = True silent = False shell_futures = True preprocessing_exc_tuple = None transformed_cell = #collapse-output #now let us try our custom except... coro = &lt;coroutine object InteractiveShell.run_cell_async ... runner = &lt;function _pseudo_sync_runner at 0x7f5c724ba040&gt; Frame run_cell in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 2877 self = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... raw_cell = #collapse-output #now let us try our custom except... store_history = True silent = False shell_futures = True result = None Frame run_cell in /usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py at line 539 self = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... args = (&#39;#collapse-output n#now let us try our custom exc... kwargs = {&#39;store_history&#39;: True, &#39;silent&#39;: False} __class__ = &lt;class &#39;ipykernel.zmqshell.ZMQInteractiveShell&#39;&gt; Frame do_execute in /usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py at line 302 self = &lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5c... code = #collapse-output #now let us try our custom except... silent = False store_history = True user_expressions = {} allow_stdin = True reply_content = {} run_cell = &lt;bound method InteractiveShell.run_cell_async of &lt;... should_run_async = &lt;bound method InteractiveShell.should_run_async of... shell = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ... Frame wrapper in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 234 args = (&lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5... kwargs = {} future = &lt;Future pending&gt; ctx_run = &lt;built-in method run of Context object at 0x7f5c6c... result = &lt;generator object IPythonKernel.do_execute at 0x7f... func = &lt;function IPythonKernel.do_execute at 0x7f5c6f6978... Frame execute_request in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py at line 540 self = &lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5c... stream = &lt;zmq.eventloop.zmqstream.ZMQStream object at 0x7f5... ident = [b&#39;e2e3826d25fb4c63876268cdc5a787ad&#39;] parent = {&#39;header&#39;: {&#39;msg_id&#39;: &#39;218114cb9837444cbd29466d87b... content = {&#39;code&#39;: &#39;#collapse-output n#now let us try our cu... code = #collapse-output #now let us try our custom except... silent = False store_history = True user_expressions = {} allow_stdin = True stop_on_error = True metadata = {&#39;started&#39;: datetime.datetime(2022, 2, 14, 9, 30, ... Frame wrapper in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 234 args = (&lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5... kwargs = {} future = &lt;Future pending&gt; ctx_run = &lt;built-in method run of Context object at 0x7f5c6c... result = &lt;generator object Kernel.execute_request at 0x7f5c... func = &lt;function Kernel.execute_request at 0x7f5c6f747f70... Frame dispatch_shell in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py at line 265 self = &lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5c... stream = &lt;zmq.eventloop.zmqstream.ZMQStream object at 0x7f5... msg = {&#39;header&#39;: {&#39;msg_id&#39;: &#39;218114cb9837444cbd29466d87b... idents = [b&#39;e2e3826d25fb4c63876268cdc5a787ad&#39;] msg_type = execute_request handler = &lt;bound method Kernel.execute_request of &lt;ipykernel... Frame wrapper in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 234 args = (&lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5... kwargs = {} future = &lt;Future pending&gt; ctx_run = &lt;built-in method run of Context object at 0x7f5c6f... result = &lt;generator object Kernel.dispatch_shell at 0x7f5c6... func = &lt;function Kernel.dispatch_shell at 0x7f5c6f7473a0&gt; Frame process_one in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py at line 362 self = &lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5c... wait = True priority = 10 t = 13 dispatch = &lt;bound method Kernel.dispatch_shell of &lt;ipykernel.... args = (&lt;zmq.eventloop.zmqstream.ZMQStream object at 0x7f... Frame run in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 775 self = &lt;tornado.gen.Runner object at 0x7f5c6c60f8e0&gt; future = None exc_info = None value = (10, 13, &lt;bound method Kernel.dispatch_shell of &lt;i... Frame inner in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 814 f = None self = &lt;tornado.gen.Runner object at 0x7f5c6c60f8e0&gt; Frame _run_callback in /usr/local/lib/python3.8/dist-packages/tornado/ioloop.py at line 741 self = &lt;tornado.platform.asyncio.AsyncIOMainLoop object a... callback = functools.partial(&lt;function Runner.handle_yield.&lt;l... Frame &lt;lambda&gt; in /usr/local/lib/python3.8/dist-packages/tornado/ioloop.py at line 688 f = &lt;Future finished result=(10, 13, &lt;bound method...7... callback = &lt;function Runner.handle_yield.&lt;locals&gt;.inner at 0x... future = &lt;Future finished result=(10, 13, &lt;bound method...7... self = &lt;tornado.platform.asyncio.AsyncIOMainLoop object a... Frame _run in /usr/lib/python3.8/asyncio/events.py at line 81 self = &lt;Handle IOLoop.add_future.&lt;locals&gt;.&lt;lambda&gt;(&lt;Futur... Frame _run_once in /usr/lib/python3.8/asyncio/base_events.py at line 1859 self = &lt;_UnixSelectorEventLoop running=True closed=False ... sched_count = 0 handle = &lt;Handle IOLoop.add_future.&lt;locals&gt;.&lt;lambda&gt;(&lt;Futur... timeout = 0 event_list = [] end_time = 113697.83311910101 ntodo = 2 i = 0 Frame run_forever in /usr/lib/python3.8/asyncio/base_events.py at line 570 self = &lt;_UnixSelectorEventLoop running=True closed=False ... old_agen_hooks = asyncgen_hooks(firstiter=None, finalizer=None) Frame start in /usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py at line 199 self = &lt;tornado.platform.asyncio.AsyncIOMainLoop object a... old_loop = &lt;_UnixSelectorEventLoop running=True closed=False ... Frame start in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py at line 612 self = &lt;ipykernel.kernelapp.IPKernelApp object at 0x7f5c7... Frame launch_instance in /usr/local/lib/python3.8/dist-packages/traitlets/config/application.py at line 845 cls = &lt;class &#39;ipykernel.kernelapp.IPKernelApp&#39;&gt; argv = None kwargs = {} app = &lt;ipykernel.kernelapp.IPKernelApp object at 0x7f5c7... Frame &lt;module&gt; in /usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py at line 16 __name__ = __main__ __doc__ = Entry point for launching an IPython kernel. This... __package__ = __loader__ = &lt;_frozen_importlib_external.SourceFileLoader objec... __spec__ = ModuleSpec(name=&#39;ipykernel_launcher&#39;, loader=&lt;_fro... __annotations__ = {} __builtins__ = &lt;module &#39;builtins&#39; (built-in)&gt; __file__ = /usr/local/lib/python3.8/dist-packages/ipykernel_l... __cached__ = /usr/local/lib/python3.8/dist-packages/__pycache__... sys = &lt;module &#39;sys&#39; (built-in)&gt; app = &lt;module &#39;ipykernel.kernelapp&#39; from &#39;/usr/local/lib... Frame _run_code in /usr/lib/python3.8/runpy.py at line 87 code = &lt;code object &lt;module&gt; at 0x7f5c7317e030, file &#34;/us... run_globals = {&#39;__name__&#39;: &#39;__main__&#39;, &#39;__doc__&#39;: &#39;Entry point f... init_globals = None mod_name = __main__ mod_spec = ModuleSpec(name=&#39;ipykernel_launcher&#39;, loader=&lt;_fro... pkg_name = script_name = None loader = &lt;_frozen_importlib_external.SourceFileLoader objec... fname = /usr/local/lib/python3.8/dist-packages/ipykernel_l... cached = /usr/local/lib/python3.8/dist-packages/__pycache__... Frame _run_module_as_main in /usr/lib/python3.8/runpy.py at line 194 mod_name = ipykernel_launcher alter_argv = 1 mod_spec = ModuleSpec(name=&#39;ipykernel_launcher&#39;, loader=&lt;_fro... code = &lt;code object &lt;module&gt; at 0x7f5c7317e030, file &#34;/us... main_globals = {&#39;__name__&#39;: &#39;__main__&#39;, &#39;__doc__&#39;: &#39;Entry point f... . . Note the output from the first stack frame in the above stack trace. It is easy now to see (items) that we received in our function. The item at index i is also available (333) on which our function crashed. Using our custom function unexpected errors are logged in a format that makes it a lot easier to find and fix the errors. Let&#39;s fix our function to handle unexpected integer values. . # let&#39;s fix our function to handle unexpected &#39;int&#39; items by converting them to &#39;str&#39; def get_items_len(items: list) -&gt; list: &quot;&quot;&quot; this function returns the length of items received in a list. &quot;&quot;&quot; items_len = [] for i in map(str, items): items_len.append(len(i)) return items_len # test it again get_items_len(data) . [1, 2, 3, 4] .",
            "url": "https://hassaanbinaslam.github.io/myblog/python/2022/02/11/python-stack-traceback-more-info.html",
            "relUrl": "/python/2022/02/11/python-stack-traceback-more-info.html",
            "date": " • Feb 11, 2022"
        }
        
    
  
    
        ,"post21": {
            "title": "Python Dictionary - Multiple ways to get items",
            "content": ". About . This notebook demonstrates multiple ways to get items from a Python dictionary. . Environment Details . from platform import python_version print(&quot;python==&quot; + python_version()) . . python==3.8.5 . Example Dictionaries . # simple dictionary car = { &quot;brand&quot;: &quot;ford&quot;, &quot;model&quot;: &quot;mustang&quot; } car . {&#39;brand&#39;: &#39;ford&#39;, &#39;model&#39;: &#39;mustang&#39;} . # nested dictionary family = { &#39;gfather&#39; : { &#39;father&#39;: { &#39;son&#39;: {&#39;love&#39;:&#39;python&#39;} } } } family . {&#39;gfather&#39;: {&#39;father&#39;: {&#39;son&#39;: {&#39;love&#39;: &#39;python&#39;}}}} . Method 1: Square brackets . A square bracket is the simplest approach to getting any item from a dictionary. You can get a value from a dictionary by providing it a key in [] brackets. For example, to get a value of model from a car . car[&#39;model&#39;] . &#39;mustang&#39; . Problem with this approach is that if the provided key is not available in the dictionary then it will throw a KeyError exception. . car[&#39;year&#39;] . KeyErrorTraceback (most recent call last) &lt;ipython-input-5-ca220af55913&gt; in &lt;module&gt; -&gt; 1 car[&#39;year&#39;] KeyError: &#39;year&#39; . To avoid KeyError, you can first check if the key is available in dictionary. . if &#39;year&#39; in car: # check if given key is available in dictionary year = car[&#39;year&#39;] # now get the value else: year = &#39;1964&#39; # (Optional) otherwise give this car a default value year . &#39;1964&#39; . An alternate approach could be to use a Try-Except block to handle the KeyError exception. . try: year = car[&#39;year&#39;] except KeyError: year = &#39;1964&#39; # give this car a default value year . &#39;1964&#39; . For nested dictionaries, you can use chained [] brackets. But beware that if any of the Keys is missing in the chain, you will get a KeyError exception. . # this will work. All keys are present. family[&#39;gfather&#39;][&#39;father&#39;][&#39;son&#39;] . {&#39;love&#39;: &#39;python&#39;} . # this will not work. &#39;mother&#39; key is not in dictionary family[&#39;gfather&#39;][&#39;mother&#39;][&#39;son&#39;] . KeyErrorTraceback (most recent call last) &lt;ipython-input-9-3d615db6bfdf&gt; in &lt;module&gt; 1 # this will not work. &#39;mother&#39; key is not in dictionary -&gt; 2 family[&#39;gfather&#39;][&#39;mother&#39;][&#39;son&#39;] KeyError: &#39;mother&#39; . Method 2: Get function . https://docs.python.org/3/library/stdtypes.html#dict.get &gt; get(key[, default]) . Get function will return the value for key if key is in the dictionary. Otherwise, it will return a default value which is None. You can provide your default value as well. . year = car.get(&#39;year&#39;, &#39;1964&#39;) year # year key is not present so get function will return a default value &#39;1964&#39; . &#39;1964&#39; . Depending on your use case there can be confusion with this approach when your item can also have None value. In that case, you will not know whether the None value was returned from the dictionary or it was the Get function. . owner = car.get(&#39;owner&#39;) owner # owner has a None value. But is this value coming from dic or from Get function? # This can be confusing for large nested dictionaries. . For nested dictionaries you can use chained Get functions. But beware that missing Key items needs to be properly handled otherwise you will still get an exception. . # this will work. All keys are present. family.get(&#39;gfather&#39;).get(&#39;father&#39;).get(&#39;son&#39;) . {&#39;love&#39;: &#39;python&#39;} . # this will still work. &#39;daughter&#39; key is missing # but since it is at the end of chain it will return a default None value family.get(&#39;gfather&#39;).get(&#39;father&#39;).get(&#39;daughter&#39;) . # this will NOT work. &#39;mother&#39; key is missing and it returned a default None value. # but since it is not at the end, and we called Get function on returned value &#39;None&#39; family.get(&#39;gfather&#39;).get(&#39;mother&#39;).get(&#39;son&#39;) . AttributeErrorTraceback (most recent call last) &lt;ipython-input-14-a35a8f091991&gt; in &lt;module&gt; 1 # this will NOT work. &#39;mother&#39; key is missing and it returned a default None value. 2 # but since it is not at the end, and we called Get function on returned value &#39;None&#39; -&gt; 3 family.get(&#39;gfather&#39;).get(&#39;mother&#39;).get(&#39;son&#39;) AttributeError: &#39;NoneType&#39; object has no attribute &#39;get&#39; . # this will work. &#39;mother&#39; key is missing and it returned a default value. # but we have properly handled all the default values with empty dictionaries. family.get(&#39;gfather&#39;, {}).get(&#39;mother&#39;, {}).get(&#39;son&#39;, {}) . {} .",
            "url": "https://hassaanbinaslam.github.io/myblog/python/2022/02/10/python-dictionary.html",
            "relUrl": "/python/2022/02/10/python-dictionary.html",
            "date": " • Feb 10, 2022"
        }
        
    
  
    
        ,"post22": {
            "title": "My First Blog Post from Jupyter Notebook",
            "content": ". Well, this is my first post using Jupyter notebook as a publishing medium. Besides this notebook, I am also using &#39;nbdev&#39; library from FastAI as tooling to convert notebooks into static HTML pages. Once pushed to GitHub they will become new posts on my blog. I need to learn more about this setup, but it is looking very interesting. . # I can also include some code directly into the blog post. No need for GitHub snippets. print(&quot;nbdev and fastpages from Fast.AI are so cool! &quot;) . . nbdev and fastpages from Fast.AI are so cool! .",
            "url": "https://hassaanbinaslam.github.io/myblog/jupyter/fastpages/2022/02/09/hello-world.html",
            "relUrl": "/jupyter/fastpages/2022/02/09/hello-world.html",
            "date": " • Feb 9, 2022"
        }
        
    
  
    
        ,"post23": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://hassaanbinaslam.github.io/myblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://hassaanbinaslam.github.io/myblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Hi there! . I am Hassaan Bin Aslam and welcome to my blog. . I started this blog to document and share my learning. I make living by working as a Machine Learning Solutions Architect. AI/ML, Cloud Architecture, DevOps are very exciting topics and also close to my heart. Every day I face interesting problems and I like to share my understanding on them here. I am passionate about AWS as a strategic cloud platform and using AI/ML to solve business problems. I’m currently focusing a lot of my time on applying DevOps practices to Machine Learning workloads (MLOps) to enable customers to adopt Machine Learning at scale. . You can find and connect with me on . LinkedIn: linkedin.com/in/hassaanbinaslam | Twitter: twitter.com/hassaanbinaslam | GitHub: github.com/hassaanbinaslam | .",
          "url": "https://hassaanbinaslam.github.io/myblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hassaanbinaslam.github.io/myblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}