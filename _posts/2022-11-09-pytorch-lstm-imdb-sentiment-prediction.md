---
keywords: fastai
description: This is a practice notebook to work with a dataset of 50,000 movie reviews from the Internet Movie Database (IMDB) and build an LSTM predictor to distinguish between positive and negative reviews.
title: Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch
toc: true 
badges: true
comments: true
categories: [pytorch, lstm]
keyword: [ml, dl, nn, pytorch, LSTM, IMDB, sentiment]
image: images/copied_from_nb/images/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.jpeg
nb_path: _notebooks/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/myblog/images/copied_from_nb/images/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.jpeg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Credits">Credits<a class="anchor-link" href="#Credits"> </a></h2><p>This notebook takes inspiration and ideas from the following sources.</p>
<ul>
<li>"Machine learning with PyTorch and Scikit-Learn" by "Sebastian Raschka, Yuxi (Hayden) Liu, and Vahid Mirjalili". You can get the book from its website: <a href="https://sebastianraschka.com/books/#machine-learning-with-pytorch-and-scikit-learn">Machine learning with PyTorch and Scikit-Learn</a>. In addition, the GitHub repository for this book has valuable notebooks: <a href="https://github.com/rasbt/machine-learning-book">github.com/rasbt/machine-learning-book</a>. Parts of the code you see in this notebook are taken from <a href="https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part2.ipynb">chapter 15</a> notebook of the same book.</li>
<li>"Intro to Deep Learning and Generative Models Course" lecture series from "Sebastian Raschka". Course website: <a href="https://sebastianraschka.com/teaching/stat453-ss2021/">stat453-ss2021</a>. YouTube Link: <a href="https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51">Intro to Deep Learning and Generative Models Course</a>. Lectures that are related to this post are <a href="https://youtu.be/k6fSgUaWUF8">L15.5 Long Short-Term Memory</a> and <a href="https://youtu.be/KgrdifrlDxg">L15.7 An RNN Sentiment Classifier in PyTorch</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Environment">Environment<a class="anchor-link" href="#Environment"> </a></h2><p>This notebook is prepared with Google Colab. For "runtime type" choose hardware accelerator as "GPU". It will take a long time to complete the training without any GPU.</p>
<p>This notebook also depends on the PyTorch library <a href="https://pytorch.org/text/stable/index.html">TorchText</a>. We will use this library to fetch IMDB review data. While using the <code>torchtext</code> latest version, I found more dependencies on other libraries like <code>torchdata</code>. Even after resolving them, it threw strange encoding errors while fetching IMDB data. So I have downgraded this library till the version I found working without external dependencies. Consequently, <code>torch</code> is also downgraded to a compatible version, but I did not find any issue while working with a lower version of PyTorch for this notebook. It is preferred to restart the runtime after the library installation is complete.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span> pip install <span class="nv">torchtext</span><span class="o">==</span><span class="m">0</span>.11.0
</pre></div>

    </div>
</div>
</div>
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Output" data-close="Show Output"></summary>
        <p>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: torchtext==0.11.0 in /usr/local/lib/python3.7/dist-packages (0.11.0)
Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.10.0)
Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.21.6)
Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (2.23.0)
Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (4.64.1)
Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0-&gt;torchtext==0.11.0) (4.1.1)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;torchtext==0.11.0) (2022.9.24)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;torchtext==0.11.0) (1.24.3)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;torchtext==0.11.0) (2.10)
Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;torchtext==0.11.0) (3.0.4)
</pre>
</div>
</div>

</div>
</div>
</p>
    </details>
</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">platform</span> <span class="kn">import</span> <span class="n">python_version</span>
<span class="kn">import</span> <span class="nn">numpy</span><span class="o">,</span> <span class="nn">matplotlib</span><span class="o">,</span> <span class="nn">pandas</span><span class="o">,</span> <span class="nn">torch</span><span class="o">,</span> <span class="nn">torchtext</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;python==&quot;</span> <span class="o">+</span> <span class="n">python_version</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;numpy==&quot;</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;torch==&quot;</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;torchtext==&quot;</span> <span class="o">+</span> <span class="n">torchtext</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;matplotlib==&quot;</span> <span class="o">+</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>python==3.7.15
numpy==1.21.6
torch==1.10.0+cu102
torchtext==0.11.0
matplotlib==3.2.2
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Data-Preparation">Data Preparation<a class="anchor-link" href="#Data-Preparation"> </a></h2><h3 id="Download-data">Download data<a class="anchor-link" href="#Download-data"> </a></h3><p>Let's download our movie review dataset. This dataset is also known as <a href="https://ai.stanford.edu/~amaas/data/sentiment/">Large Movie Review Dataset</a>, and can also be obtained in a compressed zip file from <a href="https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz">this link</a>. Using the <code>torchtext</code> library makes downloading, extracting, and reading files a lot easier. 'torchtext.datasets' comes with many more NLP related datasets, and a full list can be <a href="https://pytorch.org/text/stable/datasets.html">found here</a>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torchtext.datasets</span> <span class="kn">import</span> <span class="n">IMDB</span>
<span class="kn">from</span> <span class="nn">torch.utils.data.dataset</span> <span class="kn">import</span> <span class="n">random_split</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">train_dataset_raw</span> <span class="o">=</span> <span class="n">IMDB</span><span class="p">(</span><span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">test_dataset_raw</span> <span class="o">=</span> <span class="n">IMDB</span><span class="p">(</span><span class="n">split</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Check the size of the downloaded data.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train dataset size: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataset_raw</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test dataset size: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_dataset_raw</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train dataset size:  25000
Test dataset size:  25000
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Split-train-data-further-into-train-and-validation-set">Split train data further into train and validation set<a class="anchor-link" href="#Split-train-data-further-into-train-and-validation-set"> </a></h3><p>Both train and test datasets have 25000 reviews. Therefore, we can split the training set further into the train and validation sets.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_set_size</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">valid_set_size</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="n">train_dataset</span><span class="p">,</span> <span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">random_split</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">train_dataset_raw</span><span class="p">),</span> <span class="p">[</span><span class="mi">20000</span><span class="p">,</span> <span class="mi">5000</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="How-does-this-data-look?">How does this data look?<a class="anchor-link" href="#How-does-this-data-look?"> </a></h3><p>The data we have is in the form of tuples. The first index has the sentiment label, and the second contains the review text. Let's check the first element in our training dataset.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;pos&#39;,
 &#39;An extra is called upon to play a general in a movie about the Russian Revolution. However, he is not any ordinary extra. He is Serguis Alexander, former commanding general of the Russia armies who is now being forced to relive the same scene, which he suffered professional and personal tragedy in, to satisfy the director who was once a revolutionist in Russia and was humiliated by Alexander. It can now be the time for this broken man to finally &#34;win&#34; his penultimate battle. This is one powerful movie with meticulous direction by Von Sternberg, providing the greatest irony in Alexander\&#39;s character in every way he can. Jannings deserved his Oscar for the role with a very moving performance playing the general at his peak and at his deepest valley. Powell lends a sinister support as the revenge minded director and Brent is perfect in her role with her face and movements showing so much expression as Jannings\&#39; love. All around brilliance. Rating, 10.&#39;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Check the first index of the validation set.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">valid_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;neg&#39;,
 &#39;The Dereks did seem to struggle to find rolls for Bo after &#34;10&#34;.&lt;br /&gt;&lt;br /&gt;I used to work for a marine park in the Florida Keys. One day, the script for &#34;Ghosts Can\&#39;t Do It&#34; was circulating among the trainers in the &#34;fish house&#34; where food was prepared for the dolphins. There was one scene where a -dolphin- supposedly propositions Bo (or Bo the dolphin), asking to &#34;go make eggs.&#34; Reading the script, we -lauuughed-...&lt;br /&gt;&lt;br /&gt;We did not end up doing any portion of this movie at our facility, although our dolphins -were- in &#34;The Big Blue!&#34;&lt;br /&gt;&lt;br /&gt;This must have been very close to the end of Anthony Quinn\&#39;s life. I hope he had fun in this film, as it certainly didn\&#39;t do anything for his legacy.&#39;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Data-preprocessing-steps">Data preprocessing steps<a class="anchor-link" href="#Data-preprocessing-steps"> </a></h3><p>From these two reviews, we can deduce that</p>
<ul>
<li>We have two labels. 'pos' for a positive and 'neg' for a negative review</li>
<li>From the second review (from valid_dataset), we also get that text may contain HTML tags, special characters, and emoticons besides normal English words. It will require some preprocessing to remove them for proper word tokenization.</li>
<li>Reviews can have varying text lengths. It will require some padding to make all review texts the same size.</li>
</ul>
<p>Let's take a simple text example and apply these steps to understand why these steps are essential in preprocessing. In the last step, we will create tokens from the preprocessed text.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">example_text</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;This is awesome movie &lt;br /&gt;&lt;br /&gt;. I loved it so much :-) I</span><span class="se">\&#39;</span><span class="s1">m goona watch it again :)&#39;&#39;&#39;</span>
<span class="n">example_text</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#34;This is awesome movie &lt;br /&gt;&lt;br /&gt;. I loved it so much :-) I&#39;m goona watch it again :)&#34;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># step 1. remove HTML tags. they are not helpful in understanding the sentiments of a review</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">&#39;&lt;[^&gt;]*&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">example_text</span><span class="p">)</span>
<span class="n">text</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#34;This is awesome movie . I loved it so much :-) I&#39;m goona watch it again :)&#34;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># step 2: use lowercase for all text to keep symmetry</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
<span class="n">text</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#34;this is awesome movie . i loved it so much :-) i&#39;m goona watch it again :)&#34;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># step 3: extract emoticons. keep them as they are important sentiment signals</span>
<span class="n">emoticons</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;(?::|;|=)(?:-)?(?:\)|\(|D|P)&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
<span class="n">emoticons</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&#39;:-)&#39;, &#39;:)&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># step 4: remove punctuation marks</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">&#39;[\W]+&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
<span class="n">text</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;this is awesome movie i loved it so much i m goona watch it again &#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># step 5: put back emoticons</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">text</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">emoticons</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">text</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;this is awesome movie i loved it so much i m goona watch it again :) :)&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># step 6: generate word tokens</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="n">text</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&#39;this&#39;,
 &#39;is&#39;,
 &#39;awesome&#39;,
 &#39;movie&#39;,
 &#39;i&#39;,
 &#39;loved&#39;,
 &#39;it&#39;,
 &#39;so&#39;,
 &#39;much&#39;,
 &#39;i&#39;,
 &#39;m&#39;,
 &#39;goona&#39;,
 &#39;watch&#39;,
 &#39;it&#39;,
 &#39;again&#39;,
 &#39;:)&#39;,
 &#39;:)&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's put all the preprocessing steps in a nice function and give it a name.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># step 1. remove HTML tags. they are not helpful in understanding the sentiments of a review</span>
    <span class="c1"># step 2: use lowercase for all text to keep symmetry</span>
    <span class="c1"># step 3: extract emoticons. keep them as they are important sentiment signals</span>
    <span class="c1"># step 4: remove punctuation marks</span>
    <span class="c1"># step 5: put back emoticons</span>
    <span class="c1"># step 6: generate word tokens</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s2">&quot;&lt;[^&gt;]*&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">emoticons</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s2">&quot;(?::|;|=)(?:-)?(?:\)|\(|D|P)&quot;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s2">&quot;[\W]+&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span> <span class="o">+</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">emoticons</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="n">tokenized</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">tokenized</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Apply <code>tokenizer</code> on the <code>example_text</code> to verify the output.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">example_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">example_text</span><span class="p">)</span>
<span class="n">example_tokens</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&#39;this&#39;,
 &#39;is&#39;,
 &#39;awesome&#39;,
 &#39;movie&#39;,
 &#39;i&#39;,
 &#39;loved&#39;,
 &#39;it&#39;,
 &#39;so&#39;,
 &#39;much&#39;,
 &#39;i&#39;,
 &#39;m&#39;,
 &#39;goona&#39;,
 &#39;watch&#39;,
 &#39;it&#39;,
 &#39;again&#39;,
 &#39;:)&#39;,
 &#39;:)&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Preparing-data-dictionary">Preparing data dictionary<a class="anchor-link" href="#Preparing-data-dictionary"> </a></h3><p>We are successful in creating word tokens from our <code>example_text</code>. But there is one more problem. Some of the tokens are repeating. If we can convert these tokens into a dictionary along with their frequency count, we can significantly reduce the generated token size from these reviews. Let's do that.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="n">token_counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
<span class="n">token_counts</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">example_tokens</span><span class="p">)</span>
<span class="n">token_counts</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Counter({&#39;this&#39;: 1,
         &#39;is&#39;: 1,
         &#39;awesome&#39;: 1,
         &#39;movie&#39;: 1,
         &#39;i&#39;: 2,
         &#39;loved&#39;: 1,
         &#39;it&#39;: 2,
         &#39;so&#39;: 1,
         &#39;much&#39;: 1,
         &#39;m&#39;: 1,
         &#39;goona&#39;: 1,
         &#39;watch&#39;: 1,
         &#39;again&#39;: 1,
         &#39;:)&#39;: 2})</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's sort the output to have the most common words at the top.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sorted_by_freq_tuples</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">token_counts</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sorted_by_freq_tuples</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[(&#39;i&#39;, 2),
 (&#39;it&#39;, 2),
 (&#39;:)&#39;, 2),
 (&#39;this&#39;, 1),
 (&#39;is&#39;, 1),
 (&#39;awesome&#39;, 1),
 (&#39;movie&#39;, 1),
 (&#39;loved&#39;, 1),
 (&#39;so&#39;, 1),
 (&#39;much&#39;, 1),
 (&#39;m&#39;, 1),
 (&#39;goona&#39;, 1),
 (&#39;watch&#39;, 1),
 (&#39;again&#39;, 1)]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It shows that in our example text, the top place is taken by pronouns (i and it) followed by the emoticon. Though our data is now correctly processed, it needs to be prepared to be fed to a model. Because [machine] models love math and work with numbers exclusively. To convert our dictionary of word tokens into integers, we can take help from <code>torchtext.vocab</code>. Its purpose in the official documentation is defined as <a href="https://pytorch.org/text/stable/vocab.html">link here</a></p>
<blockquote><p>Factory method for creating a vocab object which maps tokens to indices.</p>
<p>Note that the ordering in which key value pairs were inserted in the ordered_dict will be respected when building the vocab. Therefore if sorting by token frequency is important to the user, the ordered_dict should be created in a way to reflect this.</p>
</blockquote>
<p>It highlights three points:<em> It maps tokens to indices</em> It requires an ordered dictionary (<code>OrderedDict</code>) to work</p>
<ul>
<li>Tokens in vocab at the starting indices reflect higher frequency</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># step 1: convert our sorted list of tokens to OrderedDict</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>

<span class="n">ordered_dict</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">(</span><span class="n">sorted_by_freq_tuples</span><span class="p">)</span>
<span class="n">ordered_dict</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>OrderedDict([(&#39;i&#39;, 2),
             (&#39;it&#39;, 2),
             (&#39;:)&#39;, 2),
             (&#39;this&#39;, 1),
             (&#39;is&#39;, 1),
             (&#39;awesome&#39;, 1),
             (&#39;movie&#39;, 1),
             (&#39;loved&#39;, 1),
             (&#39;so&#39;, 1),
             (&#39;much&#39;, 1),
             (&#39;m&#39;, 1),
             (&#39;goona&#39;, 1),
             (&#39;watch&#39;, 1),
             (&#39;again&#39;, 1)])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Check the length of our dictionary</span>
<span class="nb">len</span><span class="p">(</span><span class="n">ordered_dict</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>14</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># step 2: convert the ordered dict to torchtext.vocab</span>
<span class="kn">from</span> <span class="nn">torchtext.vocab</span> <span class="kn">import</span> <span class="n">vocab</span>

<span class="n">vb</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">(</span><span class="n">ordered_dict</span><span class="p">)</span>
<span class="n">vb</span><span class="o">.</span><span class="n">get_stoi</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;goona&#39;: 11,
 &#39;much&#39;: 9,
 &#39;m&#39;: 10,
 &#39;loved&#39;: 7,
 &#39;watch&#39;: 12,
 &#39;so&#39;: 8,
 &#39;movie&#39;: 6,
 &#39;it&#39;: 1,
 &#39;again&#39;: 13,
 &#39;this&#39;: 3,
 &#39;i&#39;: 0,
 &#39;awesome&#39;: 5,
 &#39;:)&#39;: 2,
 &#39;is&#39;: 4}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This generated vocabulary shows that tokens with higher frequency (<code>i</code>, <code>it</code>) have been assigned lower indices (or integers). This vocabulary will act as a lookup table for us, and during training for each word token, we will find a corresponding index from this vocab and pass it to our model.</p>
<p>We have done many steps while processing our <code>example_text</code>. Let's summarize them here before moving further</p>
<h4 id="Summary-of-data-dictionary-preparation-steps"><strong>Summary of data dictionary preparation steps</strong><a class="anchor-link" href="#Summary-of-data-dictionary-preparation-steps"> </a></h4><ol>
<li>Generate tokens from text using the function <code>tokenizer</code></li>
<li>Find the frequency of tokens using Python <a href="https://docs.python.org/3/library/collections.html#collections.Counter">collections.Counter</a></li>
<li>Sort the tokens based on their frequency in descending order</li>
<li>Put the sorted tokens in Python <a href="https://docs.python.org/3/library/collections.html#collections.OrderedDict">collections.OrderedDict</a></li>
<li>Convert the tokens into integers using <a href="https://pytorch.org/text/stable/vocab.html">torchtext.vocab</a></li>
</ol>
<p>Let's apply all these steps on our IMDB reviews training dataset.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># step 1: convert reviews into tokens</span>
<span class="c1"># step 2: find frequency of tokens</span>

<span class="n">token_counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>

<span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">train_dataset</span><span class="p">:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
    <span class="n">token_counts</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
 
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;IMDB vocab size:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_counts</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>IMDB vocab size: 69023
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>After tokenizing IMDB reviews, we find that there <code>69023</code> unique tokens.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># step 3: sort the token based on their frequency</span>
<span class="c1"># step 4: put the sorted tokens in OrderedDict</span>
<span class="c1"># step 5: convert token to integers using vocab object</span>

<span class="n">sorted_by_freq_tuples</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">token_counts</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ordered_dict</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">(</span><span class="n">sorted_by_freq_tuples</span><span class="p">)</span>

<span class="n">vb</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">(</span><span class="n">ordered_dict</span><span class="p">)</span>

<span class="n">vb</span><span class="o">.</span><span class="n">insert_token</span><span class="p">(</span><span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># special token for padding</span>
<span class="n">vb</span><span class="o">.</span><span class="n">insert_token</span><span class="p">(</span><span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># special token for unknown words</span>
<span class="n">vb</span><span class="o">.</span><span class="n">set_default_index</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># print some token indexes from vocab</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;this&quot;</span><span class="p">,</span> <span class="s2">&quot;is&quot;</span><span class="p">,</span> <span class="s2">&quot;an&quot;</span><span class="p">,</span> <span class="s2">&quot;example&quot;</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="s2">&quot; --&gt; &quot;</span><span class="p">,</span> <span class="n">vb</span><span class="p">[</span><span class="n">token</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>this  --&gt;  11
is  --&gt;  7
an  --&gt;  35
example  --&gt;  457
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We have added two extra tokens to our vocabulary.</p>
<ul>
<li>"pad" for padding. This token will come in handy when we pad our reviews to make them of the same length</li>
<li>"unk" for unknown. This token will come in handy if we find any token in the validation or test set that was not part of the train set</li>
</ul>
<p>Let's also print the tokens present at the first ten indices of our vocab object.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vb</span><span class="o">.</span><span class="n">get_itos</span><span class="p">()[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&#39;&lt;pad&gt;&#39;, &#39;&lt;unk&gt;&#39;, &#39;the&#39;, &#39;and&#39;, &#39;a&#39;, &#39;of&#39;, &#39;to&#39;, &#39;is&#39;, &#39;it&#39;, &#39;in&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It shows that articles, prepositions, and pronouns are the most common words in the training dataset. So let's also check the least common words.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vb</span><span class="o">.</span><span class="n">get_itos</span><span class="p">()[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&#39;hairband&#39;,
 &#39;ratt&#39;,
 &#39;bettiefile&#39;,
 &#39;queueing&#39;,
 &#39;johansen&#39;,
 &#39;hemmed&#39;,
 &#39;jardine&#39;,
 &#39;morland&#39;,
 &#39;seriousuly&#39;,
 &#39;fictive&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The least common words seem to be people or place names or misspelled words like 'queueing' and 'seriousuly'.</p>
<h2 id="Define-data-processing-pipelines">Define data processing pipelines<a class="anchor-link" href="#Define-data-processing-pipelines"> </a></h2><p>At this point, we have our tokenizer function and vocabulary lookup ready. For each review item from the dataset, we are supposed to perform the following preprocessing steps:</p>
<p><strong>For review text</strong></p>
<ul>
<li>Create tokens from the review text</li>
<li>Assign a unique integer to each token from the vocab lookup</li>
</ul>
<p><strong>For review label</strong></p>
<ul>
<li>Assign 1 for <code>pos</code> and 0 for <code>neg</code> label</li>
</ul>
<p>Let's create two simple functions (inline lambda) for review text and label processing.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># inline lambda functions for text and label precessing</span>
<span class="n">text_pipeline</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="n">vb</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">x</span><span class="p">)]</span>
<span class="n">label_pipeline</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s2">&quot;pos&quot;</span> <span class="k">else</span> <span class="mf">0.0</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># apply text_pipeline to example_text</span>
<span class="n">text_pipeline</span><span class="p">(</span><span class="n">example_text</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[11, 7, 1166, 18, 10, 450, 8, 37, 74, 10, 142, 1, 104, 8, 174, 2287, 2287]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Instead of processing a single review at a time, we always prefer to work with a batch of them during model training. For each review item in the batch, we will be doing the same preprocessing steps i.e. review text processing and label processing. For handling preprocessing steps at a batch level, we can create another higher-level function that applies preprocessing steps at a batch level.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># setting device on GPU if available, else CPU</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Using device:&#39;</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Using device: cuda
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># a function to apply pre-processing steps at a batch level</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">def</span> <span class="nf">collate_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="n">label_list</span><span class="p">,</span> <span class="n">text_list</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

    <span class="c1"># iterate over all reviews in a batch</span>
    <span class="k">for</span> <span class="n">_label</span><span class="p">,</span> <span class="n">_text</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
        <span class="c1"># label preprocessing</span>
        <span class="n">label_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label_pipeline</span><span class="p">(</span><span class="n">_label</span><span class="p">))</span>
        <span class="c1"># text preprocessing</span>
        <span class="n">processed_text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">text_pipeline</span><span class="p">(</span><span class="n">_text</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

        <span class="c1"># store the processed text in a list</span>
        <span class="n">text_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">processed_text</span><span class="p">)</span>
        
        <span class="c1"># store the length of processed text</span>
        <span class="c1"># this will come handy in future when we want to know the original size of a text (without padding)</span>
        <span class="n">lengths</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">processed_text</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    
    <span class="n">label_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">label_list</span><span class="p">)</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span>
    
    <span class="c1"># pad the processed reviews to make their lengths consistant</span>
    <span class="n">padded_text_list</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_sequence</span><span class="p">(</span>
        <span class="n">text_list</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># return</span>
    <span class="c1"># 1. a list of processed and padded review texts</span>
    <span class="c1"># 2. a list of processed labels</span>
    <span class="c1"># 3. a list of review text original lengths (before padding)</span>
    <span class="k">return</span> <span class="n">padded_text_list</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">label_list</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">lengths</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Sequence-padding">Sequence padding<a class="anchor-link" href="#Sequence-padding"> </a></h3><p>In the above <code>collate_batch</code> function, I added one extra padding step.</p>

<pre><code>added_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)</code></pre>
<p>We intend to make all review texts in a batch of the same length. For this, we take the maximum length of a text in a batch, all pad all the smaller text with extra dummy tokens ('pad') to make their sizes equal. Finally, with all the data in a batch of the same dimension, we convert it into a tensor matrix for faster processing.</p>
<p>To understand how PyTorch utility <code>nn.utils.rnn.pad_sequence</code> works, we can take a simple example of three tensors (a, b, c) of varying sizes (1, 3, 5).</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># initialize three tensors of varying sizes</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">])</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([1]), tensor([2, 3, 4]), tensor([5, 6, 7, 8, 9]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's pad them to make sizes consistant.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># apply padding on tensors</span>
<span class="n">pad_seq</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_sequence</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>
<span class="n">pad_seq</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[1, 2, 5],
        [0, 3, 6],
        [0, 4, 7],
        [0, 0, 8],
        [0, 0, 9]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Sequence-packing">Sequence packing<a class="anchor-link" href="#Sequence-packing"> </a></h3><p>From the above output, we can see that after padding tensors of varying sizes, we can convert them into a single matrix for faster processing. But the drawback of this approach is that we can have many, many padded tokens in our matrix. They are not helping us in any way, instead of occupying a lot of machine memory. To avoid this, we can also squish these matrixes into a much condensed form called <code>packed padded sequences</code> using PyTorch utility <code>nn.utils.rnn.pack_padded_sequence</code>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pack_pad_seq</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pack_padded_sequence</span><span class="p">(</span>
    <span class="n">pad_seq</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">enforce_sorted</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
<span class="n">pack_pad_seq</span><span class="o">.</span><span class="n">data</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([5, 2, 1, 6, 3, 7, 4, 8, 9])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here the tensor still holds all the original tensor values (1 to 9) but is very condensed and has no extra padded token. So how does this tensor know which tokens belong to which token? For this, it stores some additional information.</p>
<ul>
<li>batch sizes (or original tensor length)</li>
<li>tensor indices</li>
</ul>
<p>We can move back and forth between the padded pack and unpacked sequences using this information.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pack_pad_seq</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>PackedSequence(data=tensor([5, 2, 1, 6, 3, 7, 4, 8, 9]), batch_sizes=tensor([3, 2, 2, 1, 1]), sorted_indices=tensor([2, 1, 0]), unsorted_indices=tensor([2, 1, 0]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Run-data-preprocessing-pipelines-on-an-example-batch">Run data preprocessing pipelines on an example batch<a class="anchor-link" href="#Run-data-preprocessing-pipelines-on-an-example-batch"> </a></h3><p>Let's load our data in the PyTorch DataLoader class and create a small batch of 4 reviews. Preprocess the entire set with <code>collate_batch</code> function.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_batch</span>
<span class="p">)</span>
<span class="n">text_batch</span><span class="p">,</span> <span class="n">label_batch</span><span class="p">,</span> <span class="n">length_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;text_batch.shape: &quot;</span><span class="p">,</span> <span class="n">text_batch</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;label_batch: &quot;</span><span class="p">,</span> <span class="n">label_batch</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;length_batch: &quot;</span><span class="p">,</span> <span class="n">length_batch</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>text_batch.shape:  torch.Size([4, 218])
label_batch:  tensor([1., 1., 1., 0.], device=&#39;cuda:0&#39;)
length_batch:  tensor([165,  86, 218, 145], device=&#39;cuda:0&#39;)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><code>text_batch.shape: torch.Size([4, 218])</code> tells us that in this batch, there are four reviews (or their tokens) and all have the same length of 218</li>
<li><code>label_batch:  tensor([1., 1., 1., 0.])</code> tells us that the first three reviews are positive and the last is negative</li>
<li><code>length_batch:  tensor([165,  86, 218, 145])</code> tells us that before padding the original length of review tokens</li>
</ul>
<p>Let's check what the first review in this batch looks like after preprocessing and padding.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">text_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor([   35,  1739,     7,   449,   721,     6,   301,     4,   787,     9,
            4,    18,    44,     2,  1705,  2460,   186,    25,     7,    24,
          100,  1874,  1739,    25,     7, 34415,  3568,  1103,  7517,   787,
            5,     2,  4991, 12401,    36,     7,   148,   111,   939,     6,
        11598,     2,   172,   135,    62,    25,  3199,  1602,     3,   928,
         1500,     9,     6,  4601,     2,   155,    36,    14,   274,     4,
        42945,     9,  4991,     3,    14, 10296,    34,  3568,     8,    51,
          148,    30,     2,    58,    16,    11,  1893,   125,     6,   420,
         1214,    27, 14542,   940,    11,     7,    29,   951,    18,    17,
        15994,   459,    34,  2480, 15211,  3713,     2,   840,  3200,     9,
         3568,    13,   107,     9,   175,    94,    25,    51, 10297,  1796,
           27,   712,    16,     2,   220,    17,     4,    54,   722,   238,
          395,     2,   787,    32,    27,  5236,     3,    32,    27,  7252,
         5118,  2461,  6390,     4,  2873,  1495,    15,     2,  1054,  2874,
          155,     3,  7015,     7,   409,     9,    41,   220,    17,    41,
          390,     3,  3925,   807,    37,    74,  2858,    15, 10297,   115,
           31,   189,  3506,   667,   163,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0],
       device=&#39;cuda:0&#39;)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To complete the picture, I have re-printed the original text of the first review and manually processed a part of it. You can verify that the tokens match.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># first review</span>
<span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;pos&#39;,
 &#39;An extra is called upon to play a general in a movie about the Russian Revolution. However, he is not any ordinary extra. He is Serguis Alexander, former commanding general of the Russia armies who is now being forced to relive the same scene, which he suffered professional and personal tragedy in, to satisfy the director who was once a revolutionist in Russia and was humiliated by Alexander. It can now be the time for this broken man to finally &#34;win&#34; his penultimate battle. This is one powerful movie with meticulous direction by Von Sternberg, providing the greatest irony in Alexander\&#39;s character in every way he can. Jannings deserved his Oscar for the role with a very moving performance playing the general at his peak and at his deepest valley. Powell lends a sinister support as the revenge minded director and Brent is perfect in her role with her face and movements showing so much expression as Jannings\&#39; love. All around brilliance. Rating, 10.&#39;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># manually preprocessing a part of review text</span>
<span class="c1"># notice that the generated tokens match</span>
<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;An extra is called upon to play a general in a movie about the Russian Revolution&#39;</span>
<span class="p">[</span><span class="n">vb</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">)]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[35, 1739, 7, 449, 721, 6, 301, 4, 787, 9, 4, 18, 44, 2, 1705, 2460]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Batching-the-training,-validation,-and-test-dataset">Batching the training, validation, and test dataset<a class="anchor-link" href="#Batching-the-training,-validation,-and-test-dataset"> </a></h2><p>Let's proceed on creating DataLoaders for train, valid, and test data with <code>batch_size = 32</code></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_batch</span>
<span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_batch</span>
<span class="p">)</span>
<span class="n">test_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">test_dataset_raw</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_batch</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Define-model-training-and-evaluation-pipelines">Define model training and evaluation pipelines<a class="anchor-link" href="#Define-model-training-and-evaluation-pipelines"> </a></h2><p>I have defined two simple functions to train and evaluate the model in this section.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># model training pipeline</span>
<span class="c1"># https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part2.ipynb</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">total_acc</span><span class="p">,</span> <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">text_batch</span><span class="p">,</span> <span class="n">label_batch</span><span class="p">,</span> <span class="n">lengths</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">text_batch</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">label_batch</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">total_acc</span> <span class="o">+=</span> <span class="p">((</span><span class="n">pred</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">==</span> <span class="n">label_batch</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">label_batch</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">total_acc</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">),</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>


<span class="c1"># model evaluation pipeline</span>
<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">total_acc</span><span class="p">,</span> <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">text_batch</span><span class="p">,</span> <span class="n">label_batch</span><span class="p">,</span> <span class="n">lengths</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">text_batch</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">label_batch</span><span class="p">)</span>
            <span class="n">total_acc</span> <span class="o">+=</span> <span class="p">((</span><span class="n">pred</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">==</span> <span class="n">label_batch</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">label_batch</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">total_acc</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">),</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="RNN-model-configuration,-loss-function,-and-optimizer">RNN model configuration, loss function, and optimizer<a class="anchor-link" href="#RNN-model-configuration,-loss-function,-and-optimizer"> </a></h2><p>We have seen the review text, which can be long sequences. We will use the LSTM layer for capturing the long-term dependencies. Our sentiment analysis model is composed of the following layers</p>
<ul>
<li>Start with an <strong>Embedding layer</strong>. Placing the embedding layer is similar to one-hot-encoding, where each word token is converted to a separate feature (or vector or column). But this can lead to too many features (curse of dimensionality or dimensional explosion). To avoid this, we try to map tokens to fixed-size vectors (or columns). In such a feature matrix, different elements denote different tokens. Tokens that are closed are also placed together. Further, during training, we also learn and update the positioning of tokens. Similar tokens are placed into closer and closer locations. Such a matrix layer is termed an embedding layer.</li>
<li>After the embedding layer, there is the RNN layer (LSTM to be specific).</li>
<li>Then we have a fully connected layer followed by activation and another fully connected layer.</li>
<li>Finally, we have a logistic sigmoid layer for prediction</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part2.ipynb</span>
<span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">rnn_hidden_size</span><span class="p">,</span> <span class="n">fc_hidden_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">rnn_hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">rnn_hidden_size</span><span class="p">,</span> <span class="n">fc_hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">fc_hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">lengths</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pack_padded_sequence</span><span class="p">(</span>
            <span class="n">out</span><span class="p">,</span> <span class="n">lengths</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">enforce_sorted</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="n">out</span><span class="p">,</span> <span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">cell</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">hidden</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vb</span><span class="p">)</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">rnn_hidden_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">fc_hidden_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">rnn_hidden_size</span><span class="p">,</span> <span class="n">fc_hidden_size</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Define-model-loss-function-and-optimizer">Define model loss function and optimizer<a class="anchor-link" href="#Define-model-loss-function-and-optimizer"> </a></h3><p>For loss function (or criterion), I have used <a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html">Binary Cross Entropy</a>, and for loss optimization, I have used <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html">Adam algorithm</a></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Model-training-and-evaluation">Model training and evaluation<a class="anchor-link" href="#Model-training-and-evaluation"> </a></h2><p>Let's run the pipeline for ten epochs and compare the training and validation accuracy.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">acc_train</span><span class="p">,</span> <span class="n">loss_train</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">train_dl</span><span class="p">)</span>
    <span class="n">acc_valid</span><span class="p">,</span> <span class="n">loss_valid</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">valid_dl</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> train accuracy: </span><span class="si">{</span><span class="n">acc_train</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">; val accuracy: </span><span class="si">{</span><span class="n">acc_valid</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch 0 train accuracy: 0.6085; val accuracy: 0.6502
Epoch 1 train accuracy: 0.7206; val accuracy: 0.7462
Epoch 2 train accuracy: 0.7613; val accuracy: 0.6250
Epoch 3 train accuracy: 0.8235; val accuracy: 0.8232
Epoch 4 train accuracy: 0.8819; val accuracy: 0.8482
Epoch 5 train accuracy: 0.9132; val accuracy: 0.8526
Epoch 6 train accuracy: 0.9321; val accuracy: 0.8374
Epoch 7 train accuracy: 0.9504; val accuracy: 0.8502
Epoch 8 train accuracy: 0.9643; val accuracy: 0.8608
Epoch 9 train accuracy: 0.9747; val accuracy: 0.8636
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Evaluate-sentiments-on-random-texts">Evaluate sentiments on random texts<a class="anchor-link" href="#Evaluate-sentiments-on-random-texts"> </a></h3><p>Let's create another helper method to evaluate sentiments on random texts.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">classify_review</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">text_list</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

    <span class="c1"># process review text with text_pipeline</span>
    <span class="c1"># note: &quot;text_pipeline&quot; has dependency on data vocabulary</span>
    <span class="n">processed_text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">text_pipeline</span><span class="p">(</span><span class="n">text</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="n">text_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">processed_text</span><span class="p">)</span>

    <span class="c1"># get processed review tokens length</span>
    <span class="n">lengths</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">processed_text</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span>
        
    <span class="c1"># change the dimensions from (torch.Size([8]), torch.Size([1, 8]))</span>
    <span class="c1"># nn.utils.rnn.pad_sequence(text_list, batch_first=True) does this too</span>
    <span class="n">padded_text_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">processed_text</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># move tensors to correct device</span>
    <span class="n">padded_text_list</span> <span class="o">=</span> <span class="n">padded_text_list</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># get prediction</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">padded_text_list</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;model pred: &quot;</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>

    <span class="c1"># positive or negative review</span>
    <span class="n">review_class</span> <span class="o">=</span> <span class="s1">&#39;negative&#39;</span> <span class="c1"># else case</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">pred</span><span class="o">&gt;=</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">review_class</span> <span class="o">=</span> <span class="s2">&quot;positive&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;review type: &quot;</span><span class="p">,</span> <span class="n">review_class</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># create two random texts with strong positive and negative sentiments</span>
<span class="n">pos_review</span> <span class="o">=</span> <span class="s1">&#39;i love this movie. it was so good.&#39;</span>
<span class="n">neg_review</span> <span class="o">=</span> <span class="s1">&#39;slow and boring. waste of time.&#39;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">classify_review</span><span class="p">(</span><span class="n">pos_review</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>model pred:  tensor([[0.9388]], device=&#39;cuda:0&#39;, grad_fn=&lt;SigmoidBackward0&gt;)
review type:  positive
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">classify_review</span><span class="p">(</span><span class="n">neg_review</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>model pred:  tensor([[0.0057]], device=&#39;cuda:0&#39;, grad_fn=&lt;SigmoidBackward0&gt;)
review type:  negative
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

