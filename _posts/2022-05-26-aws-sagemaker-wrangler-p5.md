---
keywords: fastai
description: A detailed guide on AWS SageMaker Data Wrangler to prepare data for machine learning models. This is a five parts series where we will prepare, import, explore, process, and export data using AWS Data Wrangler. You are reading **Part 5:Export data for ML training**.
title: Data Preparation with SageMaker Data Wrangler (Part 5)
toc: true 
badges: true
comments: true
categories: [aws, ml, sagemaker]
keyword: [aws, ml, sagemaker, wrangler]
image: images/copied_from_nb/images/2022-05-26-aws-sagemaker-wrangler-p5.jpeg
nb_path: _notebooks/2022-05-26-aws-sagemaker-wrangler-p5.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-05-26-aws-sagemaker-wrangler-p5.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/myblog/images/copied_from_nb/images/2022-05-26-aws-sagemaker-wrangler-p5.jpeg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Enviornment">Enviornment<a class="anchor-link" href="#Enviornment"> </a></h1><p>This notebook is prepared with Amazon SageMaker Studio using <code>Python 3 (Data Science)</code> Kernel and <code>ml.t3.medium</code> instance.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="About">About<a class="anchor-link" href="#About"> </a></h1><p>This is a detailed guide on using <strong>AWS SageMaker Data Wrangler</strong> service to prepare data for machine learning models. SageMaker Data Wrangler is a multipurpose tool with which you can</p>
<ul>
<li>import data from multiple sources</li>
<li>explore data with visualizations</li>
<li>apply transformations</li>
<li>export data for ml training</li>
</ul>
<p>This guide is divided into five parts</p>
<ul>
<li><a href="https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/17/aws-sagemaker-wrangler-p1.html">Part 1: Prepare synthetic data and place it on multiple sources</a></li>
<li><a href="https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/23/aws-sagemaker-wrangler-p2.html">Part 2: Import data from multiple sources using Data Wrangler</a></li>
<li><a href="https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/24/aws-sagemaker-wrangler-p3.html">Part 3: Explore data with Data Wrangler visualizations</a></li>
<li><a href="https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/25/aws-sagemaker-wrangler-p4.html">Part 4: Preprocess data using Data Wrangler</a></li>
<li>Part 5: Export data for ML training (You are here)</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Part-5:-Export-data-for-ML-training">Part 5: Export data for ML training<a class="anchor-link" href="#Part-5:-Export-data-for-ML-training"> </a></h1><p>It is important to note that the transformations we have used are not applied to the data yet. These transformations need to be executed to get the final transformed data. When we export Data Wrangler flow it generates the code that when executed will perform the data transformations. Data Wrangler supports four export methods: Save to S3, Pipeline, Python Code, and Feature Store. In this post, we will see how to export data to S3 as this is the most common use case.</p>
<p>Open the <code>customer-churn-p4.flow</code> file from <a href="https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/25/aws-sagemaker-wrangler-p4.html">part-4</a>. On the last step click the plus sign and select <code>Export to &gt; Amazon S3 (via Jupyter Notebook)</code></p>
<p><img src="/myblog/images/copied_from_nb/images/2022-05-25-aws-sagemaker-wrangler-p5/export_s3.png" alt="export_s3.png"></p>
<p>Sagemaker Data Wrangler will auto-generate a Jupyter notebook that will contain all the required code to transform and put data on the S3 bucket.</p>
<p><img src="/myblog/images/copied_from_nb/images/2022-05-25-aws-sagemaker-wrangler-p5/export_notebook.png" alt="export_notebook.png"></p>
<p>You may review the code and make any changes otherwise run it as it is till point <strong>(Optional)Next Steps</strong>. This is the first time SageMaker will process the data and place the output on S3 bucket. SageMaker may take a couple of minutes to execute all the cells. It is important to note that this notebook will initiate a container running on a separate machine to do all the processing. The machine specs are defined in the notebook as</p>

<pre><code># Processing Job Instance count and instance type.
instance_count = 2
instance_type = "ml.m5.4xlarge"</code></pre>
<p>Once execution is complete you see the output message containing the S3 bucket location where the final output is stored.</p>
<p><img src="/myblog/images/copied_from_nb/images/2022-05-25-aws-sagemaker-wrangler-p5/export_s3_output.png" alt="export_s3_output.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The optional part of this notebook also contains code to generate xgboost model on the transformed data. To execute these steps make the following changes in the notebook.</p>
<p>Change the flag to run the optional steps.</p>

<pre><code>run_optional_steps = True</code></pre>
<p>Next, update the xgboost hyperparameters to train a binary classification model (customer churn or not?).</p>

<pre><code>hyperparameters = {
    "max_depth":"5",
    "objective": "binary:logistic",
    "num_round": "10",
}</code></pre>
<p>Execute the optional steps. Again note that these steps will initiate a container running on a separate machine ("ml.m5.2xlarge") to do the training work. The training job will take a few minutes to complete and once it is done trained model will be available on the S3 bucket for inference use. This autogenerated notebook <code>customer-churn-p4.ipynb</code> is available on GitHub here.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Summary">Summary<a class="anchor-link" href="#Summary"> </a></h1><p>In this last post of the series, we used SageMaker Data Wrangler to auto-generate code to preprocess the data and store the final output on S3 bucket. We also used the same notebook to train an xgboost model on the processed data.</p>

</div>
</div>
</div>
</div>
 

