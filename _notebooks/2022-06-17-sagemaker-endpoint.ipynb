{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serverless Inference with SageMaker Serverless Endpoints\n",
    "> How to call an ML model endpoint hosted by SageMaker using serverless technology.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [aws, ml, sagemaker]\n",
    "- keyword: [aws, ml, sagemaker]\n",
    "- image: images/copied_from_nb/images/2022-06-17-sagemaker-endpoint.jpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/2022-06-17-sagemaker-endpoint.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "You have trained and deployed a model using Amazon SageMaker. You have an endpoint and now you are wondering \"After I deploy an endpoint, where do I go from there?\" Your concerns are valid because SageMaker endpoints are not public but are scoped to an individual account. In this post, we will discuss how to make them public using AWS serverless technologies: AWS Lambda and Function URL. We will also make our endpoints serverless so our ML inference solution is serverless end-to-end.\n",
    "\n",
    "# Introduction\n",
    "\n",
    "The following diagram shows how a model is called using AWS serverless architecture.\n",
    "<br><br>\n",
    "![serverless-architecture](images/2022-06-17-sagemaker-endpoint/serverless-architecture.png)\n",
    "\n",
    "Starting from the client, a client application calls the AWS Lambda Function URL and passes parameter values. The Lambda function parses the request and passes it SageMaker model endpoint. This endpoint can be hosted on an EC2 instance or you have the option to make it serverless. Serverless endpoints behave similarly to Lambda functions. Once a request is received by the endpoint it will perform the prediction and return the predicted values to Lambda. The Lambda function then parses the returned values and sends the final response back to the client.\n",
    "\n",
    "To train a model using Amazon SageMaker you can follow my other post [Demystifying Amazon SageMaker Training for scikit-learn Lovers](https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/06/08/sagemaker-training-overview.html). There I have trained SageMaker Linear Learner model on Boston housing dataset.\n",
    "\n",
    "`Note that this post assumes that you have already trained a model and is available in SageMaker model repository.`\n",
    "\n",
    "# Deploy SageMaker Serverless Endpoint\n",
    "\n",
    "## Through SageMaker Console UI\n",
    "\n",
    "Let's first deploy our serverless endpoint through SageMaker console UI. In the next section, we will do the same through SageMaker Python SDK.\n",
    "\n",
    "Visit the SageMaker model repository to find the registered Linear Learner model. You can find the repository on the **SageMaker Inference > Model** page.\n",
    "\n",
    "![model-repo](images/2022-06-17-sagemaker-endpoint/model-repo.png)\n",
    "\n",
    "Note the mode name `linear-learner-2022-06-16-09-10-17-207` as will need it in later steps.\n",
    "\n",
    "Click on the *model name* and then **Create endpoint**\n",
    "\n",
    "![create-endpoint](images/2022-06-17-sagemaker-endpoint/create-endpoint.png)\n",
    "\n",
    "This will take you to **configure endpoint page**. Here do the following configurations.\n",
    "* Set **Endpoint name** to `2022-06-17-sagemaker-endpoint-serverless`. You may use any other unique string here.\n",
    "* From **Attach endpoint configuration** select `create a new endpoint configuration`\n",
    "* From **New endpoint configuration > Endpoint configuration** set\n",
    "  * **Endpoint configuration name** to `config-2022-06-17-sagemaker-endpoint-serverless`. You may use any other name here.\n",
    "  * **Type of endpoint** to `Serverless`\n",
    "  * From **Production variants** click on **Add Model** and then select the model name we want to deploy. In our case it is `linear-learner-2022-06-16-09-10-17-207`. Click **Save**.\n",
    "\n",
    "![add-model](images/2022-06-17-sagemaker-endpoint/add-model.png)\n",
    "\n",
    "* Then Edit the **Max Concurrency** and set it to 5.\n",
    "\n",
    "![max-concurrency](images/2022-06-17-sagemaker-endpoint/max-concurrency.png)\n",
    "\n",
    "* Click **Create endpoint configuration**\n",
    "\n",
    "![new-endpoint-config](images/2022-06-17-sagemaker-endpoint/new-endpoint-config.png)\n",
    "\n",
    "* Click **Create endpoint**\n",
    "\n",
    "![endpoint-created](images/2022-06-17-sagemaker-endpoint/endpoint-created.png)\n",
    "\n",
    "It will take a minute for the created endpoint to become ready.\n",
    "\n",
    "While we were configuring the concurrency for our endpoint we have give it a value of 5. This is because at this point there is a limit on concurrency per account across all serverless endpoints. The maximum total concurrency for an account is 20, and if you cross this limit you will get an error as shown below.\n",
    "\n",
    "![serverless-endpoints-concurrency-error](images/2022-06-17-sagemaker-endpoint/serverless-endpoints-concurrency-error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Through SageMaker Python SDK\n",
    "\n",
    "Let's do the same again but using SageMaker SDK. Deploying a model to a serverless endpoint using SDK involves the following steps:\n",
    "* Get session to SageMaker API\n",
    "* Create a serverless endpoint deployment config\n",
    "* Create a reference to a model container\n",
    "* Deploy the model on a serverless endpoint using serverless configuration\n",
    "\n",
    "Let's do it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# get a session to sagemaker api's\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"sagemaker.__version__: {sagemaker.__version__}\")\n",
    "print(f\"Session: {session}\")\n",
    "print(f\"Role: {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# define a serverless endpoint configuration\n",
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=1024, max_concurrency=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we are only defining the endpoint configuration. It will be created when we will deploy the model. Also, note that we have not passed any configuration name. It will default to the endpoint name. To read more about the serverless inference configuration read the documentation [ServerlessInferenceConfig](https://sagemaker.readthedocs.io/en/stable/api/inference/serverless.html?highlight=serverless_inference_config#module-sagemaker.serverless.serverless_inference_config)\n",
    "\n",
    "I could not find a way to give a name to endpoint configuration from SageMaker SDK. Let me know in the comments if there is a way to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# create a SageMaker model. In our case model is already registered so it will only create a reference to it\n",
    "\n",
    "from sagemaker.model import Model\n",
    "\n",
    "ll_model = Model(\n",
    "    image_uri = '382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner', # find it from the SageMaker mode repository\n",
    "    name = 'linear-learner-2022-06-16-09-10-17-207',\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While creating a SageMaker model you need to provide its container URI, name, and role. The role gives necessary permissions to SageMaker to pull the image container from the ECR repository. To read more about the Model read the docs [sagemaker.model.Model](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html?highlight=EndpointConfig#sagemaker.model.Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# define the endpoint name\n",
    "endpoint_name = '2022-06-17-sagemaker-endpoint-serverless-sdk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: linear-learner-2022-06-16-09-10-17-207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "##\n",
    "# deploy the model to serverless endpoint\n",
    "ll_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serverless_inference_config=serverless_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will take a minute or so for the serverless endpoint to get provisioned. Once it is ready (InService) you will find it on the **SageMaker Inference > Endpoints** page.\n",
    "\n",
    "![endpoint-created-sdk](images/2022-06-17-sagemaker-endpoint/endpoint-created-sdk.png)\n",
    "\n",
    "`model.deploy()` command will also create the endpoint configuration with same name as endpoint, and it can be found on **SageMaker Inference > Endpoint configurations** page\n",
    "\n",
    "![new-endpoint-config-sdk](images/2022-06-17-sagemaker-endpoint/new-endpoint-config-sdk.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Lambda Function with Function URL\n",
    "\n",
    "Our model's serverless endpoint is ready, and in this section we will make it public using AWS Lambda and Function URL. Let's create our Lambda Function. \n",
    "\n",
    "From AWS Lambda console, click **Create Function** and make the following configurations.\n",
    "\n",
    "Under **Basic Information**\n",
    "* Function name = 'linear-learner-boston-demo'\n",
    "* Runtime = 'Python 3.9'\n",
    "* Execution Role = 'Create a new role with basic Lambda permissions'\n",
    "\n",
    "Under **Advanced Settings**\n",
    "* Check 'Enable function URL'\n",
    "* 'Auth type' to None. This is for demo purposes.\n",
    "\n",
    "Click **Create Function**\n",
    "\n",
    "Once the function is created click on it to open its page. \n",
    "\n",
    "Under `Function Overview` on the bottom right there is a Function URL that we can call to access it publically.\n",
    "\n",
    "![lambda-function-created](images/2022-06-17-sagemaker-endpoint/lambda-function-created.png)\n",
    "\n",
    "For our function to call SageMaker endpoint we first need to give it some extra permissions. For this click on the Lambda **Configurations > Permissions > Role name**. This will open the IAM page for the Role, and the Policies under that role. Select the Policy attached to this Role and Click Edit.\n",
    "\n",
    "![iam-policy](images/2022-06-17-sagemaker-endpoint/iam-policy.png)\n",
    "\n",
    "On the next page add the following permissions to your policy.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Sid\": \"VisualEditor2\",\n",
    "    \"Effect\": \"Allow\",\n",
    "    \"Action\": \"sagemaker:InvokeEndpoint\",\n",
    "    \"Resource\": \"*\"\n",
    "}\n",
    "```\n",
    "\n",
    "After adding those line your final policy will look similar to this.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"logs:CreateLogGroup\",\n",
    "            \"Resource\": \"arn:aws:logs:us-east-1:801598032724:*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:logs:us-east-1:801598032724:log-group:/aws/lambda/linear-learner-boston-demo:*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"VisualEditor2\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"sagemaker:InvokeEndpoint\",\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Review policy and save your changes.\n",
    "\n",
    "\n",
    "Now go back to your Lambda console and use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "runtime= boto3.client('runtime.sagemaker')\n",
    "\n",
    "endpoint_name = '2022-06-17-sagemaker-endpoint-serverless-sdk'\n",
    "    \n",
    "def lambda_handler(event, context):\n",
    "    print(\"Received event: \" + json.dumps(event, indent=2))\n",
    "    data = json.loads(json.dumps(event))\n",
    "    payload = data['body']\n",
    "    \n",
    "    #payload = '0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98'\n",
    "    \n",
    "    response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                       ContentType='text/csv',\n",
    "                                       Body=payload)    \n",
    "    print(response)\n",
    "    \n",
    "    result = json.loads(response['Body'].read().decode())\n",
    "    print(result)\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(result)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have done here is\n",
    "\n",
    "* use boto3 SDK to create a session with SageMaker API. We did not use SageMaker SDK here because it is not available in the Lambda environment as of now. You may read more about it here [sagemaker-python-sdk in AWS Lambda](https://github.com/aws/sagemaker-python-sdk/issues/1200) \n",
    "* then we have defined the endpoint name that we want to call from this function\n",
    "* in the lambda handler we have parsed the request to get the payload\n",
    "* next we have invoked the serverless endpoint with the payload\n",
    "* then we parsed the response to get the predictions\n",
    "* finally we have returned the prediction\n",
    "\n",
    "Note that in the endpoint we have used `2022-06-17-sagemaker-endpoint-serverless-sdk` which we have created through SageMaker SDK. You may also use `2022-06-17-sagemaker-endpoint-serverless` which we created from UI as both points to the same model.\n",
    "\n",
    "Let's deploy our function code, and create a test event. Give it a name and use the following event body\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"body\": \"0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98\"\n",
    "}\n",
    "```\n",
    "\n",
    "Now test it. The first time I tested it I got a timeout exception. The reason for this is that the default timeout for the Lambda function is 3 seconds. But when the function called serverless endpoint it could not get any response during that time window. This is because of a **cold start** for the serverless endpoint. \n",
    "\n",
    ">If your endpoint does not receive traffic for a while and then your endpoint suddenly receives new requests, it can take some time for your endpoint to spin up the compute resources to process the requests. This is called a **cold start**. Since serverless endpoints provision compute resources on demand, your endpoint may experience cold starts. A cold start can also occur if your concurrent requests exceed the current concurrent request usage. The cold start time depends on your model size, how long it takes to download your model, and the start-up time of your container.\n",
    "\n",
    "![lambda-coldstart-timeout](images/2022-06-17-sagemaker-endpoint/lambda-coldstart-timeout.png)\n",
    "\n",
    "On the next test event I got a successful response from the Lambda function as shown below.\n",
    "\n",
    "![lambda-event-success](images/2022-06-17-sagemaker-endpoint/lambda-event-success.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the logs for your serverless endpoint on AWS CloudWatch under log group **/aws/sagemaker/Endpoints/[endpoint-name]**. In our case it will be `/aws/sagemaker/Endpoints/2022-06-17-sagemaker-endpoint-serverless`. If you look at the logs you will find that serverless endpoint is doing the following steps:\n",
    "\n",
    "* loading request and response encoders\n",
    "* loading the model\n",
    "* starting a gunicorn server\n",
    "* starting a server listener\n",
    "* making prediction\n",
    "* returning results\n",
    "\n",
    "![cloudwatch-logs](images/2022-06-17-sagemaker-endpoint/cloudwatch-logs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Serverless Inference through Postman\n",
    "\n",
    "At this point our inference endpoint is ready to be consumed from external applications. Let's use Postman for testing. Copy the lambda function URL and paste it in Postman Request UI. For the body use the following text.\n",
    "\n",
    "```\n",
    "0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98\n",
    "```\n",
    "Send a POST request and on SUCCESS you will get the predictions as shown below\n",
    "\n",
    "![postman-success](images/2022-06-17-sagemaker-endpoint/postman-success.png)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbac80ad2bfd54975e0c0f7ddf300156d5b24b5126f35dc262fa887f22fb28f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
