{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auS4LXzau0x1"
      },
      "source": [
        "# Two Class (Binary) Logistic Regression in Pytorch\n",
        "> This is a practice notebook for implementing a two class logistic regression model in PyTorch. We will start by generating some synthetic data and then build an end-to-end pipeline to train a model. We will also see two ways to implement logistic regression models.\n",
        "\n",
        "- toc: true \n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [pytorch]\n",
        "- keyword: [ml, dl, logistic, regression, pytorch]\n",
        "- image: images/copied_from_nb/images/2022-10-11-pytorch-two-class-logistic-regression.jpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2O8j6ezu0x9"
      },
      "source": [
        "![](images/2022-10-11-pytorch-two-class-logistic-regression.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J4Ci40Wu0x-"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this notebook, we will train a logistic regression model using PyTorch. Given below is the summary of the steps followed in this notebook.\n",
        "* Create a synthetic binary class dataset\n",
        "* Split the data into `Train` and `Validation` datasets. Then convert them into mini-batches using PyTorch `DataLoader` class\n",
        "* Create a Neural Net model configuration, an SGD optimizer, and a loss function\n",
        "* Create a pipeline that will train the model on given data and update the weights based on the loss \n",
        "* Compare the results with scikit-learn logistic regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtohdezgu0x_"
      },
      "source": [
        "## Environment\n",
        "This notebook is prepared with Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9kZUBv9DobO",
        "outputId": "c13a2607-e015-4484-d363-edc7ad386397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python==3.7.14\n",
            "sklearn==1.0.2\n",
            "numpy==1.21.6\n",
            "torch==1.12.1+cu113\n",
            "matplotlib==3.2.2\n"
          ]
        }
      ],
      "source": [
        "#collapse\n",
        "from platform import python_version\n",
        "import sklearn, numpy, matplotlib, pandas\n",
        "\n",
        "print(\"python==\" + python_version())\n",
        "print(\"sklearn==\" + sklearn.__version__)\n",
        "print(\"numpy==\" + numpy.__version__)\n",
        "print(\"torch==\" + torch.__version__)\n",
        "print(\"matplotlib==\" + matplotlib.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsza7iAIu0x_"
      },
      "source": [
        "## Credits\n",
        "This notebook takes inspiration from the book \"Deep Learning with PyTorch Step-by-Step\" by \"Daniel Voigt Godoy\". You can get the book from its website: [pytorchstepbystep](https://pytorchstepbystep.com/). The GitHub repository for this book has valuable notebooks and can be used independently: [github.com/dvgodoy/PyTorchStepByStep](https://github.com/dvgodoy/PyTorchStepByStep). Parts of the code you see in this notebook are taken [chapter 3 notebook](https://colab.research.google.com/github/dvgodoy/PyTorchStepByStep/blob/master/Chapter03.ipynb) of the same book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb14e269u0yA"
      },
      "source": [
        "## Generate synthetic data\n",
        "\n",
        "In this section, we will generate some data representing two interleaving half-circles using [sklearn.datasets.make_moons](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html). The purpose of this function is defined as\n",
        "\n",
        "> Make two interleaving half circles. A simple toy dataset to visualize clustering and classification algorithms ... It generate 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HfAPRknIu0yB"
      },
      "outputs": [],
      "source": [
        "# data generation\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_moons(n_samples=100, noise=0.3, random_state=0)\n",
        "# split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.2, random_state=13)\n",
        "\n",
        "# standardize data\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "\n",
        "X_train = sc.transform(X_train)\n",
        "X_val = sc.transform(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's view the generated data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train[:10], y_train[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-nAL3jPKyHc"
      },
      "source": [
        "Let's plot our generated data to see how it looks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "jrzdN0Dhu0yD",
        "outputId": "10282722-bfaa-4cb1-d5c4-9770223ac50e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "figure, axes = plt.subplots(1, 3, figsize=(15,5))\n",
        "figure.suptitle('Train and Validation Dataset')\n",
        "\n",
        "axes[0].set_title('Training Data')\n",
        "axes[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n",
        "\n",
        "axes[1].set_title('Validation Data')\n",
        "axes[1].scatter(X_val[:, 0], X_val[:, 1], c=y_val)\n",
        "\n",
        "axes[2].set_title('Combined Data')\n",
        "axes[2].scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n",
        "axes[2].scatter(X_val[:, 0], X_val[:, 1], c=y_val)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAIqiWOiLFR-"
      },
      "source": [
        "## Load generated data into PyTorch Dataset and DataLoader class\n",
        "\n",
        "In this section, we will load our data in PyTorch helper classes Dataset and DataLoader. PyTorch documentation defines them as: [see [basics/data_tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)]\n",
        "\n",
        "> Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
        "\n",
        "\n",
        "For this, we first need to convert NumPy data arrays to PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p-EEMQZeu0yF"
      },
      "outputs": [],
      "source": [
        "# data preparation\n",
        "\n",
        "import torch\n",
        "\n",
        "# Builds tensors from numpy arrays\n",
        "x_train_tensor = torch.as_tensor(X_train).float()\n",
        "y_train_tensor = torch.as_tensor(y_train.reshape(-1, 1)).float()\n",
        "\n",
        "x_val_tensor = torch.as_tensor(X_val).float()\n",
        "y_val_tensor = torch.as_tensor(y_val.reshape(-1, 1)).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPIRD_mATHu8"
      },
      "source": [
        "Now load the tensors into Dataset and DataLoader class. PyTorch Datasets are helper classes that contain data and labels as a list of tuples. DataLoader is another helper class to create batches from Dataset tuples. `batch_size` means the number of tuples we want in a single batch. We have used 16 here since our data is small. So each fetch from DataLoader will give us a list of 16 tuples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oNWTRbAzu0yG"
      },
      "outputs": [],
      "source": [
        "## \n",
        "# Load tensors into Dataset and DataLoader\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Builds dataset containing ALL data points\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
        "\n",
        "# Builds a loader of each set\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pipeline\n",
        "import numpy as np\n",
        "import datetime\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "class DeepLearningPipeline(object):\n",
        "    def __init__(self, model, loss_fn, optimizer):\n",
        "        # Here we define the attributes of our class\n",
        "        \n",
        "        # We start by storing the arguments as attributes \n",
        "        # to use them later\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # Let's send the model to the specified device right away\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # These attributes are defined here, but since they are\n",
        "        # not informed at the moment of creation, we keep them None\n",
        "        self.train_loader = None\n",
        "        self.val_loader = None\n",
        "        self.writer = None\n",
        "        \n",
        "        # These attributes are going to be computed internally\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.total_epochs = 0\n",
        "\n",
        "        # Creates the train_step function for our model, \n",
        "        # loss function and optimizer\n",
        "        # Note: there are NO ARGS there! It makes use of the class\n",
        "        # attributes directly\n",
        "        self.train_step_fn = self._make_train_step_fn()\n",
        "        # Creates the val_step function for our model and loss\n",
        "        self.val_step_fn = self._make_val_step_fn()\n",
        "\n",
        "    def set_loaders(self, train_loader, val_loader=None):\n",
        "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
        "        # Both loaders are then assigned to attributes of the class\n",
        "        # So they can be referred to later\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "\n",
        "    def _make_train_step_fn(self):\n",
        "        # This method does not need ARGS... it can refer to\n",
        "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
        "        \n",
        "        # Builds function that performs a step in the train loop\n",
        "        def perform_train_step_fn(x, y):\n",
        "            # Sets model to TRAIN mode\n",
        "            self.model.train()\n",
        "\n",
        "            # Step 1 - Computes our model's predicted output - forward pass\n",
        "            yhat = self.model(x)\n",
        "            # Step 2 - Computes the loss\n",
        "            loss = self.loss_fn(yhat, y)\n",
        "            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
        "            loss.backward()\n",
        "            # Step 4 - Updates parameters using gradients and the learning rate\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Returns the loss\n",
        "            return loss.item()\n",
        "\n",
        "        # Returns the function that will be called inside the train loop\n",
        "        return perform_train_step_fn\n",
        "    \n",
        "    def _make_val_step_fn(self):\n",
        "        # Builds function that performs a step in the validation loop\n",
        "        def perform_val_step_fn(x, y):\n",
        "            # Sets model to EVAL mode\n",
        "            self.model.eval()\n",
        "\n",
        "            # Step 1 - Computes our model's predicted output - forward pass\n",
        "            yhat = self.model(x)\n",
        "            # Step 2 - Computes the loss\n",
        "            loss = self.loss_fn(yhat, y)\n",
        "            # There is no need to compute Steps 3 and 4, \n",
        "            # since we don't update parameters during evaluation\n",
        "            return loss.item()\n",
        "\n",
        "        return perform_val_step_fn\n",
        "            \n",
        "    def _mini_batch(self, validation=False):\n",
        "        # The mini-batch can be used with both loaders\n",
        "        # The argument `validation`defines which loader and \n",
        "        # corresponding step function is going to be used\n",
        "        if validation:\n",
        "            data_loader = self.val_loader\n",
        "            step_fn = self.val_step_fn\n",
        "        else:\n",
        "            data_loader = self.train_loader\n",
        "            step_fn = self.train_step_fn\n",
        "\n",
        "        if data_loader is None:\n",
        "            return None\n",
        "            \n",
        "        # Once the data loader and step function, this is the \n",
        "        # same mini-batch loop we had before\n",
        "        mini_batch_losses = []\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch = x_batch.to(self.device)\n",
        "            y_batch = y_batch.to(self.device)\n",
        "\n",
        "            mini_batch_loss = step_fn(x_batch, y_batch)\n",
        "            mini_batch_losses.append(mini_batch_loss)\n",
        "\n",
        "        loss = np.mean(mini_batch_losses)\n",
        "        return loss\n",
        "\n",
        "    def set_seed(self, seed=42):\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False    \n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "    \n",
        "    def train(self, n_epochs, seed=42):\n",
        "        # To ensure reproducibility of the training process\n",
        "        self.set_seed(seed)\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            # Keeps track of the numbers of epochs\n",
        "            # by updating the corresponding attribute\n",
        "            self.total_epochs += 1\n",
        "\n",
        "            # inner loop\n",
        "            # Performs training using mini-batches\n",
        "            loss = self._mini_batch(validation=False)\n",
        "            self.losses.append(loss)\n",
        "\n",
        "            # VALIDATION\n",
        "            # no gradients in validation!\n",
        "            with torch.no_grad():\n",
        "                # Performs evaluation using mini-batches\n",
        "                val_loss = self._mini_batch(validation=True)\n",
        "                self.val_losses.append(val_loss)\n",
        "\n",
        "            # If a SummaryWriter has been set...\n",
        "            if self.writer:\n",
        "                scalars = {'training': loss}\n",
        "                if val_loss is not None:\n",
        "                    scalars.update({'validation': val_loss})\n",
        "                # Records both losses for each epoch under the main tag \"loss\"\n",
        "                self.writer.add_scalars(main_tag='loss',\n",
        "                                        tag_scalar_dict=scalars,\n",
        "                                        global_step=epoch)\n",
        "\n",
        "        if self.writer:\n",
        "            # Closes the writer\n",
        "            self.writer.close()\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Set is to evaluation mode for predictions\n",
        "        self.model.eval() \n",
        "        # Takes aNumpy input and make it a float tensor\n",
        "        x_tensor = torch.as_tensor(x).float()\n",
        "        # Send input to device and uses model for prediction\n",
        "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
        "        # Set it back to train mode\n",
        "        self.model.train()\n",
        "        # Detaches it, brings it to CPU and back to Numpy\n",
        "        return y_hat_tensor.detach().cpu().numpy()\n",
        "\n",
        "    def plot_losses(self):\n",
        "        fig = plt.figure(figsize=(10, 4))\n",
        "        plt.plot(self.losses, label='Training Loss', c='b')\n",
        "        plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# configure model\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "lr = 0.1\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model_1 = nn.Sequential()\n",
        "model_1.add_module('linear', nn.Linear(2, 1))\n",
        "\n",
        "# Defines a SGD optimizer to update the parameters\n",
        "optimizer_1 = optim.SGD(model_1.parameters(), lr=lr)\n",
        "\n",
        "# Defines a BCE loss function\n",
        "loss_fn_1 = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_epochs = 100\n",
        "\n",
        "dlp_1 = DeepLearningPipeline(model_1, loss_fn_1, optimizer_1)\n",
        "dlp_1.set_loaders(train_loader, val_loader)\n",
        "dlp_1.train(n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = dlp_1.plot_losses()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model_1.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "logits_val = dlp_1.predict(X_val)\n",
        "logits_val_tensor = torch.from_numpy(logits_val)\n",
        "probabilities_val = torch.sigmoid(logits_val_tensor).squeeze()\n",
        "cm_thresh50 = confusion_matrix(y_val, (probabilities_val >= 0.5))\n",
        "cm_thresh50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_2 = nn.Sequential()\n",
        "model_2.add_module('linear', nn.Linear(2, 1))\n",
        "model_2.add_module('sigmoid', nn.Sigmoid())\n",
        "\n",
        "# Defines a SGD optimizer to update the parameters\n",
        "optimizer_2 = optim.SGD(model_2.parameters(), lr=lr)\n",
        "\n",
        "# Defines a BCE loss function\n",
        "loss_fn_2 = nn.BCELoss(reduction='mean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_epochs = 100\n",
        "\n",
        "dlp_2 = DeepLearningPipeline(model_2, loss_fn_2, optimizer_2)\n",
        "dlp_2.set_loaders(train_loader, val_loader)\n",
        "dlp_2.train(n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = dlp_2.plot_losses()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model_2.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "probabilities_val = dlp_2.predict(X_val).squeeze()\n",
        "cm_thresh50 = confusion_matrix(y_val, (probabilities_val >= 0.5))\n",
        "cm_thresh50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# comparison with sklearn logistic regression\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "probabilities_val = logreg.predict(X_val)\n",
        "cm_thresh50 = confusion_matrix(y_val, (probabilities_val >= 0.5))\n",
        "cm_thresh50"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "instance_type": "ml.t3.medium",
    "kernelspec": {
      "display_name": "Python 3.8.8 ('myblog')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "bbac80ad2bfd54975e0c0f7ddf300156d5b24b5126f35dc262fa887f22fb28f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
