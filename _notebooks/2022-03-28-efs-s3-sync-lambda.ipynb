{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS EFS Sync to S3 Using Lambda\n",
    "> A tutorial to synchronize EFS with S3 bucket using a Lambda function.\n",
    "\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [aws, lambda, efs, s3, synchonization]\n",
    "- keywords: [aws, lambda, s3, efs, sync, python]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "This post is to document all the steps required to synchronize [AWS EFS](https://aws.amazon.com/efs/) with an [S3 bucket](https://aws.amazon.com/s3/) using a [lambda function](https://aws.amazon.com/lambda/). The flow of information is from S3 to EFS and not vice versa.\n",
    "\n",
    "The approach is whenever a new file is uploaded or deleted from the S3 bucket, it will create an event notification. This event will trigger a lambda function. This lambda function will have the efs file system mounted to it. Lambda function synchronizes the files from S3 to EFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python = 3.9.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an S3 bucket\n",
    "Let's first create an S3 bucket that will contain our data, and this is the bucket we would like to be in sync with EFS. I am naming the bucket `mydata-202203`. You may name it as you please. Choose a region of your choice and leave the rest of the settings as defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Lambda function\n",
    "\n",
    "Now create a lambda function that will receive event notifications from the S3 bucket, and sync files on efs. I am naming it `mydata-sync` and our runtime will be `Python 3.9`. Keep the rest of the settings as default, and create the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create S3 event notifications\n",
    "From the bucket, `mydata-sync` go to `Properties`. Scroll down to `Event notifications` and click **create**. Give any name to the event. I am calling it `object-sync`. From the provided event types select\n",
    "* s3:ObjectCreated:Put\n",
    "* s3:ObjectRemoved:Delete\n",
    "\n",
    "From the section **Destination** select `Lambda Function`, and from the list choose the lambda function name we created in the last section **mydata-sync** \n",
    "\n",
    "Click **Save Changes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test S3 notifications\n",
    "Let's now test if S3 event notifications are being received by our lambda function. For this update lambda function code and simply print the event received. After updating the lambda function, make sure to deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    print(event)\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('Hello from Lambda!')\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now upload some files in our S3 bucket, and it should trigger our lambda function. For testing, I have uploaded an empty `test1.txt` file in our bucket. Once successfully uploaded I check the Lambda function logs to see if any event is received. For this go to lambda function **mydata-sync** > Monitor > Logs > **View logs in CloudWatch**. For the CloudWatch console view the latest log stream. Below is the event I have received in the logs\n",
    "\n",
    "```\n",
    "{'Records': [{'eventVersion': '2.1', 'eventSource': 'aws:s3', 'awsRegion': 'us-east-1', 'eventTime': '2022-03-28T16:08:00.896Z', 'eventName': 'ObjectCreated:Put', 'userIdentity': {'principalId': 'AWS:AIDA3VIXXJNKIVU6P5NY3'}, 'requestParameters': {'sourceIPAddress': '202.163.113.76'}, 'responseElements': {'x-amz-request-id': '39MD61ZS00SNK2RT', 'x-amz-id-2': 'U+zPUWOrfzTuVi7kbaBONLHoJXKqUICsVqyKBg4yPKYbUV7pQLGc4Z5A2fSIVvDFtSJHC6v29EDJoXhypWsj2wXanUu8YrLocr3z+yK1qoo='}, 's3': {'s3SchemaVersion': '1.0', 'configurationId': 'object-sync', 'bucket': {'name': 'mydata-202203', 'ownerIdentity': {'principalId': 'AYAQOSFZ1VPK'}, 'arn': 'arn:aws:s3:::mydata-202203'}, 'object': {'key': 'test1.txt', 'size': 0, 'eTag': 'd41d8cd98f00b204e9800998ecf8427e', 'sequencer': '006241DD60D67A4556'}}}]}\n",
    "```\n",
    "let's load this event in a dictionary and find some important parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Records': [{'eventVersion': '2.1',\n",
       "   'eventSource': 'aws:s3',\n",
       "   'awsRegion': 'us-east-1',\n",
       "   'eventTime': '2022-03-28T16:08:00.896Z',\n",
       "   'eventName': 'ObjectCreated:Put',\n",
       "   'userIdentity': {'principalId': 'AWS:AIDA3VIXXJNKIVU6P5NY3'},\n",
       "   'requestParameters': {'sourceIPAddress': '202.163.113.76'},\n",
       "   'responseElements': {'x-amz-request-id': '39MD61ZS00SNK2RT',\n",
       "    'x-amz-id-2': 'U+zPUWOrfzTuVi7kbaBONLHoJXKqUICsVqyKBg4yPKYbUV7pQLGc4Z5A2fSIVvDFtSJHC6v29EDJoXhypWsj2wXanUu8YrLocr3z+yK1qoo='},\n",
       "   's3': {'s3SchemaVersion': '1.0',\n",
       "    'configurationId': 'object-sync',\n",
       "    'bucket': {'name': 'mydata-202203',\n",
       "     'ownerIdentity': {'principalId': 'AYAQOSFZ1VPK'},\n",
       "     'arn': 'arn:aws:s3:::mydata-202203'},\n",
       "    'object': {'key': 'test1.txt',\n",
       "     'size': 0,\n",
       "     'eTag': 'd41d8cd98f00b204e9800998ecf8427e',\n",
       "     'sequencer': '006241DD60D67A4556'}}}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event = {'Records': [{'eventVersion': '2.1', 'eventSource': 'aws:s3', 'awsRegion': 'us-east-1', 'eventTime': '2022-03-28T16:08:00.896Z', 'eventName': 'ObjectCreated:Put', 'userIdentity': {'principalId': 'AWS:AIDA3VIXXJNKIVU6P5NY3'}, 'requestParameters': {'sourceIPAddress': '202.163.113.76'}, 'responseElements': {'x-amz-request-id': '39MD61ZS00SNK2RT', 'x-amz-id-2': 'U+zPUWOrfzTuVi7kbaBONLHoJXKqUICsVqyKBg4yPKYbUV7pQLGc4Z5A2fSIVvDFtSJHC6v29EDJoXhypWsj2wXanUu8YrLocr3z+yK1qoo='}, 's3': {'s3SchemaVersion': '1.0', 'configurationId': 'object-sync', 'bucket': {'name': 'mydata-202203', 'ownerIdentity': {'principalId': 'AYAQOSFZ1VPK'}, 'arn': 'arn:aws:s3:::mydata-202203'}, 'object': {'key': 'test1.txt', 'size': 0, 'eTag': 'd41d8cd98f00b204e9800998ecf8427e', 'sequencer': '006241DD60D67A4556'}}}]}\n",
    "event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ObjectCreated:Put'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "# event name\n",
    "event['Records'][0]['eventName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mydata-202203'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "# bucket name\n",
    "event['Records'][0]['s3']['bucket']['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test1.txt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "# uploaded object key\n",
    "event['Records'][0]['s3']['object']['key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we have seen that we are receiving notifications from S3 bucket so let's now move on to the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an EFS\n",
    "\n",
    "From EFS console give name as `mydata-efs`. I am using default VPC for this post. Use `Regional` availability settings. Click **Create**. Once file system is created, click on **Access points** and create an access point for this efs to be mounted in other service. For access point use following settings\n",
    "* name = mydata-ap\n",
    "* root dir path = /efs\n",
    "* POSIX User\n",
    "  * POSIX UID = 1000\n",
    "  * Group ID = 1000\n",
    "* Root directory creation permissions\n",
    "  * Owner user id = 1000\n",
    "  * Owner group id = 1000\n",
    "  * POSIX permissions = 777\n",
    "\n",
    "Click **Create**.\n",
    "\n",
    "Here I have used the root dir path as `/efs` this means that from this access point my access will be limited to folder `/efs`. If you want to provide full access to all folders then set to root path to `/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on EFS security group settings\n",
    "\n",
    "In the last section, I have used a default VPC security group (sg) while creating EFS. default sg allows traffic for all protocols and all ports, both for inbound and outbound traffic. But if you are using a custom security group then make sure that you have an inbound rule for \n",
    "* Type = NFS\n",
    "* Protocol = TCP\n",
    "* Port range = 2049\n",
    "\n",
    "Otherwise, you will not be able to access EFS using NFS clients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount EFS to Lambda Function\n",
    "\n",
    "To mount an EFS to the Lambda function requires some additional steps. \n",
    "\n",
    "First add permissions to Lambda function. From lambda function > Configurations > Permissions > **Execution role**. Click on the execution role to open it in IAM concole. For the selected role attach an additional policy `AmazonElasticFileSystemFullAccess`. \n",
    "\n",
    "Second, add the lambda to a VPC group in which efs was created. We have created efs in default VPC so let's add lambda to it. For this from lambda Configurations > **VPC** click edit. For the next pane select default VPC, all subnets, default VPC security group, and click **save**.\n",
    "\n",
    "Now we can add EFS to lambda. Go to lambda Configurations > File Systems > **Add file system**. Select the file system **mydata-efs** and associated access point **mydata-ap** and local mount point as `/mnt/efs`. The local mount point is the mounted directory from where we can access our EFS from inside the lambda environment. Click **Save**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check EFS mount point from Lambda\n",
    "\n",
    "Let's verify from lambda that EFS has been mounted and can we access it. So update the lambda code as below and deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    mount_path = '/mnt/efs'\n",
    "    if os.path.exists(mount_path):\n",
    "        print(f\"{mount_path} exists\")\n",
    "        print(os.listdir('/mnt/efs'))\n",
    "    \n",
    "    print(event)\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('Hello from Lambda!')\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test this code using a test event `S3 Put`. For this go to lambda Test > Create new event > Template (**s3-put**). 'S3 Put' test event is similar to the one we saw in the last section. We can use this request template to simulate the event received from S3 bucket. Once the test is successfully executed, check the log output.\n",
    "\n",
    "```\n",
    "START RequestId: 2e307a14-f373-46d5-b763-594d5f406ae6 Version: $LATEST\n",
    "/mnt/efs exists\n",
    "[]\n",
    "{'Records': [{'eventVersion': '2.0', 'eventSource': 'aws:s3', 'awsRegion': 'us-east-1', 'eventTime': '1970-01-01T00:00:00.000Z', 'eventName': 'ObjectCreated:Put', 'userIdentity': {'principalId': 'EXAMPLE'}, 'requestParameters': {'sourceIPAddress': '127.0.0.1'}, 'responseElements': {'x-amz-request-id': 'EXAMPLE123456789', 'x-amz-id-2': 'EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH'}, 's3': {'s3SchemaVersion': '1.0', 'configurationId': 'testConfigRule', 'bucket': {'name': 'example-bucket', 'ownerIdentity': {'principalId': 'EXAMPLE'}, 'arn': 'arn:aws:s3:::example-bucket'}, 'object': {'key': 'test%2Fkey', 'size': 1024, 'eTag': '0123456789abcdef0123456789abcdef', 'sequencer': '0A1B2C3D4E5F678901'}}}]}\n",
    "END RequestId: 2e307a14-f373-46d5-b763-594d5f406ae6\n",
    "REPORT RequestId: 2e307a14-f373-46d5-b763-594d5f406ae6\tDuration: 7.02 ms\tBilled Duration: 8 ms\tMemory Size: 128 MB\tMax Memory Used: 37 MB\tInit Duration: 93.81 ms\t\n",
    "```\n",
    "\n",
    "From the logs we can see that the mounted EFS directory exists `/mnt/efs` but currently the folder is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure VPC endpoint for S3\n",
    "Till now we have configured S3 notifications to trigger a lambda function and also mounted EFS to it. Our next step is to process the event received in lambda, and download the file from S3 to EFS. But since our lambda function is configured for a VPC we cannot connect to S3 from it. Even though we can still receive S3 event notification, when we try to connect to S3 to download any file we will get a timeout error. To fix this we will create a VPC endpoint for S3 bucket.\n",
    "\n",
    "For this go to VPC console > Endpoints > **Create endpoint**, and set the following\n",
    "* name = mydata-ep\n",
    "* service category = aws services\n",
    "* services = com.amazonaws.us-east-1.s3 (Gateway)\n",
    "* vpc = default\n",
    "* route table = default (main route table)\n",
    "* policy = full access\n",
    "\n",
    "Click **Create endpoint**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure S3 permissions for Lambda\n",
    "\n",
    "For lambda to be able to connect to S3 we also need to give it proper permissions. For this go to Lambda > Configurations > Permissions > Execution Role > **click on role name**. From the IAM Role console select add permissions, and then select `AmazonS3FullAccess`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process S3 event notifications\n",
    "Our lambda and EFS are ready and we can now process S3 events. Update the lambda code as below to process S3 events. It will download and delete from EFS to keep it in sync with S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    event_name = event[\"Records\"][0][\"eventName\"]\n",
    "    bucket_name = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]\n",
    "    object_key = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]\n",
    "\n",
    "    efs_file_name = \"/mnt/efs/\" + object_key\n",
    "\n",
    "    # S3 put\n",
    "    if event_name == \"ObjectCreated:Put\":\n",
    "        s3.download_file(bucket_name, object_key, efs_file_name)\n",
    "        print(f\"file downloaded: {efs_file_name}\")\n",
    "\n",
    "    # S3 delete\n",
    "    if event_name == \"ObjectRemoved:Delete\":\n",
    "        # check if file exists on efs\n",
    "        if os.path.exists(efs_file_name):\n",
    "            os.remove(efs_file_name)\n",
    "            print(f\"file deleted: {efs_file_name}\")\n",
    "\n",
    "    return {\"statusCode\": 200, \"body\": json.dumps(event)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this code using the S3-put test event we used last time. Modify the event for bucket name and object key as below.\n",
    "```json\n",
    "{\n",
    "  \"Records\": [\n",
    "    {\n",
    "      \"eventVersion\": \"2.0\",\n",
    "      \"eventSource\": \"aws:s3\",\n",
    "      \"awsRegion\": \"us-east-1\",\n",
    "      \"eventTime\": \"1970-01-01T00:00:00.000Z\",\n",
    "      \"eventName\": \"ObjectCreated:Put\",\n",
    "      \"userIdentity\": {\n",
    "        \"principalId\": \"EXAMPLE\"\n",
    "      },\n",
    "      \"requestParameters\": {\n",
    "        \"sourceIPAddress\": \"127.0.0.1\"\n",
    "      },\n",
    "      \"responseElements\": {\n",
    "        \"x-amz-request-id\": \"EXAMPLE123456789\",\n",
    "        \"x-amz-id-2\": \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH\"\n",
    "      },\n",
    "      \"s3\": {\n",
    "        \"s3SchemaVersion\": \"1.0\",\n",
    "        \"configurationId\": \"testConfigRule\",\n",
    "        \"bucket\": {\n",
    "          \"name\": \"mydata-202203\",\n",
    "          \"ownerIdentity\": {\n",
    "            \"principalId\": \"EXAMPLE\"\n",
    "          },\n",
    "          \"arn\": \"arn:aws:s3:::example-bucket\"\n",
    "        },\n",
    "        \"object\": {\n",
    "          \"key\": \"test1.txt\",\n",
    "          \"size\": 1024,\n",
    "          \"eTag\": \"0123456789abcdef0123456789abcdef\",\n",
    "          \"sequencer\": \"0A1B2C3D4E5F678901\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Click **test**. From the output logs, we can see that our code was able to download the file from S3 bucket and write it on EFS.\n",
    "\n",
    "```\n",
    "START RequestId: 7e9c0dc2-f970-426e-8372-e59b07f5536c Version: $LATEST\n",
    "file downloaded: /mnt/efs/test1.txt\n",
    "END RequestId: 7e9c0dc2-f970-426e-8372-e59b07f5536c\n",
    "REPORT RequestId: 7e9c0dc2-f970-426e-8372-e59b07f5536c  Duration: 370.00 ms Billed Duration: 371 ms Memory Size: 128 MB Max Memory Used: 72 MB  Init Duration: 367.68 ms\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you get any permission errors then it could be due to the mounting path errors. Please do check the access point path and lambda mount path. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify file on EFS\n",
    "\n",
    "We can verify files on EFS by directly mounting them to an EC2 machine and verifying from there. So let's do that. \n",
    "\n",
    "Create an EC2 machine\n",
    "* AMI = Amazon Linux 2 AMI (HVM) - Kernel 5.10, SSD Volume Type\n",
    "* Intance type = t2.micro (free tier)\n",
    "* Instance details\n",
    "  * Network = default VPC\n",
    "  * Auto-assign Public IP = Enable\n",
    "* Review and Lanunch > Launch > Proceed without key pair.\n",
    "\n",
    "Once the instance is up and running, click on it and connect using EC2 instance connect option. Create a dir 'efs' using the command\n",
    "```\n",
    "mkdir efs\n",
    "```\n",
    "In a separate tab open EFS, and click on the file system we have created. Click **Attach**. From \"Mount via DNS\" copy command for NFS client. paste that in EC2 bash terminal\n",
    "```\n",
    "sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-0c9526e2f48ece247.efs.us-east-1.amazonaws.com:/ efs\n",
    "```\n",
    "Once successfully mounted verify that the file 'test1.txt' exists in EFS. We can also delete the file from S3 and similarly verify from EFS that the file has been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "A summary of all the steps\n",
    "* Create an S3 bucket\n",
    "* Create a Lambda function\n",
    "* Create event notifications on the S3 bucket to trigger the lambda function\n",
    "* Create an EFS file system and its access point. Check the security group setting for inbound rules for NFS traffic\n",
    "* Add EFS and S3 permissions to lambda\n",
    "* Add lambda to VPC\n",
    "* Create VPC endpoint for S3 bucket\n",
    "* Update lambda code to process event notifications\n",
    "* Use EC2 to mount EFS and verify the files"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "592d70e68660334c1e2b3db2d131a28b3e4c5aef0ad9ff036ec5b5af6e985aa1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('sc_mlflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
