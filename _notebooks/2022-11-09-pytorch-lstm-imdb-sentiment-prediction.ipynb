{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auS4LXzau0x1"
      },
      "source": [
        "# Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch\n",
        "> This is a practice notebook.\n",
        "\n",
        "- toc: true \n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [pytorch, lstm]\n",
        "- keyword: [ml, dl, nn, pytorch, LSTM, IMDB, sentiment]\n",
        "- image: images/copied_from_nb/images/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.jpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2O8j6ezu0x9"
      },
      "source": [
        "![](images/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsza7iAIu0x_"
      },
      "source": [
        "## Credits\n",
        "This notebook takes inspiration and ideas from the following sources.\n",
        "* The outstanding book \"Deep Learning with PyTorch Step-by-Step\" by \"Daniel Voigt Godoy\". You can get the book from its website: [pytorchstepbystep](https://pytorchstepbystep.com/). In addition, the GitHub repository for this book has valuable notebooks: [github.com/dvgodoy/PyTorchStepByStep](https://github.com/dvgodoy/PyTorchStepByStep). Parts of the code you see in this notebook are taken from [chapter 3](https://colab.research.google.com/github/dvgodoy/PyTorchStepByStep/blob/master/Chapter03.ipynb) and [chapter 8](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter08.ipynb) notebooks of the same book.\n",
        "* Very helpful Kaggle notebook from 'TARON ZAKARYAN' to predict stock prices using LSTM. [Link here](https://www.kaggle.com/code/taronzakaryan/predicting-stock-price-using-lstm-model-pytorch/notebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtohdezgu0x_"
      },
      "source": [
        "## Environment\n",
        "This notebook is prepared with Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9kZUBv9DobO",
        "outputId": "9398dac5-fcad-4e91-a8ba-28a7d71831e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python==3.7.15\n",
            "numpy==1.21.6\n",
            "torch==1.12.1+cu113\n",
            "matplotlib==3.2.2\n",
            "seaborn==0.11.2\n"
          ]
        }
      ],
      "source": [
        "#collapse\n",
        "from platform import python_version\n",
        "import numpy, matplotlib, pandas, torch, seaborn\n",
        "\n",
        "print(\"python==\" + python_version())\n",
        "print(\"numpy==\" + numpy.__version__)\n",
        "print(\"torch==\" + torch.__version__)\n",
        "print(\"matplotlib==\" + matplotlib.__version__)\n",
        "print(\"seaborn==\" + seaborn.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J4Ci40Wu0x-"
      },
      "source": [
        "## Introduction\n",
        "Recurrent Neural Network (RNN) is great for exploiting data that involves one-dimensional (1D) ordered structures. We call these 1D-ordered structures `sequences`. Two main sequence problems are `Time series` and `Natural Language Processing (NLP)`. RNN and its variants are developed to work for both types of sequence problems, but in this notebook we will only deal with time series sequences.\n",
        "\n",
        "I have divided this notebook into two sections. In the first section, our focus will be on understanding the structure of sequences and generating training sets and batches from them. We will develop a simple (synthetic) sequence data and then create its training set. Next, we will make batches using PyTorch DataLoaders and write a training pipeline. We will end this section by training an RNN on this data. \n",
        "\n",
        "In the next section, our focus will be more on the internals of different neural architectures for sequence data problems. We will use stock price data and train multiple networks (RNN, GRU, LSTM, CNN) on it while understanding their features and behavior."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "instance_type": "ml.t3.medium",
    "kernelspec": {
      "display_name": "Python 3.8.8 ('myblog')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "bbac80ad2bfd54975e0c0f7ddf300156d5b24b5126f35dc262fa887f22fb28f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
