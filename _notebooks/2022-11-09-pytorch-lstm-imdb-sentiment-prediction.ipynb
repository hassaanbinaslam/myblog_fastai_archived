{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auS4LXzau0x1"
      },
      "source": [
        "# Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch\n",
        "> This is a practice notebook to work with a dataset of 50,000 movie reviews from the Internet Movie Database (IMDB) and build an LSTM predictor to distinguish between positive and negative reviews.\n",
        "\n",
        "- toc: true \n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [pytorch, lstm]\n",
        "- keyword: [ml, dl, nn, pytorch, LSTM, IMDB, sentiment]\n",
        "- image: images/copied_from_nb/images/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.jpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2O8j6ezu0x9"
      },
      "source": [
        "![](images/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsza7iAIu0x_"
      },
      "source": [
        "## Credits\n",
        "This notebook takes inspiration and ideas from the following sources.\n",
        "\n",
        "* \"Machine learning with PyTorch and Scikit-Learn\" by \"Sebastian Raschka, Yuxi (Hayden) Liu, and Vahid Mirjalili\". You can get the book from its website: [Machine learning with PyTorch and Scikit-Learn](https://sebastianraschka.com/books/#machine-learning-with-pytorch-and-scikit-learn). In addition, the GitHub repository for this book has valuable notebooks: [github.com/rasbt/machine-learning-book](https://github.com/rasbt/machine-learning-book). Parts of the code you see in this notebook are taken from [chapter 15](https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part2.ipynb) notebook of the same book.\n",
        "* \"Intro to Deep Learning and Generative Models Course\" lecture series from \"Sebastian Raschka\". Course website: [stat453-ss2021](https://sebastianraschka.com/teaching/stat453-ss2021/). YouTube Link: [Intro to Deep Learning and Generative Models Course](https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51). Lectures that are related to this post are [L15.5 Long Short-Term Memory](https://youtu.be/k6fSgUaWUF8) and [L15.7 An RNN Sentiment Classifier in PyTorch](https://youtu.be/KgrdifrlDxg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtohdezgu0x_"
      },
      "source": [
        "## Environment\n",
        "This notebook is prepared with Google Colab. For \"runtime type\" choose hardware accelerator as \"GPU\". It will take a long time to complete the training without any GPU. \n",
        "\n",
        "This notebook also depends on the PyTorch library [TorchText](https://pytorch.org/text/stable/index.html). We will use this library to fetch IMDB review data. While using the `torchtext` latest version, I found more dependencies on other libraries like `torchdata`. Even after resolving them, it threw strange encoding errors while fetching IMDB data. So I have downgraded this library till the version I found working without external dependencies. Consequently, `torch` is also downgraded to a compatible version, but I did not find any issue while working with a lower version of PyTorch for this notebook. It is preferred to restart the runtime after the library installation is complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c23jvqQYYpr0",
        "outputId": "6a37f823-61e1-44cb-a21f-9f0aef76f1b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext==0.11.0 in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchtext==0.11.0) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "#collapse-output\n",
        "! pip install torchtext==0.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9kZUBv9DobO",
        "outputId": "85394c24-663a-427b-e8c9-d2084d2f9735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python==3.7.15\n",
            "numpy==1.21.6\n",
            "torch==1.10.0+cu102\n",
            "torchtext==0.11.0\n",
            "matplotlib==3.2.2\n"
          ]
        }
      ],
      "source": [
        "#collapse\n",
        "from platform import python_version\n",
        "import numpy, matplotlib, pandas, torch, torchtext\n",
        "\n",
        "print(\"python==\" + python_version())\n",
        "print(\"numpy==\" + numpy.__version__)\n",
        "print(\"torch==\" + torch.__version__)\n",
        "print(\"torchtext==\" + torchtext.__version__)\n",
        "print(\"matplotlib==\" + matplotlib.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBWOSLktKMoD"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "### Download data\n",
        "Let's download our movie review dataset. This dataset is also known as [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/), and can also be obtained in a compressed zip file from [this link](https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz). Using the `torchtext` library makes downloading, extracting, and reading files a lot easier. 'torchtext.datasets' comes with many more NLP related datasets, and a full list can be [found here](https://pytorch.org/text/stable/datasets.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ib_HGLXwuxIa"
      },
      "outputs": [],
      "source": [
        "from torchtext.datasets import IMDB\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "train_dataset_raw = IMDB(split=\"train\")\n",
        "test_dataset_raw = IMDB(split=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hkKuykqX-zB"
      },
      "source": [
        "Check the size of the downloaded data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuEsWZ4QwPop",
        "outputId": "15d68147-95cf-458c-a25c-b9aeec98e462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size:  25000\n",
            "Test dataset size:  25000\n"
          ]
        }
      ],
      "source": [
        "print(\"Train dataset size: \", len(train_dataset_raw))\n",
        "print(\"Test dataset size: \", len(test_dataset_raw))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs1BlO8aYH2L"
      },
      "source": [
        "### Split train data further into train and validation set\n",
        "\n",
        "Both train and test datasets have 25000 reviews. Therefore, we can split the training set further into the train and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K7L2Shr4wJqw"
      },
      "outputs": [],
      "source": [
        "train_set_size = 20000\n",
        "valid_set_size = 5000\n",
        "\n",
        "train_dataset, valid_dataset = random_split(list(train_dataset_raw), [20000, 5000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6m5omgfaQQl"
      },
      "source": [
        "### How does this data look?\n",
        "The data we have is in the form of tuples. The first index has the sentiment label, and the second contains the review text. Let's check the first element in our training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofjOhYWIJlYm",
        "outputId": "adb1305a-df0c-46a8-9af4-972b3c5277cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('pos',\n",
              " 'An extra is called upon to play a general in a movie about the Russian Revolution. However, he is not any ordinary extra. He is Serguis Alexander, former commanding general of the Russia armies who is now being forced to relive the same scene, which he suffered professional and personal tragedy in, to satisfy the director who was once a revolutionist in Russia and was humiliated by Alexander. It can now be the time for this broken man to finally \"win\" his penultimate battle. This is one powerful movie with meticulous direction by Von Sternberg, providing the greatest irony in Alexander\\'s character in every way he can. Jannings deserved his Oscar for the role with a very moving performance playing the general at his peak and at his deepest valley. Powell lends a sinister support as the revenge minded director and Brent is perfect in her role with her face and movements showing so much expression as Jannings\\' love. All around brilliance. Rating, 10.')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBLVo-41eZmz"
      },
      "source": [
        "Check the first index of the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSHczkBWJqJI",
        "outputId": "96ea82d5-8662-4285-d7e3-9f516b181f27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('neg',\n",
              " 'The Dereks did seem to struggle to find rolls for Bo after \"10\".<br /><br />I used to work for a marine park in the Florida Keys. One day, the script for \"Ghosts Can\\'t Do It\" was circulating among the trainers in the \"fish house\" where food was prepared for the dolphins. There was one scene where a -dolphin- supposedly propositions Bo (or Bo the dolphin), asking to \"go make eggs.\" Reading the script, we -lauuughed-...<br /><br />We did not end up doing any portion of this movie at our facility, although our dolphins -were- in \"The Big Blue!\"<br /><br />This must have been very close to the end of Anthony Quinn\\'s life. I hope he had fun in this film, as it certainly didn\\'t do anything for his legacy.')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIv9ObZSe2wd"
      },
      "source": [
        "### Data preprocessing steps\n",
        "From these two reviews, we can deduce that\n",
        "* We have two labels. 'pos' for a positive and 'neg' for a negative review\n",
        "* From the second review (from valid_dataset), we also get that text may contain HTML tags, special characters, and emoticons besides normal English words. It will require some preprocessing to remove them for proper word tokenization.\n",
        "* Reviews can have varying text lengths. It will require some padding to make all review texts the same size.\n",
        "\n",
        "Let's take a simple text example and apply these steps to understand why these steps are essential in preprocessing. In the last step, we will create tokens from the preprocessed text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BISxm5lvCY7i",
        "outputId": "740babb1-00b7-4066-caa2-5cc5fe47053e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"This is awesome movie <br /><br />. I loved it so much :-) I'm goona watch it again :)\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_text = '''This is awesome movie <br /><br />. I loved it so much :-) I\\'m goona watch it again :)'''\n",
        "example_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_yIEFVrGl2Jh",
        "outputId": "4e5cedfc-ba91-4b55-8dcb-235861b72be5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"This is awesome movie . I loved it so much :-) I'm goona watch it again :)\""
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##\n",
        "# step 1. remove HTML tags. they are not helpful in understanding the sentiments of a review\n",
        "import re\n",
        "\n",
        "text = re.sub('<[^>]*>', '', example_text)\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yV5KofsfmBLv",
        "outputId": "27e40280-07b0-431f-e150-1b9c2ea0cf82"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"this is awesome movie . i loved it so much :-) i'm goona watch it again :)\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##\n",
        "# step 2: use lowercase for all text to keep symmetry\n",
        "text = text.lower()\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MT9-9VJmcxg",
        "outputId": "33a74799-341c-4d4b-a28a-4dc230e02e4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[':-)', ':)']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##\n",
        "# step 3: extract emoticons. keep them as they are important sentiment signals\n",
        "emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "emoticons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_GAqhLBinl7a",
        "outputId": "31ee748e-ac8c-4681-ae89-28360309f786"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'this is awesome movie i loved it so much i m goona watch it again '"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##\n",
        "# step 4: remove punctuation marks\n",
        "text = re.sub('[\\W]+', ' ', text)\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tDrEVvQNm9K9",
        "outputId": "d67ff36b-d664-4e14-dea4-9a18b637e6d7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'this is awesome movie i loved it so much i m goona watch it again :) :)'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##\n",
        "# step 5: put back emoticons\n",
        "text = text + ' '.join(emoticons).replace('-', '')\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPu_IsM6nYHm",
        "outputId": "aba0338c-a7a8-44a8-fc64-7f9b4210e8be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['this',\n",
              " 'is',\n",
              " 'awesome',\n",
              " 'movie',\n",
              " 'i',\n",
              " 'loved',\n",
              " 'it',\n",
              " 'so',\n",
              " 'much',\n",
              " 'i',\n",
              " 'm',\n",
              " 'goona',\n",
              " 'watch',\n",
              " 'it',\n",
              " 'again',\n",
              " ':)',\n",
              " ':)']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##\n",
        "# step 6: generate word tokens\n",
        "text = text.split()\n",
        "text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmOoau_hp42q"
      },
      "source": [
        "Let's put all the preprocessing steps in a nice function and give it a name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-mX3i1Z1orIr"
      },
      "outputs": [],
      "source": [
        "def tokenizer(text):\n",
        "    # step 1. remove HTML tags. they are not helpful in understanding the sentiments of a review\n",
        "    # step 2: use lowercase for all text to keep symmetry\n",
        "    # step 3: extract emoticons. keep them as they are important sentiment signals\n",
        "    # step 4: remove punctuation marks\n",
        "    # step 5: put back emoticons\n",
        "    # step 6: generate word tokens\n",
        "    text = re.sub(\"<[^>]*>\", \"\", text)\n",
        "    text = text.lower()\n",
        "    emoticons = re.findall(\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text)\n",
        "    text = re.sub(\"[\\W]+\", \" \", text)\n",
        "    text = text + \" \".join(emoticons).replace(\"-\", \"\")\n",
        "    tokenized = text.split()\n",
        "    return tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_nKDW-Wq5BY"
      },
      "source": [
        "Apply `tokenizer` on the `example_text` to verify the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpBTxDAUp3kn",
        "outputId": "c57eb03d-680b-4c55-d3a1-dedfd4eb9ce8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['this',\n",
              " 'is',\n",
              " 'awesome',\n",
              " 'movie',\n",
              " 'i',\n",
              " 'loved',\n",
              " 'it',\n",
              " 'so',\n",
              " 'much',\n",
              " 'i',\n",
              " 'm',\n",
              " 'goona',\n",
              " 'watch',\n",
              " 'it',\n",
              " 'again',\n",
              " ':)',\n",
              " ':)']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_tokens = tokenizer(example_text)\n",
        "example_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l82qer6lsLWc"
      },
      "source": [
        "### Preparing data dictionary\n",
        "We are successful in creating word tokens from our `example_text`. But there is one more problem. Some of the tokens are repeating. If we can convert these tokens into a dictionary along with their frequency count, we can significantly reduce the generated token size from these reviews. Let's do that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-xGGlgwuwO2",
        "outputId": "aec65607-3078-4f6f-8139-5ff9c2951097"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'this': 1,\n",
              "         'is': 1,\n",
              "         'awesome': 1,\n",
              "         'movie': 1,\n",
              "         'i': 2,\n",
              "         'loved': 1,\n",
              "         'it': 2,\n",
              "         'so': 1,\n",
              "         'much': 1,\n",
              "         'm': 1,\n",
              "         'goona': 1,\n",
              "         'watch': 1,\n",
              "         'again': 1,\n",
              "         ':)': 2})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "token_counts = Counter()\n",
        "token_counts.update(example_tokens)\n",
        "token_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-siY_CC5wtpC"
      },
      "source": [
        "Let's sort the output to have the most common words at the top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPDKW9BUq2TZ",
        "outputId": "4b6c783a-7352-4024-d298-93d831f4bd83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('i', 2),\n",
              " ('it', 2),\n",
              " (':)', 2),\n",
              " ('this', 1),\n",
              " ('is', 1),\n",
              " ('awesome', 1),\n",
              " ('movie', 1),\n",
              " ('loved', 1),\n",
              " ('so', 1),\n",
              " ('much', 1),\n",
              " ('m', 1),\n",
              " ('goona', 1),\n",
              " ('watch', 1),\n",
              " ('again', 1)]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_by_freq_tuples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebG5M5g7ywIn"
      },
      "source": [
        "It shows that in our example text, the top place is taken by pronouns (i and it) followed by the emoticon. Though our data is now correctly processed, it needs to be prepared to be fed to a model. Because [machine] models love math and work with numbers exclusively. To convert our dictionary of word tokens into integers, we can take help from `torchtext.vocab`. Its purpose in the official documentation is defined as [link here](https://pytorch.org/text/stable/vocab.html)\n",
        "\n",
        "> Factory method for creating a vocab object which maps tokens to indices. \n",
        "\n",
        "> Note that the ordering in which key value pairs were inserted in the ordered_dict will be respected when building the vocab. Therefore if sorting by token frequency is important to the user, the ordered_dict should be created in a way to reflect this.\n",
        "\n",
        "It highlights three points:\n",
        "* It maps tokens to indices\n",
        "* It requires an ordered dictionary (`OrderedDict`) to work\n",
        "* Tokens in vocab at the starting indices reflect higher frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7Zi3wIb7JD7",
        "outputId": "d97dfd64-f37e-4372-cd52-339466927997"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('i', 2),\n",
              "             ('it', 2),\n",
              "             (':)', 2),\n",
              "             ('this', 1),\n",
              "             ('is', 1),\n",
              "             ('awesome', 1),\n",
              "             ('movie', 1),\n",
              "             ('loved', 1),\n",
              "             ('so', 1),\n",
              "             ('much', 1),\n",
              "             ('m', 1),\n",
              "             ('goona', 1),\n",
              "             ('watch', 1),\n",
              "             ('again', 1)])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##\n",
        "# step 1: convert our sorted list of tokens to OrderedDict\n",
        "from collections import OrderedDict\n",
        "\n",
        "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "ordered_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SKnIiQvwONc",
        "outputId": "7ec70e90-8c13-4b00-a6cd-abed87943542"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##\n",
        "# Check the length of our dictionary\n",
        "len(ordered_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WAqHHu4vW-B",
        "outputId": "1c03856f-5e50-4a0d-9218-52929ae78b0a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'goona': 11,\n",
              " 'much': 9,\n",
              " 'm': 10,\n",
              " 'loved': 7,\n",
              " 'watch': 12,\n",
              " 'so': 8,\n",
              " 'movie': 6,\n",
              " 'it': 1,\n",
              " 'again': 13,\n",
              " 'this': 3,\n",
              " 'i': 0,\n",
              " 'awesome': 5,\n",
              " ':)': 2,\n",
              " 'is': 4}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##\n",
        "# step 2: convert the ordered dict to torchtext.vocab\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "vb = vocab(ordered_dict)\n",
        "vb.get_stoi()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPm2upP-7n02"
      },
      "source": [
        "This generated vocabulary shows that tokens with higher frequency (`i`, `it`) have been assigned lower indices (or integers). This vocabulary will act as a lookup table for us, and during training for each word token, we will find a corresponding index from this vocab and pass it to our model. \n",
        "\n",
        "We have done many steps while processing our `example_text`. Let's summarize them here before moving further\n",
        "\n",
        "#### **Summary of data dictionary preparation steps**\n",
        "\n",
        "1.   Generate tokens from text using the function `tokenizer`\n",
        "2.   Find the frequency of tokens using Python [collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter)\n",
        "3. Sort the tokens based on their frequency in descending order\n",
        "4. Put the sorted tokens in Python [collections.OrderedDict](https://docs.python.org/3/library/collections.html#collections.OrderedDict)\n",
        "5. Convert the tokens into integers using [torchtext.vocab](https://pytorch.org/text/stable/vocab.html)\n",
        "\n",
        "Let's apply all these steps on our IMDB reviews training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nrsi4alivzsn",
        "outputId": "48bd4363-f820-4a43-93dc-0bf56c35eb0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IMDB vocab size: 69023\n"
          ]
        }
      ],
      "source": [
        "##\n",
        "# step 1: convert reviews into tokens\n",
        "# step 2: find frequency of tokens\n",
        "\n",
        "token_counts = Counter()\n",
        "\n",
        "for label, line in train_dataset:\n",
        "    tokens = tokenizer(line)\n",
        "    token_counts.update(tokens)\n",
        " \n",
        "print('IMDB vocab size:', len(token_counts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYHOV47d2nPM"
      },
      "source": [
        "After tokenizing IMDB reviews, we find that there `69023` unique tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuaysCgS-sgY",
        "outputId": "2da48263-8c05-456e-9e30-5e595919a618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "this  -->  11\n",
            "is  -->  7\n",
            "an  -->  35\n",
            "example  -->  457\n"
          ]
        }
      ],
      "source": [
        "##\n",
        "# step 3: sort the token based on their frequency\n",
        "# step 4: put the sorted tokens in OrderedDict\n",
        "# step 5: convert token to integers using vocab object\n",
        "\n",
        "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "\n",
        "vb = vocab(ordered_dict)\n",
        "\n",
        "vb.insert_token(\"<pad>\", 0)  # special token for padding\n",
        "vb.insert_token(\"<unk>\", 1)  # special token for unknown words\n",
        "vb.set_default_index(1)\n",
        "\n",
        "# print some token indexes from vocab\n",
        "for token in [\"this\", \"is\", \"an\", \"example\"]:\n",
        "    print(token, \" --> \", vb[token])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3vX2UfvDHCD"
      },
      "source": [
        "We have added two extra tokens to our vocabulary.\n",
        "* \"pad\" for padding. This token will come in handy when we pad our reviews to make them of the same length\n",
        "* \"unk\" for unknown. This token will come in handy if we find any token in the validation or test set that was not part of the train set\n",
        "\n",
        "Let's also print the tokens present at the first ten indices of our vocab object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIFypu57CwC5",
        "outputId": "8a12125d-d609-43ed-d83e-7e855b55bc33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<pad>', '<unk>', 'the', 'and', 'a', 'of', 'to', 'is', 'it', 'in']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vb.get_itos()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDBG6TCOi9Ut"
      },
      "source": [
        "It shows that articles, prepositions, and pronouns are the most common words in the training dataset. So let's also check the least common words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP5G1DAyDbIH",
        "outputId": "b3edc3ee-3cd0-4a46-9044-16e65da37883"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hairband',\n",
              " 'ratt',\n",
              " 'bettiefile',\n",
              " 'queueing',\n",
              " 'johansen',\n",
              " 'hemmed',\n",
              " 'jardine',\n",
              " 'morland',\n",
              " 'seriousuly',\n",
              " 'fictive']"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vb.get_itos()[-10:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4URcUtz84dGk"
      },
      "source": [
        "The least common words seem to be people or place names or misspelled words like 'queueing' and 'seriousuly'.\n",
        "\n",
        "## Define data processing pipelines\n",
        "At this point, we have our tokenizer function and vocabulary lookup ready. For each review item from the dataset, we are supposed to perform the following preprocessing steps:\n",
        "\n",
        "**For review text**\n",
        "* Create tokens from the review text\n",
        "* Assign a unique integer to each token from the vocab lookup\n",
        "\n",
        "**For review label**\n",
        "* Assign 1 for `pos` and 0 for `neg` label\n",
        "\n",
        "Let's create two simple functions (inline lambda) for review text and label processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SuaVYQ-sgttl"
      },
      "outputs": [],
      "source": [
        "##\n",
        "# inline lambda functions for text and label precessing\n",
        "text_pipeline = lambda x: [vb[token] for token in tokenizer(x)]\n",
        "label_pipeline = lambda x: 1.0 if x == \"pos\" else 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ_VwRvDHtci",
        "outputId": "eed8b2b9-ec2e-4c77-b64f-0003ddc95b36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[11, 7, 1166, 18, 10, 450, 8, 37, 74, 10, 142, 1, 104, 8, 174, 2287, 2287]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##\n",
        "# apply text_pipeline to example_text\n",
        "text_pipeline(example_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyysz5YQ6hxM"
      },
      "source": [
        "Instead of processing a single review at a time, we always prefer to work with a batch of them during model training. For each review item in the batch, we will be doing the same preprocessing steps i.e. review text processing and label processing. For handling preprocessing steps at a batch level, we can create another higher-level function that applies preprocessing steps at a batch level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAthQbaE9f4E",
        "outputId": "5d3efdc2-19ae-4615-8db6-4e652d720822"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "##\n",
        "# setting device on GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9UViOgiB6JBC"
      },
      "outputs": [],
      "source": [
        "##\n",
        "# a function to apply pre-processing steps at a batch level\n",
        "import torch.nn as nn\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, lengths = [], [], []\n",
        "\n",
        "    # iterate over all reviews in a batch\n",
        "    for _label, _text in batch:\n",
        "        # label preprocessing\n",
        "        label_list.append(label_pipeline(_label))\n",
        "        # text preprocessing\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "\n",
        "        # store the processed text in a list\n",
        "        text_list.append(processed_text)\n",
        "        \n",
        "        # store the length of processed text\n",
        "        # this will come handy in future when we want to know the original size of a text (without padding)\n",
        "        lengths.append(processed_text.size(0))\n",
        "    \n",
        "    label_list = torch.tensor(label_list)\n",
        "    lengths = torch.tensor(lengths)\n",
        "    \n",
        "    # pad the processed reviews to make their lengths consistant\n",
        "    padded_text_list = nn.utils.rnn.pad_sequence(\n",
        "        text_list, batch_first=True)\n",
        "    \n",
        "    # return\n",
        "    # 1. a list of processed and padded review texts\n",
        "    # 2. a list of processed labels\n",
        "    # 3. a list of review text original lengths (before padding)\n",
        "    return padded_text_list.to(device), label_list.to(device), lengths.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVSa0yK99y8J"
      },
      "source": [
        "### Sequence padding\n",
        "\n",
        "In the above `collate_batch` function, I added one extra padding step. \n",
        "```\n",
        "added_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
        "```\n",
        "We intend to make all review texts in a batch of the same length. For this, we take the maximum length of a text in a batch, all pad all the smaller text with extra dummy tokens ('pad') to make their sizes equal. Finally, with all the data in a batch of the same dimension, we convert it into a tensor matrix for faster processing.\n",
        "\n",
        "To understand how PyTorch utility `nn.utils.rnn.pad_sequence` works, we can take a simple example of three tensors (a, b, c) of varying sizes (1, 3, 5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvXEBZrd9yQ-",
        "outputId": "6df8f266-52c7-41ce-eb2f-7535a775d08d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([1]), tensor([2, 3, 4]), tensor([5, 6, 7, 8, 9]))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##\n",
        "# initialize three tensors of varying sizes\n",
        "a = torch.tensor([1])\n",
        "b = torch.tensor([2, 3, 4])\n",
        "c = torch.tensor([5, 6, 7, 8, 9])\n",
        "a, b, c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtp1KHQ9B6Jn"
      },
      "source": [
        "Now let's pad them to make sizes consistant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2Fgwj3VBNww",
        "outputId": "f3e0a572-1acf-45cc-f869-f5d15bdb1a65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1, 2, 5],\n",
              "        [0, 3, 6],\n",
              "        [0, 4, 7],\n",
              "        [0, 0, 8],\n",
              "        [0, 0, 9]])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##\n",
        "# apply padding on tensors\n",
        "pad_seq = nn.utils.rnn.pad_sequence([a, b, c])\n",
        "pad_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaZyrhblCHSy"
      },
      "source": [
        "### Sequence packing\n",
        "From the above output, we can see that after padding tensors of varying sizes, we can convert them into a single matrix for faster processing. But the drawback of this approach is that we can have many, many padded tokens in our matrix. They are not helping us in any way, instead of occupying a lot of machine memory. To avoid this, we can also squish these matrixes into a much condensed form called `packed padded sequences` using PyTorch utility `nn.utils.rnn.pack_padded_sequence`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T966AVl2BbrA",
        "outputId": "f45f9d61-3b4f-492c-cd4c-b5ba6f115f63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([5, 2, 1, 6, 3, 7, 4, 8, 9])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pack_pad_seq = nn.utils.rnn.pack_padded_sequence(\n",
        "    pad_seq, [1, 3, 5], enforce_sorted=False, batch_first=False\n",
        ")\n",
        "pack_pad_seq.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfht5yquD6Q8"
      },
      "source": [
        "Here the tensor still holds all the original tensor values (1 to 9) but is very condensed and has no extra padded token. So how does this tensor know which tokens belong to which token? For this, it stores some additional information.\n",
        "* batch sizes (or original tensor length)\n",
        "* tensor indices\n",
        "\n",
        "We can move back and forth between the padded pack and unpacked sequences using this information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWvqG8y1Drib",
        "outputId": "4041dad4-e6f7-48c9-8153-bd4cf94fc075"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([5, 2, 1, 6, 3, 7, 4, 8, 9]), batch_sizes=tensor([3, 2, 2, 1, 1]), sorted_indices=tensor([2, 1, 0]), unsorted_indices=tensor([2, 1, 0]))"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pack_pad_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH4YGLTrNAM0"
      },
      "source": [
        "### Run data preprocessing pipelines on an example batch\n",
        "Let's load our data in the PyTorch DataLoader class and create a small batch of 4 reviews. Preprocess the entire set with `collate_batch` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgmQ8IYZE3PW",
        "outputId": "7c1bb941-285f-4f23-8dae-3626c58c2b68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text_batch.shape:  torch.Size([4, 218])\n",
            "label_batch:  tensor([1., 1., 1., 0.], device='cuda:0')\n",
            "length_batch:  tensor([165,  86, 218, 145], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch\n",
        ")\n",
        "text_batch, label_batch, length_batch = next(iter(dataloader))\n",
        "\n",
        "print(\"text_batch.shape: \", text_batch.shape)\n",
        "print(\"label_batch: \", label_batch)\n",
        "print(\"length_batch: \", length_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRJ63CKNRr1o"
      },
      "source": [
        "* `text_batch.shape: torch.Size([4, 218])` tells us that in this batch, there are four reviews (or their tokens) and all have the same length of 218\n",
        "* `label_batch:  tensor([1., 1., 1., 0.])` tells us that the first three reviews are positive and the last is negative\n",
        "* `length_batch:  tensor([165,  86, 218, 145])` tells us that before padding the original length of review tokens\n",
        "\n",
        "Let's check what the first review in this batch looks like after preprocessing and padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmPSno8XFNvC",
        "outputId": "5615d0a2-06d7-4d6d-cf90-8f2cc5710aae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([   35,  1739,     7,   449,   721,     6,   301,     4,   787,     9,\n",
            "            4,    18,    44,     2,  1705,  2460,   186,    25,     7,    24,\n",
            "          100,  1874,  1739,    25,     7, 34415,  3568,  1103,  7517,   787,\n",
            "            5,     2,  4991, 12401,    36,     7,   148,   111,   939,     6,\n",
            "        11598,     2,   172,   135,    62,    25,  3199,  1602,     3,   928,\n",
            "         1500,     9,     6,  4601,     2,   155,    36,    14,   274,     4,\n",
            "        42945,     9,  4991,     3,    14, 10296,    34,  3568,     8,    51,\n",
            "          148,    30,     2,    58,    16,    11,  1893,   125,     6,   420,\n",
            "         1214,    27, 14542,   940,    11,     7,    29,   951,    18,    17,\n",
            "        15994,   459,    34,  2480, 15211,  3713,     2,   840,  3200,     9,\n",
            "         3568,    13,   107,     9,   175,    94,    25,    51, 10297,  1796,\n",
            "           27,   712,    16,     2,   220,    17,     4,    54,   722,   238,\n",
            "          395,     2,   787,    32,    27,  5236,     3,    32,    27,  7252,\n",
            "         5118,  2461,  6390,     4,  2873,  1495,    15,     2,  1054,  2874,\n",
            "          155,     3,  7015,     7,   409,     9,    41,   220,    17,    41,\n",
            "          390,     3,  3925,   807,    37,    74,  2858,    15, 10297,   115,\n",
            "           31,   189,  3506,   667,   163,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(text_batch[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCEQqCIWSl-U"
      },
      "source": [
        "To complete the picture, I have re-printed the original text of the first review and manually processed a part of it. You can verify that the tokens match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLeaQJTlNsCy",
        "outputId": "5e7f0099-c07f-4a4b-e2c9-3297074308b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('pos',\n",
              " 'An extra is called upon to play a general in a movie about the Russian Revolution. However, he is not any ordinary extra. He is Serguis Alexander, former commanding general of the Russia armies who is now being forced to relive the same scene, which he suffered professional and personal tragedy in, to satisfy the director who was once a revolutionist in Russia and was humiliated by Alexander. It can now be the time for this broken man to finally \"win\" his penultimate battle. This is one powerful movie with meticulous direction by Von Sternberg, providing the greatest irony in Alexander\\'s character in every way he can. Jannings deserved his Oscar for the role with a very moving performance playing the general at his peak and at his deepest valley. Powell lends a sinister support as the revenge minded director and Brent is perfect in her role with her face and movements showing so much expression as Jannings\\' love. All around brilliance. Rating, 10.')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##\n",
        "# first review\n",
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr2L4UkRQ1CX",
        "outputId": "63e0d0b1-d478-4bab-959a-a1654903b0f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[35, 1739, 7, 449, 721, 6, 301, 4, 787, 9, 4, 18, 44, 2, 1705, 2460]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##\n",
        "# manually preprocessing a part of review text\n",
        "# notice that the generated tokens match\n",
        "text = 'An extra is called upon to play a general in a movie about the Russian Revolution'\n",
        "[vb[token] for token in tokenizer(text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH9VlvnlTH6l"
      },
      "source": [
        "## Batching the training, validation, and test dataset\n",
        "\n",
        "Let's proceed on creating DataLoaders for train, valid, and test data with `batch_size = 32`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "CR6eKF3_RbyA"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dl = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n",
        ")\n",
        "valid_dl = DataLoader(\n",
        "    valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
        ")\n",
        "test_dl = DataLoader(\n",
        "    test_dataset_raw, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnuIywwGTsqF"
      },
      "source": [
        "## Define model training and evaluation pipelines\n",
        "I have defined two simple functions to train and evaluate the model in this section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "GpTak2XHTi_L"
      },
      "outputs": [],
      "source": [
        "##\n",
        "# model training pipeline\n",
        "# https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part2.ipynb\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_loss = 0, 0\n",
        "    for text_batch, label_batch, lengths in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(text_batch, lengths)[:, 0]\n",
        "        loss = loss_fn(pred, label_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n",
        "        total_loss += loss.item() * label_batch.size(0)\n",
        "    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)\n",
        "\n",
        "\n",
        "# model evaluation pipeline\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_loss = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for text_batch, label_batch, lengths in dataloader:\n",
        "            pred = model(text_batch, lengths)[:, 0]\n",
        "            loss = loss_fn(pred, label_batch)\n",
        "            total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n",
        "            total_loss += loss.item() * label_batch.size(0)\n",
        "    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQrhPBw3TZ2o"
      },
      "source": [
        "## RNN model configuration, loss function, and optimizer\n",
        "We have seen the review text, which can be long sequences. We will use the LSTM layer for capturing the long-term dependencies. Our sentiment analysis model is composed of the following layers\n",
        "* Start with an **Embedding layer**. Placing the embedding layer is similar to one-hot-encoding, where each word token is converted to a separate feature (or vector or column). But this can lead to too many features (curse of dimensionality or dimensional explosion). To avoid this, we try to map tokens to fixed-size vectors (or columns). In such a feature matrix, different elements denote different tokens. Tokens that are closed are also placed together. Further, during training, we also learn and update the positioning of tokens. Similar tokens are placed into closer and closer locations. Such a matrix layer is termed an embedding layer.\n",
        "* After the embedding layer, there is the RNN layer (LSTM to be specific).\n",
        "* Then we have a fully connected layer followed by activation and another fully connected layer.\n",
        "* Finally, we have a logistic sigmoid layer for prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "k36IeQGJTT3a"
      },
      "outputs": [],
      "source": [
        "##\n",
        "# https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part2.ipynb\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
        "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text, lengths):\n",
        "        out = self.embedding(text)\n",
        "        out = nn.utils.rnn.pack_padded_sequence(\n",
        "            out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True\n",
        "        )\n",
        "        out, (hidden, cell) = self.rnn(out)\n",
        "        out = hidden[-1, :, :]\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "K8KLG602WJUh"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vb)\n",
        "embed_dim = 20\n",
        "rnn_hidden_size = 64\n",
        "fc_hidden_size = 64\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxKYEeNwT1vL"
      },
      "source": [
        "### Define model loss function and optimizer\n",
        "For loss function (or criterion), I have used [Binary Cross Entropy](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html), and for loss optimization, I have used [Adam algorithm](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "QV0gDqZETw3-"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqGMq6JuaFSV"
      },
      "source": [
        "## Model training and evaluation\n",
        "Let's run the pipeline for ten epochs and compare the training and validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5PGkqLpT6MX",
        "outputId": "3c53356b-77ab-49e2-92ff-5e8086d3b9c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 train accuracy: 0.6085; val accuracy: 0.6502\n",
            "Epoch 1 train accuracy: 0.7206; val accuracy: 0.7462\n",
            "Epoch 2 train accuracy: 0.7613; val accuracy: 0.6250\n",
            "Epoch 3 train accuracy: 0.8235; val accuracy: 0.8232\n",
            "Epoch 4 train accuracy: 0.8819; val accuracy: 0.8482\n",
            "Epoch 5 train accuracy: 0.9132; val accuracy: 0.8526\n",
            "Epoch 6 train accuracy: 0.9321; val accuracy: 0.8374\n",
            "Epoch 7 train accuracy: 0.9504; val accuracy: 0.8502\n",
            "Epoch 8 train accuracy: 0.9643; val accuracy: 0.8608\n",
            "Epoch 9 train accuracy: 0.9747; val accuracy: 0.8636\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    acc_train, loss_train = train(train_dl)\n",
        "    acc_valid, loss_valid = evaluate(valid_dl)\n",
        "    print(\n",
        "        f\"Epoch {epoch} train accuracy: {acc_train:.4f}; val accuracy: {acc_valid:.4f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6swkupX8DpRC"
      },
      "source": [
        "### Evaluate sentiments on random texts\n",
        "Let's create another helper method to evaluate sentiments on random texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "zk4VwWGV_BhV"
      },
      "outputs": [],
      "source": [
        "def classify_review(text):\n",
        "    text_list, lengths = [], []\n",
        "\n",
        "    # process review text with text_pipeline\n",
        "    # note: \"text_pipeline\" has dependency on data vocabulary\n",
        "    processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
        "    text_list.append(processed_text)\n",
        "\n",
        "    # get processed review tokens length\n",
        "    lengths.append(processed_text.size(0))\n",
        "    lengths = torch.tensor(lengths)\n",
        "        \n",
        "    # change the dimensions from (torch.Size([8]), torch.Size([1, 8]))\n",
        "    # nn.utils.rnn.pad_sequence(text_list, batch_first=True) does this too\n",
        "    padded_text_list = torch.unsqueeze(processed_text, 0)\n",
        "\n",
        "    # move tensors to correct device\n",
        "    padded_text_list = padded_text_list.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "\n",
        "    # get prediction\n",
        "    model.eval()\n",
        "    pred = model(padded_text_list, lengths)\n",
        "    print(\"model pred: \", pred)\n",
        "\n",
        "    # positive or negative review\n",
        "    review_class = 'negative' # else case\n",
        "    if (pred>=0.5) == 1:\n",
        "        review_class = \"positive\"\n",
        "\n",
        "    print(\"review type: \", review_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "xqX24x-IzErD"
      },
      "outputs": [],
      "source": [
        "##\n",
        "# create two random texts with strong positive and negative sentiments\n",
        "pos_review = 'i love this movie. it was so good.'\n",
        "neg_review = 'slow and boring. waste of time.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oejN6rJ_pS8",
        "outputId": "0f2c5b34-3ce2-4304-da0a-30855050c1dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model pred:  tensor([[0.9388]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
            "review type:  positive\n"
          ]
        }
      ],
      "source": [
        "classify_review(pos_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_M1VhbMBygl",
        "outputId": "2dee82f4-b8fa-44f8-d5aa-ffddc54488d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model pred:  tensor([[0.0057]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
            "review type:  negative\n"
          ]
        }
      ],
      "source": [
        "classify_review(neg_review)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "instance_type": "ml.t3.medium",
    "kernelspec": {
      "display_name": "Python 3.8.8 ('myblog')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "bbac80ad2bfd54975e0c0f7ddf300156d5b24b5126f35dc262fa887f22fb28f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
