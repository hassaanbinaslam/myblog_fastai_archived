{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation with SageMaker Data Wrangler (Part 4)\n",
    "> A detailed guide on AWS SageMaker Data Wrangler to prepare data for machine learning models. This is a five parts series where we will prepare, import, explore, process, and export data using AWS Data Wrangler. You are reading **Part 4:Preprocess data using Data Wrangler**.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [aws, ml, sagemaker]\n",
    "- keyword: [aws, ml, sagemaker, wrangler]\n",
    "- image: images/copied_from_nb/images/2022-05-25-aws-sagemaker-wrangler-p4.jpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/2022-05-25-aws-sagemaker-wrangler-p4.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviornment\n",
    "\n",
    "This notebook is prepared with Amazon SageMaker Studio using `Python 3 (Data Science)` Kernel and `ml.t3.medium` instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "This is a detailed guide on using **AWS SageMaker Data Wrangler** service to prepare data for machine learning models. SageMaker Data Wrangler is a multipurpose tool with which you can\n",
    "* import data from multiple sources\n",
    "* explore data with visualizations\n",
    "* apply transformations\n",
    "* export data for ml training\n",
    "\n",
    "This guide is divided into five parts\n",
    "* [Part 1: Prepare synthetic data and place it on multiple sources](https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/17/aws-sagemaker-wrangler-p1.html)\n",
    "* [Part 2: Import data from multiple sources using Data Wrangler](https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/23/aws-sagemaker-wrangler-p2.html)\n",
    "* [Part 3: Explore data with Data Wrangler visualizations](https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/24/aws-sagemaker-wrangler-p3.html)\n",
    "* **Part 4: Preprocess data using Data Wrangler (You are here)**\n",
    "* [Part 5: Export data for ML training](https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/26/aws-sagemaker-wrangler-p5.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Preprocess data using Data Wrangler\n",
    "\n",
    "We will continue from where we left in [part-3](https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/24/aws-sagemaker-wrangler-p3.html). Open [customer-churn.flow](https://github.com/hassaanbinaslam/myblog/blob/master/_notebooks/datasets/2022-05-23-aws-sagemaker-wrangler-p2/customer-churn-p3.flow) file in AWS SageMaker Data Wrangler console. Once opened our flow will look like this\n",
    "\n",
    "![customer-churn.png](images/2022-05-25-aws-sagemaker-wrangler-p4/customer-churn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add the following transformations to our code.\n",
    "\n",
    "* Remove redundant columns\n",
    "* Remove features with low predictive power\n",
    "* Transform feature values to correct format\n",
    "* Encode categorical features\n",
    "* Move the target label to the start\n",
    "\n",
    "## Remove redundant columns\n",
    "When we made joins between tables (see [part-2](https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/23/aws-sagemaker-wrangler-p2.html)) it resulted in some redundant columns CustomerID_* . We will remove them first. For this click on plus sign beside 2nd Join, and select **Add Transform**. From the next transform UI clink **Add Step** and then search for transformer **Manage Column**. Inside Manage Columns transformer select\n",
    "\n",
    "* Transform = Drop Column\n",
    "* Columns to drop = CustomerID_0, CustomerID_1\n",
    "\n",
    "Click preview and Add.\n",
    "![drop-columns.png](images/2022-05-25-aws-sagemaker-wrangler-p4/drop-columns.png)\n",
    "\n",
    "## Remove features with low predictive power\n",
    "In [part-3](https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/24/aws-sagemaker-wrangler-p3.html) we used Quick Model to get the predictive power of features. When we analyze features with low importance we find that `Phone` is one such feature that does not hold much information for the model. For a model, a phone number is just some random collection of numbers and does not hold any meaning. There are other features with low importance too but they still hold some information for the model. So let's drop `Phone`. The steps will be same as in the last part.\n",
    "\n",
    "## Transform feature values to correct format\n",
    "`Churn?` is our target label but its value has an extra '.' at the end. If we remove that symbol then it can easily be converted to a Boolean type. So let's do that. From the transformers list this time choose `Format String` and select\n",
    "\n",
    "* Transform = Remove Symbols\n",
    "* Input Columns = Churn?\n",
    "* Symbols = `.`\n",
    "\n",
    "Click Preview and Add.\n",
    "\n",
    "![format-strings.png](images/2022-05-25-aws-sagemaker-wrangler-p4/format-strings.png)\n",
    "\n",
    "Now that the data is in the correct format (True/False) we can apply another transformer on it to convert it to Boolean feature. So select **PARSE COLUMN AS TYPE** transformer and configure\n",
    "\n",
    "* Column = Churn?\n",
    "* From = String\n",
    "* To = Boolean\n",
    "\n",
    "Click Preview and then Add.\n",
    "\n",
    "## Encode categorical features\n",
    "\n",
    "At this point we have only two columns with String datatype: **State** and **Area Code**. If we look at the Area Code it has high variance and little feature importance. It is better to drop this feature. So Add another transformer and drop Area Code. For State we will apply one-hot encoding. So for this select transformer **Encode Categorical** and configure\n",
    "\n",
    "* Transform = One-hot encode\n",
    "* Input Columns = State\n",
    "* Output style = Columns\n",
    "\n",
    "Leave the rest of the options as default. Click Preview and Add.\n",
    "\n",
    "![one-hot-encode.png](images/2022-05-25-aws-sagemaker-wrangler-p4/one-hot-encode.png)\n",
    "\n",
    "## Move the target label to the start\n",
    "\n",
    "SageMaker requires that the target label should be the first column in the dataset. So add another transformer **Manage columns** and configure\n",
    "\n",
    "* Transform = Move column\n",
    "* Move Type = Move to start\n",
    "* Column to move = Churn?\n",
    "\n",
    "![move-target.png](images/2022-05-25-aws-sagemaker-wrangler-p4/move-target.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance\n",
    "\n",
    "We have done some key transformations. We can use Quick Model again to analyze the model performance at this point. We have done a similar analysis in [part-3](https://hassaanbinaslam.github.io/myblog/aws/ml/sagemaker/2022/05/24/aws-sagemaker-wrangler-p3.html) so let's do it again and compare the results. From the last transformation step, click plus sign and choose **Add Analysis**\n",
    "\n",
    "![quick_model_2.png](images/2022-05-25-aws-sagemaker-wrangler-p4/quick_model_2.png)\n",
    "\n",
    "We can see from the results that these transformations have a positive impact on the model performance and the F1 score has moved up from 0.841 to 0.861.\n",
    "\n",
    "# Summary\n",
    "\n",
    "In this post we have seen how we can apply a transformation to our data and can use Quick Model to quickly analyze the model performance. `customer-churn-p4.flow` file used in this post can be found on the GitHub here. In the next post, we will discuss how to export data from Data Wrangler to different destinations."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
