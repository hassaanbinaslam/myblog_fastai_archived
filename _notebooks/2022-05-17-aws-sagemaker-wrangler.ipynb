{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation with SageMaker Data Wrangler\n",
    "> A detailed guide on AWS SageMaker Wrangler to prepare data for machine learning models.\n",
    "\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [aws, ml, sagemaker]\n",
    "- keyword: [aws, ml, sagemaker, wrangler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "This post is a detailed guide on using AWS SageMaker Data Wrangler service to prepare data for machine learning models. SageMaker Data Wrangler is a multipurpose tool with which you can\n",
    "* import data from multiple sources\n",
    "* explore data with visualizations\n",
    "* apply transformations\n",
    "* export data for ml training\n",
    "\n",
    "This will be a five part post\n",
    "* Part 1: Prepare a synthatic data and place it on multiple sources\n",
    "* Part 2: Import data from multiple sources using Data Wrangler\n",
    "* Part 3: Explore data with Data Wrangler visualizations\n",
    "* Part 4: Preprocess data using Data Wrangler\n",
    "* Part 5: Export data for ML training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Prepare a synthatic data and place it on multiple sources\n",
    "\n",
    "Let's prepare some dataset and place it on S3 bucket and Athena tables. Then we will use Data Wrangler to pull and join data from these two sources. The idea is to simulate some real project challenges where data is not coming from a single source but is distributed in multiple stores and in different formats. It is usually the preprocessing pipeline job to get data from these sources, join and preprocess it. \n",
    "\n",
    "## Data\n",
    "Mobile operators have historical records on which customers ultimately ended up churning and which continued using the service. We can use this historical information to construct an ML model of one mobile operator’s churn using a process called training. After training the model, we can pass the profile information of an arbitrary customer (the same profile information that we used to train the model) to the model, and have the model predict whether this customer is going to churn. Of course, we expect the model to make mistakes. After all, predicting the future is tricky business! But we’ll learn how to deal with prediction errors.\n",
    "\n",
    "The dataset we use is publicly available and was mentioned in the book Discovering Knowledge in Data by Daniel T. Larose. It is attributed by the author to the University of California Irvine Repository of Machine Learning Datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
