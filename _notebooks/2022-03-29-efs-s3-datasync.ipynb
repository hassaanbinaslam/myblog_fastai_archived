{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS EFS Sync to S3 Using DataSync\n",
    "> A tutorial to synchronize EFS with S3 bucket using DataSync service.\n",
    "\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [aws, lambda, efs, s3, synchonization, datasync]\n",
    "- keywords: [aws, lambda, s3, efs, sync, python, datasync]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "This post is to document all the steps required to synchronize [AWS EFS](https://aws.amazon.com/efs/) with an [S3 bucket](https://aws.amazon.com/s3/) using [DataSync service](https://aws.amazon.com/datasync/). The flow of information is from S3 to EFS and not vice versa.\n",
    "\n",
    "We will discuss two approaches to trigger the datasync service\n",
    "* Trigger-based. Whenever there is a new file uploaded in S3 bucket\n",
    "* Schedule-based. A trigger will run datasync at scheduled intervals\n",
    "  \n",
    "Once a datasync service is invoked it will take care of syncing files from S3 bucket to EFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python = 3.9.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for trigger based approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![trigger-based-approach](images/2022-03-29-efs-s3-datasync/trigger-based-approach.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an S3 bucket\n",
    "Let's first create an S3 bucket that will contain our data, and this is the bucket we would like to be in sync with EFS. I am naming the bucket as `mydata-202203`. You may name it as you please. Choose a region of your choice and leave the rest of the settings as defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an EFS\n",
    "\n",
    "From EFS console give name as `mydata-efs`. I am using default VPC for this post. Use `Regional` availability settings. Click **Create**. Once file system is created, click on **Access points** and create an access point for this efs to be mounted in other service. For access point use following settings\n",
    "* name = mydata-ap\n",
    "* root dir path = /\n",
    "* POSIX User\n",
    "  * POSIX UID = 1000\n",
    "  * Group ID = 1000\n",
    "* Root directory creation permissions\n",
    "  * Owner user id = 1000\n",
    "  * Owner group id = 1000\n",
    "  * POSIX permissions = 777\n",
    "\n",
    "Click **Create**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on EFS security group settings\n",
    "\n",
    "In the last section, I have used a default VPC security group (sg) while creating EFS. Default sg allows traffic for all protocols and all ports, both inbound and outbound. But if you are using a custom security group then make sure that you have an inbound rule for \n",
    "* Type = NFS\n",
    "* Protocol = TCP\n",
    "* Port range = 2049\n",
    "\n",
    "Otherwise, you will not be able to access EFS using NFS clients, and if you find an error similar to the below then it means you need to check the security group settings.\n",
    "\n",
    "![security-group-error](images/2022-03-29-efs-s3-datasync/security-group-error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataSync service task\n",
    "\n",
    "- configure source location = create a new location\n",
    "    - location type = Amazon S3\n",
    "    - region = us-east-1\n",
    "    - s3 bucket = mydata-202203\n",
    "    - s3 storage class = standard\n",
    "    - folder = [leave empty]\n",
    "    - IAM role = click on auto generate\n",
    "- configure destination location = create a new location\n",
    "    - location type = EFS\n",
    "    - region = us-east-1\n",
    "    - efs file system = mydata-efs\n",
    "    - mount path = /efs\n",
    "    - subnet = us-east-1a\n",
    "    - security group = default\n",
    "- configure settings\n",
    "    - task name = mydata-datasync\n",
    "    - task execution configuration\n",
    "        - verify data = verify only the data transferred\n",
    "        - set bandwidth limit = use available\n",
    "    - data transfer configuration\n",
    "        - data to scan = entire source location\n",
    "        - transfer mode = transfer only the data that has changed\n",
    "        - uncheck \"keep deleted files\"\n",
    "        - check \"overwrite files\"\n",
    "    - schedule\n",
    "        - frequency = not scheduled\n",
    "    - task logging\n",
    "        - cloudwatch log group = autogenerate\n",
    "\n",
    "Click \"next\". Review and Launch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DataSync Service\n",
    "\n",
    "Let's test datasync service by manually starting it. If S3 bucket is empty then datasync will throw an exception as below\n",
    "\n",
    "![s3-empty](images/2022-03-29-efs-s3-datasync/s3-empty.png)\n",
    "\n",
    "This is not an issue. Just place some files (**test1.txt** in my case) in the bucket and start the datasync service again. If it executes successfully then you will get a message as `Execution Status = Success`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSync can work without Internet Gateway or VPC Endpoint\n",
    "\n",
    "One thing I noticed is that DataSync service can work even without the presence of an internet gateway or S3 service endpoint. EFS is VPC bound and S3 is global but DataSync can still communicate with both of them. This was different for Lambda. Once Lambda is configured for a VPC then it is not able to access S3 without an internet gateway or VPC endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify EFS by mounting it to the EC2 machine\n",
    "\n",
    "In the last section, we ran DataSync and it successfully copied files from S3 to EFS. So let's verify our files from EFS by mounting it to an EC2 instance.\n",
    "\n",
    "Create an EC2 machine\n",
    "* AMI = Amazon Linux 2 AMI (HVM) - Kernel 5.10, SSD Volume Type\n",
    "* Intance type = t2.micro (free tier)\n",
    "* Instance details\n",
    "  * Network = default VPC\n",
    "  * Auto-assign Public IP = Enable\n",
    "* Review and Lanunch > Launch > Proceed without key pair.\n",
    "\n",
    "Once the instance is up and running, click on it and connect using EC2 instance connect option. Create a dir 'efs' using the command\n",
    "```\n",
    "mkdir efs\n",
    "```\n",
    "In a separate tab open EFS, and click on the file system we have created. Click **Attach**. From \"Mount via DNS\" copy command for NFS client. paste that in EC2 bash terminal\n",
    "```\n",
    "sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-01acd308743098251.efs.us-east-1.amazonaws.com:/ efs\n",
    "```\n",
    "Once successfully mounted, verify that the file '**test1.txt**' exists in EFS.\n",
    "\n",
    "![efs-verify](images/2022-03-29-efs-s3-datasync/efs-verify.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lambda function to trigger DataSync task\n",
    "\n",
    "Now let's create a lambda function that will trigger the datasync task. This function will itself be triggered by an S3 event notification whenever a file is uploaded or deleted.\n",
    "\n",
    "* Create a lambda function as\n",
    "* name = datasync-trigger-s3\n",
    "* runtime = Python 3.9\n",
    "\n",
    "Leave the rest of the settings as default, update the code as below, and deploy.\n",
    "\n",
    "In the code, we are first filtering the object key for which the event is generated. Then we trigger the datasync task and pass the object key as a filter string. With the filter key provided datasync job will only sync provided object from S3 to EFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "import boto3  \n",
    "import os  \n",
    "  \n",
    "DataSync_task_arn = 'arn:aws:datasync:us-east-1:801598032724:task/task-0c04a4a15668b6b8a'  \n",
    "DataSync = boto3.client('datasync')\n",
    "      \n",
    "def lambda_handler(event, context):  \n",
    "    objectKey = ''  \n",
    "    try:  \n",
    "        objectKey = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]  \n",
    "    except KeyError:  \n",
    "        raise KeyError(\"Received invalid event - unable to locate Object key to upload.\", event)  \n",
    "          \n",
    "    response = DataSync.start_task_execution(  \n",
    "        TaskArn=DataSync_task_arn,  \n",
    "        OverrideOptions={  \n",
    "            'OverwriteMode' : 'ALWAYS',\n",
    "            'PreserveDeletedFiles' : 'REMOVE',\n",
    "        },  \n",
    "        Includes=[  \n",
    "            {  \n",
    "                'FilterType': 'SIMPLE_PATTERN',  \n",
    "                'Value': '/' + os.path.basename(objectKey)  \n",
    "            }  \n",
    "        ]  \n",
    "    )  \n",
    "      \n",
    "    print(f\"response= {response}\")\n",
    "    return {  \n",
    "        'response' : response  \n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add policy `AWSDataSyncFullAccess` to this lambda function role otherwise it will not be able to trigger datasync task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure S3 bucket event notifications\n",
    "\n",
    "Our lambda function is ready. Now we can enable S3 bucket event notifications as put the lambda function as a target. For this from S3 bucket Properties > Event notifications > **Create event notifications**\n",
    "\n",
    "* event name = object-put-delete\n",
    "* event type = s3:ObjectCreated:Put, and s3:ObjectRemoved:Delete\n",
    "* destination = lambda function (datasync-trigger-s3)\n",
    "\n",
    "Click **Save changes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DataSync task through S3 events trigger\n",
    "\n",
    "Now let's test our trigger by placing a new file in S3 bucket. In my case it is '**test2.txt**'. Once file is successfully uploaded we can check the EC2 instance to verify the file presence.\n",
    "\n",
    "![ec2-verify-files](images/2022-03-29-efs-s3-datasync/ec2-verify-files.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also verify that the datasync job was triggered from lambda CloudWatch logs.\n",
    "\n",
    "```JS\n",
    "response= {'TaskExecutionArn': 'arn:aws:datasync:us-east-1:801598032724:task/task-0c04a4a15668b6b8a/execution/exec-020e456f670ca2419', 'ResponseMetadata': {'RequestId': 'c8166ce4-ef14-415c-beff-09cc7720f4a3', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 30 Mar 2022 13:27:45 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '123', 'connection': 'keep-alive', 'x-amzn-requestid': 'c8166ce4-ef14-415c-beff-09cc7720f4a3'}, 'RetryAttempts': 0}}\n",
    "```\n",
    "\n",
    "In the logs we have task execution id `exec-020e456f670ca2419` , and we can use that to verify task's status from datasync console. \n",
    "\n",
    "![datasync-task-status](images/2022-03-29-efs-s3-datasync/datasync-task-status.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for scheduled based approach\n",
    "\n",
    "We have seen in the last section that we can use S3 event notifications to trigger datasync tasks. Now we will discuss a schedule-based trigger for datasync task. This can be done in two ways\n",
    "* While creating a datasync task we can define a frequency for it to follow. But the limitation on this is that it can not be lower than a 1 hour window.\n",
    "* If we want to schedule a datasync task on a smaller than 1 hour window then we can use AWS EventBridge (previously CloudWatch Events) to trigger a lambda function that can inturn invoke a datasync task. In the coming section, we will follow this approach.\n",
    "\n",
    "![scheduled-based-approach](images/2022-03-29-efs-s3-datasync/scheduled-based-approach.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a lambda function\n",
    "Let's create a new lambda function with the following code. This lambda will invoke the datasync task.\n",
    "Add permissions to this lambda `AWSDataSyncFullAccess`\n",
    "\n",
    "* function name = datasync-trigger-scheduled\n",
    "* runtime = Python 3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "import boto3  \n",
    "import os  \n",
    "  \n",
    "DataSync_task_arn = 'arn:aws:datasync:us-east-1:801598032724:task/task-0c04a4a15668b6b8a'  \n",
    "DataSync = boto3.client('datasync')\n",
    "      \n",
    "def lambda_handler(event, context):\n",
    "    response = DataSync.start_task_execution(  \n",
    "        TaskArn=DataSync_task_arn,  \n",
    "        OverrideOptions={  \n",
    "            'OverwriteMode' : 'ALWAYS',\n",
    "            'PreserveDeletedFiles' : 'REMOVE',\n",
    "        }\n",
    "    )  \n",
    "    \n",
    "    print(f\"response= {response}\")\n",
    "    return {  \n",
    "        'response' : response  \n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create EventBridge event\n",
    "\n",
    "Go to EventBridge Events > Rules > select **Create Rule**\n",
    "- Define rule details\n",
    "    - name = datasync-trigger\n",
    "    - event bus = default\n",
    "    - rule type = scheduled\n",
    "- Define schedule\n",
    "    - Sample event = {}\n",
    "    - Schedule Pattern\n",
    "        - Rate expression = 5 min\n",
    "- Select Targets\n",
    "    - target = Lambda function\n",
    "    - function = datasync-trigger-scheduled\n",
    "    \n",
    "Click **Next** and **Create Rule**\n",
    "\n",
    "EventBridge will automatically add a policy statement to lambda function (datasync-trigger-scheduled) allowing it to trigger lambda. You can verify the policy from lambda Configurations > Permissions > **Resource based policy**. If no resource policy exists then you need to manually add a policy to allow EventBridge to invoke it. For this click on Resource based policy > Policy statements > **Add permissions**.\n",
    "* policy statement = AWS service\n",
    "* service = EventBridge\n",
    "* statement id = eventbridge-1000 (or any unique id)\n",
    "* principal = events.amazonaws.com\n",
    "* source ARN = arn:aws:events:us-east-1:801598032724:rule/datasync-trigger (arn for eventbridge event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify event and datasync task execution\n",
    "\n",
    "* We have configured eventbridge to fire an event after every 5 min we can verify it from eventbrige monitoring tab and its cloudwatch logs.\n",
    "* Lambda function invocations can be verified from its cloudwatch logs\n",
    "* Datasync task execution status can be verified from its history tab and cloudwatch logs."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "592d70e68660334c1e2b3db2d131a28b3e4c5aef0ad9ff036ec5b5af6e985aa1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('sc_mlflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
